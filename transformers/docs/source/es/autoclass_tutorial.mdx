<!--Copyright 2022 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with
the License. You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on
an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the
specific language governing permissions and limitations under the License.
-->

# Carga instancias preentrenadas con un AutoClass

Con tantas arquitecturas diferentes de Transformer puede ser retador crear una para tu checkpoint. Como parte de la filosof√≠a central de ü§ó Transformers para hacer que la biblioteca sea f√°cil, simple y flexible de usar; una `AutoClass` autom√°ticamente infiere y carga la arquitectura correcta desde un checkpoint dado. El m√©todo `from_pretrained` te permite cargar r√°pidamente un modelo preentrenado para cualquier arquitectura, por lo que no tendr√°s que dedicar tiempo y recursos para entrenar uno desde cero. Producir este tipo de c√≥digo con checkpoint implica que si funciona con uno, funcionar√° tambi√©n con otro (siempre que haya sido entrenado para una tarea similar) incluso si la arquitectura es distinta.

<Tip>

Recuerda, la arquitectura se refiere al esqueleto del modelo y los checkpoints son los pesos para una arquitectura dada. Por ejemplo, [BERT](https://huggingface.co/bert-base-uncased) es una arquitectura, mientras que `bert-base-uncased` es un checkpoint. Modelo es un t√©rmino general que puede significar una arquitectura o un checkpoint.

</Tip>

En este tutorial, aprende a:

* Cargar un tokenizador preentrenado.
* Cargar un extractor de caracter√≠sticas (feature extractor) preentrenado.
* Cargar un procesador preentrenado.
* Cargar un modelo preentrenado.

## AutoTokenizer

Casi cualquier tarea de Natural Language Processing comienza con un tokenizador. Un tokenizador convierte tu input a un formato que puede ser procesado por el modelo.

Carga un tokenizador con [`AutoTokenizer.from_pretrained`]:

```py
>>> from transformers import AutoTokenizer

>>> tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")
```

Luego tokeniza tu input como lo mostrado a continuaci√≥n:

```py
>>> sequence = "In a hole in the ground there lived a hobbit."
>>> print(tokenizer(sequence))
{'input_ids': [101, 1999, 1037, 4920, 1999, 1996, 2598, 2045, 2973, 1037, 7570, 10322, 4183, 1012, 102], 
 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 
 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}
```

## AutoFeatureExtractor

Para tareas de audio y visi√≥n, un extractor de caracter√≠sticas procesa la se√±al de audio o imagen al formato de input correcto.

Carga un extractor de caracter√≠sticas con [`AutoFeatureExtractor.from_pretrained`]:

```py
>>> from transformers import AutoFeatureExtractor

>>> feature_extractor = AutoFeatureExtractor.from_pretrained(
...     "ehcalabres/wav2vec2-lg-xlsr-en-speech-emotion-recognition"
... )
```

## AutoProcessor

Las tareas multimodales requieren un procesador que combine dos tipos de herramientas de preprocesamiento. Por ejemplo, el modelo [LayoutLMV2](model_doc/layoutlmv2) requiere que un extractor de caracter√≠sticas maneje las im√°genes y que un tokenizador maneje el texto; un procesador combina ambas.

Carga un procesador con [`AutoProcessor.from_pretrained`]:

```py
>>> from transformers import AutoProcessor

>>> processor = AutoProcessor.from_pretrained("microsoft/layoutlmv2-base-uncased")
```

## AutoModel

<frameworkcontent>
<pt>
Finalmente, las clases `AutoModelFor` te permiten cargar un modelo preentrenado para una tarea dada (revisa [aqu√≠](model_doc/auto) para conocer la lista completa de tareas disponibles). Por ejemplo, cargue un modelo para clasificaci√≥n de secuencias con [`AutoModelForSequenceClassification.from_pretrained`]:

```py
>>> from transformers import AutoModelForSequenceClassification

>>> model = AutoModelForSequenceClassification.from_pretrained("distilbert-base-uncased")
```

Reutilice f√°cilmente el mismo checkpoint para cargar una aquitectura para alguna tarea diferente:

```py
>>> from transformers import AutoModelForTokenClassification

>>> model = AutoModelForTokenClassification.from_pretrained("distilbert-base-uncased")
```

Generalmente recomendamos utilizar las clases `AutoTokenizer` y `AutoModelFor` para cargar instancias preentrenadas de modelos. √âsto asegurar√° que cargues la arquitectura correcta en cada ocasi√≥n. En el siguiente [tutorial](preprocessing), aprende a usar tu tokenizador reci√©n cargado, el extractor de caracter√≠sticas y el procesador para preprocesar un dataset para fine-tuning.
</pt>
<tf>
Finalmente, la clase `TFAutoModelFor` te permite cargar tu modelo preentrenado para una tarea dada (revisa [aqu√≠](model_doc/auto) para conocer la lista completa de tareas disponibles). Por ejemplo, carga un modelo para clasificaci√≥n de secuencias con [`TFAutoModelForSequenceClassification.from_pretrained`]:

```py
>>> from transformers import TFAutoModelForSequenceClassification

>>> model = TFAutoModelForSequenceClassification.from_pretrained("distilbert-base-uncased")
```

Reutilice f√°cilmente el mismo checkpoint para cargar una aquitectura para alguna tarea diferente:

```py
>>> from transformers import TFAutoModelForTokenClassification

>>> model = TFAutoModelForTokenClassification.from_pretrained("distilbert-base-uncased")
```

Generalmente recomendamos utilizar las clases `AutoTokenizer` y `TFAutoModelFor` para cargar instancias de modelos preentrenados. √âsto asegurar√° que cargues la arquitectura correcta cada vez. En el siguiente [tutorial](preprocessing), aprende a usar tu tokenizador reci√©n cargado, el extractor de caracter√≠sticas y el procesador para preprocesar un dataset para fine-tuning.
</tf>
</frameworkcontent>
