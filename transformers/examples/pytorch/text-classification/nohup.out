08/27/2022 21:48:09 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 2distributed training: False, 16-bits training: True
08/27/2022 21:48:09 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=2,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=IntervalStrategy.NO,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_min_num_params=0,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=1e-06,
length_column_name=length,
load_best_model_at_end=False,
local_rank=-1,
log_level=-1,
log_level_replica=-1,
log_on_each_node=True,
logging_dir=tmp/baseline/CoLA/bert-base-uncased/HPO_S0/1e-06/256/10/runs/Aug27_21-48-09_ip-10-21-39-60,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
no_cuda=False,
num_train_epochs=10.0,
optim=OptimizerNames.ADAMW_HF,
output_dir=tmp/baseline/CoLA/bert-base-uncased/HPO_S0/1e-06/256/10,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=128,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=tmp/baseline/CoLA/bert-base-uncased/HPO_S0/1e-06/256/10,
save_on_each_node=False,
save_steps=500,
save_strategy=IntervalStrategy.STEPS,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tf32=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
xpu_backend=None,
)
08/27/2022 21:48:10 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 2distributed training: False, 16-bits training: True
08/27/2022 21:48:10 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=2,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=IntervalStrategy.NO,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_min_num_params=0,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=1e-06,
length_column_name=length,
load_best_model_at_end=False,
local_rank=-1,
log_level=-1,
log_level_replica=-1,
log_on_each_node=True,
logging_dir=tmp/baseline/RTE/bert-base-uncased/HPO_S0/1e-06/256/10/runs/Aug27_21-48-09_ip-10-21-39-60,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
no_cuda=False,
num_train_epochs=10.0,
optim=OptimizerNames.ADAMW_HF,
output_dir=tmp/baseline/RTE/bert-base-uncased/HPO_S0/1e-06/256/10,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=128,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=tmp/baseline/RTE/bert-base-uncased/HPO_S0/1e-06/256/10,
save_on_each_node=False,
save_steps=500,
save_strategy=IntervalStrategy.STEPS,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tf32=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
xpu_backend=None,
)
08/27/2022 21:48:10 - INFO - datasets.info - Loading Dataset Infos from /home/ubuntu/.cache/huggingface/modules/datasets_modules/datasets/glue/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad
08/27/2022 21:48:10 - INFO - datasets.builder - Overwrite dataset info from restored data version.
08/27/2022 21:48:10 - INFO - datasets.info - Loading Dataset info from /home/ubuntu/.cache/huggingface/datasets/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad
08/27/2022 21:48:10 - WARNING - datasets.builder - Reusing dataset glue (/home/ubuntu/.cache/huggingface/datasets/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)
08/27/2022 21:48:10 - INFO - datasets.info - Loading Dataset info from /home/ubuntu/.cache/huggingface/datasets/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 785.99it/s]
[INFO|configuration_utils.py:657] 2022-08-27 21:48:10,285 >> loading configuration file /home/ubuntu/checkpoints/exp/CoLA/config.json
[INFO|configuration_utils.py:708] 2022-08-27 21:48:10,286 >> Model config BertConfig {
  "_name_or_path": "/home/ubuntu/checkpoints/exp/CoLA",
  "architectures": [
    "BertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "crypten": false,
  "finetuning_task": "cola",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "unacceptable",
    "1": "acceptable"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "acceptable": 1,
    "unacceptable": 0
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "softmax_act": "softmax",
  "torch_dtype": "float32",
  "transformers_version": "4.20.0.dev0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|tokenization_utils_base.py:1701] 2022-08-27 21:48:10,287 >> Didn't find file /home/ubuntu/checkpoints/exp/CoLA/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1779] 2022-08-27 21:48:10,287 >> loading file /home/ubuntu/checkpoints/exp/CoLA/vocab.txt
[INFO|tokenization_utils_base.py:1779] 2022-08-27 21:48:10,287 >> loading file /home/ubuntu/checkpoints/exp/CoLA/tokenizer.json
[INFO|tokenization_utils_base.py:1779] 2022-08-27 21:48:10,287 >> loading file None
[INFO|tokenization_utils_base.py:1779] 2022-08-27 21:48:10,287 >> loading file /home/ubuntu/checkpoints/exp/CoLA/special_tokens_map.json
[INFO|tokenization_utils_base.py:1779] 2022-08-27 21:48:10,287 >> loading file /home/ubuntu/checkpoints/exp/CoLA/tokenizer_config.json
[INFO|modeling_utils.py:2047] 2022-08-27 21:48:10,326 >> loading weights file /home/ubuntu/checkpoints/exp/CoLA/pytorch_model.bin
08/27/2022 21:48:10 - INFO - datasets.info - Loading Dataset Infos from /home/ubuntu/.cache/huggingface/modules/datasets_modules/datasets/glue/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad
08/27/2022 21:48:10 - INFO - datasets.builder - Overwrite dataset info from restored data version.
08/27/2022 21:48:10 - INFO - datasets.info - Loading Dataset info from /home/ubuntu/.cache/huggingface/datasets/glue/rte/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad
08/27/2022 21:48:10 - WARNING - datasets.builder - Reusing dataset glue (/home/ubuntu/.cache/huggingface/datasets/glue/rte/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)
08/27/2022 21:48:10 - INFO - datasets.info - Loading Dataset info from /home/ubuntu/.cache/huggingface/datasets/glue/rte/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 490.91it/s]
[INFO|configuration_utils.py:657] 2022-08-27 21:48:10,483 >> loading configuration file /home/ubuntu/checkpoints/exp/RTE/config.json
[INFO|configuration_utils.py:708] 2022-08-27 21:48:10,484 >> Model config BertConfig {
  "_name_or_path": "/home/ubuntu/checkpoints/exp/RTE",
  "architectures": [
    "BertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "crypten": false,
  "finetuning_task": "rte",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "entailment",
    "1": "not_entailment"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "entailment": 0,
    "not_entailment": 1
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "softmax_act": "softmax",
  "torch_dtype": "float32",
  "transformers_version": "4.20.0.dev0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|tokenization_utils_base.py:1701] 2022-08-27 21:48:10,485 >> Didn't find file /home/ubuntu/checkpoints/exp/RTE/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1779] 2022-08-27 21:48:10,485 >> loading file /home/ubuntu/checkpoints/exp/RTE/vocab.txt
[INFO|tokenization_utils_base.py:1779] 2022-08-27 21:48:10,485 >> loading file /home/ubuntu/checkpoints/exp/RTE/tokenizer.json
[INFO|tokenization_utils_base.py:1779] 2022-08-27 21:48:10,485 >> loading file None
[INFO|tokenization_utils_base.py:1779] 2022-08-27 21:48:10,485 >> loading file /home/ubuntu/checkpoints/exp/RTE/special_tokens_map.json
[INFO|tokenization_utils_base.py:1779] 2022-08-27 21:48:10,485 >> loading file /home/ubuntu/checkpoints/exp/RTE/tokenizer_config.json
[INFO|modeling_utils.py:2047] 2022-08-27 21:48:10,532 >> loading weights file /home/ubuntu/checkpoints/exp/RTE/pytorch_model.bin
[INFO|modeling_bert.py:194] 2022-08-27 21:48:10,634 >> initializing embedding using nn.Embedding
[INFO|modeling_bert.py:194] 2022-08-27 21:48:10,855 >> initializing embedding using nn.Embedding
[INFO|modeling_utils.py:2417] 2022-08-27 21:48:11,921 >> All model checkpoint weights were used when initializing BertForSequenceClassification.

[INFO|modeling_utils.py:2426] 2022-08-27 21:48:11,922 >> All the weights of BertForSequenceClassification were initialized from the model checkpoint at /home/ubuntu/checkpoints/exp/CoLA.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.
using model config: BertConfig {
  "_name_or_path": "/home/ubuntu/checkpoints/exp/CoLA",
  "architectures": [
    "BertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "crypten": false,
  "finetuning_task": "cola",
  "gradient_checkpointing": false,
  "hidden_act": "quad",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "unacceptable",
    "1": "acceptable"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "acceptable": 1,
    "unacceptable": 0
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "softmax_act": "2quad",
  "torch_dtype": "float32",
  "transformers_version": "4.20.0.dev0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

BertConfig {
  "_name_or_path": "/home/ubuntu/checkpoints/exp/CoLA",
  "architectures": [
    "BertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "crypten": false,
  "finetuning_task": "cola",
  "gradient_checkpointing": false,
  "hidden_act": "quad",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "unacceptable",
    "1": "acceptable"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "acceptable": 1,
    "unacceptable": 0
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "softmax_act": "2quad",
  "torch_dtype": "float32",
  "transformers_version": "4.20.0.dev0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

model architecture: BertForSequenceClassification(
  (bert): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): QuadActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): QuadActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): QuadActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): QuadActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): QuadActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): QuadActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): QuadActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): QuadActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): QuadActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): QuadActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): QuadActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): QuadActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (dropout): Dropout(p=0.1, inplace=False)
  (classifier): Linear(in_features=768, out_features=2, bias=True)
)[INFO|modeling_utils.py:2417] 2022-08-27 21:48:11,986 >> All model checkpoint weights were used when initializing BertForSequenceClassification.

[INFO|modeling_utils.py:2426] 2022-08-27 21:48:11,987 >> All the weights of BertForSequenceClassification were initialized from the model checkpoint at /home/ubuntu/checkpoints/exp/RTE.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.
using model config: BertConfig {
  "_name_or_path": "/home/ubuntu/checkpoints/exp/RTE",
  "architectures": [
    "BertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "crypten": false,
  "finetuning_task": "rte",
  "gradient_checkpointing": false,
  "hidden_act": "quad",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "entailment",
    "1": "not_entailment"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "entailment": 0,
    "not_entailment": 1
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "softmax_act": "2quad",
  "torch_dtype": "float32",
  "transformers_version": "4.20.0.dev0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

BertConfig {
  "_name_or_path": "/home/ubuntu/checkpoints/exp/RTE",
  "architectures": [
    "BertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "crypten": false,
  "finetuning_task": "rte",
  "gradient_checkpointing": false,
  "hidden_act": "quad",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "entailment",
    "1": "not_entailment"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "entailment": 0,
    "not_entailment": 1
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "softmax_act": "2quad",
  "torch_dtype": "float32",
  "transformers_version": "4.20.0.dev0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

model architecture: BertForSequenceClassification(
  (bert): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): QuadActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): QuadActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): QuadActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): QuadActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): QuadActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): QuadActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): QuadActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): QuadActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): QuadActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): QuadActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): QuadActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): QuadActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (dropout): Dropout(p=0.1, inplace=False)
  (classifier): Linear(in_features=768, out_features=2, bias=True)
)
08/27/2022 21:48:12 - WARNING - datasets.fingerprint - Parameter 'function'=<function main.<locals>.preprocess_function at 0x7fe29494d488> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
Running tokenizer on dataset:   0%|          | 0/9 [00:00<?, ?ba/s]
08/27/2022 21:48:12 - WARNING - datasets.fingerprint - Parameter 'function'=<function main.<locals>.preprocess_function at 0x7f03050b5488> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
Running tokenizer on dataset:   0%|          | 0/3 [00:00<?, ?ba/s]08/27/2022 21:48:12 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/ubuntu/.cache/huggingface/datasets/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-1c80317fa3b1799d.arrow
Running tokenizer on dataset:  11%|█         | 1/9 [00:00<00:00,  8.68ba/s]08/27/2022 21:48:12 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/ubuntu/.cache/huggingface/datasets/glue/rte/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-1c80317fa3b1799d.arrow
Running tokenizer on dataset:  33%|███▎      | 1/3 [00:00<00:00,  5.33ba/s]Running tokenizer on dataset:  22%|██▏       | 2/9 [00:00<00:01,  6.29ba/s]Running tokenizer on dataset:  44%|████▍     | 4/9 [00:00<00:00,  9.67ba/s]Running tokenizer on dataset:  67%|██████▋   | 2/3 [00:00<00:00,  3.95ba/s]Running tokenizer on dataset:  67%|██████▋   | 6/9 [00:00<00:00, 11.18ba/s]Running tokenizer on dataset: 100%|██████████| 3/3 [00:00<00:00,  5.29ba/s]08/27/2022 21:48:12 - INFO - datasets.fingerprint - Parameter 'function'=<function main.<locals>.preprocess_function at 0x7f03050b1950> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead.

Running tokenizer on dataset:   0%|          | 0/1 [00:00<?, ?ba/s]08/27/2022 21:48:12 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/ubuntu/.cache/huggingface/datasets/glue/rte/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-bdd640fb06671ad1.arrow
Running tokenizer on dataset: 100%|██████████| 1/1 [00:00<00:00, 28.20ba/s]08/27/2022 21:48:12 - INFO - datasets.fingerprint - Parameter 'function'=<function main.<locals>.preprocess_function at 0x7f03050b5840> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead.

Running tokenizer on dataset:   0%|          | 0/3 [00:00<?, ?ba/s]08/27/2022 21:48:12 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/ubuntu/.cache/huggingface/datasets/glue/rte/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-3eb13b9046685257.arrow
Running tokenizer on dataset:  89%|████████▉ | 8/9 [00:00<00:00, 11.83ba/s]Running tokenizer on dataset: 100%|██████████| 9/9 [00:00<00:00, 11.36ba/s]08/27/2022 21:48:12 - INFO - datasets.fingerprint - Parameter 'function'=<function main.<locals>.preprocess_function at 0x7fe29494d510> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead.

Running tokenizer on dataset:   0%|          | 0/2 [00:00<?, ?ba/s]08/27/2022 21:48:12 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/ubuntu/.cache/huggingface/datasets/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-bdd640fb06671ad1.arrow
Running tokenizer on dataset:  67%|██████▋   | 2/3 [00:00<00:00, 11.72ba/s]Running tokenizer on dataset: 100%|██████████| 2/2 [00:00<00:00, 22.59ba/s]08/27/2022 21:48:12 - INFO - datasets.fingerprint - Parameter 'function'=<function main.<locals>.preprocess_function at 0x7fe29494d488> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead.

Running tokenizer on dataset:   0%|          | 0/2 [00:00<?, ?ba/s]08/27/2022 21:48:12 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/ubuntu/.cache/huggingface/datasets/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-3eb13b9046685257.arrow
Running tokenizer on dataset: 100%|██████████| 3/3 [00:00<00:00, 10.58ba/s]Running tokenizer on dataset: 100%|██████████| 2/2 [00:00<00:00, 26.48ba/s]128
08/27/2022 21:48:18 - INFO - __main__ - Sample 914 of the training set: {'sentence1': "Because of Reagan's economic strategy, the federal budget deficit ballooned.", 'sentence2': "Reagan's economic strategy led to huge federal budget deficits.", 'label': 0, 'idx': 914, 'input_ids': [101, 2138, 1997, 11531, 1005, 1055, 3171, 5656, 1010, 1996, 2976, 5166, 15074, 13212, 2098, 1012, 102, 11531, 1005, 1055, 3171, 5656, 2419, 2000, 4121, 2976, 5166, 15074, 2015, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.
128
08/27/2022 21:48:18 - INFO - __main__ - Sample 3657 of the training set: {'sentence': 'John sounded in the park.', 'label': 0, 'idx': 3657, 'input_ids': [101, 2198, 5015, 1999, 1996, 2380, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.
128
08/27/2022 21:48:23 - INFO - __main__ - Sample 571 of the training set: {'sentence1': 'So far the British have preferred a policy of a voluntary approach to restricting advertising and high taxes on tobacco products.', 'sentence2': 'Sales have declined due to restrictions on advertising.', 'label': 1, 'idx': 571, 'input_ids': [101, 2061, 2521, 1996, 2329, 2031, 6871, 1037, 3343, 1997, 1037, 10758, 3921, 2000, 26996, 6475, 1998, 2152, 7773, 2006, 9098, 3688, 1012, 102, 4341, 2031, 6430, 2349, 2000, 9259, 2006, 6475, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.
128
08/27/2022 21:48:23 - INFO - __main__ - Sample 2286 of the training set: {'sentence': 'Clouds cleared from the sky.', 'label': 1, 'idx': 2286, 'input_ids': [101, 8044, 5985, 2013, 1996, 3712, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.
128
08/27/2022 21:48:28 - INFO - __main__ - Sample 419 of the training set: {'sentence1': 'Protesters, many from organized pro-government groups but including many ordinary citizens, carried anti-American banners and chanted slogans attacking U.N. Secretary-General Kofi Annan for his close alignment with U.S. policy.', 'sentence2': 'Protesters confiscated anti-American banners and chanted slogans attacking U.N. Secretary-General Kofi Annan for his close alignment with U.S. policy.', 'label': 1, 'idx': 419, 'input_ids': [101, 13337, 1010, 2116, 2013, 4114, 4013, 1011, 2231, 2967, 2021, 2164, 2116, 6623, 4480, 1010, 3344, 3424, 1011, 2137, 23562, 1998, 16883, 2098, 14558, 2015, 7866, 1057, 1012, 1050, 1012, 3187, 1011, 2236, 12849, 8873, 4698, 2078, 2005, 2010, 2485, 12139, 2007, 1057, 1012, 1055, 1012, 3343, 1012, 102, 13337, 17182, 3424, 1011, 2137, 23562, 1998, 16883, 2098, 14558, 2015, 7866, 1057, 1012, 1050, 1012, 3187, 1011, 2236, 12849, 8873, 4698, 2078, 2005, 2010, 2485, 12139, 2007, 1057, 1012, 1055, 1012, 3343, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.
128
08/27/2022 21:48:28 - INFO - __main__ - Sample 1679 of the training set: {'sentence': 'It is this hat that that he was wearing is certain.', 'label': 0, 'idx': 1679, 'input_ids': [101, 2009, 2003, 2023, 6045, 2008, 2008, 2002, 2001, 4147, 2003, 3056, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.

using training arge: TrainingArguments(
_n_gpu=2,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=IntervalStrategy.NO,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_min_num_params=0,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=1e-06,
length_column_name=length,
load_best_model_at_end=False,
local_rank=-1,
log_level=-1,
log_level_replica=-1,
log_on_each_node=True,
logging_dir=tmp/baseline/CoLA/bert-base-uncased/HPO_S0/1e-06/256/10/runs/Aug27_21-48-09_ip-10-21-39-60,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
no_cuda=False,
num_train_epochs=10.0,
optim=OptimizerNames.ADAMW_HF,
output_dir=tmp/baseline/CoLA/bert-base-uncased/HPO_S0/1e-06/256/10,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=128,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=tmp/baseline/CoLA/bert-base-uncased/HPO_S0/1e-06/256/10,
save_on_each_node=False,
save_steps=500,
save_strategy=IntervalStrategy.STEPS,
save_total_limit=0,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tf32=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
xpu_backend=None,
)
Traceback (most recent call last):
  File "run_glue.py", line 645, in <module>
    main()
  File "run_glue.py", line 544, in main
    data_collator=data_collator,
  File "/home/ubuntu/transformers_private/src/transformers/trainer.py", line 423, in __init__
    self._move_model_to_device(model, args.device)
  File "/home/ubuntu/transformers_private/src/transformers/trainer.py", line 605, in _move_model_to_device
    model = model.to(device)
  File "/home/ubuntu/anaconda3/envs/transformers/lib/python3.7/site-packages/torch/nn/modules/module.py", line 927, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/transformers/lib/python3.7/site-packages/torch/nn/modules/module.py", line 579, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/transformers/lib/python3.7/site-packages/torch/nn/modules/module.py", line 579, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/transformers/lib/python3.7/site-packages/torch/nn/modules/module.py", line 579, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/transformers/lib/python3.7/site-packages/torch/nn/modules/module.py", line 602, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/transformers/lib/python3.7/site-packages/torch/nn/modules/module.py", line 925, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Traceback (most recent call last):
  File "baseline.py", line 78, in <module>
    result = json.load(open(result_path))
FileNotFoundError: [Errno 2] No such file or directory: 'tmp/baseline/CoLA/bert-base-uncased/HPO_S0/1e-06/256/10/eval_results.json'

[INFO|trainer.py:506] 2022-08-27 21:48:33,927 >> Using amp half precision backend
[INFO|trainer.py:628] 2022-08-27 21:48:33,930 >> The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: idx, sentence2, sentence1. If idx, sentence2, sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.
/home/ubuntu/transformers_private/src/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  FutureWarning,
[INFO|trainer.py:1430] 2022-08-27 21:48:33,958 >> ***** Running training *****
[INFO|trainer.py:1431] 2022-08-27 21:48:33,958 >>   Num examples = 2490
[INFO|trainer.py:1432] 2022-08-27 21:48:33,958 >>   Num Epochs = 10
[INFO|trainer.py:1433] 2022-08-27 21:48:33,958 >>   Instantaneous batch size per device = 128
[INFO|trainer.py:1434] 2022-08-27 21:48:33,958 >>   Total train batch size (w. parallel, distributed & accumulation) = 256
[INFO|trainer.py:1435] 2022-08-27 21:48:33,958 >>   Gradient Accumulation steps = 1
[INFO|trainer.py:1436] 2022-08-27 21:48:33,958 >>   Total optimization steps = 100
using training arge: TrainingArguments(
_n_gpu=2,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=IntervalStrategy.NO,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_min_num_params=0,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=1e-06,
length_column_name=length,
load_best_model_at_end=False,
local_rank=-1,
log_level=-1,
log_level_replica=-1,
log_on_each_node=True,
logging_dir=tmp/baseline/RTE/bert-base-uncased/HPO_S0/1e-06/256/10/runs/Aug27_21-48-09_ip-10-21-39-60,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
no_cuda=False,
num_train_epochs=10.0,
optim=OptimizerNames.ADAMW_HF,
output_dir=tmp/baseline/RTE/bert-base-uncased/HPO_S0/1e-06/256/10,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=128,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=tmp/baseline/RTE/bert-base-uncased/HPO_S0/1e-06/256/10,
save_on_each_node=False,
save_steps=500,
save_strategy=IntervalStrategy.STEPS,
save_total_limit=0,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tf32=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
xpu_backend=None,
)
  0%|          | 0/100 [00:00<?, ?it/s]/home/ubuntu/anaconda3/envs/transformers/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
  1%|          | 1/100 [00:05<08:42,  5.28s/it]  2%|▏         | 2/100 [00:05<04:01,  2.46s/it]  3%|▎         | 3/100 [00:06<02:31,  1.56s/it]  4%|▍         | 4/100 [00:06<01:49,  1.14s/it]  5%|▌         | 5/100 [00:07<01:26,  1.10it/s]  6%|▌         | 6/100 [00:07<01:11,  1.31it/s]  7%|▋         | 7/100 [00:08<01:03,  1.48it/s]  8%|▊         | 8/100 [00:08<00:56,  1.62it/s]  9%|▉         | 9/100 [00:09<00:52,  1.73it/s] 10%|█         | 10/100 [00:09<00:46,  1.92it/s] 11%|█         | 11/100 [00:10<00:45,  1.95it/s] 12%|█▏        | 12/100 [00:10<00:44,  1.98it/s] 13%|█▎        | 13/100 [00:11<00:43,  1.99it/s] 14%|█▍        | 14/100 [00:11<00:42,  2.01it/s] 15%|█▌        | 15/100 [00:12<00:42,  2.01it/s] 16%|█▌        | 16/100 [00:12<00:41,  2.02it/s] 17%|█▋        | 17/100 [00:13<00:41,  2.02it/s] 18%|█▊        | 18/100 [00:13<00:40,  2.02it/s] 19%|█▉        | 19/100 [00:14<00:39,  2.03it/s] 20%|██        | 20/100 [00:14<00:36,  2.17it/s] 21%|██        | 21/100 [00:14<00:37,  2.13it/s] 22%|██▏       | 22/100 [00:15<00:37,  2.10it/s] 23%|██▎       | 23/100 [00:15<00:37,  2.08it/s] 24%|██▍       | 24/100 [00:16<00:36,  2.07it/s] 25%|██▌       | 25/100 [00:16<00:36,  2.06it/s] 26%|██▌       | 26/100 [00:17<00:36,  2.05it/s] 27%|██▋       | 27/100 [00:17<00:35,  2.05it/s] 28%|██▊       | 28/100 [00:18<00:35,  2.05it/s] 29%|██▉       | 29/100 [00:18<00:34,  2.05it/s] 30%|███       | 30/100 [00:19<00:32,  2.19it/s] 31%|███       | 31/100 [00:19<00:32,  2.14it/s] 32%|███▏      | 32/100 [00:20<00:32,  2.11it/s] 33%|███▎      | 33/100 [00:20<00:32,  2.09it/s] 34%|███▍      | 34/100 [00:21<00:31,  2.08it/s] 35%|███▌      | 35/100 [00:21<00:33,  1.94it/s] 36%|███▌      | 36/100 [00:22<00:32,  1.97it/s] 37%|███▋      | 37/100 [00:22<00:31,  1.99it/s] 38%|███▊      | 38/100 [00:23<00:30,  2.00it/s] 39%|███▉      | 39/100 [00:23<00:30,  2.02it/s] 40%|████      | 40/100 [00:24<00:27,  2.16it/s] 41%|████      | 41/100 [00:24<00:27,  2.12it/s] 42%|████▏     | 42/100 [00:25<00:27,  2.10it/s] 43%|████▎     | 43/100 [00:25<00:27,  2.08it/s] 44%|████▍     | 44/100 [00:26<00:27,  2.07it/s] 45%|████▌     | 45/100 [00:26<00:26,  2.06it/s] 46%|████▌     | 46/100 [00:27<00:26,  2.06it/s] 47%|████▋     | 47/100 [00:27<00:25,  2.05it/s] 48%|████▊     | 48/100 [00:28<00:25,  2.05it/s] 49%|████▉     | 49/100 [00:28<00:24,  2.04it/s] 50%|█████     | 50/100 [00:28<00:22,  2.18it/s] 51%|█████     | 51/100 [00:29<00:22,  2.13it/s] 52%|█████▏    | 52/100 [00:29<00:22,  2.10it/s] 53%|█████▎    | 53/100 [00:30<00:22,  2.08it/s] 54%|█████▍    | 54/100 [00:30<00:22,  2.07it/s] 55%|█████▌    | 55/100 [00:31<00:21,  2.06it/s] 56%|█████▌    | 56/100 [00:31<00:21,  2.05it/s] 57%|█████▋    | 57/100 [00:32<00:21,  2.05it/s] 58%|█████▊    | 58/100 [00:32<00:20,  2.04it/s] 59%|█████▉    | 59/100 [00:33<00:20,  2.04it/s] 60%|██████    | 60/100 [00:33<00:18,  2.18it/s] 61%|██████    | 61/100 [00:34<00:18,  2.13it/s] 62%|██████▏   | 62/100 [00:34<00:18,  2.10it/s] 63%|██████▎   | 63/100 [00:35<00:17,  2.08it/s] 64%|██████▍   | 64/100 [00:35<00:17,  2.07it/s] 65%|██████▌   | 65/100 [00:36<00:16,  2.06it/s] 66%|██████▌   | 66/100 [00:36<00:16,  2.06it/s] 67%|██████▋   | 67/100 [00:37<00:16,  2.05it/s] 68%|██████▊   | 68/100 [00:37<00:15,  2.05it/s] 69%|██████▉   | 69/100 [00:38<00:15,  2.05it/s] 70%|███████   | 70/100 [00:38<00:13,  2.18it/s] 71%|███████   | 71/100 [00:39<00:13,  2.14it/s] 72%|███████▏  | 72/100 [00:39<00:13,  2.11it/s] 73%|███████▎  | 73/100 [00:39<00:12,  2.09it/s] 74%|███████▍  | 74/100 [00:40<00:12,  2.07it/s] 75%|███████▌  | 75/100 [00:40<00:12,  2.06it/s] 76%|███████▌  | 76/100 [00:41<00:11,  2.06it/s] 77%|███████▋  | 77/100 [00:41<00:11,  2.05it/s] 78%|███████▊  | 78/100 [00:42<00:10,  2.05it/s] 79%|███████▉  | 79/100 [00:42<00:10,  2.04it/s] 80%|████████  | 80/100 [00:43<00:09,  2.18it/s] 81%|████████  | 81/100 [00:43<00:08,  2.14it/s] 82%|████████▏ | 82/100 [00:44<00:08,  2.11it/s] 83%|████████▎ | 83/100 [00:44<00:08,  2.08it/s] 84%|████████▍ | 84/100 [00:45<00:07,  2.07it/s] 85%|████████▌ | 85/100 [00:45<00:07,  2.06it/s] 86%|████████▌ | 86/100 [00:46<00:06,  2.05it/s] 87%|████████▋ | 87/100 [00:46<00:06,  2.05it/s] 88%|████████▊ | 88/100 [00:47<00:05,  2.04it/s] 89%|████████▉ | 89/100 [00:47<00:05,  2.04it/s] 90%|█████████ | 90/100 [00:48<00:04,  2.18it/s] 91%|█████████ | 91/100 [00:48<00:04,  2.14it/s] 92%|█████████▏| 92/100 [00:49<00:03,  2.11it/s] 93%|█████████▎| 93/100 [00:49<00:03,  2.09it/s] 94%|█████████▍| 94/100 [00:50<00:02,  2.07it/s] 95%|█████████▌| 95/100 [00:50<00:02,  2.06it/s] 96%|█████████▌| 96/100 [00:51<00:01,  2.05it/s] 97%|█████████▋| 97/100 [00:51<00:01,  2.05it/s] 98%|█████████▊| 98/100 [00:52<00:00,  2.04it/s] 99%|█████████▉| 99/100 [00:52<00:00,  2.04it/s]100%|██████████| 100/100 [00:52<00:00,  2.18it/s][INFO|trainer.py:1679] 2022-08-27 21:49:26,896 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


                                                 100%|██████████| 100/100 [00:52<00:00,  2.18it/s]100%|██████████| 100/100 [00:52<00:00,  1.89it/s]
[INFO|trainer.py:2409] 2022-08-27 21:49:26,897 >> Saving model checkpoint to tmp/baseline/RTE/bert-base-uncased/HPO_S0/1e-06/256/10
[INFO|configuration_utils.py:446] 2022-08-27 21:49:26,904 >> Configuration saved in tmp/baseline/RTE/bert-base-uncased/HPO_S0/1e-06/256/10/config.json
[INFO|modeling_utils.py:1602] 2022-08-27 21:49:34,445 >> Model weights saved in tmp/baseline/RTE/bert-base-uncased/HPO_S0/1e-06/256/10/pytorch_model.bin
[INFO|tokenization_utils_base.py:2108] 2022-08-27 21:49:34,451 >> tokenizer config file saved in tmp/baseline/RTE/bert-base-uncased/HPO_S0/1e-06/256/10/tokenizer_config.json
[INFO|tokenization_utils_base.py:2114] 2022-08-27 21:49:34,456 >> Special tokens file saved in tmp/baseline/RTE/bert-base-uncased/HPO_S0/1e-06/256/10/special_tokens_map.json
{'train_runtime': 52.9381, 'train_samples_per_second': 470.361, 'train_steps_per_second': 1.889, 'train_loss': 0.7006800842285156, 'epoch': 10.0}
***** train metrics *****
  epoch                    =       10.0
  train_loss               =     0.7007
  train_runtime            = 0:00:52.93
  train_samples            =       2490
  train_samples_per_second =    470.361
  train_steps_per_second   =      1.889
08/27/2022 21:49:34 - INFO - __main__ - *** Evaluate ***
[INFO|trainer.py:628] 2022-08-27 21:49:34,547 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: idx, sentence2, sentence1. If idx, sentence2, sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:2659] 2022-08-27 21:49:34,550 >> ***** Running Evaluation *****
[INFO|trainer.py:2661] 2022-08-27 21:49:34,550 >>   Num examples = 277
[INFO|trainer.py:2664] 2022-08-27 21:49:34,550 >>   Batch size = 16
  0%|          | 0/18 [00:00<?, ?it/s] 17%|█▋        | 3/18 [00:00<00:00, 28.80it/s] 33%|███▎      | 6/18 [00:00<00:00, 22.53it/s] 50%|█████     | 9/18 [00:00<00:00, 16.62it/s] 61%|██████    | 11/18 [00:00<00:00, 17.36it/s] 72%|███████▏  | 13/18 [00:00<00:00, 17.95it/s] 83%|████████▎ | 15/18 [00:00<00:00, 18.34it/s] 94%|█████████▍| 17/18 [00:00<00:00, 18.54it/s]08/27/2022 21:49:35 - INFO - datasets.metric - Removing /home/ubuntu/.cache/huggingface/metrics/glue/rte/default_experiment-1-0.arrow
100%|██████████| 18/18 [00:00<00:00, 18.62it/s]
***** eval metrics *****
  epoch                   =       10.0
  eval_accuracy           =     0.4729
  eval_loss               =     0.6937
  eval_runtime            = 0:00:01.02
  eval_samples            =        277
  eval_samples_per_second =    270.325
  eval_steps_per_second   =     17.566
08/27/2022 21:49:39 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 2distributed training: False, 16-bits training: True
08/27/2022 21:49:39 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=2,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=IntervalStrategy.NO,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_min_num_params=0,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=1e-06,
length_column_name=length,
load_best_model_at_end=False,
local_rank=-1,
log_level=-1,
log_level_replica=-1,
log_on_each_node=True,
logging_dir=tmp/baseline/RTE/bert-base-uncased/HPO_S0/1e-06/256/30/runs/Aug27_21-49-39_ip-10-21-39-60,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
no_cuda=False,
num_train_epochs=30.0,
optim=OptimizerNames.ADAMW_HF,
output_dir=tmp/baseline/RTE/bert-base-uncased/HPO_S0/1e-06/256/30,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=128,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=tmp/baseline/RTE/bert-base-uncased/HPO_S0/1e-06/256/30,
save_on_each_node=False,
save_steps=500,
save_strategy=IntervalStrategy.STEPS,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tf32=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
xpu_backend=None,
)
08/27/2022 21:49:39 - INFO - datasets.info - Loading Dataset Infos from /home/ubuntu/.cache/huggingface/modules/datasets_modules/datasets/glue/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad
08/27/2022 21:49:39 - INFO - datasets.builder - Overwrite dataset info from restored data version.
08/27/2022 21:49:39 - INFO - datasets.info - Loading Dataset info from /home/ubuntu/.cache/huggingface/datasets/glue/rte/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad
08/27/2022 21:49:39 - WARNING - datasets.builder - Reusing dataset glue (/home/ubuntu/.cache/huggingface/datasets/glue/rte/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)
08/27/2022 21:49:39 - INFO - datasets.info - Loading Dataset info from /home/ubuntu/.cache/huggingface/datasets/glue/rte/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 572.76it/s]
[INFO|configuration_utils.py:657] 2022-08-27 21:49:39,737 >> loading configuration file /home/ubuntu/checkpoints/exp/RTE/config.json
[INFO|configuration_utils.py:708] 2022-08-27 21:49:39,738 >> Model config BertConfig {
  "_name_or_path": "/home/ubuntu/checkpoints/exp/RTE",
  "architectures": [
    "BertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "crypten": false,
  "finetuning_task": "rte",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "entailment",
    "1": "not_entailment"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "entailment": 0,
    "not_entailment": 1
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "softmax_act": "softmax",
  "torch_dtype": "float32",
  "transformers_version": "4.20.0.dev0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|tokenization_utils_base.py:1701] 2022-08-27 21:49:39,739 >> Didn't find file /home/ubuntu/checkpoints/exp/RTE/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1779] 2022-08-27 21:49:39,739 >> loading file /home/ubuntu/checkpoints/exp/RTE/vocab.txt
[INFO|tokenization_utils_base.py:1779] 2022-08-27 21:49:39,739 >> loading file /home/ubuntu/checkpoints/exp/RTE/tokenizer.json
[INFO|tokenization_utils_base.py:1779] 2022-08-27 21:49:39,739 >> loading file None
[INFO|tokenization_utils_base.py:1779] 2022-08-27 21:49:39,739 >> loading file /home/ubuntu/checkpoints/exp/RTE/special_tokens_map.json
[INFO|tokenization_utils_base.py:1779] 2022-08-27 21:49:39,740 >> loading file /home/ubuntu/checkpoints/exp/RTE/tokenizer_config.json
[INFO|modeling_utils.py:2047] 2022-08-27 21:49:39,782 >> loading weights file /home/ubuntu/checkpoints/exp/RTE/pytorch_model.bin
[INFO|modeling_bert.py:194] 2022-08-27 21:49:40,134 >> initializing embedding using nn.Embedding
[INFO|modeling_utils.py:2417] 2022-08-27 21:49:41,307 >> All model checkpoint weights were used when initializing BertForSequenceClassification.

[INFO|modeling_utils.py:2426] 2022-08-27 21:49:41,307 >> All the weights of BertForSequenceClassification were initialized from the model checkpoint at /home/ubuntu/checkpoints/exp/RTE.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.
using model config: BertConfig {
  "_name_or_path": "/home/ubuntu/checkpoints/exp/RTE",
  "architectures": [
    "BertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "crypten": false,
  "finetuning_task": "rte",
  "gradient_checkpointing": false,
  "hidden_act": "quad",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "entailment",
    "1": "not_entailment"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "entailment": 0,
    "not_entailment": 1
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "softmax_act": "2quad",
  "torch_dtype": "float32",
  "transformers_version": "4.20.0.dev0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

BertConfig {
  "_name_or_path": "/home/ubuntu/checkpoints/exp/RTE",
  "architectures": [
    "BertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "crypten": false,
  "finetuning_task": "rte",
  "gradient_checkpointing": false,
  "hidden_act": "quad",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "entailment",
    "1": "not_entailment"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "entailment": 0,
    "not_entailment": 1
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "softmax_act": "2quad",
  "torch_dtype": "float32",
  "transformers_version": "4.20.0.dev0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

model architecture: BertForSequenceClassification(
  (bert): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): QuadActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): QuadActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): QuadActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): QuadActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): QuadActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): QuadActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): QuadActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): QuadActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): QuadActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): QuadActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): QuadActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): QuadActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (dropout): Dropout(p=0.1, inplace=False)
  (classifier): Linear(in_features=768, out_features=2, bias=True)
)
08/27/2022 21:49:41 - WARNING - datasets.fingerprint - Parameter 'function'=<function main.<locals>.preprocess_function at 0x7f3341916488> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
Running tokenizer on dataset:   0%|          | 0/3 [00:00<?, ?ba/s]08/27/2022 21:49:41 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/ubuntu/.cache/huggingface/datasets/glue/rte/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-1c80317fa3b1799d.arrow
Running tokenizer on dataset:  33%|███▎      | 1/3 [00:00<00:00,  7.97ba/s]Running tokenizer on dataset:  67%|██████▋   | 2/3 [00:00<00:00,  6.16ba/s]Running tokenizer on dataset: 100%|██████████| 3/3 [00:00<00:00,  8.32ba/s]08/27/2022 21:49:41 - INFO - datasets.fingerprint - Parameter 'function'=<function main.<locals>.preprocess_function at 0x7f3341912950> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead.

Running tokenizer on dataset:   0%|          | 0/1 [00:00<?, ?ba/s]08/27/2022 21:49:41 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/ubuntu/.cache/huggingface/datasets/glue/rte/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-bdd640fb06671ad1.arrow
Running tokenizer on dataset: 100%|██████████| 1/1 [00:00<00:00, 35.50ba/s]08/27/2022 21:49:41 - INFO - datasets.fingerprint - Parameter 'function'=<function main.<locals>.preprocess_function at 0x7f3341916840> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead.

Running tokenizer on dataset:   0%|          | 0/3 [00:00<?, ?ba/s]08/27/2022 21:49:41 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/ubuntu/.cache/huggingface/datasets/glue/rte/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-3eb13b9046685257.arrow
Running tokenizer on dataset:  67%|██████▋   | 2/3 [00:00<00:00, 12.41ba/s]Running tokenizer on dataset: 100%|██████████| 3/3 [00:00<00:00, 11.17ba/s]128
08/27/2022 21:49:47 - INFO - __main__ - Sample 914 of the training set: {'sentence1': "Because of Reagan's economic strategy, the federal budget deficit ballooned.", 'sentence2': "Reagan's economic strategy led to huge federal budget deficits.", 'label': 0, 'idx': 914, 'input_ids': [101, 2138, 1997, 11531, 1005, 1055, 3171, 5656, 1010, 1996, 2976, 5166, 15074, 13212, 2098, 1012, 102, 11531, 1005, 1055, 3171, 5656, 2419, 2000, 4121, 2976, 5166, 15074, 2015, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.
128
08/27/2022 21:49:52 - INFO - __main__ - Sample 571 of the training set: {'sentence1': 'So far the British have preferred a policy of a voluntary approach to restricting advertising and high taxes on tobacco products.', 'sentence2': 'Sales have declined due to restrictions on advertising.', 'label': 1, 'idx': 571, 'input_ids': [101, 2061, 2521, 1996, 2329, 2031, 6871, 1037, 3343, 1997, 1037, 10758, 3921, 2000, 26996, 6475, 1998, 2152, 7773, 2006, 9098, 3688, 1012, 102, 4341, 2031, 6430, 2349, 2000, 9259, 2006, 6475, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.
128
08/27/2022 21:49:57 - INFO - __main__ - Sample 419 of the training set: {'sentence1': 'Protesters, many from organized pro-government groups but including many ordinary citizens, carried anti-American banners and chanted slogans attacking U.N. Secretary-General Kofi Annan for his close alignment with U.S. policy.', 'sentence2': 'Protesters confiscated anti-American banners and chanted slogans attacking U.N. Secretary-General Kofi Annan for his close alignment with U.S. policy.', 'label': 1, 'idx': 419, 'input_ids': [101, 13337, 1010, 2116, 2013, 4114, 4013, 1011, 2231, 2967, 2021, 2164, 2116, 6623, 4480, 1010, 3344, 3424, 1011, 2137, 23562, 1998, 16883, 2098, 14558, 2015, 7866, 1057, 1012, 1050, 1012, 3187, 1011, 2236, 12849, 8873, 4698, 2078, 2005, 2010, 2485, 12139, 2007, 1057, 1012, 1055, 1012, 3343, 1012, 102, 13337, 17182, 3424, 1011, 2137, 23562, 1998, 16883, 2098, 14558, 2015, 7866, 1057, 1012, 1050, 1012, 3187, 1011, 2236, 12849, 8873, 4698, 2078, 2005, 2010, 2485, 12139, 2007, 1057, 1012, 1055, 1012, 3343, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.

[INFO|trainer.py:506] 2022-08-27 21:50:01,752 >> Using amp half precision backend
[INFO|trainer.py:628] 2022-08-27 21:50:01,753 >> The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1, sentence2, idx. If sentence1, sentence2, idx are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.
/home/ubuntu/transformers_private/src/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  FutureWarning,
[INFO|trainer.py:1430] 2022-08-27 21:50:01,766 >> ***** Running training *****
[INFO|trainer.py:1431] 2022-08-27 21:50:01,766 >>   Num examples = 2490
[INFO|trainer.py:1432] 2022-08-27 21:50:01,766 >>   Num Epochs = 30
[INFO|trainer.py:1433] 2022-08-27 21:50:01,766 >>   Instantaneous batch size per device = 128
[INFO|trainer.py:1434] 2022-08-27 21:50:01,766 >>   Total train batch size (w. parallel, distributed & accumulation) = 256
[INFO|trainer.py:1435] 2022-08-27 21:50:01,766 >>   Gradient Accumulation steps = 1
[INFO|trainer.py:1436] 2022-08-27 21:50:01,766 >>   Total optimization steps = 300
using training arge: TrainingArguments(
_n_gpu=2,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=IntervalStrategy.NO,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_min_num_params=0,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=1e-06,
length_column_name=length,
load_best_model_at_end=False,
local_rank=-1,
log_level=-1,
log_level_replica=-1,
log_on_each_node=True,
logging_dir=tmp/baseline/RTE/bert-base-uncased/HPO_S0/1e-06/256/30/runs/Aug27_21-49-39_ip-10-21-39-60,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
no_cuda=False,
num_train_epochs=30.0,
optim=OptimizerNames.ADAMW_HF,
output_dir=tmp/baseline/RTE/bert-base-uncased/HPO_S0/1e-06/256/30,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=128,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=tmp/baseline/RTE/bert-base-uncased/HPO_S0/1e-06/256/30,
save_on_each_node=False,
save_steps=500,
save_strategy=IntervalStrategy.STEPS,
save_total_limit=0,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tf32=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
xpu_backend=None,
)
  0%|          | 0/300 [00:00<?, ?it/s]/home/ubuntu/anaconda3/envs/transformers/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
  0%|          | 1/300 [00:05<25:38,  5.15s/it]  1%|          | 2/300 [00:05<11:58,  2.41s/it]  1%|          | 3/300 [00:06<07:34,  1.53s/it]  1%|▏         | 4/300 [00:06<05:31,  1.12s/it]  2%|▏         | 5/300 [00:07<04:22,  1.12it/s]  2%|▏         | 6/300 [00:07<03:41,  1.33it/s]  2%|▏         | 7/300 [00:08<03:15,  1.50it/s]  3%|▎         | 8/300 [00:08<02:57,  1.64it/s]  3%|▎         | 9/300 [00:09<02:46,  1.75it/s]  3%|▎         | 10/300 [00:09<02:29,  1.95it/s]  4%|▎         | 11/300 [00:09<02:26,  1.98it/s]  4%|▍         | 12/300 [00:10<02:24,  2.00it/s]  4%|▍         | 13/300 [00:10<02:22,  2.01it/s]  5%|▍         | 14/300 [00:11<02:21,  2.02it/s]  5%|▌         | 15/300 [00:11<02:20,  2.03it/s]  5%|▌         | 16/300 [00:12<02:19,  2.04it/s]  6%|▌         | 17/300 [00:12<02:18,  2.04it/s]  6%|▌         | 18/300 [00:13<02:17,  2.05it/s]  6%|▋         | 19/300 [00:13<02:17,  2.05it/s]  7%|▋         | 20/300 [00:14<02:07,  2.19it/s]  7%|▋         | 21/300 [00:14<02:10,  2.15it/s]  7%|▋         | 22/300 [00:15<02:11,  2.12it/s]  8%|▊         | 23/300 [00:15<02:12,  2.10it/s]  8%|▊         | 24/300 [00:16<02:12,  2.08it/s]  8%|▊         | 25/300 [00:16<02:12,  2.08it/s]  9%|▊         | 26/300 [00:17<02:12,  2.07it/s]  9%|▉         | 27/300 [00:17<02:12,  2.07it/s]  9%|▉         | 28/300 [00:18<02:11,  2.06it/s] 10%|▉         | 29/300 [00:18<02:11,  2.06it/s] 10%|█         | 30/300 [00:18<02:02,  2.20it/s] 10%|█         | 31/300 [00:19<02:04,  2.15it/s] 11%|█         | 32/300 [00:19<02:06,  2.12it/s] 11%|█         | 33/300 [00:20<02:07,  2.10it/s] 11%|█▏        | 34/300 [00:20<02:07,  2.09it/s] 12%|█▏        | 35/300 [00:21<02:14,  1.97it/s] 12%|█▏        | 36/300 [00:21<02:12,  2.00it/s] 12%|█▏        | 37/300 [00:22<02:10,  2.01it/s] 13%|█▎        | 38/300 [00:22<02:09,  2.02it/s] 13%|█▎        | 39/300 [00:23<02:08,  2.03it/s] 13%|█▎        | 40/300 [00:23<01:59,  2.18it/s] 14%|█▎        | 41/300 [00:24<02:01,  2.14it/s] 14%|█▍        | 42/300 [00:24<02:02,  2.11it/s] 14%|█▍        | 43/300 [00:25<02:02,  2.09it/s] 15%|█▍        | 44/300 [00:25<02:03,  2.08it/s] 15%|█▌        | 45/300 [00:26<02:03,  2.07it/s] 15%|█▌        | 46/300 [00:26<02:03,  2.06it/s] 16%|█▌        | 47/300 [00:27<02:03,  2.06it/s] 16%|█▌        | 48/300 [00:27<02:02,  2.05it/s] 16%|█▋        | 49/300 [00:28<02:02,  2.05it/s] 17%|█▋        | 50/300 [00:28<01:54,  2.19it/s] 17%|█▋        | 51/300 [00:29<01:55,  2.15it/s] 17%|█▋        | 52/300 [00:29<01:57,  2.12it/s] 18%|█▊        | 53/300 [00:30<01:57,  2.10it/s] 18%|█▊        | 54/300 [00:30<01:57,  2.09it/s] 18%|█▊        | 55/300 [00:31<01:57,  2.08it/s] 19%|█▊        | 56/300 [00:31<01:57,  2.07it/s] 19%|█▉        | 57/300 [00:32<01:57,  2.07it/s] 19%|█▉        | 58/300 [00:32<01:57,  2.06it/s] 20%|█▉        | 59/300 [00:32<01:56,  2.06it/s] 20%|██        | 60/300 [00:33<01:49,  2.20it/s] 20%|██        | 61/300 [00:33<01:50,  2.15it/s] 21%|██        | 62/300 [00:34<01:52,  2.12it/s] 21%|██        | 63/300 [00:34<01:52,  2.10it/s] 21%|██▏       | 64/300 [00:35<01:52,  2.09it/s] 22%|██▏       | 65/300 [00:35<01:53,  2.08it/s] 22%|██▏       | 66/300 [00:36<01:52,  2.07it/s] 22%|██▏       | 67/300 [00:36<01:52,  2.07it/s] 23%|██▎       | 68/300 [00:37<01:52,  2.06it/s] 23%|██▎       | 69/300 [00:37<01:52,  2.06it/s] 23%|██▎       | 70/300 [00:38<01:44,  2.20it/s] 24%|██▎       | 71/300 [00:38<01:46,  2.15it/s] 24%|██▍       | 72/300 [00:39<01:47,  2.12it/s] 24%|██▍       | 73/300 [00:39<01:48,  2.10it/s] 25%|██▍       | 74/300 [00:40<01:48,  2.09it/s] 25%|██▌       | 75/300 [00:40<01:48,  2.08it/s] 25%|██▌       | 76/300 [00:41<01:48,  2.07it/s] 26%|██▌       | 77/300 [00:41<01:48,  2.06it/s] 26%|██▌       | 78/300 [00:42<01:47,  2.06it/s] 26%|██▋       | 79/300 [00:42<01:47,  2.06it/s] 27%|██▋       | 80/300 [00:42<01:40,  2.20it/s] 27%|██▋       | 81/300 [00:43<01:41,  2.15it/s] 27%|██▋       | 82/300 [00:43<01:42,  2.12it/s] 28%|██▊       | 83/300 [00:44<01:43,  2.10it/s] 28%|██▊       | 84/300 [00:44<01:43,  2.09it/s] 28%|██▊       | 85/300 [00:45<01:43,  2.07it/s] 29%|██▊       | 86/300 [00:45<01:43,  2.07it/s] 29%|██▉       | 87/300 [00:46<01:43,  2.06it/s] 29%|██▉       | 88/300 [00:46<01:42,  2.06it/s] 30%|██▉       | 89/300 [00:47<01:42,  2.06it/s] 30%|███       | 90/300 [00:47<01:35,  2.20it/s] 30%|███       | 91/300 [00:48<01:37,  2.15it/s] 31%|███       | 92/300 [00:48<01:38,  2.12it/s] 31%|███       | 93/300 [00:49<01:38,  2.10it/s] 31%|███▏      | 94/300 [00:49<01:38,  2.09it/s] 32%|███▏      | 95/300 [00:50<01:38,  2.08it/s] 32%|███▏      | 96/300 [00:50<01:38,  2.07it/s] 32%|███▏      | 97/300 [00:51<01:38,  2.06it/s] 33%|███▎      | 98/300 [00:51<01:38,  2.06it/s] 33%|███▎      | 99/300 [00:52<01:37,  2.06it/s] 33%|███▎      | 100/300 [00:52<01:30,  2.20it/s] 34%|███▎      | 101/300 [00:52<01:32,  2.15it/s] 34%|███▍      | 102/300 [00:53<01:33,  2.12it/s] 34%|███▍      | 103/300 [00:53<01:33,  2.10it/s] 35%|███▍      | 104/300 [00:54<01:33,  2.09it/s] 35%|███▌      | 105/300 [00:54<01:33,  2.08it/s] 35%|███▌      | 106/300 [00:55<01:38,  1.98it/s] 36%|███▌      | 107/300 [00:55<01:36,  2.00it/s] 36%|███▌      | 108/300 [00:56<01:35,  2.01it/s] 36%|███▋      | 109/300 [00:56<01:34,  2.03it/s] 37%|███▋      | 110/300 [00:57<01:27,  2.17it/s] 37%|███▋      | 111/300 [00:57<01:28,  2.13it/s] 37%|███▋      | 112/300 [00:58<01:29,  2.11it/s] 38%|███▊      | 113/300 [00:58<01:29,  2.09it/s] 38%|███▊      | 114/300 [00:59<01:29,  2.08it/s] 38%|███▊      | 115/300 [00:59<01:29,  2.07it/s] 39%|███▊      | 116/300 [01:00<01:29,  2.06it/s] 39%|███▉      | 117/300 [01:00<01:28,  2.06it/s] 39%|███▉      | 118/300 [01:01<01:28,  2.06it/s] 40%|███▉      | 119/300 [01:01<01:28,  2.06it/s] 40%|████      | 120/300 [01:02<01:21,  2.20it/s] 40%|████      | 121/300 [01:02<01:23,  2.15it/s] 41%|████      | 122/300 [01:03<01:23,  2.12it/s] 41%|████      | 123/300 [01:03<01:24,  2.10it/s] 41%|████▏     | 124/300 [01:03<01:24,  2.09it/s] 42%|████▏     | 125/300 [01:04<01:24,  2.08it/s] 42%|████▏     | 126/300 [01:04<01:24,  2.07it/s] 42%|████▏     | 127/300 [01:05<01:23,  2.07it/s] 43%|████▎     | 128/300 [01:05<01:23,  2.06it/s] 43%|████▎     | 129/300 [01:06<01:23,  2.06it/s] 43%|████▎     | 130/300 [01:06<01:17,  2.20it/s] 44%|████▎     | 131/300 [01:07<01:18,  2.15it/s] 44%|████▍     | 132/300 [01:07<01:19,  2.12it/s] 44%|████▍     | 133/300 [01:08<01:19,  2.10it/s] 45%|████▍     | 134/300 [01:08<01:19,  2.09it/s] 45%|████▌     | 135/300 [01:09<01:19,  2.08it/s] 45%|████▌     | 136/300 [01:09<01:19,  2.07it/s] 46%|████▌     | 137/300 [01:10<01:18,  2.06it/s] 46%|████▌     | 138/300 [01:10<01:18,  2.06it/s] 46%|████▋     | 139/300 [01:11<01:18,  2.06it/s] 47%|████▋     | 140/300 [01:11<01:12,  2.20it/s] 47%|████▋     | 141/300 [01:12<01:13,  2.15it/s] 47%|████▋     | 142/300 [01:12<01:14,  2.12it/s] 48%|████▊     | 143/300 [01:13<01:14,  2.10it/s] 48%|████▊     | 144/300 [01:13<01:14,  2.09it/s] 48%|████▊     | 145/300 [01:14<01:14,  2.08it/s] 49%|████▊     | 146/300 [01:14<01:14,  2.07it/s] 49%|████▉     | 147/300 [01:14<01:13,  2.07it/s] 49%|████▉     | 148/300 [01:15<01:13,  2.06it/s] 50%|████▉     | 149/300 [01:15<01:13,  2.06it/s] 50%|█████     | 150/300 [01:16<01:08,  2.20it/s] 50%|█████     | 151/300 [01:16<01:09,  2.15it/s] 51%|█████     | 152/300 [01:17<01:09,  2.12it/s] 51%|█████     | 153/300 [01:17<01:09,  2.10it/s] 51%|█████▏    | 154/300 [01:18<01:09,  2.09it/s] 52%|█████▏    | 155/300 [01:18<01:09,  2.08it/s] 52%|█████▏    | 156/300 [01:19<01:09,  2.07it/s] 52%|█████▏    | 157/300 [01:19<01:09,  2.07it/s] 53%|█████▎    | 158/300 [01:20<01:08,  2.06it/s] 53%|█████▎    | 159/300 [01:20<01:08,  2.06it/s] 53%|█████▎    | 160/300 [01:21<01:03,  2.20it/s] 54%|█████▎    | 161/300 [01:21<01:04,  2.15it/s] 54%|█████▍    | 162/300 [01:22<01:05,  2.12it/s] 54%|█████▍    | 163/300 [01:22<01:05,  2.10it/s] 55%|█████▍    | 164/300 [01:23<01:05,  2.08it/s] 55%|█████▌    | 165/300 [01:23<01:05,  2.07it/s] 55%|█████▌    | 166/300 [01:24<01:04,  2.06it/s] 56%|█████▌    | 167/300 [01:24<01:04,  2.06it/s] 56%|█████▌    | 168/300 [01:25<01:04,  2.06it/s] 56%|█████▋    | 169/300 [01:25<01:03,  2.05it/s] 57%|█████▋    | 170/300 [01:25<00:59,  2.19it/s] 57%|█████▋    | 171/300 [01:26<01:00,  2.15it/s] 57%|█████▋    | 172/300 [01:26<01:00,  2.12it/s] 58%|█████▊    | 173/300 [01:27<01:00,  2.10it/s] 58%|█████▊    | 174/300 [01:27<01:00,  2.08it/s] 58%|█████▊    | 175/300 [01:28<01:00,  2.07it/s] 59%|█████▊    | 176/300 [01:28<01:00,  2.06it/s] 59%|█████▉    | 177/300 [01:29<00:59,  2.06it/s] 59%|█████▉    | 178/300 [01:29<01:02,  1.96it/s] 60%|█████▉    | 179/300 [01:30<01:00,  1.99it/s] 60%|██████    | 180/300 [01:30<00:56,  2.12it/s] 60%|██████    | 181/300 [01:31<00:56,  2.10it/s] 61%|██████    | 182/300 [01:31<00:56,  2.09it/s] 61%|██████    | 183/300 [01:32<00:56,  2.08it/s] 61%|██████▏   | 184/300 [01:32<00:56,  2.07it/s] 62%|██████▏   | 185/300 [01:33<00:55,  2.06it/s] 62%|██████▏   | 186/300 [01:33<00:55,  2.06it/s] 62%|██████▏   | 187/300 [01:34<00:54,  2.06it/s] 63%|██████▎   | 188/300 [01:34<00:54,  2.06it/s] 63%|██████▎   | 189/300 [01:35<00:54,  2.06it/s] 63%|██████▎   | 190/300 [01:35<00:50,  2.19it/s] 64%|██████▎   | 191/300 [01:35<00:50,  2.15it/s] 64%|██████▍   | 192/300 [01:36<00:50,  2.12it/s] 64%|██████▍   | 193/300 [01:36<00:50,  2.10it/s] 65%|██████▍   | 194/300 [01:37<00:50,  2.09it/s] 65%|██████▌   | 195/300 [01:37<00:50,  2.08it/s] 65%|██████▌   | 196/300 [01:38<00:50,  2.07it/s] 66%|██████▌   | 197/300 [01:38<00:49,  2.06it/s] 66%|██████▌   | 198/300 [01:39<00:49,  2.06it/s] 66%|██████▋   | 199/300 [01:39<00:49,  2.06it/s] 67%|██████▋   | 200/300 [01:40<00:45,  2.20it/s] 67%|██████▋   | 201/300 [01:40<00:45,  2.15it/s] 67%|██████▋   | 202/300 [01:41<00:46,  2.12it/s] 68%|██████▊   | 203/300 [01:41<00:46,  2.10it/s] 68%|██████▊   | 204/300 [01:42<00:45,  2.09it/s] 68%|██████▊   | 205/300 [01:42<00:45,  2.08it/s] 69%|██████▊   | 206/300 [01:43<00:45,  2.07it/s] 69%|██████▉   | 207/300 [01:43<00:44,  2.07it/s] 69%|██████▉   | 208/300 [01:44<00:44,  2.06it/s] 70%|██████▉   | 209/300 [01:44<00:44,  2.06it/s] 70%|███████   | 210/300 [01:45<00:40,  2.20it/s] 70%|███████   | 211/300 [01:45<00:41,  2.15it/s] 71%|███████   | 212/300 [01:46<00:41,  2.12it/s] 71%|███████   | 213/300 [01:46<00:41,  2.10it/s] 71%|███████▏  | 214/300 [01:46<00:41,  2.09it/s] 72%|███████▏  | 215/300 [01:47<00:40,  2.08it/s] 72%|███████▏  | 216/300 [01:47<00:40,  2.07it/s] 72%|███████▏  | 217/300 [01:48<00:40,  2.07it/s] 73%|███████▎  | 218/300 [01:48<00:39,  2.06it/s] 73%|███████▎  | 219/300 [01:49<00:39,  2.06it/s] 73%|███████▎  | 220/300 [01:49<00:36,  2.20it/s] 74%|███████▎  | 221/300 [01:50<00:36,  2.15it/s] 74%|███████▍  | 222/300 [01:50<00:36,  2.12it/s] 74%|███████▍  | 223/300 [01:51<00:36,  2.10it/s] 75%|███████▍  | 224/300 [01:51<00:36,  2.08it/s] 75%|███████▌  | 225/300 [01:52<00:36,  2.07it/s] 75%|███████▌  | 226/300 [01:52<00:35,  2.07it/s] 76%|███████▌  | 227/300 [01:53<00:35,  2.06it/s] 76%|███████▌  | 228/300 [01:53<00:34,  2.06it/s] 76%|███████▋  | 229/300 [01:54<00:34,  2.06it/s] 77%|███████▋  | 230/300 [01:54<00:31,  2.19it/s] 77%|███████▋  | 231/300 [01:55<00:32,  2.15it/s] 77%|███████▋  | 232/300 [01:55<00:32,  2.12it/s] 78%|███████▊  | 233/300 [01:56<00:31,  2.10it/s] 78%|███████▊  | 234/300 [01:56<00:31,  2.08it/s] 78%|███████▊  | 235/300 [01:57<00:31,  2.07it/s] 79%|███████▊  | 236/300 [01:57<00:30,  2.07it/s] 79%|███████▉  | 237/300 [01:57<00:30,  2.06it/s] 79%|███████▉  | 238/300 [01:58<00:30,  2.06it/s] 80%|███████▉  | 239/300 [01:58<00:29,  2.06it/s] 80%|████████  | 240/300 [01:59<00:27,  2.19it/s] 80%|████████  | 241/300 [01:59<00:27,  2.15it/s] 81%|████████  | 242/300 [02:00<00:27,  2.12it/s] 81%|████████  | 243/300 [02:00<00:27,  2.10it/s] 81%|████████▏ | 244/300 [02:01<00:26,  2.08it/s] 82%|████████▏ | 245/300 [02:01<00:26,  2.07it/s] 82%|████████▏ | 246/300 [02:02<00:26,  2.07it/s] 82%|████████▏ | 247/300 [02:02<00:25,  2.06it/s] 83%|████████▎ | 248/300 [02:03<00:25,  2.06it/s] 83%|████████▎ | 249/300 [02:03<00:24,  2.06it/s] 83%|████████▎ | 250/300 [02:04<00:23,  2.09it/s] 84%|████████▎ | 251/300 [02:04<00:23,  2.08it/s] 84%|████████▍ | 252/300 [02:05<00:23,  2.07it/s] 84%|████████▍ | 253/300 [02:05<00:22,  2.06it/s] 85%|████████▍ | 254/300 [02:06<00:22,  2.06it/s] 85%|████████▌ | 255/300 [02:06<00:21,  2.05it/s] 85%|████████▌ | 256/300 [02:07<00:21,  2.05it/s] 86%|████████▌ | 257/300 [02:07<00:20,  2.05it/s] 86%|████████▌ | 258/300 [02:08<00:20,  2.05it/s] 86%|████████▋ | 259/300 [02:08<00:20,  2.05it/s] 87%|████████▋ | 260/300 [02:08<00:18,  2.19it/s] 87%|████████▋ | 261/300 [02:09<00:18,  2.14it/s] 87%|████████▋ | 262/300 [02:09<00:17,  2.11it/s] 88%|████████▊ | 263/300 [02:10<00:17,  2.09it/s] 88%|████████▊ | 264/300 [02:10<00:17,  2.08it/s] 88%|████████▊ | 265/300 [02:11<00:16,  2.07it/s] 89%|████████▊ | 266/300 [02:11<00:16,  2.06it/s] 89%|████████▉ | 267/300 [02:12<00:16,  2.06it/s] 89%|████████▉ | 268/300 [02:12<00:15,  2.06it/s] 90%|████████▉ | 269/300 [02:13<00:15,  2.05it/s] 90%|█████████ | 270/300 [02:13<00:13,  2.19it/s] 90%|█████████ | 271/300 [02:14<00:13,  2.15it/s] 91%|█████████ | 272/300 [02:14<00:13,  2.12it/s] 91%|█████████ | 273/300 [02:15<00:12,  2.10it/s] 91%|█████████▏| 274/300 [02:15<00:12,  2.08it/s] 92%|█████████▏| 275/300 [02:16<00:12,  2.07it/s] 92%|█████████▏| 276/300 [02:16<00:11,  2.07it/s] 92%|█████████▏| 277/300 [02:17<00:11,  2.06it/s] 93%|█████████▎| 278/300 [02:17<00:10,  2.06it/s] 93%|█████████▎| 279/300 [02:18<00:10,  2.06it/s] 93%|█████████▎| 280/300 [02:18<00:09,  2.19it/s] 94%|█████████▎| 281/300 [02:19<00:08,  2.15it/s] 94%|█████████▍| 282/300 [02:19<00:08,  2.12it/s] 94%|█████████▍| 283/300 [02:19<00:08,  2.10it/s] 95%|█████████▍| 284/300 [02:20<00:07,  2.08it/s] 95%|█████████▌| 285/300 [02:20<00:07,  2.07it/s] 95%|█████████▌| 286/300 [02:21<00:06,  2.07it/s] 96%|█████████▌| 287/300 [02:21<00:06,  2.06it/s] 96%|█████████▌| 288/300 [02:22<00:05,  2.06it/s] 96%|█████████▋| 289/300 [02:22<00:05,  2.05it/s] 97%|█████████▋| 290/300 [02:23<00:04,  2.19it/s] 97%|█████████▋| 291/300 [02:23<00:04,  2.15it/s] 97%|█████████▋| 292/300 [02:24<00:03,  2.12it/s] 98%|█████████▊| 293/300 [02:24<00:03,  2.10it/s] 98%|█████████▊| 294/300 [02:25<00:02,  2.08it/s] 98%|█████████▊| 295/300 [02:25<00:02,  2.07it/s] 99%|█████████▊| 296/300 [02:26<00:01,  2.06it/s] 99%|█████████▉| 297/300 [02:26<00:01,  2.06it/s] 99%|█████████▉| 298/300 [02:27<00:00,  2.06it/s]100%|█████████▉| 299/300 [02:27<00:00,  2.05it/s]100%|██████████| 300/300 [02:28<00:00,  2.19it/s][INFO|trainer.py:1679] 2022-08-27 21:52:29,857 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


                                                 100%|██████████| 300/300 [02:28<00:00,  2.19it/s]100%|██████████| 300/300 [02:28<00:00,  2.03it/s]
[INFO|trainer.py:2409] 2022-08-27 21:52:29,858 >> Saving model checkpoint to tmp/baseline/RTE/bert-base-uncased/HPO_S0/1e-06/256/30
[INFO|configuration_utils.py:446] 2022-08-27 21:52:29,860 >> Configuration saved in tmp/baseline/RTE/bert-base-uncased/HPO_S0/1e-06/256/30/config.json
[INFO|modeling_utils.py:1602] 2022-08-27 21:52:30,757 >> Model weights saved in tmp/baseline/RTE/bert-base-uncased/HPO_S0/1e-06/256/30/pytorch_model.bin
[INFO|tokenization_utils_base.py:2108] 2022-08-27 21:52:30,758 >> tokenizer config file saved in tmp/baseline/RTE/bert-base-uncased/HPO_S0/1e-06/256/30/tokenizer_config.json
[INFO|tokenization_utils_base.py:2114] 2022-08-27 21:52:30,758 >> Special tokens file saved in tmp/baseline/RTE/bert-base-uncased/HPO_S0/1e-06/256/30/special_tokens_map.json
{'train_runtime': 148.0916, 'train_samples_per_second': 504.417, 'train_steps_per_second': 2.026, 'train_loss': 0.6985655212402344, 'epoch': 30.0}
***** train metrics *****
  epoch                    =       30.0
  train_loss               =     0.6986
  train_runtime            = 0:02:28.09
  train_samples            =       2490
  train_samples_per_second =    504.417
  train_steps_per_second   =      2.026
08/27/2022 21:52:30 - INFO - __main__ - *** Evaluate ***
[INFO|trainer.py:628] 2022-08-27 21:52:30,807 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1, sentence2, idx. If sentence1, sentence2, idx are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:2659] 2022-08-27 21:52:30,810 >> ***** Running Evaluation *****
[INFO|trainer.py:2661] 2022-08-27 21:52:30,810 >>   Num examples = 277
[INFO|trainer.py:2664] 2022-08-27 21:52:30,810 >>   Batch size = 16
  0%|          | 0/18 [00:00<?, ?it/s] 22%|██▏       | 4/18 [00:00<00:00, 26.77it/s] 39%|███▉      | 7/18 [00:00<00:00, 22.90it/s] 56%|█████▌    | 10/18 [00:00<00:00, 21.60it/s] 72%|███████▏  | 13/18 [00:00<00:00, 20.98it/s] 89%|████████▉ | 16/18 [00:00<00:00, 17.22it/s]08/27/2022 21:52:31 - INFO - datasets.metric - Removing /home/ubuntu/.cache/huggingface/metrics/glue/rte/default_experiment-1-0.arrow
100%|██████████| 18/18 [00:00<00:00, 19.29it/s]
***** eval metrics *****
  epoch                   =       30.0
  eval_accuracy           =     0.4729
  eval_loss               =     0.6934
  eval_runtime            = 0:00:00.98
  eval_samples            =        277
  eval_samples_per_second =    280.337
  eval_steps_per_second   =     18.217
08/27/2022 21:52:35 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 2distributed training: False, 16-bits training: True
08/27/2022 21:52:35 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=2,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=IntervalStrategy.NO,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_min_num_params=0,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=1e-06,
length_column_name=length,
load_best_model_at_end=False,
local_rank=-1,
log_level=-1,
log_level_replica=-1,
log_on_each_node=True,
logging_dir=tmp/baseline/RTE/bert-base-uncased/HPO_S0/1e-06/256/100/runs/Aug27_21-52-35_ip-10-21-39-60,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
no_cuda=False,
num_train_epochs=100.0,
optim=OptimizerNames.ADAMW_HF,
output_dir=tmp/baseline/RTE/bert-base-uncased/HPO_S0/1e-06/256/100,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=128,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=tmp/baseline/RTE/bert-base-uncased/HPO_S0/1e-06/256/100,
save_on_each_node=False,
save_steps=500,
save_strategy=IntervalStrategy.STEPS,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tf32=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
xpu_backend=None,
)
08/27/2022 21:52:35 - INFO - datasets.info - Loading Dataset Infos from /home/ubuntu/.cache/huggingface/modules/datasets_modules/datasets/glue/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad
08/27/2022 21:52:35 - INFO - datasets.builder - Overwrite dataset info from restored data version.
08/27/2022 21:52:35 - INFO - datasets.info - Loading Dataset info from /home/ubuntu/.cache/huggingface/datasets/glue/rte/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad
08/27/2022 21:52:36 - WARNING - datasets.builder - Reusing dataset glue (/home/ubuntu/.cache/huggingface/datasets/glue/rte/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)
08/27/2022 21:52:36 - INFO - datasets.info - Loading Dataset info from /home/ubuntu/.cache/huggingface/datasets/glue/rte/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 743.89it/s]
[INFO|configuration_utils.py:657] 2022-08-27 21:52:36,035 >> loading configuration file /home/ubuntu/checkpoints/exp/RTE/config.json
[INFO|configuration_utils.py:708] 2022-08-27 21:52:36,036 >> Model config BertConfig {
  "_name_or_path": "/home/ubuntu/checkpoints/exp/RTE",
  "architectures": [
    "BertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "crypten": false,
  "finetuning_task": "rte",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "entailment",
    "1": "not_entailment"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "entailment": 0,
    "not_entailment": 1
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "softmax_act": "softmax",
  "torch_dtype": "float32",
  "transformers_version": "4.20.0.dev0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|tokenization_utils_base.py:1701] 2022-08-27 21:52:36,036 >> Didn't find file /home/ubuntu/checkpoints/exp/RTE/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1779] 2022-08-27 21:52:36,036 >> loading file /home/ubuntu/checkpoints/exp/RTE/vocab.txt
[INFO|tokenization_utils_base.py:1779] 2022-08-27 21:52:36,036 >> loading file /home/ubuntu/checkpoints/exp/RTE/tokenizer.json
[INFO|tokenization_utils_base.py:1779] 2022-08-27 21:52:36,036 >> loading file None
[INFO|tokenization_utils_base.py:1779] 2022-08-27 21:52:36,036 >> loading file /home/ubuntu/checkpoints/exp/RTE/special_tokens_map.json
[INFO|tokenization_utils_base.py:1779] 2022-08-27 21:52:36,036 >> loading file /home/ubuntu/checkpoints/exp/RTE/tokenizer_config.json
[INFO|modeling_utils.py:2047] 2022-08-27 21:52:36,077 >> loading weights file /home/ubuntu/checkpoints/exp/RTE/pytorch_model.bin
[INFO|modeling_bert.py:194] 2022-08-27 21:52:36,435 >> initializing embedding using nn.Embedding
[INFO|modeling_utils.py:2417] 2022-08-27 21:52:37,587 >> All model checkpoint weights were used when initializing BertForSequenceClassification.

[INFO|modeling_utils.py:2426] 2022-08-27 21:52:37,587 >> All the weights of BertForSequenceClassification were initialized from the model checkpoint at /home/ubuntu/checkpoints/exp/RTE.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.
using model config: BertConfig {
  "_name_or_path": "/home/ubuntu/checkpoints/exp/RTE",
  "architectures": [
    "BertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "crypten": false,
  "finetuning_task": "rte",
  "gradient_checkpointing": false,
  "hidden_act": "quad",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "entailment",
    "1": "not_entailment"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "entailment": 0,
    "not_entailment": 1
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "softmax_act": "2quad",
  "torch_dtype": "float32",
  "transformers_version": "4.20.0.dev0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

BertConfig {
  "_name_or_path": "/home/ubuntu/checkpoints/exp/RTE",
  "architectures": [
    "BertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "crypten": false,
  "finetuning_task": "rte",
  "gradient_checkpointing": false,
  "hidden_act": "quad",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "entailment",
    "1": "not_entailment"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "entailment": 0,
    "not_entailment": 1
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "softmax_act": "2quad",
  "torch_dtype": "float32",
  "transformers_version": "4.20.0.dev0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

model architecture: BertForSequenceClassification(
  (bert): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): QuadActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): QuadActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): QuadActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): QuadActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): QuadActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): QuadActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): QuadActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): QuadActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): QuadActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): QuadActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): QuadActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): QuadActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (dropout): Dropout(p=0.1, inplace=False)
  (classifier): Linear(in_features=768, out_features=2, bias=True)
)
08/27/2022 21:52:37 - WARNING - datasets.fingerprint - Parameter 'function'=<function main.<locals>.preprocess_function at 0x7f9f7b633488> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
Running tokenizer on dataset:   0%|          | 0/3 [00:00<?, ?ba/s]08/27/2022 21:52:37 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/ubuntu/.cache/huggingface/datasets/glue/rte/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-1c80317fa3b1799d.arrow
Running tokenizer on dataset:  33%|███▎      | 1/3 [00:00<00:00,  7.89ba/s]Running tokenizer on dataset:  67%|██████▋   | 2/3 [00:00<00:00,  7.18ba/s]Running tokenizer on dataset: 100%|██████████| 3/3 [00:00<00:00,  9.31ba/s]08/27/2022 21:52:38 - INFO - datasets.fingerprint - Parameter 'function'=<function main.<locals>.preprocess_function at 0x7f9f7b62f950> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead.

Running tokenizer on dataset:   0%|          | 0/1 [00:00<?, ?ba/s]08/27/2022 21:52:38 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/ubuntu/.cache/huggingface/datasets/glue/rte/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-bdd640fb06671ad1.arrow
Running tokenizer on dataset: 100%|██████████| 1/1 [00:00<00:00, 34.75ba/s]08/27/2022 21:52:38 - INFO - datasets.fingerprint - Parameter 'function'=<function main.<locals>.preprocess_function at 0x7f9f7b633840> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead.

Running tokenizer on dataset:   0%|          | 0/3 [00:00<?, ?ba/s]08/27/2022 21:52:38 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/ubuntu/.cache/huggingface/datasets/glue/rte/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-3eb13b9046685257.arrow
Running tokenizer on dataset:  67%|██████▋   | 2/3 [00:00<00:00, 12.60ba/s]Running tokenizer on dataset: 100%|██████████| 3/3 [00:00<00:00, 11.38ba/s]128
08/27/2022 21:52:43 - INFO - __main__ - Sample 914 of the training set: {'sentence1': "Because of Reagan's economic strategy, the federal budget deficit ballooned.", 'sentence2': "Reagan's economic strategy led to huge federal budget deficits.", 'label': 0, 'idx': 914, 'input_ids': [101, 2138, 1997, 11531, 1005, 1055, 3171, 5656, 1010, 1996, 2976, 5166, 15074, 13212, 2098, 1012, 102, 11531, 1005, 1055, 3171, 5656, 2419, 2000, 4121, 2976, 5166, 15074, 2015, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.
128
08/27/2022 21:52:48 - INFO - __main__ - Sample 571 of the training set: {'sentence1': 'So far the British have preferred a policy of a voluntary approach to restricting advertising and high taxes on tobacco products.', 'sentence2': 'Sales have declined due to restrictions on advertising.', 'label': 1, 'idx': 571, 'input_ids': [101, 2061, 2521, 1996, 2329, 2031, 6871, 1037, 3343, 1997, 1037, 10758, 3921, 2000, 26996, 6475, 1998, 2152, 7773, 2006, 9098, 3688, 1012, 102, 4341, 2031, 6430, 2349, 2000, 9259, 2006, 6475, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.
128
08/27/2022 21:52:53 - INFO - __main__ - Sample 419 of the training set: {'sentence1': 'Protesters, many from organized pro-government groups but including many ordinary citizens, carried anti-American banners and chanted slogans attacking U.N. Secretary-General Kofi Annan for his close alignment with U.S. policy.', 'sentence2': 'Protesters confiscated anti-American banners and chanted slogans attacking U.N. Secretary-General Kofi Annan for his close alignment with U.S. policy.', 'label': 1, 'idx': 419, 'input_ids': [101, 13337, 1010, 2116, 2013, 4114, 4013, 1011, 2231, 2967, 2021, 2164, 2116, 6623, 4480, 1010, 3344, 3424, 1011, 2137, 23562, 1998, 16883, 2098, 14558, 2015, 7866, 1057, 1012, 1050, 1012, 3187, 1011, 2236, 12849, 8873, 4698, 2078, 2005, 2010, 2485, 12139, 2007, 1057, 1012, 1055, 1012, 3343, 1012, 102, 13337, 17182, 3424, 1011, 2137, 23562, 1998, 16883, 2098, 14558, 2015, 7866, 1057, 1012, 1050, 1012, 3187, 1011, 2236, 12849, 8873, 4698, 2078, 2005, 2010, 2485, 12139, 2007, 1057, 1012, 1055, 1012, 3343, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.

[INFO|trainer.py:506] 2022-08-27 21:52:57,844 >> Using amp half precision backend
[INFO|trainer.py:628] 2022-08-27 21:52:57,846 >> The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence2, sentence1, idx. If sentence2, sentence1, idx are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.
/home/ubuntu/transformers_private/src/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  FutureWarning,
[INFO|trainer.py:1430] 2022-08-27 21:52:57,857 >> ***** Running training *****
[INFO|trainer.py:1431] 2022-08-27 21:52:57,857 >>   Num examples = 2490
[INFO|trainer.py:1432] 2022-08-27 21:52:57,857 >>   Num Epochs = 100
[INFO|trainer.py:1433] 2022-08-27 21:52:57,857 >>   Instantaneous batch size per device = 128
[INFO|trainer.py:1434] 2022-08-27 21:52:57,857 >>   Total train batch size (w. parallel, distributed & accumulation) = 256
[INFO|trainer.py:1435] 2022-08-27 21:52:57,858 >>   Gradient Accumulation steps = 1
[INFO|trainer.py:1436] 2022-08-27 21:52:57,858 >>   Total optimization steps = 1000
using training arge: TrainingArguments(
_n_gpu=2,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=IntervalStrategy.NO,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_min_num_params=0,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=1e-06,
length_column_name=length,
load_best_model_at_end=False,
local_rank=-1,
log_level=-1,
log_level_replica=-1,
log_on_each_node=True,
logging_dir=tmp/baseline/RTE/bert-base-uncased/HPO_S0/1e-06/256/100/runs/Aug27_21-52-35_ip-10-21-39-60,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
no_cuda=False,
num_train_epochs=100.0,
optim=OptimizerNames.ADAMW_HF,
output_dir=tmp/baseline/RTE/bert-base-uncased/HPO_S0/1e-06/256/100,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=128,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=tmp/baseline/RTE/bert-base-uncased/HPO_S0/1e-06/256/100,
save_on_each_node=False,
save_steps=500,
save_strategy=IntervalStrategy.STEPS,
save_total_limit=0,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tf32=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
xpu_backend=None,
)
  0%|          | 0/1000 [00:00<?, ?it/s]/home/ubuntu/anaconda3/envs/transformers/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
  0%|          | 1/1000 [00:04<1:19:00,  4.75s/it]  0%|          | 2/1000 [00:05<37:14,  2.24s/it]    0%|          | 3/1000 [00:05<23:52,  1.44s/it]  0%|          | 4/1000 [00:06<17:35,  1.06s/it]  0%|          | 5/1000 [00:06<14:07,  1.17it/s]  1%|          | 6/1000 [00:07<12:01,  1.38it/s]  1%|          | 7/1000 [00:07<10:40,  1.55it/s]  1%|          | 8/1000 [00:08<09:48,  1.69it/s]  1%|          | 9/1000 [00:08<09:12,  1.79it/s]  1%|          | 10/1000 [00:08<08:17,  1.99it/s]  1%|          | 11/1000 [00:09<08:10,  2.02it/s]  1%|          | 12/1000 [00:09<08:05,  2.04it/s]  1%|▏         | 13/1000 [00:10<08:01,  2.05it/s]  1%|▏         | 14/1000 [00:10<07:58,  2.06it/s]  2%|▏         | 15/1000 [00:11<07:57,  2.06it/s]  2%|▏         | 16/1000 [00:11<07:56,  2.07it/s]  2%|▏         | 17/1000 [00:12<07:54,  2.07it/s]  2%|▏         | 18/1000 [00:12<07:54,  2.07it/s]  2%|▏         | 19/1000 [00:13<07:53,  2.07it/s]  2%|▏         | 20/1000 [00:13<07:22,  2.22it/s]  2%|▏         | 21/1000 [00:14<07:32,  2.17it/s]  2%|▏         | 22/1000 [00:14<07:37,  2.14it/s]  2%|▏         | 23/1000 [00:15<07:40,  2.12it/s]  2%|▏         | 24/1000 [00:15<07:42,  2.11it/s]  2%|▎         | 25/1000 [00:16<07:44,  2.10it/s]  3%|▎         | 26/1000 [00:16<07:44,  2.09it/s]  3%|▎         | 27/1000 [00:17<07:45,  2.09it/s]  3%|▎         | 28/1000 [00:17<07:45,  2.09it/s]  3%|▎         | 29/1000 [00:18<07:45,  2.09it/s]  3%|▎         | 30/1000 [00:18<07:15,  2.23it/s]  3%|▎         | 31/1000 [00:18<07:24,  2.18it/s]  3%|▎         | 32/1000 [00:19<07:30,  2.15it/s]  3%|▎         | 33/1000 [00:19<07:34,  2.13it/s]  3%|▎         | 34/1000 [00:20<07:36,  2.11it/s]  4%|▎         | 35/1000 [00:20<08:02,  2.00it/s]  4%|▎         | 36/1000 [00:21<07:56,  2.02it/s]  4%|▎         | 37/1000 [00:21<07:52,  2.04it/s]  4%|▍         | 38/1000 [00:22<07:48,  2.05it/s]  4%|▍         | 39/1000 [00:22<07:46,  2.06it/s]  4%|▍         | 40/1000 [00:23<07:15,  2.21it/s]  4%|▍         | 41/1000 [00:23<07:22,  2.17it/s]  4%|▍         | 42/1000 [00:24<07:28,  2.14it/s]  4%|▍         | 43/1000 [00:24<07:31,  2.12it/s]  4%|▍         | 44/1000 [00:25<07:33,  2.11it/s]  4%|▍         | 45/1000 [00:25<07:34,  2.10it/s]  5%|▍         | 46/1000 [00:26<07:35,  2.09it/s]  5%|▍         | 47/1000 [00:26<07:35,  2.09it/s]  5%|▍         | 48/1000 [00:27<07:36,  2.09it/s]  5%|▍         | 49/1000 [00:27<07:36,  2.08it/s]  5%|▌         | 50/1000 [00:27<07:07,  2.22it/s]  5%|▌         | 51/1000 [00:28<07:15,  2.18it/s]  5%|▌         | 52/1000 [00:28<07:21,  2.15it/s]  5%|▌         | 53/1000 [00:29<07:25,  2.12it/s]  5%|▌         | 54/1000 [00:29<07:28,  2.11it/s]  6%|▌         | 55/1000 [00:30<07:30,  2.10it/s]  6%|▌         | 56/1000 [00:30<07:31,  2.09it/s]  6%|▌         | 57/1000 [00:31<07:31,  2.09it/s]  6%|▌         | 58/1000 [00:31<07:32,  2.08it/s]  6%|▌         | 59/1000 [00:32<07:31,  2.08it/s]  6%|▌         | 60/1000 [00:32<07:02,  2.22it/s]  6%|▌         | 61/1000 [00:33<07:11,  2.18it/s]  6%|▌         | 62/1000 [00:33<07:17,  2.15it/s]  6%|▋         | 63/1000 [00:34<07:20,  2.13it/s]  6%|▋         | 64/1000 [00:34<07:23,  2.11it/s]  6%|▋         | 65/1000 [00:35<07:24,  2.10it/s]  7%|▋         | 66/1000 [00:35<07:25,  2.10it/s]  7%|▋         | 67/1000 [00:35<07:26,  2.09it/s]  7%|▋         | 68/1000 [00:36<07:26,  2.09it/s]  7%|▋         | 69/1000 [00:36<07:27,  2.08it/s]  7%|▋         | 70/1000 [00:37<06:58,  2.22it/s]  7%|▋         | 71/1000 [00:37<07:06,  2.18it/s]  7%|▋         | 72/1000 [00:38<07:12,  2.15it/s]  7%|▋         | 73/1000 [00:38<07:16,  2.13it/s]  7%|▋         | 74/1000 [00:39<07:18,  2.11it/s]  8%|▊         | 75/1000 [00:39<07:20,  2.10it/s]  8%|▊         | 76/1000 [00:40<07:20,  2.10it/s]  8%|▊         | 77/1000 [00:40<07:21,  2.09it/s]  8%|▊         | 78/1000 [00:41<07:21,  2.09it/s]  8%|▊         | 79/1000 [00:41<07:21,  2.09it/s]  8%|▊         | 80/1000 [00:42<06:52,  2.23it/s]  8%|▊         | 81/1000 [00:42<07:01,  2.18it/s]  8%|▊         | 82/1000 [00:42<07:07,  2.15it/s]  8%|▊         | 83/1000 [00:43<07:10,  2.13it/s]  8%|▊         | 84/1000 [00:43<07:13,  2.11it/s]  8%|▊         | 85/1000 [00:44<07:15,  2.10it/s]  9%|▊         | 86/1000 [00:44<07:16,  2.10it/s]  9%|▊         | 87/1000 [00:45<07:16,  2.09it/s]  9%|▉         | 88/1000 [00:45<07:16,  2.09it/s]  9%|▉         | 89/1000 [00:46<07:16,  2.09it/s]  9%|▉         | 90/1000 [00:46<06:48,  2.23it/s]  9%|▉         | 91/1000 [00:47<06:56,  2.18it/s]  9%|▉         | 92/1000 [00:47<07:02,  2.15it/s]  9%|▉         | 93/1000 [00:48<07:06,  2.13it/s]  9%|▉         | 94/1000 [00:48<07:08,  2.11it/s] 10%|▉         | 95/1000 [00:49<07:10,  2.10it/s] 10%|▉         | 96/1000 [00:49<07:11,  2.10it/s] 10%|▉         | 97/1000 [00:50<07:11,  2.09it/s] 10%|▉         | 98/1000 [00:50<07:11,  2.09it/s] 10%|▉         | 99/1000 [00:51<07:12,  2.08it/s] 10%|█         | 100/1000 [00:51<06:44,  2.22it/s] 10%|█         | 101/1000 [00:51<06:52,  2.18it/s] 10%|█         | 102/1000 [00:52<06:57,  2.15it/s] 10%|█         | 103/1000 [00:52<07:02,  2.13it/s] 10%|█         | 104/1000 [00:53<07:04,  2.11it/s] 10%|█         | 105/1000 [00:53<07:05,  2.10it/s] 11%|█         | 106/1000 [00:54<07:26,  2.00it/s] 11%|█         | 107/1000 [00:54<07:21,  2.02it/s] 11%|█         | 108/1000 [00:55<07:17,  2.04it/s] 11%|█         | 109/1000 [00:55<07:14,  2.05it/s] 11%|█         | 110/1000 [00:56<06:44,  2.20it/s] 11%|█         | 111/1000 [00:56<06:50,  2.17it/s] 11%|█         | 112/1000 [00:57<06:54,  2.14it/s] 11%|█▏        | 113/1000 [00:57<06:57,  2.12it/s] 11%|█▏        | 114/1000 [00:58<06:59,  2.11it/s] 12%|█▏        | 115/1000 [00:58<07:00,  2.10it/s] 12%|█▏        | 116/1000 [00:59<07:01,  2.10it/s] 12%|█▏        | 117/1000 [00:59<07:01,  2.09it/s] 12%|█▏        | 118/1000 [01:00<07:02,  2.09it/s] 12%|█▏        | 119/1000 [01:00<07:01,  2.09it/s] 12%|█▏        | 120/1000 [01:00<06:34,  2.23it/s] 12%|█▏        | 121/1000 [01:01<06:42,  2.18it/s] 12%|█▏        | 122/1000 [01:01<06:47,  2.15it/s] 12%|█▏        | 123/1000 [01:02<06:51,  2.13it/s] 12%|█▏        | 124/1000 [01:02<06:54,  2.11it/s] 12%|█▎        | 125/1000 [01:03<06:55,  2.11it/s] 13%|█▎        | 126/1000 [01:03<06:56,  2.10it/s] 13%|█▎        | 127/1000 [01:04<06:57,  2.09it/s] 13%|█▎        | 128/1000 [01:04<06:57,  2.09it/s] 13%|█▎        | 129/1000 [01:05<06:57,  2.09it/s] 13%|█▎        | 130/1000 [01:05<06:30,  2.23it/s] 13%|█▎        | 131/1000 [01:06<06:38,  2.18it/s] 13%|█▎        | 132/1000 [01:06<06:43,  2.15it/s] 13%|█▎        | 133/1000 [01:07<06:46,  2.13it/s] 13%|█▎        | 134/1000 [01:07<06:49,  2.11it/s] 14%|█▎        | 135/1000 [01:08<06:50,  2.10it/s] 14%|█▎        | 136/1000 [01:08<06:51,  2.10it/s] 14%|█▎        | 137/1000 [01:08<06:52,  2.09it/s] 14%|█▍        | 138/1000 [01:09<06:52,  2.09it/s] 14%|█▍        | 139/1000 [01:09<06:52,  2.09it/s] 14%|█▍        | 140/1000 [01:10<06:25,  2.23it/s] 14%|█▍        | 141/1000 [01:10<06:33,  2.18it/s] 14%|█▍        | 142/1000 [01:11<06:38,  2.15it/s] 14%|█▍        | 143/1000 [01:11<06:42,  2.13it/s] 14%|█▍        | 144/1000 [01:12<06:44,  2.12it/s] 14%|█▍        | 145/1000 [01:12<06:46,  2.11it/s] 15%|█▍        | 146/1000 [01:13<06:47,  2.10it/s] 15%|█▍        | 147/1000 [01:13<06:47,  2.09it/s] 15%|█▍        | 148/1000 [01:14<06:48,  2.09it/s] 15%|█▍        | 149/1000 [01:14<06:48,  2.09it/s] 15%|█▌        | 150/1000 [01:15<06:21,  2.23it/s] 15%|█▌        | 151/1000 [01:15<06:29,  2.18it/s] 15%|█▌        | 152/1000 [01:15<06:34,  2.15it/s] 15%|█▌        | 153/1000 [01:16<06:37,  2.13it/s] 15%|█▌        | 154/1000 [01:16<06:40,  2.11it/s] 16%|█▌        | 155/1000 [01:17<06:41,  2.10it/s] 16%|█▌        | 156/1000 [01:17<06:42,  2.10it/s] 16%|█▌        | 157/1000 [01:18<06:42,  2.09it/s] 16%|█▌        | 158/1000 [01:18<06:42,  2.09it/s] 16%|█▌        | 159/1000 [01:19<06:42,  2.09it/s] 16%|█▌        | 160/1000 [01:19<06:17,  2.23it/s] 16%|█▌        | 161/1000 [01:20<06:25,  2.18it/s] 16%|█▌        | 162/1000 [01:20<06:30,  2.15it/s] 16%|█▋        | 163/1000 [01:21<06:33,  2.13it/s] 16%|█▋        | 164/1000 [01:21<06:35,  2.11it/s] 16%|█▋        | 165/1000 [01:22<06:37,  2.10it/s] 17%|█▋        | 166/1000 [01:22<06:38,  2.10it/s] 17%|█▋        | 167/1000 [01:23<06:38,  2.09it/s] 17%|█▋        | 168/1000 [01:23<06:38,  2.09it/s] 17%|█▋        | 169/1000 [01:24<06:38,  2.09it/s] 17%|█▋        | 170/1000 [01:24<06:12,  2.23it/s] 17%|█▋        | 171/1000 [01:24<06:20,  2.18it/s] 17%|█▋        | 172/1000 [01:25<06:24,  2.15it/s] 17%|█▋        | 173/1000 [01:25<06:28,  2.13it/s] 17%|█▋        | 174/1000 [01:26<06:31,  2.11it/s] 18%|█▊        | 175/1000 [01:26<06:32,  2.10it/s] 18%|█▊        | 176/1000 [01:27<06:33,  2.09it/s] 18%|█▊        | 177/1000 [01:27<06:34,  2.09it/s] 18%|█▊        | 178/1000 [01:28<06:54,  1.98it/s] 18%|█▊        | 179/1000 [01:28<06:48,  2.01it/s] 18%|█▊        | 180/1000 [01:29<06:18,  2.16it/s] 18%|█▊        | 181/1000 [01:29<06:23,  2.14it/s] 18%|█▊        | 182/1000 [01:30<06:26,  2.12it/s] 18%|█▊        | 183/1000 [01:30<06:28,  2.10it/s] 18%|█▊        | 184/1000 [01:31<06:29,  2.09it/s] 18%|█▊        | 185/1000 [01:31<06:30,  2.09it/s] 19%|█▊        | 186/1000 [01:32<06:30,  2.08it/s] 19%|█▊        | 187/1000 [01:32<06:30,  2.08it/s] 19%|█▉        | 188/1000 [01:33<06:30,  2.08it/s] 19%|█▉        | 189/1000 [01:33<06:30,  2.08it/s] 19%|█▉        | 190/1000 [01:33<06:04,  2.22it/s] 19%|█▉        | 191/1000 [01:34<06:11,  2.18it/s] 19%|█▉        | 192/1000 [01:34<06:16,  2.15it/s] 19%|█▉        | 193/1000 [01:35<06:19,  2.13it/s] 19%|█▉        | 194/1000 [01:35<06:21,  2.11it/s] 20%|█▉        | 195/1000 [01:36<06:22,  2.10it/s] 20%|█▉        | 196/1000 [01:36<06:23,  2.10it/s] 20%|█▉        | 197/1000 [01:37<06:24,  2.09it/s] 20%|█▉        | 198/1000 [01:37<06:24,  2.09it/s] 20%|█▉        | 199/1000 [01:38<06:24,  2.08it/s] 20%|██        | 200/1000 [01:38<05:59,  2.23it/s] 20%|██        | 201/1000 [01:39<06:06,  2.18it/s] 20%|██        | 202/1000 [01:39<06:11,  2.15it/s] 20%|██        | 203/1000 [01:40<06:14,  2.13it/s] 20%|██        | 204/1000 [01:40<06:16,  2.11it/s] 20%|██        | 205/1000 [01:41<06:17,  2.10it/s] 21%|██        | 206/1000 [01:41<06:18,  2.10it/s] 21%|██        | 207/1000 [01:42<06:18,  2.09it/s] 21%|██        | 208/1000 [01:42<06:18,  2.09it/s] 21%|██        | 209/1000 [01:42<06:18,  2.09it/s] 21%|██        | 210/1000 [01:43<05:54,  2.23it/s] 21%|██        | 211/1000 [01:43<06:01,  2.18it/s] 21%|██        | 212/1000 [01:44<06:06,  2.15it/s] 21%|██▏       | 213/1000 [01:44<06:09,  2.13it/s] 21%|██▏       | 214/1000 [01:45<06:11,  2.12it/s] 22%|██▏       | 215/1000 [01:45<06:12,  2.11it/s] 22%|██▏       | 216/1000 [01:46<06:13,  2.10it/s] 22%|██▏       | 217/1000 [01:46<06:14,  2.09it/s] 22%|██▏       | 218/1000 [01:47<06:14,  2.09it/s] 22%|██▏       | 219/1000 [01:47<06:14,  2.09it/s] 22%|██▏       | 220/1000 [01:48<05:49,  2.23it/s] 22%|██▏       | 221/1000 [01:48<05:56,  2.18it/s] 22%|██▏       | 222/1000 [01:49<06:01,  2.15it/s] 22%|██▏       | 223/1000 [01:49<06:04,  2.13it/s] 22%|██▏       | 224/1000 [01:49<06:06,  2.12it/s] 22%|██▎       | 225/1000 [01:50<06:07,  2.11it/s] 23%|██▎       | 226/1000 [01:50<06:08,  2.10it/s] 23%|██▎       | 227/1000 [01:51<06:08,  2.09it/s] 23%|██▎       | 228/1000 [01:51<06:09,  2.09it/s] 23%|██▎       | 229/1000 [01:52<06:09,  2.09it/s] 23%|██▎       | 230/1000 [01:52<05:45,  2.23it/s] 23%|██▎       | 231/1000 [01:53<05:52,  2.18it/s] 23%|██▎       | 232/1000 [01:53<05:56,  2.15it/s] 23%|██▎       | 233/1000 [01:54<05:59,  2.13it/s] 23%|██▎       | 234/1000 [01:54<06:01,  2.12it/s] 24%|██▎       | 235/1000 [01:55<06:03,  2.11it/s] 24%|██▎       | 236/1000 [01:55<06:03,  2.10it/s] 24%|██▎       | 237/1000 [01:56<06:04,  2.10it/s] 24%|██▍       | 238/1000 [01:56<06:04,  2.09it/s] 24%|██▍       | 239/1000 [01:57<06:04,  2.09it/s] 24%|██▍       | 240/1000 [01:57<05:40,  2.23it/s] 24%|██▍       | 241/1000 [01:57<05:47,  2.19it/s] 24%|██▍       | 242/1000 [01:58<05:51,  2.15it/s] 24%|██▍       | 243/1000 [01:58<05:55,  2.13it/s] 24%|██▍       | 244/1000 [01:59<05:57,  2.12it/s] 24%|██▍       | 245/1000 [01:59<05:58,  2.11it/s] 25%|██▍       | 246/1000 [02:00<05:59,  2.10it/s] 25%|██▍       | 247/1000 [02:00<05:59,  2.10it/s] 25%|██▍       | 248/1000 [02:01<05:59,  2.09it/s] 25%|██▍       | 249/1000 [02:01<05:59,  2.09it/s] 25%|██▌       | 250/1000 [02:02<05:52,  2.13it/s] 25%|██▌       | 251/1000 [02:02<05:54,  2.11it/s] 25%|██▌       | 252/1000 [02:03<05:56,  2.10it/s] 25%|██▌       | 253/1000 [02:03<05:56,  2.10it/s] 25%|██▌       | 254/1000 [02:04<05:56,  2.09it/s] 26%|██▌       | 255/1000 [02:04<05:57,  2.09it/s] 26%|██▌       | 256/1000 [02:05<05:57,  2.08it/s] 26%|██▌       | 257/1000 [02:05<05:56,  2.08it/s] 26%|██▌       | 258/1000 [02:06<05:56,  2.08it/s] 26%|██▌       | 259/1000 [02:06<05:55,  2.08it/s] 26%|██▌       | 260/1000 [02:06<05:32,  2.22it/s] 26%|██▌       | 261/1000 [02:07<05:39,  2.18it/s] 26%|██▌       | 262/1000 [02:07<05:43,  2.15it/s] 26%|██▋       | 263/1000 [02:08<05:46,  2.13it/s] 26%|██▋       | 264/1000 [02:08<05:48,  2.11it/s] 26%|██▋       | 265/1000 [02:09<05:49,  2.10it/s] 27%|██▋       | 266/1000 [02:09<05:49,  2.10it/s] 27%|██▋       | 267/1000 [02:10<05:50,  2.09it/s] 27%|██▋       | 268/1000 [02:10<05:50,  2.09it/s] 27%|██▋       | 269/1000 [02:11<05:49,  2.09it/s] 27%|██▋       | 270/1000 [02:11<05:27,  2.23it/s] 27%|██▋       | 271/1000 [02:12<05:33,  2.18it/s] 27%|██▋       | 272/1000 [02:12<05:38,  2.15it/s] 27%|██▋       | 273/1000 [02:13<05:40,  2.13it/s] 27%|██▋       | 274/1000 [02:13<05:42,  2.12it/s] 28%|██▊       | 275/1000 [02:14<05:44,  2.11it/s] 28%|██▊       | 276/1000 [02:14<05:44,  2.10it/s] 28%|██▊       | 277/1000 [02:14<05:45,  2.09it/s] 28%|██▊       | 278/1000 [02:15<05:45,  2.09it/s] 28%|██▊       | 279/1000 [02:15<05:45,  2.09it/s] 28%|██▊       | 280/1000 [02:16<05:22,  2.23it/s] 28%|██▊       | 281/1000 [02:16<05:29,  2.18it/s] 28%|██▊       | 282/1000 [02:17<05:33,  2.15it/s] 28%|██▊       | 283/1000 [02:17<05:36,  2.13it/s] 28%|██▊       | 284/1000 [02:18<05:38,  2.12it/s] 28%|██▊       | 285/1000 [02:18<05:39,  2.11it/s] 29%|██▊       | 286/1000 [02:19<05:40,  2.10it/s] 29%|██▊       | 287/1000 [02:19<05:40,  2.09it/s] 29%|██▉       | 288/1000 [02:20<05:40,  2.09it/s] 29%|██▉       | 289/1000 [02:20<05:40,  2.09it/s] 29%|██▉       | 290/1000 [02:21<05:18,  2.23it/s] 29%|██▉       | 291/1000 [02:21<05:24,  2.18it/s] 29%|██▉       | 292/1000 [02:21<05:28,  2.15it/s] 29%|██▉       | 293/1000 [02:22<05:31,  2.13it/s] 29%|██▉       | 294/1000 [02:22<05:33,  2.12it/s] 30%|██▉       | 295/1000 [02:23<05:35,  2.10it/s] 30%|██▉       | 296/1000 [02:23<05:35,  2.10it/s] 30%|██▉       | 297/1000 [02:24<05:35,  2.09it/s] 30%|██▉       | 298/1000 [02:24<05:35,  2.09it/s] 30%|██▉       | 299/1000 [02:25<05:35,  2.09it/s] 30%|███       | 300/1000 [02:25<05:13,  2.23it/s] 30%|███       | 301/1000 [02:26<05:20,  2.18it/s] 30%|███       | 302/1000 [02:26<05:24,  2.15it/s] 30%|███       | 303/1000 [02:27<05:27,  2.13it/s] 30%|███       | 304/1000 [02:27<05:29,  2.11it/s] 30%|███       | 305/1000 [02:28<05:30,  2.10it/s] 31%|███       | 306/1000 [02:28<05:30,  2.10it/s] 31%|███       | 307/1000 [02:29<05:31,  2.09it/s] 31%|███       | 308/1000 [02:29<05:31,  2.09it/s] 31%|███       | 309/1000 [02:30<05:31,  2.09it/s] 31%|███       | 310/1000 [02:30<05:09,  2.23it/s] 31%|███       | 311/1000 [02:30<05:16,  2.18it/s] 31%|███       | 312/1000 [02:31<05:20,  2.14it/s] 31%|███▏      | 313/1000 [02:31<05:23,  2.12it/s] 31%|███▏      | 314/1000 [02:32<05:25,  2.11it/s] 32%|███▏      | 315/1000 [02:32<05:26,  2.10it/s] 32%|███▏      | 316/1000 [02:33<05:27,  2.09it/s] 32%|███▏      | 317/1000 [02:33<05:28,  2.08it/s] 32%|███▏      | 318/1000 [02:34<05:28,  2.08it/s] 32%|███▏      | 319/1000 [02:34<05:28,  2.08it/s] 32%|███▏      | 320/1000 [02:35<05:06,  2.22it/s] 32%|███▏      | 321/1000 [02:35<05:12,  2.17it/s] 32%|███▏      | 322/1000 [02:36<05:34,  2.03it/s] 32%|███▏      | 323/1000 [02:36<05:31,  2.04it/s] 32%|███▏      | 324/1000 [02:37<05:29,  2.05it/s] 32%|███▎      | 325/1000 [02:37<05:28,  2.06it/s] 33%|███▎      | 326/1000 [02:38<05:26,  2.06it/s] 33%|███▎      | 327/1000 [02:38<05:25,  2.07it/s] 33%|███▎      | 328/1000 [02:39<05:24,  2.07it/s] 33%|███▎      | 329/1000 [02:39<05:23,  2.08it/s] 33%|███▎      | 330/1000 [02:39<05:01,  2.22it/s] 33%|███▎      | 331/1000 [02:40<05:07,  2.18it/s] 33%|███▎      | 332/1000 [02:40<05:11,  2.15it/s] 33%|███▎      | 333/1000 [02:41<05:13,  2.13it/s] 33%|███▎      | 334/1000 [02:41<05:14,  2.11it/s] 34%|███▎      | 335/1000 [02:42<05:16,  2.10it/s] 34%|███▎      | 336/1000 [02:42<05:16,  2.10it/s] 34%|███▎      | 337/1000 [02:43<05:16,  2.09it/s] 34%|███▍      | 338/1000 [02:43<05:16,  2.09it/s] 34%|███▍      | 339/1000 [02:44<05:16,  2.09it/s] 34%|███▍      | 340/1000 [02:44<04:56,  2.23it/s] 34%|███▍      | 341/1000 [02:45<05:01,  2.18it/s] 34%|███▍      | 342/1000 [02:45<05:05,  2.15it/s] 34%|███▍      | 343/1000 [02:46<05:08,  2.13it/s] 34%|███▍      | 344/1000 [02:46<05:10,  2.11it/s] 34%|███▍      | 345/1000 [02:47<05:11,  2.11it/s] 35%|███▍      | 346/1000 [02:47<05:11,  2.10it/s] 35%|███▍      | 347/1000 [02:48<05:11,  2.09it/s] 35%|███▍      | 348/1000 [02:48<05:11,  2.09it/s] 35%|███▍      | 349/1000 [02:48<05:11,  2.09it/s] 35%|███▌      | 350/1000 [02:49<04:51,  2.23it/s] 35%|███▌      | 351/1000 [02:49<04:57,  2.18it/s] 35%|███▌      | 352/1000 [02:50<05:00,  2.15it/s] 35%|███▌      | 353/1000 [02:50<05:03,  2.13it/s] 35%|███▌      | 354/1000 [02:51<05:05,  2.12it/s] 36%|███▌      | 355/1000 [02:51<05:06,  2.11it/s] 36%|███▌      | 356/1000 [02:52<05:06,  2.10it/s] 36%|███▌      | 357/1000 [02:52<05:06,  2.10it/s] 36%|███▌      | 358/1000 [02:53<05:07,  2.09it/s] 36%|███▌      | 359/1000 [02:53<05:06,  2.09it/s] 36%|███▌      | 360/1000 [02:54<04:46,  2.23it/s] 36%|███▌      | 361/1000 [02:54<04:52,  2.18it/s] 36%|███▌      | 362/1000 [02:54<04:56,  2.15it/s] 36%|███▋      | 363/1000 [02:55<04:58,  2.13it/s] 36%|███▋      | 364/1000 [02:55<05:00,  2.12it/s] 36%|███▋      | 365/1000 [02:56<05:01,  2.11it/s] 37%|███▋      | 366/1000 [02:56<05:01,  2.10it/s] 37%|███▋      | 367/1000 [02:57<05:02,  2.09it/s] 37%|███▋      | 368/1000 [02:57<05:02,  2.09it/s] 37%|███▋      | 369/1000 [02:58<05:01,  2.09it/s] 37%|███▋      | 370/1000 [02:58<04:42,  2.23it/s] 37%|███▋      | 371/1000 [02:59<04:47,  2.19it/s] 37%|███▋      | 372/1000 [02:59<04:51,  2.15it/s] 37%|███▋      | 373/1000 [03:00<04:54,  2.13it/s] 37%|███▋      | 374/1000 [03:00<04:55,  2.12it/s] 38%|███▊      | 375/1000 [03:01<04:56,  2.11it/s] 38%|███▊      | 376/1000 [03:01<04:57,  2.10it/s] 38%|███▊      | 377/1000 [03:02<04:57,  2.10it/s] 38%|███▊      | 378/1000 [03:02<04:57,  2.09it/s] 38%|███▊      | 379/1000 [03:03<04:57,  2.08it/s] 38%|███▊      | 380/1000 [03:03<04:38,  2.23it/s] 38%|███▊      | 381/1000 [03:03<04:43,  2.18it/s] 38%|███▊      | 382/1000 [03:04<04:47,  2.15it/s] 38%|███▊      | 383/1000 [03:04<04:50,  2.13it/s] 38%|███▊      | 384/1000 [03:05<04:51,  2.11it/s] 38%|███▊      | 385/1000 [03:05<04:53,  2.10it/s] 39%|███▊      | 386/1000 [03:06<04:53,  2.09it/s] 39%|███▊      | 387/1000 [03:06<04:53,  2.09it/s] 39%|███▉      | 388/1000 [03:07<04:53,  2.09it/s] 39%|███▉      | 389/1000 [03:07<04:53,  2.08it/s] 39%|███▉      | 390/1000 [03:08<04:33,  2.23it/s] 39%|███▉      | 391/1000 [03:08<04:39,  2.18it/s] 39%|███▉      | 392/1000 [03:09<04:42,  2.15it/s] 39%|███▉      | 393/1000 [03:09<04:45,  2.13it/s] 39%|███▉      | 394/1000 [03:10<04:57,  2.04it/s] 40%|███▉      | 395/1000 [03:10<04:54,  2.05it/s] 40%|███▉      | 396/1000 [03:11<04:53,  2.06it/s] 40%|███▉      | 397/1000 [03:11<04:52,  2.06it/s] 40%|███▉      | 398/1000 [03:12<04:50,  2.07it/s] 40%|███▉      | 399/1000 [03:12<04:50,  2.07it/s] 40%|████      | 400/1000 [03:12<04:30,  2.22it/s] 40%|████      | 401/1000 [03:13<04:35,  2.17it/s] 40%|████      | 402/1000 [03:13<04:38,  2.15it/s] 40%|████      | 403/1000 [03:14<04:40,  2.13it/s] 40%|████      | 404/1000 [03:14<04:42,  2.11it/s] 40%|████      | 405/1000 [03:15<04:42,  2.10it/s] 41%|████      | 406/1000 [03:15<04:43,  2.10it/s] 41%|████      | 407/1000 [03:16<04:43,  2.09it/s] 41%|████      | 408/1000 [03:16<04:43,  2.09it/s] 41%|████      | 409/1000 [03:17<04:43,  2.09it/s] 41%|████      | 410/1000 [03:17<04:24,  2.23it/s] 41%|████      | 411/1000 [03:18<04:29,  2.18it/s] 41%|████      | 412/1000 [03:18<04:33,  2.15it/s] 41%|████▏     | 413/1000 [03:19<04:35,  2.13it/s] 41%|████▏     | 414/1000 [03:19<04:36,  2.12it/s] 42%|████▏     | 415/1000 [03:20<04:37,  2.11it/s] 42%|████▏     | 416/1000 [03:20<04:38,  2.10it/s] 42%|████▏     | 417/1000 [03:20<04:38,  2.09it/s] 42%|████▏     | 418/1000 [03:21<04:38,  2.09it/s] 42%|████▏     | 419/1000 [03:21<04:38,  2.09it/s] 42%|████▏     | 420/1000 [03:22<04:20,  2.23it/s] 42%|████▏     | 421/1000 [03:22<04:25,  2.18it/s] 42%|████▏     | 422/1000 [03:23<04:28,  2.15it/s] 42%|████▏     | 423/1000 [03:23<04:30,  2.13it/s] 42%|████▏     | 424/1000 [03:24<04:32,  2.12it/s] 42%|████▎     | 425/1000 [03:24<04:33,  2.10it/s] 43%|████▎     | 426/1000 [03:25<04:34,  2.09it/s] 43%|████▎     | 427/1000 [03:25<04:34,  2.09it/s] 43%|████▎     | 428/1000 [03:26<04:34,  2.08it/s] 43%|████▎     | 429/1000 [03:26<04:34,  2.08it/s] 43%|████▎     | 430/1000 [03:27<04:16,  2.22it/s] 43%|████▎     | 431/1000 [03:27<04:21,  2.17it/s] 43%|████▎     | 432/1000 [03:27<04:25,  2.14it/s] 43%|████▎     | 433/1000 [03:28<04:27,  2.12it/s] 43%|████▎     | 434/1000 [03:28<04:28,  2.10it/s] 44%|████▎     | 435/1000 [03:29<04:29,  2.09it/s] 44%|████▎     | 436/1000 [03:29<04:30,  2.09it/s] 44%|████▎     | 437/1000 [03:30<04:30,  2.08it/s] 44%|████▍     | 438/1000 [03:30<04:30,  2.08it/s] 44%|████▍     | 439/1000 [03:31<04:29,  2.08it/s] 44%|████▍     | 440/1000 [03:31<04:12,  2.22it/s] 44%|████▍     | 441/1000 [03:32<04:17,  2.17it/s] 44%|████▍     | 442/1000 [03:32<04:20,  2.14it/s] 44%|████▍     | 443/1000 [03:33<04:22,  2.12it/s] 44%|████▍     | 444/1000 [03:33<04:23,  2.11it/s] 44%|████▍     | 445/1000 [03:34<04:23,  2.10it/s] 45%|████▍     | 446/1000 [03:34<04:24,  2.10it/s] 45%|████▍     | 447/1000 [03:35<04:24,  2.09it/s] 45%|████▍     | 448/1000 [03:35<04:24,  2.09it/s] 45%|████▍     | 449/1000 [03:36<04:23,  2.09it/s] 45%|████▌     | 450/1000 [03:36<04:06,  2.23it/s] 45%|████▌     | 451/1000 [03:36<04:11,  2.18it/s] 45%|████▌     | 452/1000 [03:37<04:14,  2.15it/s] 45%|████▌     | 453/1000 [03:37<04:17,  2.13it/s] 45%|████▌     | 454/1000 [03:38<04:18,  2.11it/s] 46%|████▌     | 455/1000 [03:38<04:19,  2.10it/s] 46%|████▌     | 456/1000 [03:39<04:19,  2.10it/s] 46%|████▌     | 457/1000 [03:39<04:19,  2.09it/s] 46%|████▌     | 458/1000 [03:40<04:19,  2.09it/s] 46%|████▌     | 459/1000 [03:40<04:19,  2.09it/s] 46%|████▌     | 460/1000 [03:41<04:02,  2.23it/s] 46%|████▌     | 461/1000 [03:41<04:06,  2.18it/s] 46%|████▌     | 462/1000 [03:42<04:09,  2.15it/s] 46%|████▋     | 463/1000 [03:42<04:12,  2.13it/s] 46%|████▋     | 464/1000 [03:43<04:13,  2.11it/s] 46%|████▋     | 465/1000 [03:43<04:24,  2.02it/s] 47%|████▋     | 466/1000 [03:44<04:21,  2.04it/s] 47%|████▋     | 467/1000 [03:44<04:19,  2.05it/s] 47%|████▋     | 468/1000 [03:45<04:18,  2.06it/s] 47%|████▋     | 469/1000 [03:45<04:16,  2.07it/s] 47%|████▋     | 470/1000 [03:45<03:59,  2.21it/s] 47%|████▋     | 471/1000 [03:46<04:03,  2.17it/s] 47%|████▋     | 472/1000 [03:46<04:06,  2.14it/s] 47%|████▋     | 473/1000 [03:47<04:07,  2.13it/s] 47%|████▋     | 474/1000 [03:47<04:09,  2.11it/s] 48%|████▊     | 475/1000 [03:48<04:09,  2.10it/s] 48%|████▊     | 476/1000 [03:48<04:09,  2.10it/s] 48%|████▊     | 477/1000 [03:49<04:10,  2.09it/s] 48%|████▊     | 478/1000 [03:49<04:09,  2.09it/s] 48%|████▊     | 479/1000 [03:50<04:09,  2.09it/s] 48%|████▊     | 480/1000 [03:50<03:53,  2.23it/s] 48%|████▊     | 481/1000 [03:51<03:57,  2.18it/s] 48%|████▊     | 482/1000 [03:51<04:00,  2.15it/s] 48%|████▊     | 483/1000 [03:52<04:02,  2.13it/s] 48%|████▊     | 484/1000 [03:52<04:03,  2.12it/s] 48%|████▊     | 485/1000 [03:53<04:04,  2.11it/s] 49%|████▊     | 486/1000 [03:53<04:04,  2.10it/s] 49%|████▊     | 487/1000 [03:53<04:04,  2.09it/s] 49%|████▉     | 488/1000 [03:54<04:04,  2.09it/s] 49%|████▉     | 489/1000 [03:54<04:04,  2.09it/s] 49%|████▉     | 490/1000 [03:55<03:48,  2.23it/s] 49%|████▉     | 491/1000 [03:55<03:53,  2.18it/s] 49%|████▉     | 492/1000 [03:56<03:56,  2.15it/s] 49%|████▉     | 493/1000 [03:56<03:58,  2.13it/s] 49%|████▉     | 494/1000 [03:57<03:59,  2.11it/s] 50%|████▉     | 495/1000 [03:57<03:59,  2.10it/s] 50%|████▉     | 496/1000 [03:58<04:00,  2.10it/s] 50%|████▉     | 497/1000 [03:58<04:00,  2.09it/s] 50%|████▉     | 498/1000 [03:59<04:00,  2.09it/s] 50%|████▉     | 499/1000 [03:59<03:59,  2.09it/s] 50%|█████     | 500/1000 [04:00<03:44,  2.23it/s]                                                   50%|█████     | 500/1000 [04:00<03:44,  2.23it/s][INFO|trainer.py:2409] 2022-08-27 21:56:57,877 >> Saving model checkpoint to tmp/baseline/RTE/bert-base-uncased/HPO_S0/1e-06/256/100/checkpoint-500
[INFO|configuration_utils.py:446] 2022-08-27 21:56:57,878 >> Configuration saved in tmp/baseline/RTE/bert-base-uncased/HPO_S0/1e-06/256/100/checkpoint-500/config.json
[INFO|modeling_utils.py:1602] 2022-08-27 21:56:58,734 >> Model weights saved in tmp/baseline/RTE/bert-base-uncased/HPO_S0/1e-06/256/100/checkpoint-500/pytorch_model.bin
[INFO|tokenization_utils_base.py:2108] 2022-08-27 21:56:58,735 >> tokenizer config file saved in tmp/baseline/RTE/bert-base-uncased/HPO_S0/1e-06/256/100/checkpoint-500/tokenizer_config.json
[INFO|tokenization_utils_base.py:2114] 2022-08-27 21:56:58,735 >> Special tokens file saved in tmp/baseline/RTE/bert-base-uncased/HPO_S0/1e-06/256/100/checkpoint-500/special_tokens_map.json
/home/ubuntu/anaconda3/envs/transformers/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 50%|█████     | 501/1000 [04:03<10:15,  1.23s/it] 50%|█████     | 502/1000 [04:03<08:21,  1.01s/it] 50%|█████     | 503/1000 [04:04<07:02,  1.18it/s] 50%|█████     | 504/1000 [04:04<06:06,  1.35it/s] 50%|█████     | 505/1000 [04:05<05:27,  1.51it/s] 51%|█████     | 506/1000 [04:05<05:00,  1.65it/s] 51%|█████     | 507/1000 [04:05<04:40,  1.76it/s] 51%|█████     | 508/1000 [04:06<04:27,  1.84it/s] 51%|█████     | 509/1000 [04:06<04:17,  1.91it/s] 51%|█████     | 510/1000 [04:07<03:55,  2.08it/s] 51%|█████     | 511/1000 [04:07<03:54,  2.08it/s] 51%|█████     | 512/1000 [04:08<03:54,  2.08it/s] 51%|█████▏    | 513/1000 [04:08<03:54,  2.08it/s] 51%|█████▏    | 514/1000 [04:09<03:53,  2.08it/s] 52%|█████▏    | 515/1000 [04:09<03:53,  2.08it/s] 52%|█████▏    | 516/1000 [04:10<03:52,  2.08it/s] 52%|█████▏    | 517/1000 [04:10<03:52,  2.08it/s] 52%|█████▏    | 518/1000 [04:11<03:51,  2.08it/s] 52%|█████▏    | 519/1000 [04:11<03:51,  2.08it/s] 52%|█████▏    | 520/1000 [04:12<03:35,  2.23it/s] 52%|█████▏    | 521/1000 [04:12<03:39,  2.18it/s] 52%|█████▏    | 522/1000 [04:12<03:42,  2.15it/s] 52%|█████▏    | 523/1000 [04:13<03:44,  2.13it/s] 52%|█████▏    | 524/1000 [04:13<03:45,  2.11it/s] 52%|█████▎    | 525/1000 [04:14<03:45,  2.10it/s] 53%|█████▎    | 526/1000 [04:14<03:45,  2.10it/s] 53%|█████▎    | 527/1000 [04:15<03:45,  2.09it/s] 53%|█████▎    | 528/1000 [04:15<03:45,  2.09it/s] 53%|█████▎    | 529/1000 [04:16<03:45,  2.09it/s] 53%|█████▎    | 530/1000 [04:16<03:30,  2.23it/s] 53%|█████▎    | 531/1000 [04:17<03:34,  2.18it/s] 53%|█████▎    | 532/1000 [04:17<03:37,  2.15it/s] 53%|█████▎    | 533/1000 [04:18<03:39,  2.13it/s] 53%|█████▎    | 534/1000 [04:18<03:40,  2.12it/s] 54%|█████▎    | 535/1000 [04:19<03:40,  2.11it/s] 54%|█████▎    | 536/1000 [04:19<03:41,  2.10it/s] 54%|█████▎    | 537/1000 [04:20<03:41,  2.09it/s] 54%|█████▍    | 538/1000 [04:20<03:40,  2.09it/s] 54%|█████▍    | 539/1000 [04:21<03:51,  1.99it/s] 54%|█████▍    | 540/1000 [04:21<03:33,  2.15it/s] 54%|█████▍    | 541/1000 [04:21<03:35,  2.13it/s] 54%|█████▍    | 542/1000 [04:22<03:36,  2.12it/s] 54%|█████▍    | 543/1000 [04:22<03:37,  2.11it/s] 54%|█████▍    | 544/1000 [04:23<03:37,  2.10it/s] 55%|█████▍    | 545/1000 [04:23<03:37,  2.10it/s] 55%|█████▍    | 546/1000 [04:24<03:37,  2.09it/s] 55%|█████▍    | 547/1000 [04:24<03:36,  2.09it/s] 55%|█████▍    | 548/1000 [04:25<03:36,  2.09it/s] 55%|█████▍    | 549/1000 [04:25<03:36,  2.09it/s] 55%|█████▌    | 550/1000 [04:26<03:21,  2.23it/s] 55%|█████▌    | 551/1000 [04:26<03:25,  2.18it/s] 55%|█████▌    | 552/1000 [04:27<03:28,  2.15it/s] 55%|█████▌    | 553/1000 [04:27<03:29,  2.13it/s] 55%|█████▌    | 554/1000 [04:28<03:30,  2.12it/s] 56%|█████▌    | 555/1000 [04:28<03:31,  2.11it/s] 56%|█████▌    | 556/1000 [04:29<03:31,  2.10it/s] 56%|█████▌    | 557/1000 [04:29<03:31,  2.10it/s] 56%|█████▌    | 558/1000 [04:30<03:31,  2.09it/s] 56%|█████▌    | 559/1000 [04:30<03:31,  2.09it/s] 56%|█████▌    | 560/1000 [04:30<03:17,  2.23it/s] 56%|█████▌    | 561/1000 [04:31<03:21,  2.18it/s] 56%|█████▌    | 562/1000 [04:31<03:23,  2.15it/s] 56%|█████▋    | 563/1000 [04:32<03:25,  2.13it/s] 56%|█████▋    | 564/1000 [04:32<03:26,  2.12it/s] 56%|█████▋    | 565/1000 [04:33<03:26,  2.11it/s] 57%|█████▋    | 566/1000 [04:33<03:26,  2.10it/s] 57%|█████▋    | 567/1000 [04:34<03:26,  2.10it/s] 57%|█████▋    | 568/1000 [04:34<03:26,  2.09it/s] 57%|█████▋    | 569/1000 [04:35<03:26,  2.09it/s] 57%|█████▋    | 570/1000 [04:35<03:12,  2.23it/s] 57%|█████▋    | 571/1000 [04:36<03:16,  2.19it/s] 57%|█████▋    | 572/1000 [04:36<03:18,  2.15it/s] 57%|█████▋    | 573/1000 [04:37<03:20,  2.13it/s] 57%|█████▋    | 574/1000 [04:37<03:21,  2.12it/s] 57%|█████▊    | 575/1000 [04:37<03:21,  2.11it/s] 58%|█████▊    | 576/1000 [04:38<03:21,  2.10it/s] 58%|█████▊    | 577/1000 [04:38<03:21,  2.09it/s] 58%|█████▊    | 578/1000 [04:39<03:21,  2.09it/s] 58%|█████▊    | 579/1000 [04:39<03:21,  2.09it/s] 58%|█████▊    | 580/1000 [04:40<03:08,  2.23it/s] 58%|█████▊    | 581/1000 [04:40<03:11,  2.18it/s] 58%|█████▊    | 582/1000 [04:41<03:14,  2.15it/s] 58%|█████▊    | 583/1000 [04:41<03:15,  2.13it/s] 58%|█████▊    | 584/1000 [04:42<03:16,  2.12it/s] 58%|█████▊    | 585/1000 [04:42<03:16,  2.11it/s] 59%|█████▊    | 586/1000 [04:43<03:17,  2.10it/s] 59%|█████▊    | 587/1000 [04:43<03:17,  2.09it/s] 59%|█████▉    | 588/1000 [04:44<03:16,  2.09it/s] 59%|█████▉    | 589/1000 [04:44<03:16,  2.09it/s] 59%|█████▉    | 590/1000 [04:44<03:03,  2.23it/s] 59%|█████▉    | 591/1000 [04:45<03:07,  2.19it/s] 59%|█████▉    | 592/1000 [04:45<03:09,  2.16it/s] 59%|█████▉    | 593/1000 [04:46<03:10,  2.13it/s] 59%|█████▉    | 594/1000 [04:46<03:11,  2.12it/s] 60%|█████▉    | 595/1000 [04:47<03:12,  2.11it/s] 60%|█████▉    | 596/1000 [04:47<03:12,  2.10it/s] 60%|█████▉    | 597/1000 [04:48<03:12,  2.10it/s] 60%|█████▉    | 598/1000 [04:48<03:12,  2.09it/s] 60%|█████▉    | 599/1000 [04:49<03:11,  2.09it/s] 60%|██████    | 600/1000 [04:49<02:59,  2.23it/s] 60%|██████    | 601/1000 [04:50<03:02,  2.19it/s] 60%|██████    | 602/1000 [04:50<03:04,  2.16it/s] 60%|██████    | 603/1000 [04:51<03:06,  2.13it/s] 60%|██████    | 604/1000 [04:51<03:07,  2.12it/s] 60%|██████    | 605/1000 [04:52<03:07,  2.11it/s] 61%|██████    | 606/1000 [04:52<03:07,  2.10it/s] 61%|██████    | 607/1000 [04:53<03:07,  2.09it/s] 61%|██████    | 608/1000 [04:53<03:07,  2.09it/s] 61%|██████    | 609/1000 [04:53<03:07,  2.09it/s] 61%|██████    | 610/1000 [04:54<02:59,  2.17it/s] 61%|██████    | 611/1000 [04:54<03:01,  2.14it/s] 61%|██████    | 612/1000 [04:55<03:02,  2.12it/s] 61%|██████▏   | 613/1000 [04:55<03:03,  2.11it/s] 61%|██████▏   | 614/1000 [04:56<03:03,  2.10it/s] 62%|██████▏   | 615/1000 [04:56<03:03,  2.10it/s] 62%|██████▏   | 616/1000 [04:57<03:03,  2.09it/s] 62%|██████▏   | 617/1000 [04:57<03:03,  2.09it/s] 62%|██████▏   | 618/1000 [04:58<03:02,  2.09it/s] 62%|██████▏   | 619/1000 [04:58<03:02,  2.09it/s] 62%|██████▏   | 620/1000 [04:59<02:50,  2.23it/s] 62%|██████▏   | 621/1000 [04:59<02:53,  2.18it/s] 62%|██████▏   | 622/1000 [05:00<02:55,  2.15it/s] 62%|██████▏   | 623/1000 [05:00<02:57,  2.13it/s] 62%|██████▏   | 624/1000 [05:01<02:57,  2.12it/s] 62%|██████▎   | 625/1000 [05:01<02:58,  2.11it/s] 63%|██████▎   | 626/1000 [05:01<02:58,  2.10it/s] 63%|██████▎   | 627/1000 [05:02<02:58,  2.09it/s] 63%|██████▎   | 628/1000 [05:02<02:58,  2.09it/s] 63%|██████▎   | 629/1000 [05:03<02:58,  2.08it/s] 63%|██████▎   | 630/1000 [05:03<02:46,  2.22it/s] 63%|██████▎   | 631/1000 [05:04<02:49,  2.18it/s] 63%|██████▎   | 632/1000 [05:04<02:51,  2.15it/s] 63%|██████▎   | 633/1000 [05:05<02:52,  2.13it/s] 63%|██████▎   | 634/1000 [05:05<02:53,  2.11it/s] 64%|██████▎   | 635/1000 [05:06<02:53,  2.10it/s] 64%|██████▎   | 636/1000 [05:06<02:53,  2.10it/s] 64%|██████▎   | 637/1000 [05:07<02:53,  2.09it/s] 64%|██████▍   | 638/1000 [05:07<02:53,  2.09it/s] 64%|██████▍   | 639/1000 [05:08<02:52,  2.09it/s] 64%|██████▍   | 640/1000 [05:08<02:41,  2.23it/s] 64%|██████▍   | 641/1000 [05:08<02:45,  2.17it/s] 64%|██████▍   | 642/1000 [05:09<02:47,  2.14it/s] 64%|██████▍   | 643/1000 [05:09<02:48,  2.12it/s] 64%|██████▍   | 644/1000 [05:10<02:49,  2.10it/s] 64%|██████▍   | 645/1000 [05:10<02:49,  2.09it/s] 65%|██████▍   | 646/1000 [05:11<02:49,  2.08it/s] 65%|██████▍   | 647/1000 [05:11<02:50,  2.08it/s] 65%|██████▍   | 648/1000 [05:12<02:50,  2.07it/s] 65%|██████▍   | 649/1000 [05:12<02:50,  2.06it/s] 65%|██████▌   | 650/1000 [05:13<02:38,  2.21it/s] 65%|██████▌   | 651/1000 [05:13<02:41,  2.15it/s] 65%|██████▌   | 652/1000 [05:14<02:43,  2.12it/s] 65%|██████▌   | 653/1000 [05:14<02:44,  2.10it/s] 65%|██████▌   | 654/1000 [05:15<02:45,  2.09it/s] 66%|██████▌   | 655/1000 [05:15<02:46,  2.08it/s] 66%|██████▌   | 656/1000 [05:16<02:46,  2.07it/s] 66%|██████▌   | 657/1000 [05:16<02:45,  2.07it/s] 66%|██████▌   | 658/1000 [05:17<02:45,  2.07it/s] 66%|██████▌   | 659/1000 [05:17<02:45,  2.06it/s] 66%|██████▌   | 660/1000 [05:18<02:34,  2.20it/s] 66%|██████▌   | 661/1000 [05:18<02:37,  2.16it/s] 66%|██████▌   | 662/1000 [05:18<02:39,  2.12it/s] 66%|██████▋   | 663/1000 [05:19<02:40,  2.10it/s] 66%|██████▋   | 664/1000 [05:19<02:41,  2.08it/s] 66%|██████▋   | 665/1000 [05:20<02:41,  2.07it/s] 67%|██████▋   | 666/1000 [05:20<02:41,  2.07it/s] 67%|██████▋   | 667/1000 [05:21<02:40,  2.07it/s] 67%|██████▋   | 668/1000 [05:21<02:40,  2.07it/s] 67%|██████▋   | 669/1000 [05:22<02:40,  2.06it/s] 67%|██████▋   | 670/1000 [05:22<02:30,  2.20it/s] 67%|██████▋   | 671/1000 [05:23<02:32,  2.15it/s] 67%|██████▋   | 672/1000 [05:23<02:34,  2.12it/s] 67%|██████▋   | 673/1000 [05:24<02:35,  2.10it/s] 67%|██████▋   | 674/1000 [05:24<02:36,  2.08it/s] 68%|██████▊   | 675/1000 [05:25<02:36,  2.07it/s] 68%|██████▊   | 676/1000 [05:25<02:36,  2.06it/s] 68%|██████▊   | 677/1000 [05:26<02:36,  2.06it/s] 68%|██████▊   | 678/1000 [05:26<02:36,  2.05it/s] 68%|██████▊   | 679/1000 [05:27<02:36,  2.05it/s] 68%|██████▊   | 680/1000 [05:27<02:25,  2.20it/s] 68%|██████▊   | 681/1000 [05:28<02:28,  2.15it/s] 68%|██████▊   | 682/1000 [05:28<02:37,  2.02it/s] 68%|██████▊   | 683/1000 [05:29<02:37,  2.02it/s] 68%|██████▊   | 684/1000 [05:29<02:35,  2.03it/s] 68%|██████▊   | 685/1000 [05:30<02:34,  2.04it/s] 69%|██████▊   | 686/1000 [05:30<02:33,  2.04it/s] 69%|██████▊   | 687/1000 [05:31<02:33,  2.04it/s] 69%|██████▉   | 688/1000 [05:31<02:32,  2.05it/s] 69%|██████▉   | 689/1000 [05:32<02:31,  2.05it/s] 69%|██████▉   | 690/1000 [05:32<02:21,  2.19it/s] 69%|██████▉   | 691/1000 [05:32<02:24,  2.15it/s] 69%|██████▉   | 692/1000 [05:33<02:25,  2.11it/s] 69%|██████▉   | 693/1000 [05:33<02:26,  2.10it/s] 69%|██████▉   | 694/1000 [05:34<02:26,  2.09it/s] 70%|██████▉   | 695/1000 [05:34<02:26,  2.08it/s] 70%|██████▉   | 696/1000 [05:35<02:26,  2.07it/s] 70%|██████▉   | 697/1000 [05:35<02:26,  2.07it/s] 70%|██████▉   | 698/1000 [05:36<02:25,  2.07it/s] 70%|██████▉   | 699/1000 [05:36<02:25,  2.07it/s] 70%|███████   | 700/1000 [05:37<02:15,  2.21it/s] 70%|███████   | 701/1000 [05:37<02:18,  2.16it/s] 70%|███████   | 702/1000 [05:38<02:19,  2.13it/s] 70%|███████   | 703/1000 [05:38<02:20,  2.11it/s] 70%|███████   | 704/1000 [05:39<02:21,  2.09it/s] 70%|███████   | 705/1000 [05:39<02:21,  2.08it/s] 71%|███████   | 706/1000 [05:40<02:22,  2.07it/s] 71%|███████   | 707/1000 [05:40<02:21,  2.06it/s] 71%|███████   | 708/1000 [05:41<02:21,  2.06it/s] 71%|███████   | 709/1000 [05:41<02:21,  2.06it/s] 71%|███████   | 710/1000 [05:41<02:11,  2.20it/s] 71%|███████   | 711/1000 [05:42<02:14,  2.15it/s] 71%|███████   | 712/1000 [05:42<02:15,  2.13it/s] 71%|███████▏  | 713/1000 [05:43<02:16,  2.11it/s] 71%|███████▏  | 714/1000 [05:43<02:16,  2.09it/s] 72%|███████▏  | 715/1000 [05:44<02:16,  2.08it/s] 72%|███████▏  | 716/1000 [05:44<02:16,  2.08it/s] 72%|███████▏  | 717/1000 [05:45<02:16,  2.07it/s] 72%|███████▏  | 718/1000 [05:45<02:16,  2.06it/s] 72%|███████▏  | 719/1000 [05:46<02:16,  2.06it/s] 72%|███████▏  | 720/1000 [05:46<02:06,  2.21it/s] 72%|███████▏  | 721/1000 [05:47<02:09,  2.16it/s] 72%|███████▏  | 722/1000 [05:47<02:10,  2.13it/s] 72%|███████▏  | 723/1000 [05:48<02:11,  2.10it/s] 72%|███████▏  | 724/1000 [05:48<02:11,  2.09it/s] 72%|███████▎  | 725/1000 [05:49<02:12,  2.08it/s] 73%|███████▎  | 726/1000 [05:49<02:11,  2.08it/s] 73%|███████▎  | 727/1000 [05:50<02:11,  2.07it/s] 73%|███████▎  | 728/1000 [05:50<02:11,  2.07it/s] 73%|███████▎  | 729/1000 [05:51<02:10,  2.07it/s] 73%|███████▎  | 730/1000 [05:51<02:02,  2.21it/s] 73%|███████▎  | 731/1000 [05:51<02:04,  2.16it/s] 73%|███████▎  | 732/1000 [05:52<02:05,  2.13it/s] 73%|███████▎  | 733/1000 [05:52<02:06,  2.10it/s] 73%|███████▎  | 734/1000 [05:53<02:07,  2.08it/s] 74%|███████▎  | 735/1000 [05:53<02:07,  2.08it/s] 74%|███████▎  | 736/1000 [05:54<02:07,  2.07it/s] 74%|███████▎  | 737/1000 [05:54<02:07,  2.06it/s] 74%|███████▍  | 738/1000 [05:55<02:07,  2.06it/s] 74%|███████▍  | 739/1000 [05:55<02:06,  2.06it/s] 74%|███████▍  | 740/1000 [05:56<01:58,  2.20it/s] 74%|███████▍  | 741/1000 [05:56<02:00,  2.15it/s] 74%|███████▍  | 742/1000 [05:57<02:01,  2.13it/s] 74%|███████▍  | 743/1000 [05:57<02:02,  2.10it/s] 74%|███████▍  | 744/1000 [05:58<02:02,  2.09it/s] 74%|███████▍  | 745/1000 [05:58<02:02,  2.08it/s] 75%|███████▍  | 746/1000 [05:59<02:02,  2.07it/s] 75%|███████▍  | 747/1000 [05:59<02:03,  2.06it/s] 75%|███████▍  | 748/1000 [06:00<02:02,  2.05it/s] 75%|███████▍  | 749/1000 [06:00<02:02,  2.05it/s] 75%|███████▌  | 750/1000 [06:00<01:53,  2.19it/s] 75%|███████▌  | 751/1000 [06:01<01:55,  2.15it/s] 75%|███████▌  | 752/1000 [06:01<01:57,  2.12it/s] 75%|███████▌  | 753/1000 [06:02<01:57,  2.10it/s] 75%|███████▌  | 754/1000 [06:02<02:00,  2.04it/s] 76%|███████▌  | 755/1000 [06:03<02:00,  2.04it/s] 76%|███████▌  | 756/1000 [06:03<02:00,  2.03it/s] 76%|███████▌  | 757/1000 [06:04<01:59,  2.04it/s] 76%|███████▌  | 758/1000 [06:04<01:58,  2.04it/s] 76%|███████▌  | 759/1000 [06:05<01:57,  2.04it/s] 76%|███████▌  | 760/1000 [06:05<01:49,  2.19it/s] 76%|███████▌  | 761/1000 [06:06<01:51,  2.14it/s] 76%|███████▌  | 762/1000 [06:06<01:52,  2.11it/s] 76%|███████▋  | 763/1000 [06:07<01:52,  2.10it/s] 76%|███████▋  | 764/1000 [06:07<01:53,  2.08it/s] 76%|███████▋  | 765/1000 [06:08<01:53,  2.08it/s] 77%|███████▋  | 766/1000 [06:08<01:52,  2.07it/s] 77%|███████▋  | 767/1000 [06:09<01:52,  2.07it/s] 77%|███████▋  | 768/1000 [06:09<01:52,  2.07it/s] 77%|███████▋  | 769/1000 [06:10<01:52,  2.05it/s] 77%|███████▋  | 770/1000 [06:10<01:44,  2.19it/s] 77%|███████▋  | 771/1000 [06:11<01:46,  2.15it/s] 77%|███████▋  | 772/1000 [06:11<01:47,  2.12it/s] 77%|███████▋  | 773/1000 [06:12<01:48,  2.10it/s] 77%|███████▋  | 774/1000 [06:12<01:48,  2.08it/s] 78%|███████▊  | 775/1000 [06:12<01:48,  2.07it/s] 78%|███████▊  | 776/1000 [06:13<01:48,  2.07it/s] 78%|███████▊  | 777/1000 [06:13<01:48,  2.06it/s] 78%|███████▊  | 778/1000 [06:14<01:47,  2.06it/s] 78%|███████▊  | 779/1000 [06:14<01:47,  2.06it/s] 78%|███████▊  | 780/1000 [06:15<01:39,  2.20it/s] 78%|███████▊  | 781/1000 [06:15<01:41,  2.16it/s] 78%|███████▊  | 782/1000 [06:16<01:42,  2.12it/s] 78%|███████▊  | 783/1000 [06:16<01:43,  2.10it/s] 78%|███████▊  | 784/1000 [06:17<01:43,  2.09it/s] 78%|███████▊  | 785/1000 [06:17<01:43,  2.08it/s] 79%|███████▊  | 786/1000 [06:18<01:43,  2.07it/s] 79%|███████▊  | 787/1000 [06:18<01:43,  2.07it/s] 79%|███████▉  | 788/1000 [06:19<01:43,  2.06it/s] 79%|███████▉  | 789/1000 [06:19<01:42,  2.06it/s] 79%|███████▉  | 790/1000 [06:20<01:35,  2.20it/s] 79%|███████▉  | 791/1000 [06:20<01:37,  2.15it/s] 79%|███████▉  | 792/1000 [06:21<01:38,  2.12it/s] 79%|███████▉  | 793/1000 [06:21<01:38,  2.10it/s] 79%|███████▉  | 794/1000 [06:22<01:38,  2.09it/s] 80%|███████▉  | 795/1000 [06:22<01:38,  2.08it/s] 80%|███████▉  | 796/1000 [06:23<01:38,  2.07it/s] 80%|███████▉  | 797/1000 [06:23<01:38,  2.07it/s] 80%|███████▉  | 798/1000 [06:23<01:37,  2.07it/s] 80%|███████▉  | 799/1000 [06:24<01:37,  2.07it/s] 80%|████████  | 800/1000 [06:24<01:30,  2.21it/s] 80%|████████  | 801/1000 [06:25<01:32,  2.15it/s] 80%|████████  | 802/1000 [06:25<01:33,  2.13it/s] 80%|████████  | 803/1000 [06:26<01:33,  2.10it/s] 80%|████████  | 804/1000 [06:26<01:33,  2.09it/s] 80%|████████  | 805/1000 [06:27<01:33,  2.08it/s] 81%|████████  | 806/1000 [06:27<01:33,  2.07it/s] 81%|████████  | 807/1000 [06:28<01:33,  2.07it/s] 81%|████████  | 808/1000 [06:28<01:32,  2.07it/s] 81%|████████  | 809/1000 [06:29<01:32,  2.07it/s] 81%|████████  | 810/1000 [06:29<01:25,  2.21it/s] 81%|████████  | 811/1000 [06:30<01:27,  2.16it/s] 81%|████████  | 812/1000 [06:30<01:28,  2.13it/s] 81%|████████▏ | 813/1000 [06:31<01:28,  2.11it/s] 81%|████████▏ | 814/1000 [06:31<01:28,  2.10it/s] 82%|████████▏ | 815/1000 [06:32<01:28,  2.08it/s] 82%|████████▏ | 816/1000 [06:32<01:28,  2.08it/s] 82%|████████▏ | 817/1000 [06:32<01:28,  2.08it/s] 82%|████████▏ | 818/1000 [06:33<01:27,  2.08it/s] 82%|████████▏ | 819/1000 [06:33<01:27,  2.08it/s] 82%|████████▏ | 820/1000 [06:34<01:21,  2.22it/s] 82%|████████▏ | 821/1000 [06:34<01:22,  2.17it/s] 82%|████████▏ | 822/1000 [06:35<01:23,  2.13it/s] 82%|████████▏ | 823/1000 [06:35<01:23,  2.11it/s] 82%|████████▏ | 824/1000 [06:36<01:23,  2.10it/s] 82%|████████▎ | 825/1000 [06:36<01:23,  2.09it/s] 83%|████████▎ | 826/1000 [06:37<01:27,  1.98it/s] 83%|████████▎ | 827/1000 [06:37<01:26,  2.01it/s] 83%|████████▎ | 828/1000 [06:38<01:25,  2.02it/s] 83%|████████▎ | 829/1000 [06:38<01:24,  2.03it/s] 83%|████████▎ | 830/1000 [06:39<01:18,  2.17it/s] 83%|████████▎ | 831/1000 [06:39<01:19,  2.14it/s] 83%|████████▎ | 832/1000 [06:40<01:19,  2.11it/s] 83%|████████▎ | 833/1000 [06:40<01:19,  2.10it/s] 83%|████████▎ | 834/1000 [06:41<01:19,  2.08it/s] 84%|████████▎ | 835/1000 [06:41<01:19,  2.08it/s] 84%|████████▎ | 836/1000 [06:42<01:19,  2.07it/s] 84%|████████▎ | 837/1000 [06:42<01:18,  2.07it/s] 84%|████████▍ | 838/1000 [06:43<01:18,  2.06it/s] 84%|████████▍ | 839/1000 [06:43<01:18,  2.06it/s] 84%|████████▍ | 840/1000 [06:43<01:12,  2.20it/s] 84%|████████▍ | 841/1000 [06:44<01:13,  2.16it/s] 84%|████████▍ | 842/1000 [06:44<01:14,  2.12it/s] 84%|████████▍ | 843/1000 [06:45<01:14,  2.10it/s] 84%|████████▍ | 844/1000 [06:45<01:14,  2.09it/s] 84%|████████▍ | 845/1000 [06:46<01:14,  2.08it/s] 85%|████████▍ | 846/1000 [06:46<01:14,  2.08it/s] 85%|████████▍ | 847/1000 [06:47<01:13,  2.07it/s] 85%|████████▍ | 848/1000 [06:47<01:13,  2.06it/s] 85%|████████▍ | 849/1000 [06:48<01:13,  2.07it/s] 85%|████████▌ | 850/1000 [06:48<01:07,  2.21it/s] 85%|████████▌ | 851/1000 [06:49<01:09,  2.16it/s] 85%|████████▌ | 852/1000 [06:49<01:09,  2.12it/s] 85%|████████▌ | 853/1000 [06:50<01:09,  2.10it/s] 85%|████████▌ | 854/1000 [06:50<01:10,  2.08it/s] 86%|████████▌ | 855/1000 [06:51<01:09,  2.08it/s] 86%|████████▌ | 856/1000 [06:51<01:09,  2.07it/s] 86%|████████▌ | 857/1000 [06:52<01:09,  2.06it/s] 86%|████████▌ | 858/1000 [06:52<01:08,  2.06it/s] 86%|████████▌ | 859/1000 [06:53<01:08,  2.06it/s] 86%|████████▌ | 860/1000 [06:53<01:03,  2.20it/s] 86%|████████▌ | 861/1000 [06:53<01:04,  2.16it/s] 86%|████████▌ | 862/1000 [06:54<01:05,  2.12it/s] 86%|████████▋ | 863/1000 [06:54<01:04,  2.11it/s] 86%|████████▋ | 864/1000 [06:55<01:04,  2.09it/s] 86%|████████▋ | 865/1000 [06:55<01:05,  2.07it/s] 87%|████████▋ | 866/1000 [06:56<01:04,  2.07it/s] 87%|████████▋ | 867/1000 [06:56<01:04,  2.06it/s] 87%|████████▋ | 868/1000 [06:57<01:04,  2.05it/s] 87%|████████▋ | 869/1000 [06:57<01:03,  2.05it/s] 87%|████████▋ | 870/1000 [06:58<00:59,  2.20it/s] 87%|████████▋ | 871/1000 [06:58<01:00,  2.15it/s] 87%|████████▋ | 872/1000 [06:59<01:00,  2.12it/s] 87%|████████▋ | 873/1000 [06:59<01:00,  2.10it/s] 87%|████████▋ | 874/1000 [07:00<01:00,  2.09it/s] 88%|████████▊ | 875/1000 [07:00<01:00,  2.08it/s] 88%|████████▊ | 876/1000 [07:01<00:59,  2.08it/s] 88%|████████▊ | 877/1000 [07:01<00:59,  2.07it/s] 88%|████████▊ | 878/1000 [07:02<00:58,  2.07it/s] 88%|████████▊ | 879/1000 [07:02<00:58,  2.07it/s] 88%|████████▊ | 880/1000 [07:02<00:54,  2.20it/s] 88%|████████▊ | 881/1000 [07:03<00:55,  2.16it/s] 88%|████████▊ | 882/1000 [07:03<00:55,  2.12it/s] 88%|████████▊ | 883/1000 [07:04<00:55,  2.09it/s] 88%|████████▊ | 884/1000 [07:04<00:55,  2.08it/s] 88%|████████▊ | 885/1000 [07:05<00:55,  2.07it/s] 89%|████████▊ | 886/1000 [07:05<00:55,  2.07it/s] 89%|████████▊ | 887/1000 [07:06<00:54,  2.06it/s] 89%|████████▉ | 888/1000 [07:06<00:54,  2.06it/s] 89%|████████▉ | 889/1000 [07:07<00:54,  2.05it/s] 89%|████████▉ | 890/1000 [07:07<00:50,  2.20it/s] 89%|████████▉ | 891/1000 [07:08<00:50,  2.15it/s] 89%|████████▉ | 892/1000 [07:08<00:51,  2.11it/s] 89%|████████▉ | 893/1000 [07:09<00:51,  2.09it/s] 89%|████████▉ | 894/1000 [07:09<00:50,  2.08it/s] 90%|████████▉ | 895/1000 [07:10<00:50,  2.08it/s] 90%|████████▉ | 896/1000 [07:10<00:50,  2.07it/s] 90%|████████▉ | 897/1000 [07:11<00:49,  2.06it/s] 90%|████████▉ | 898/1000 [07:11<00:51,  1.99it/s] 90%|████████▉ | 899/1000 [07:12<00:50,  2.01it/s] 90%|█████████ | 900/1000 [07:12<00:46,  2.16it/s] 90%|█████████ | 901/1000 [07:13<00:46,  2.12it/s] 90%|█████████ | 902/1000 [07:13<00:46,  2.09it/s] 90%|█████████ | 903/1000 [07:14<00:46,  2.08it/s] 90%|█████████ | 904/1000 [07:14<00:46,  2.07it/s] 90%|█████████ | 905/1000 [07:15<00:46,  2.06it/s] 91%|█████████ | 906/1000 [07:15<00:45,  2.06it/s] 91%|█████████ | 907/1000 [07:15<00:45,  2.06it/s] 91%|█████████ | 908/1000 [07:16<00:44,  2.06it/s] 91%|█████████ | 909/1000 [07:16<00:44,  2.06it/s] 91%|█████████ | 910/1000 [07:17<00:40,  2.20it/s] 91%|█████████ | 911/1000 [07:17<00:41,  2.15it/s] 91%|█████████ | 912/1000 [07:18<00:41,  2.12it/s] 91%|█████████▏| 913/1000 [07:18<00:41,  2.10it/s] 91%|█████████▏| 914/1000 [07:19<00:41,  2.09it/s] 92%|█████████▏| 915/1000 [07:19<00:40,  2.09it/s] 92%|█████████▏| 916/1000 [07:20<00:40,  2.09it/s] 92%|█████████▏| 917/1000 [07:20<00:39,  2.09it/s] 92%|█████████▏| 918/1000 [07:21<00:39,  2.09it/s] 92%|█████████▏| 919/1000 [07:21<00:38,  2.08it/s] 92%|█████████▏| 920/1000 [07:22<00:35,  2.23it/s] 92%|█████████▏| 921/1000 [07:22<00:36,  2.18it/s] 92%|█████████▏| 922/1000 [07:23<00:36,  2.15it/s] 92%|█████████▏| 923/1000 [07:23<00:36,  2.13it/s] 92%|█████████▏| 924/1000 [07:23<00:35,  2.11it/s] 92%|█████████▎| 925/1000 [07:24<00:35,  2.10it/s] 93%|█████████▎| 926/1000 [07:24<00:35,  2.10it/s] 93%|█████████▎| 927/1000 [07:25<00:34,  2.09it/s] 93%|█████████▎| 928/1000 [07:25<00:34,  2.09it/s] 93%|█████████▎| 929/1000 [07:26<00:34,  2.09it/s] 93%|█████████▎| 930/1000 [07:26<00:31,  2.23it/s] 93%|█████████▎| 931/1000 [07:27<00:31,  2.18it/s] 93%|█████████▎| 932/1000 [07:27<00:31,  2.15it/s] 93%|█████████▎| 933/1000 [07:28<00:31,  2.13it/s] 93%|█████████▎| 934/1000 [07:28<00:31,  2.12it/s] 94%|█████████▎| 935/1000 [07:29<00:30,  2.11it/s] 94%|█████████▎| 936/1000 [07:29<00:30,  2.10it/s] 94%|█████████▎| 937/1000 [07:30<00:30,  2.09it/s] 94%|█████████▍| 938/1000 [07:30<00:29,  2.09it/s] 94%|█████████▍| 939/1000 [07:31<00:29,  2.08it/s] 94%|█████████▍| 940/1000 [07:31<00:26,  2.23it/s] 94%|█████████▍| 941/1000 [07:31<00:27,  2.18it/s] 94%|█████████▍| 942/1000 [07:32<00:26,  2.15it/s] 94%|█████████▍| 943/1000 [07:32<00:26,  2.13it/s] 94%|█████████▍| 944/1000 [07:33<00:26,  2.11it/s] 94%|█████████▍| 945/1000 [07:33<00:26,  2.11it/s] 95%|█████████▍| 946/1000 [07:34<00:25,  2.10it/s] 95%|█████████▍| 947/1000 [07:34<00:25,  2.09it/s] 95%|█████████▍| 948/1000 [07:35<00:24,  2.09it/s] 95%|█████████▍| 949/1000 [07:35<00:24,  2.09it/s] 95%|█████████▌| 950/1000 [07:36<00:22,  2.23it/s] 95%|█████████▌| 951/1000 [07:36<00:22,  2.18it/s] 95%|█████████▌| 952/1000 [07:37<00:22,  2.15it/s] 95%|█████████▌| 953/1000 [07:37<00:22,  2.13it/s] 95%|█████████▌| 954/1000 [07:38<00:21,  2.12it/s] 96%|█████████▌| 955/1000 [07:38<00:21,  2.11it/s] 96%|█████████▌| 956/1000 [07:39<00:20,  2.10it/s] 96%|█████████▌| 957/1000 [07:39<00:20,  2.10it/s] 96%|█████████▌| 958/1000 [07:40<00:20,  2.09it/s] 96%|█████████▌| 959/1000 [07:40<00:19,  2.09it/s] 96%|█████████▌| 960/1000 [07:40<00:17,  2.23it/s] 96%|█████████▌| 961/1000 [07:41<00:17,  2.18it/s] 96%|█████████▌| 962/1000 [07:41<00:17,  2.15it/s] 96%|█████████▋| 963/1000 [07:42<00:17,  2.13it/s] 96%|█████████▋| 964/1000 [07:42<00:17,  2.12it/s] 96%|█████████▋| 965/1000 [07:43<00:16,  2.11it/s] 97%|█████████▋| 966/1000 [07:43<00:16,  2.10it/s] 97%|█████████▋| 967/1000 [07:44<00:15,  2.10it/s] 97%|█████████▋| 968/1000 [07:44<00:15,  2.09it/s] 97%|█████████▋| 969/1000 [07:45<00:14,  2.09it/s] 97%|█████████▋| 970/1000 [07:45<00:14,  2.12it/s] 97%|█████████▋| 971/1000 [07:46<00:13,  2.11it/s] 97%|█████████▋| 972/1000 [07:46<00:13,  2.10it/s] 97%|█████████▋| 973/1000 [07:47<00:12,  2.09it/s] 97%|█████████▋| 974/1000 [07:47<00:12,  2.09it/s] 98%|█████████▊| 975/1000 [07:48<00:11,  2.09it/s] 98%|█████████▊| 976/1000 [07:48<00:11,  2.09it/s] 98%|█████████▊| 977/1000 [07:49<00:11,  2.09it/s] 98%|█████████▊| 978/1000 [07:49<00:10,  2.08it/s] 98%|█████████▊| 979/1000 [07:49<00:10,  2.08it/s] 98%|█████████▊| 980/1000 [07:50<00:08,  2.23it/s] 98%|█████████▊| 981/1000 [07:50<00:08,  2.18it/s] 98%|█████████▊| 982/1000 [07:51<00:08,  2.15it/s] 98%|█████████▊| 983/1000 [07:51<00:07,  2.13it/s] 98%|█████████▊| 984/1000 [07:52<00:07,  2.12it/s] 98%|█████████▊| 985/1000 [07:52<00:07,  2.11it/s] 99%|█████████▊| 986/1000 [07:53<00:06,  2.10it/s] 99%|█████████▊| 987/1000 [07:53<00:06,  2.09it/s] 99%|█████████▉| 988/1000 [07:54<00:05,  2.09it/s] 99%|█████████▉| 989/1000 [07:54<00:05,  2.09it/s] 99%|█████████▉| 990/1000 [07:55<00:04,  2.23it/s] 99%|█████████▉| 991/1000 [07:55<00:04,  2.18it/s] 99%|█████████▉| 992/1000 [07:56<00:03,  2.15it/s] 99%|█████████▉| 993/1000 [07:56<00:03,  2.13it/s] 99%|█████████▉| 994/1000 [07:56<00:02,  2.12it/s]100%|█████████▉| 995/1000 [07:57<00:02,  2.11it/s]100%|█████████▉| 996/1000 [07:57<00:01,  2.10it/s]100%|█████████▉| 997/1000 [07:58<00:01,  2.09it/s]100%|█████████▉| 998/1000 [07:58<00:00,  2.09it/s]100%|█████████▉| 999/1000 [07:59<00:00,  2.09it/s]100%|██████████| 1000/1000 [07:59<00:00,  2.23it/s]                                                   100%|██████████| 1000/1000 [07:59<00:00,  2.23it/s][INFO|trainer.py:2409] 2022-08-27 22:00:57,605 >> Saving model checkpoint to tmp/baseline/RTE/bert-base-uncased/HPO_S0/1e-06/256/100/checkpoint-1000
[INFO|configuration_utils.py:446] 2022-08-27 22:00:57,606 >> Configuration saved in tmp/baseline/RTE/bert-base-uncased/HPO_S0/1e-06/256/100/checkpoint-1000/config.json
[INFO|modeling_utils.py:1602] 2022-08-27 22:00:58,439 >> Model weights saved in tmp/baseline/RTE/bert-base-uncased/HPO_S0/1e-06/256/100/checkpoint-1000/pytorch_model.bin
[INFO|tokenization_utils_base.py:2108] 2022-08-27 22:00:58,440 >> tokenizer config file saved in tmp/baseline/RTE/bert-base-uncased/HPO_S0/1e-06/256/100/checkpoint-1000/tokenizer_config.json
[INFO|tokenization_utils_base.py:2114] 2022-08-27 22:00:58,440 >> Special tokens file saved in tmp/baseline/RTE/bert-base-uncased/HPO_S0/1e-06/256/100/checkpoint-1000/special_tokens_map.json
[INFO|trainer.py:1679] 2022-08-27 22:01:00,146 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


                                                   100%|██████████| 1000/1000 [08:02<00:00,  2.23it/s]100%|██████████| 1000/1000 [08:02<00:00,  2.07it/s]
[INFO|trainer.py:2409] 2022-08-27 22:01:00,147 >> Saving model checkpoint to tmp/baseline/RTE/bert-base-uncased/HPO_S0/1e-06/256/100
[INFO|configuration_utils.py:446] 2022-08-27 22:01:00,148 >> Configuration saved in tmp/baseline/RTE/bert-base-uncased/HPO_S0/1e-06/256/100/config.json
[INFO|modeling_utils.py:1602] 2022-08-27 22:01:00,977 >> Model weights saved in tmp/baseline/RTE/bert-base-uncased/HPO_S0/1e-06/256/100/pytorch_model.bin
[INFO|tokenization_utils_base.py:2108] 2022-08-27 22:01:00,978 >> tokenizer config file saved in tmp/baseline/RTE/bert-base-uncased/HPO_S0/1e-06/256/100/tokenizer_config.json
[INFO|tokenization_utils_base.py:2114] 2022-08-27 22:01:00,978 >> Special tokens file saved in tmp/baseline/RTE/bert-base-uncased/HPO_S0/1e-06/256/100/special_tokens_map.json
{'loss': 0.6987, 'learning_rate': 6.249999999999999e-07, 'epoch': 50.0}
{'loss': 0.6969, 'learning_rate': 0.0, 'epoch': 100.0}
{'train_runtime': 482.2888, 'train_samples_per_second': 516.288, 'train_steps_per_second': 2.073, 'train_loss': 0.6978011169433593, 'epoch': 100.0}
***** train metrics *****
  epoch                    =      100.0
  train_loss               =     0.6978
  train_runtime            = 0:08:02.28
  train_samples            =       2490
  train_samples_per_second =    516.288
  train_steps_per_second   =      2.073
08/27/2022 22:01:01 - INFO - __main__ - *** Evaluate ***
[INFO|trainer.py:628] 2022-08-27 22:01:01,022 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence2, sentence1, idx. If sentence2, sentence1, idx are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:2659] 2022-08-27 22:01:01,025 >> ***** Running Evaluation *****
[INFO|trainer.py:2661] 2022-08-27 22:01:01,025 >>   Num examples = 277
[INFO|trainer.py:2664] 2022-08-27 22:01:01,025 >>   Batch size = 16
/home/ubuntu/anaconda3/envs/transformers/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
  0%|          | 0/18 [00:00<?, ?it/s] 22%|██▏       | 4/18 [00:00<00:00, 27.53it/s] 39%|███▉      | 7/18 [00:00<00:00, 23.72it/s] 56%|█████▌    | 10/18 [00:00<00:00, 22.30it/s] 72%|███████▏  | 13/18 [00:00<00:00, 21.72it/s] 89%|████████▉ | 16/18 [00:00<00:00, 21.29it/s]08/27/2022 22:01:01 - INFO - datasets.metric - Removing /home/ubuntu/.cache/huggingface/metrics/glue/rte/default_experiment-1-0.arrow
100%|██████████| 18/18 [00:00<00:00, 21.80it/s]
***** eval metrics *****
  epoch                   =      100.0
  eval_accuracy           =     0.4729
  eval_loss               =     0.6935
  eval_runtime            = 0:00:00.87
  eval_samples            =        277
  eval_samples_per_second =    315.723
  eval_steps_per_second   =     20.516
08/27/2022 22:01:05 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 2distributed training: False, 16-bits training: True
08/27/2022 22:01:05 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=2,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=IntervalStrategy.NO,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_min_num_params=0,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=1e-06,
length_column_name=length,
load_best_model_at_end=False,
local_rank=-1,
log_level=-1,
log_level_replica=-1,
log_on_each_node=True,
logging_dir=tmp/baseline/RTE/bert-base-uncased/HPO_S0/1e-06/64/10/runs/Aug27_22-01-05_ip-10-21-39-60,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
no_cuda=False,
num_train_epochs=10.0,
optim=OptimizerNames.ADAMW_HF,
output_dir=tmp/baseline/RTE/bert-base-uncased/HPO_S0/1e-06/64/10,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=tmp/baseline/RTE/bert-base-uncased/HPO_S0/1e-06/64/10,
save_on_each_node=False,
save_steps=500,
save_strategy=IntervalStrategy.STEPS,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tf32=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
xpu_backend=None,
)
08/27/2022 22:01:06 - INFO - datasets.info - Loading Dataset Infos from /home/ubuntu/.cache/huggingface/modules/datasets_modules/datasets/glue/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad
08/27/2022 22:01:06 - INFO - datasets.builder - Overwrite dataset info from restored data version.
08/27/2022 22:01:06 - INFO - datasets.info - Loading Dataset info from /home/ubuntu/.cache/huggingface/datasets/glue/rte/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad
08/27/2022 22:01:06 - WARNING - datasets.builder - Reusing dataset glue (/home/ubuntu/.cache/huggingface/datasets/glue/rte/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)
08/27/2022 22:01:06 - INFO - datasets.info - Loading Dataset info from /home/ubuntu/.cache/huggingface/datasets/glue/rte/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 587.68it/s]
[INFO|configuration_utils.py:657] 2022-08-27 22:01:06,124 >> loading configuration file /home/ubuntu/checkpoints/exp/RTE/config.json
[INFO|configuration_utils.py:708] 2022-08-27 22:01:06,126 >> Model config BertConfig {
  "_name_or_path": "/home/ubuntu/checkpoints/exp/RTE",
  "architectures": [
    "BertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "crypten": false,
  "finetuning_task": "rte",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "entailment",
    "1": "not_entailment"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "entailment": 0,
    "not_entailment": 1
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "softmax_act": "softmax",
  "torch_dtype": "float32",
  "transformers_version": "4.20.0.dev0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|tokenization_utils_base.py:1701] 2022-08-27 22:01:06,126 >> Didn't find file /home/ubuntu/checkpoints/exp/RTE/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1779] 2022-08-27 22:01:06,126 >> loading file /home/ubuntu/checkpoints/exp/RTE/vocab.txt
[INFO|tokenization_utils_base.py:1779] 2022-08-27 22:01:06,126 >> loading file /home/ubuntu/checkpoints/exp/RTE/tokenizer.json
[INFO|tokenization_utils_base.py:1779] 2022-08-27 22:01:06,126 >> loading file None
[INFO|tokenization_utils_base.py:1779] 2022-08-27 22:01:06,126 >> loading file /home/ubuntu/checkpoints/exp/RTE/special_tokens_map.json
[INFO|tokenization_utils_base.py:1779] 2022-08-27 22:01:06,126 >> loading file /home/ubuntu/checkpoints/exp/RTE/tokenizer_config.json
[INFO|modeling_utils.py:2047] 2022-08-27 22:01:06,169 >> loading weights file /home/ubuntu/checkpoints/exp/RTE/pytorch_model.bin
[INFO|modeling_bert.py:194] 2022-08-27 22:01:06,503 >> initializing embedding using nn.Embedding
[INFO|modeling_utils.py:2417] 2022-08-27 22:01:07,628 >> All model checkpoint weights were used when initializing BertForSequenceClassification.

[INFO|modeling_utils.py:2426] 2022-08-27 22:01:07,628 >> All the weights of BertForSequenceClassification were initialized from the model checkpoint at /home/ubuntu/checkpoints/exp/RTE.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.
using model config: BertConfig {
  "_name_or_path": "/home/ubuntu/checkpoints/exp/RTE",
  "architectures": [
    "BertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "crypten": false,
  "finetuning_task": "rte",
  "gradient_checkpointing": false,
  "hidden_act": "quad",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "entailment",
    "1": "not_entailment"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "entailment": 0,
    "not_entailment": 1
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "softmax_act": "2quad",
  "torch_dtype": "float32",
  "transformers_version": "4.20.0.dev0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

BertConfig {
  "_name_or_path": "/home/ubuntu/checkpoints/exp/RTE",
  "architectures": [
    "BertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "crypten": false,
  "finetuning_task": "rte",
  "gradient_checkpointing": false,
  "hidden_act": "quad",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "entailment",
    "1": "not_entailment"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "entailment": 0,
    "not_entailment": 1
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "softmax_act": "2quad",
  "torch_dtype": "float32",
  "transformers_version": "4.20.0.dev0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

model architecture: BertForSequenceClassification(
  (bert): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): QuadActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): QuadActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): QuadActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): QuadActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): QuadActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): QuadActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): QuadActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): QuadActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): QuadActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): QuadActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): QuadActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): QuadActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (dropout): Dropout(p=0.1, inplace=False)
  (classifier): Linear(in_features=768, out_features=2, bias=True)
)
08/27/2022 22:01:07 - WARNING - datasets.fingerprint - Parameter 'function'=<function main.<locals>.preprocess_function at 0x7f5ec906a488> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
Running tokenizer on dataset:   0%|          | 0/3 [00:00<?, ?ba/s]08/27/2022 22:01:07 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/ubuntu/.cache/huggingface/datasets/glue/rte/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-1c80317fa3b1799d.arrow
Running tokenizer on dataset:  33%|███▎      | 1/3 [00:00<00:00,  7.62ba/s]Running tokenizer on dataset:  67%|██████▋   | 2/3 [00:00<00:00,  5.76ba/s]Running tokenizer on dataset: 100%|██████████| 3/3 [00:00<00:00,  7.88ba/s]08/27/2022 22:01:08 - INFO - datasets.fingerprint - Parameter 'function'=<function main.<locals>.preprocess_function at 0x7f5ec9066950> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead.

Running tokenizer on dataset:   0%|          | 0/1 [00:00<?, ?ba/s]08/27/2022 22:01:08 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/ubuntu/.cache/huggingface/datasets/glue/rte/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-bdd640fb06671ad1.arrow
Running tokenizer on dataset: 100%|██████████| 1/1 [00:00<00:00, 32.33ba/s]08/27/2022 22:01:08 - INFO - datasets.fingerprint - Parameter 'function'=<function main.<locals>.preprocess_function at 0x7f5ec906a840> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead.

Running tokenizer on dataset:   0%|          | 0/3 [00:00<?, ?ba/s]08/27/2022 22:01:08 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/ubuntu/.cache/huggingface/datasets/glue/rte/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-3eb13b9046685257.arrow
Running tokenizer on dataset:  67%|██████▋   | 2/3 [00:00<00:00, 11.55ba/s]Running tokenizer on dataset: 100%|██████████| 3/3 [00:00<00:00, 10.85ba/s]128
08/27/2022 22:01:13 - INFO - __main__ - Sample 914 of the training set: {'sentence1': "Because of Reagan's economic strategy, the federal budget deficit ballooned.", 'sentence2': "Reagan's economic strategy led to huge federal budget deficits.", 'label': 0, 'idx': 914, 'input_ids': [101, 2138, 1997, 11531, 1005, 1055, 3171, 5656, 1010, 1996, 2976, 5166, 15074, 13212, 2098, 1012, 102, 11531, 1005, 1055, 3171, 5656, 2419, 2000, 4121, 2976, 5166, 15074, 2015, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.
128
08/27/2022 22:01:18 - INFO - __main__ - Sample 571 of the training set: {'sentence1': 'So far the British have preferred a policy of a voluntary approach to restricting advertising and high taxes on tobacco products.', 'sentence2': 'Sales have declined due to restrictions on advertising.', 'label': 1, 'idx': 571, 'input_ids': [101, 2061, 2521, 1996, 2329, 2031, 6871, 1037, 3343, 1997, 1037, 10758, 3921, 2000, 26996, 6475, 1998, 2152, 7773, 2006, 9098, 3688, 1012, 102, 4341, 2031, 6430, 2349, 2000, 9259, 2006, 6475, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.
128
08/27/2022 22:01:23 - INFO - __main__ - Sample 419 of the training set: {'sentence1': 'Protesters, many from organized pro-government groups but including many ordinary citizens, carried anti-American banners and chanted slogans attacking U.N. Secretary-General Kofi Annan for his close alignment with U.S. policy.', 'sentence2': 'Protesters confiscated anti-American banners and chanted slogans attacking U.N. Secretary-General Kofi Annan for his close alignment with U.S. policy.', 'label': 1, 'idx': 419, 'input_ids': [101, 13337, 1010, 2116, 2013, 4114, 4013, 1011, 2231, 2967, 2021, 2164, 2116, 6623, 4480, 1010, 3344, 3424, 1011, 2137, 23562, 1998, 16883, 2098, 14558, 2015, 7866, 1057, 1012, 1050, 1012, 3187, 1011, 2236, 12849, 8873, 4698, 2078, 2005, 2010, 2485, 12139, 2007, 1057, 1012, 1055, 1012, 3343, 1012, 102, 13337, 17182, 3424, 1011, 2137, 23562, 1998, 16883, 2098, 14558, 2015, 7866, 1057, 1012, 1050, 1012, 3187, 1011, 2236, 12849, 8873, 4698, 2078, 2005, 2010, 2485, 12139, 2007, 1057, 1012, 1055, 1012, 3343, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.

[INFO|trainer.py:506] 2022-08-27 22:01:28,050 >> Using amp half precision backend
[INFO|trainer.py:628] 2022-08-27 22:01:28,052 >> The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1, idx, sentence2. If sentence1, idx, sentence2 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.
/home/ubuntu/transformers_private/src/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  FutureWarning,
[INFO|trainer.py:1430] 2022-08-27 22:01:28,066 >> ***** Running training *****
[INFO|trainer.py:1431] 2022-08-27 22:01:28,066 >>   Num examples = 2490
[INFO|trainer.py:1432] 2022-08-27 22:01:28,067 >>   Num Epochs = 10
[INFO|trainer.py:1433] 2022-08-27 22:01:28,067 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1434] 2022-08-27 22:01:28,067 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1435] 2022-08-27 22:01:28,067 >>   Gradient Accumulation steps = 1
[INFO|trainer.py:1436] 2022-08-27 22:01:28,067 >>   Total optimization steps = 390
using training arge: TrainingArguments(
_n_gpu=2,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=IntervalStrategy.NO,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_min_num_params=0,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=1e-06,
length_column_name=length,
load_best_model_at_end=False,
local_rank=-1,
log_level=-1,
log_level_replica=-1,
log_on_each_node=True,
logging_dir=tmp/baseline/RTE/bert-base-uncased/HPO_S0/1e-06/64/10/runs/Aug27_22-01-05_ip-10-21-39-60,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
no_cuda=False,
num_train_epochs=10.0,
optim=OptimizerNames.ADAMW_HF,
output_dir=tmp/baseline/RTE/bert-base-uncased/HPO_S0/1e-06/64/10,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=tmp/baseline/RTE/bert-base-uncased/HPO_S0/1e-06/64/10,
save_on_each_node=False,
save_steps=500,
save_strategy=IntervalStrategy.STEPS,
save_total_limit=0,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tf32=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
xpu_backend=None,
)
  0%|          | 0/390 [00:00<?, ?it/s]/home/ubuntu/anaconda3/envs/transformers/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
  0%|          | 1/390 [00:04<30:11,  4.66s/it]  1%|          | 2/390 [00:04<13:19,  2.06s/it]  1%|          | 3/390 [00:05<07:48,  1.21s/it]  1%|          | 4/390 [00:05<05:12,  1.23it/s]  1%|▏         | 5/390 [00:05<03:47,  1.70it/s]  2%|▏         | 6/390 [00:05<02:55,  2.18it/s]  2%|▏         | 7/390 [00:05<02:23,  2.68it/s]  2%|▏         | 8/390 [00:06<02:01,  3.15it/s]  2%|▏         | 9/390 [00:06<01:46,  3.57it/s]  3%|▎         | 10/390 [00:06<01:37,  3.92it/s]  3%|▎         | 11/390 [00:06<01:30,  4.20it/s]  3%|▎         | 12/390 [00:06<01:25,  4.42it/s]  3%|▎         | 13/390 [00:07<01:22,  4.59it/s]  4%|▎         | 14/390 [00:07<01:19,  4.72it/s]  4%|▍         | 15/390 [00:07<01:17,  4.81it/s]  4%|▍         | 16/390 [00:07<01:16,  4.87it/s]  4%|▍         | 17/390 [00:07<01:15,  4.92it/s]  5%|▍         | 18/390 [00:08<01:15,  4.96it/s]  5%|▍         | 19/390 [00:08<01:14,  4.98it/s]  5%|▌         | 20/390 [00:08<01:13,  5.00it/s]  5%|▌         | 21/390 [00:08<01:13,  5.01it/s]  6%|▌         | 22/390 [00:08<01:13,  5.02it/s]  6%|▌         | 23/390 [00:09<01:13,  5.02it/s]  6%|▌         | 24/390 [00:09<01:12,  5.03it/s]  6%|▋         | 25/390 [00:09<01:12,  5.03it/s]  7%|▋         | 26/390 [00:09<01:12,  5.01it/s]  7%|▋         | 27/390 [00:09<01:12,  5.02it/s]  7%|▋         | 28/390 [00:10<01:12,  5.02it/s]  7%|▋         | 29/390 [00:10<01:11,  5.02it/s]  8%|▊         | 30/390 [00:10<01:11,  5.03it/s]  8%|▊         | 31/390 [00:10<01:11,  5.03it/s]  8%|▊         | 32/390 [00:10<01:11,  5.03it/s]  8%|▊         | 33/390 [00:11<01:10,  5.03it/s]  9%|▊         | 34/390 [00:11<01:10,  5.04it/s]  9%|▉         | 35/390 [00:11<01:10,  5.03it/s]  9%|▉         | 36/390 [00:11<01:10,  5.03it/s]  9%|▉         | 37/390 [00:11<01:10,  5.03it/s] 10%|▉         | 38/390 [00:12<01:09,  5.03it/s] 10%|█         | 39/390 [00:12<01:08,  5.09it/s] 10%|█         | 40/390 [00:12<01:09,  5.07it/s] 11%|█         | 41/390 [00:12<01:08,  5.06it/s] 11%|█         | 42/390 [00:12<01:08,  5.05it/s] 11%|█         | 43/390 [00:13<01:08,  5.05it/s] 11%|█▏        | 44/390 [00:13<01:08,  5.05it/s] 12%|█▏        | 45/390 [00:13<01:08,  5.04it/s] 12%|█▏        | 46/390 [00:13<01:08,  5.04it/s] 12%|█▏        | 47/390 [00:13<01:08,  5.04it/s] 12%|█▏        | 48/390 [00:14<01:07,  5.04it/s] 13%|█▎        | 49/390 [00:14<01:16,  4.47it/s] 13%|█▎        | 50/390 [00:14<01:13,  4.63it/s] 13%|█▎        | 51/390 [00:14<01:11,  4.75it/s] 13%|█▎        | 52/390 [00:14<01:09,  4.83it/s] 14%|█▎        | 53/390 [00:15<01:08,  4.89it/s] 14%|█▍        | 54/390 [00:15<01:08,  4.94it/s] 14%|█▍        | 55/390 [00:15<01:07,  4.97it/s] 14%|█▍        | 56/390 [00:15<01:06,  4.99it/s] 15%|█▍        | 57/390 [00:15<01:06,  5.01it/s] 15%|█▍        | 58/390 [00:16<01:06,  5.02it/s] 15%|█▌        | 59/390 [00:16<01:05,  5.03it/s] 15%|█▌        | 60/390 [00:16<01:05,  5.03it/s] 16%|█▌        | 61/390 [00:16<01:05,  5.04it/s] 16%|█▌        | 62/390 [00:16<01:05,  5.04it/s] 16%|█▌        | 63/390 [00:17<01:04,  5.04it/s] 16%|█▋        | 64/390 [00:17<01:04,  5.04it/s] 17%|█▋        | 65/390 [00:17<01:04,  5.04it/s] 17%|█▋        | 66/390 [00:17<01:04,  5.04it/s] 17%|█▋        | 67/390 [00:17<01:04,  5.04it/s] 17%|█▋        | 68/390 [00:18<01:03,  5.04it/s] 18%|█▊        | 69/390 [00:18<01:03,  5.04it/s] 18%|█▊        | 70/390 [00:18<01:03,  5.04it/s] 18%|█▊        | 71/390 [00:18<01:03,  5.04it/s] 18%|█▊        | 72/390 [00:18<01:03,  5.04it/s] 19%|█▊        | 73/390 [00:19<01:02,  5.04it/s] 19%|█▉        | 74/390 [00:19<01:02,  5.04it/s] 19%|█▉        | 75/390 [00:19<01:02,  5.04it/s] 19%|█▉        | 76/390 [00:19<01:02,  5.04it/s] 20%|█▉        | 77/390 [00:19<01:02,  5.04it/s] 20%|██        | 78/390 [00:20<01:01,  5.10it/s] 20%|██        | 79/390 [00:20<01:01,  5.08it/s] 21%|██        | 80/390 [00:20<01:01,  5.06it/s] 21%|██        | 81/390 [00:20<01:01,  5.05it/s] 21%|██        | 82/390 [00:20<01:00,  5.05it/s] 21%|██▏       | 83/390 [00:21<01:00,  5.05it/s] 22%|██▏       | 84/390 [00:21<01:00,  5.04it/s] 22%|██▏       | 85/390 [00:21<01:00,  5.04it/s] 22%|██▏       | 86/390 [00:21<01:00,  5.04it/s] 22%|██▏       | 87/390 [00:21<01:00,  5.04it/s] 23%|██▎       | 88/390 [00:22<00:59,  5.04it/s] 23%|██▎       | 89/390 [00:22<00:59,  5.03it/s] 23%|██▎       | 90/390 [00:22<00:59,  5.03it/s] 23%|██▎       | 91/390 [00:22<00:59,  5.03it/s] 24%|██▎       | 92/390 [00:22<00:59,  5.04it/s] 24%|██▍       | 93/390 [00:23<00:58,  5.04it/s] 24%|██▍       | 94/390 [00:23<00:58,  5.04it/s] 24%|██▍       | 95/390 [00:23<00:58,  5.03it/s] 25%|██▍       | 96/390 [00:23<00:58,  5.03it/s] 25%|██▍       | 97/390 [00:23<00:58,  5.03it/s] 25%|██▌       | 98/390 [00:24<00:57,  5.03it/s] 25%|██▌       | 99/390 [00:24<00:57,  5.04it/s] 26%|██▌       | 100/390 [00:24<00:57,  5.04it/s] 26%|██▌       | 101/390 [00:24<00:57,  5.06it/s] 26%|██▌       | 102/390 [00:24<01:03,  4.56it/s] 26%|██▋       | 103/390 [00:25<01:00,  4.72it/s] 27%|██▋       | 104/390 [00:25<00:59,  4.83it/s] 27%|██▋       | 105/390 [00:25<00:57,  4.92it/s] 27%|██▋       | 106/390 [00:25<00:57,  4.98it/s] 27%|██▋       | 107/390 [00:25<00:56,  5.02it/s] 28%|██▊       | 108/390 [00:26<00:55,  5.05it/s] 28%|██▊       | 109/390 [00:26<00:55,  5.07it/s] 28%|██▊       | 110/390 [00:26<00:55,  5.09it/s] 28%|██▊       | 111/390 [00:26<00:54,  5.10it/s] 29%|██▊       | 112/390 [00:26<00:54,  5.11it/s] 29%|██▉       | 113/390 [00:27<00:54,  5.11it/s] 29%|██▉       | 114/390 [00:27<00:53,  5.12it/s] 29%|██▉       | 115/390 [00:27<00:53,  5.12it/s] 30%|██▉       | 116/390 [00:27<00:53,  5.12it/s] 30%|███       | 117/390 [00:27<00:52,  5.18it/s] 30%|███       | 118/390 [00:28<00:52,  5.17it/s] 31%|███       | 119/390 [00:28<00:52,  5.15it/s] 31%|███       | 120/390 [00:28<00:52,  5.15it/s] 31%|███       | 121/390 [00:28<00:52,  5.14it/s] 31%|███▏      | 122/390 [00:28<00:52,  5.14it/s] 32%|███▏      | 123/390 [00:28<00:51,  5.14it/s] 32%|███▏      | 124/390 [00:29<00:51,  5.13it/s] 32%|███▏      | 125/390 [00:29<00:51,  5.13it/s] 32%|███▏      | 126/390 [00:29<00:51,  5.13it/s] 33%|███▎      | 127/390 [00:29<00:51,  5.13it/s] 33%|███▎      | 128/390 [00:29<00:51,  5.13it/s] 33%|███▎      | 129/390 [00:30<00:50,  5.13it/s] 33%|███▎      | 130/390 [00:30<00:50,  5.13it/s] 34%|███▎      | 131/390 [00:30<00:50,  5.13it/s] 34%|███▍      | 132/390 [00:30<00:50,  5.13it/s] 34%|███▍      | 133/390 [00:30<00:50,  5.13it/s] 34%|███▍      | 134/390 [00:31<00:49,  5.13it/s] 35%|███▍      | 135/390 [00:31<00:49,  5.13it/s] 35%|███▍      | 136/390 [00:31<00:49,  5.13it/s] 35%|███▌      | 137/390 [00:31<00:49,  5.13it/s] 35%|███▌      | 138/390 [00:31<00:49,  5.13it/s] 36%|███▌      | 139/390 [00:32<00:48,  5.13it/s] 36%|███▌      | 140/390 [00:32<00:48,  5.13it/s] 36%|███▌      | 141/390 [00:32<00:48,  5.12it/s] 36%|███▋      | 142/390 [00:32<00:48,  5.12it/s] 37%|███▋      | 143/390 [00:32<00:48,  5.12it/s] 37%|███▋      | 144/390 [00:33<00:48,  5.12it/s] 37%|███▋      | 145/390 [00:33<00:47,  5.12it/s] 37%|███▋      | 146/390 [00:33<00:47,  5.11it/s] 38%|███▊      | 147/390 [00:33<00:47,  5.12it/s] 38%|███▊      | 148/390 [00:33<00:47,  5.12it/s] 38%|███▊      | 149/390 [00:34<00:47,  5.11it/s] 38%|███▊      | 150/390 [00:34<00:46,  5.11it/s] 39%|███▊      | 151/390 [00:34<00:46,  5.11it/s] 39%|███▉      | 152/390 [00:34<00:46,  5.11it/s] 39%|███▉      | 153/390 [00:34<00:46,  5.11it/s] 39%|███▉      | 154/390 [00:35<00:46,  5.11it/s] 40%|███▉      | 155/390 [00:35<00:46,  5.10it/s] 40%|████      | 156/390 [00:35<00:45,  5.15it/s] 40%|████      | 157/390 [00:35<00:45,  5.14it/s] 41%|████      | 158/390 [00:35<00:45,  5.14it/s] 41%|████      | 159/390 [00:36<00:45,  5.13it/s] 41%|████      | 160/390 [00:36<00:44,  5.13it/s] 41%|████▏     | 161/390 [00:36<00:44,  5.13it/s] 42%|████▏     | 162/390 [00:36<00:44,  5.13it/s] 42%|████▏     | 163/390 [00:36<00:44,  5.13it/s] 42%|████▏     | 164/390 [00:36<00:44,  5.12it/s] 42%|████▏     | 165/390 [00:37<00:43,  5.12it/s] 43%|████▎     | 166/390 [00:37<00:43,  5.13it/s] 43%|████▎     | 167/390 [00:37<00:43,  5.13it/s] 43%|████▎     | 168/390 [00:37<00:43,  5.13it/s] 43%|████▎     | 169/390 [00:37<00:43,  5.13it/s] 44%|████▎     | 170/390 [00:38<00:42,  5.13it/s] 44%|████▍     | 171/390 [00:38<00:42,  5.13it/s] 44%|████▍     | 172/390 [00:38<00:42,  5.13it/s] 44%|████▍     | 173/390 [00:38<00:42,  5.13it/s] 45%|████▍     | 174/390 [00:38<00:42,  5.13it/s] 45%|████▍     | 175/390 [00:39<00:41,  5.13it/s] 45%|████▌     | 176/390 [00:39<00:41,  5.13it/s] 45%|████▌     | 177/390 [00:39<00:41,  5.13it/s] 46%|████▌     | 178/390 [00:39<00:41,  5.13it/s] 46%|████▌     | 179/390 [00:39<00:41,  5.13it/s] 46%|████▌     | 180/390 [00:40<00:40,  5.13it/s] 46%|████▋     | 181/390 [00:40<00:40,  5.13it/s] 47%|████▋     | 182/390 [00:40<00:40,  5.13it/s] 47%|████▋     | 183/390 [00:40<00:40,  5.13it/s] 47%|████▋     | 184/390 [00:40<00:40,  5.13it/s] 47%|████▋     | 185/390 [00:41<00:39,  5.13it/s] 48%|████▊     | 186/390 [00:41<00:39,  5.12it/s] 48%|████▊     | 187/390 [00:41<00:39,  5.12it/s] 48%|████▊     | 188/390 [00:41<00:39,  5.13it/s] 48%|████▊     | 189/390 [00:41<00:39,  5.12it/s] 49%|████▊     | 190/390 [00:42<00:39,  5.13it/s] 49%|████▉     | 191/390 [00:42<00:38,  5.13it/s] 49%|████▉     | 192/390 [00:42<00:38,  5.12it/s] 49%|████▉     | 193/390 [00:42<00:38,  5.12it/s] 50%|████▉     | 194/390 [00:42<00:38,  5.12it/s] 50%|█████     | 195/390 [00:43<00:37,  5.18it/s] 50%|█████     | 196/390 [00:43<00:37,  5.16it/s] 51%|█████     | 197/390 [00:43<00:37,  5.15it/s] 51%|█████     | 198/390 [00:43<00:37,  5.15it/s] 51%|█████     | 199/390 [00:43<00:41,  4.61it/s] 51%|█████▏    | 200/390 [00:44<00:40,  4.75it/s] 52%|█████▏    | 201/390 [00:44<00:38,  4.86it/s] 52%|█████▏    | 202/390 [00:44<00:38,  4.93it/s] 52%|█████▏    | 203/390 [00:44<00:37,  4.99it/s] 52%|█████▏    | 204/390 [00:44<00:36,  5.03it/s] 53%|█████▎    | 205/390 [00:45<00:36,  5.06it/s] 53%|█████▎    | 206/390 [00:45<00:36,  5.08it/s] 53%|█████▎    | 207/390 [00:45<00:35,  5.09it/s] 53%|█████▎    | 208/390 [00:45<00:35,  5.10it/s] 54%|█████▎    | 209/390 [00:45<00:35,  5.11it/s] 54%|█████▍    | 210/390 [00:46<00:35,  5.11it/s] 54%|█████▍    | 211/390 [00:46<00:34,  5.11it/s] 54%|█████▍    | 212/390 [00:46<00:34,  5.12it/s] 55%|█████▍    | 213/390 [00:46<00:34,  5.12it/s] 55%|█████▍    | 214/390 [00:46<00:34,  5.12it/s] 55%|█████▌    | 215/390 [00:47<00:34,  5.12it/s] 55%|█████▌    | 216/390 [00:47<00:33,  5.12it/s] 56%|█████▌    | 217/390 [00:47<00:33,  5.13it/s] 56%|█████▌    | 218/390 [00:47<00:33,  5.13it/s] 56%|█████▌    | 219/390 [00:47<00:33,  5.13it/s] 56%|█████▋    | 220/390 [00:47<00:33,  5.13it/s] 57%|█████▋    | 221/390 [00:48<00:33,  5.12it/s] 57%|█████▋    | 222/390 [00:48<00:32,  5.12it/s] 57%|█████▋    | 223/390 [00:48<00:32,  5.12it/s] 57%|█████▋    | 224/390 [00:48<00:32,  5.12it/s] 58%|█████▊    | 225/390 [00:48<00:32,  5.13it/s] 58%|█████▊    | 226/390 [00:49<00:31,  5.13it/s] 58%|█████▊    | 227/390 [00:49<00:31,  5.13it/s] 58%|█████▊    | 228/390 [00:49<00:31,  5.13it/s] 59%|█████▊    | 229/390 [00:49<00:31,  5.10it/s] 59%|█████▉    | 230/390 [00:49<00:31,  5.11it/s] 59%|█████▉    | 231/390 [00:50<00:31,  5.12it/s] 59%|█████▉    | 232/390 [00:50<00:30,  5.12it/s] 60%|█████▉    | 233/390 [00:50<00:30,  5.12it/s] 60%|██████    | 234/390 [00:50<00:30,  5.18it/s] 60%|██████    | 235/390 [00:50<00:30,  5.16it/s] 61%|██████    | 236/390 [00:51<00:29,  5.14it/s] 61%|██████    | 237/390 [00:51<00:29,  5.14it/s] 61%|██████    | 238/390 [00:51<00:29,  5.13it/s] 61%|██████▏   | 239/390 [00:51<00:29,  5.13it/s] 62%|██████▏   | 240/390 [00:51<00:29,  5.13it/s] 62%|██████▏   | 241/390 [00:52<00:29,  5.13it/s] 62%|██████▏   | 242/390 [00:52<00:28,  5.12it/s] 62%|██████▏   | 243/390 [00:52<00:28,  5.13it/s] 63%|██████▎   | 244/390 [00:52<00:28,  5.12it/s] 63%|██████▎   | 245/390 [00:52<00:28,  5.12it/s] 63%|██████▎   | 246/390 [00:53<00:28,  5.10it/s] 63%|██████▎   | 247/390 [00:53<00:27,  5.11it/s] 64%|██████▎   | 248/390 [00:53<00:27,  5.11it/s] 64%|██████▍   | 249/390 [00:53<00:27,  5.12it/s] 64%|██████▍   | 250/390 [00:53<00:27,  5.12it/s] 64%|██████▍   | 251/390 [00:54<00:27,  5.12it/s] 65%|██████▍   | 252/390 [00:54<00:30,  4.58it/s] 65%|██████▍   | 253/390 [00:54<00:28,  4.73it/s] 65%|██████▌   | 254/390 [00:54<00:28,  4.84it/s] 65%|██████▌   | 255/390 [00:54<00:27,  4.92it/s] 66%|██████▌   | 256/390 [00:55<00:26,  4.98it/s] 66%|██████▌   | 257/390 [00:55<00:26,  5.02it/s] 66%|██████▌   | 258/390 [00:55<00:26,  5.05it/s] 66%|██████▋   | 259/390 [00:55<00:25,  5.07it/s] 67%|██████▋   | 260/390 [00:55<00:25,  5.08it/s] 67%|██████▋   | 261/390 [00:56<00:25,  5.09it/s] 67%|██████▋   | 262/390 [00:56<00:25,  5.10it/s] 67%|██████▋   | 263/390 [00:56<00:24,  5.11it/s] 68%|██████▊   | 264/390 [00:56<00:24,  5.11it/s] 68%|██████▊   | 265/390 [00:56<00:24,  5.11it/s] 68%|██████▊   | 266/390 [00:57<00:24,  5.11it/s] 68%|██████▊   | 267/390 [00:57<00:24,  5.12it/s] 69%|██████▊   | 268/390 [00:57<00:23,  5.12it/s] 69%|██████▉   | 269/390 [00:57<00:23,  5.12it/s] 69%|██████▉   | 270/390 [00:57<00:23,  5.12it/s] 69%|██████▉   | 271/390 [00:58<00:23,  5.12it/s] 70%|██████▉   | 272/390 [00:58<00:23,  5.12it/s] 70%|███████   | 273/390 [00:58<00:22,  5.18it/s] 70%|███████   | 274/390 [00:58<00:22,  5.16it/s] 71%|███████   | 275/390 [00:58<00:22,  5.15it/s] 71%|███████   | 276/390 [00:58<00:22,  5.14it/s] 71%|███████   | 277/390 [00:59<00:22,  5.13it/s] 71%|███████▏  | 278/390 [00:59<00:21,  5.13it/s] 72%|███████▏  | 279/390 [00:59<00:21,  5.13it/s] 72%|███████▏  | 280/390 [00:59<00:21,  5.13it/s] 72%|███████▏  | 281/390 [00:59<00:21,  5.13it/s] 72%|███████▏  | 282/390 [01:00<00:21,  5.12it/s] 73%|███████▎  | 283/390 [01:00<00:20,  5.12it/s] 73%|███████▎  | 284/390 [01:00<00:20,  5.12it/s] 73%|███████▎  | 285/390 [01:00<00:20,  5.12it/s] 73%|███████▎  | 286/390 [01:00<00:20,  5.12it/s] 74%|███████▎  | 287/390 [01:01<00:20,  5.12it/s] 74%|███████▍  | 288/390 [01:01<00:19,  5.12it/s] 74%|███████▍  | 289/390 [01:01<00:19,  5.12it/s] 74%|███████▍  | 290/390 [01:01<00:19,  5.12it/s] 75%|███████▍  | 291/390 [01:01<00:19,  5.12it/s] 75%|███████▍  | 292/390 [01:02<00:19,  5.12it/s] 75%|███████▌  | 293/390 [01:02<00:18,  5.12it/s] 75%|███████▌  | 294/390 [01:02<00:18,  5.12it/s] 76%|███████▌  | 295/390 [01:02<00:18,  5.12it/s] 76%|███████▌  | 296/390 [01:02<00:18,  5.12it/s] 76%|███████▌  | 297/390 [01:03<00:18,  5.12it/s] 76%|███████▋  | 298/390 [01:03<00:17,  5.12it/s] 77%|███████▋  | 299/390 [01:03<00:17,  5.12it/s] 77%|███████▋  | 300/390 [01:03<00:17,  5.13it/s] 77%|███████▋  | 301/390 [01:03<00:17,  5.13it/s] 77%|███████▋  | 302/390 [01:04<00:17,  5.12it/s] 78%|███████▊  | 303/390 [01:04<00:16,  5.13it/s] 78%|███████▊  | 304/390 [01:04<00:16,  5.13it/s] 78%|███████▊  | 305/390 [01:04<00:16,  5.13it/s] 78%|███████▊  | 306/390 [01:04<00:16,  5.13it/s] 79%|███████▊  | 307/390 [01:05<00:16,  5.13it/s] 79%|███████▉  | 308/390 [01:05<00:15,  5.13it/s] 79%|███████▉  | 309/390 [01:05<00:15,  5.13it/s] 79%|███████▉  | 310/390 [01:05<00:15,  5.13it/s] 80%|███████▉  | 311/390 [01:05<00:15,  5.13it/s] 80%|████████  | 312/390 [01:06<00:15,  5.19it/s] 80%|████████  | 313/390 [01:06<00:14,  5.16it/s] 81%|████████  | 314/390 [01:06<00:14,  5.16it/s] 81%|████████  | 315/390 [01:06<00:14,  5.15it/s] 81%|████████  | 316/390 [01:06<00:14,  5.14it/s] 81%|████████▏ | 317/390 [01:06<00:14,  5.13it/s] 82%|████████▏ | 318/390 [01:07<00:14,  5.13it/s] 82%|████████▏ | 319/390 [01:07<00:13,  5.13it/s] 82%|████████▏ | 320/390 [01:07<00:13,  5.13it/s] 82%|████████▏ | 321/390 [01:07<00:13,  5.13it/s] 83%|████████▎ | 322/390 [01:07<00:13,  5.13it/s] 83%|████████▎ | 323/390 [01:08<00:13,  5.13it/s] 83%|████████▎ | 324/390 [01:08<00:12,  5.13it/s] 83%|████████▎ | 325/390 [01:08<00:12,  5.12it/s] 84%|████████▎ | 326/390 [01:08<00:12,  5.12it/s] 84%|████████▍ | 327/390 [01:08<00:12,  5.12it/s] 84%|████████▍ | 328/390 [01:09<00:12,  5.13it/s] 84%|████████▍ | 329/390 [01:09<00:11,  5.12it/s] 85%|████████▍ | 330/390 [01:09<00:11,  5.13it/s] 85%|████████▍ | 331/390 [01:09<00:11,  5.13it/s] 85%|████████▌ | 332/390 [01:09<00:11,  5.13it/s] 85%|████████▌ | 333/390 [01:10<00:11,  5.13it/s] 86%|████████▌ | 334/390 [01:10<00:10,  5.12it/s] 86%|████████▌ | 335/390 [01:10<00:10,  5.12it/s] 86%|████████▌ | 336/390 [01:10<00:10,  5.12it/s] 86%|████████▋ | 337/390 [01:10<00:10,  5.12it/s] 87%|████████▋ | 338/390 [01:11<00:10,  5.12it/s] 87%|████████▋ | 339/390 [01:11<00:09,  5.12it/s] 87%|████████▋ | 340/390 [01:11<00:09,  5.12it/s] 87%|████████▋ | 341/390 [01:11<00:09,  5.12it/s] 88%|████████▊ | 342/390 [01:11<00:09,  5.12it/s] 88%|████████▊ | 343/390 [01:12<00:09,  5.12it/s] 88%|████████▊ | 344/390 [01:12<00:08,  5.12it/s] 88%|████████▊ | 345/390 [01:12<00:08,  5.12it/s] 89%|████████▊ | 346/390 [01:12<00:08,  5.12it/s] 89%|████████▉ | 347/390 [01:12<00:08,  5.12it/s] 89%|████████▉ | 348/390 [01:13<00:08,  5.12it/s] 89%|████████▉ | 349/390 [01:13<00:08,  4.58it/s] 90%|████████▉ | 350/390 [01:13<00:08,  4.73it/s] 90%|█████████ | 351/390 [01:13<00:07,  4.89it/s] 90%|█████████ | 352/390 [01:13<00:07,  4.96it/s] 91%|█████████ | 353/390 [01:14<00:07,  5.01it/s] 91%|█████████ | 354/390 [01:14<00:07,  5.04it/s] 91%|█████████ | 355/390 [01:14<00:06,  5.07it/s] 91%|█████████▏| 356/390 [01:14<00:06,  5.09it/s] 92%|█████████▏| 357/390 [01:14<00:06,  5.10it/s] 92%|█████████▏| 358/390 [01:15<00:06,  5.11it/s] 92%|█████████▏| 359/390 [01:15<00:06,  5.12it/s] 92%|█████████▏| 360/390 [01:15<00:05,  5.12it/s] 93%|█████████▎| 361/390 [01:15<00:05,  5.12it/s] 93%|█████████▎| 362/390 [01:15<00:05,  5.12it/s] 93%|█████████▎| 363/390 [01:16<00:05,  5.12it/s] 93%|█████████▎| 364/390 [01:16<00:05,  5.12it/s] 94%|█████████▎| 365/390 [01:16<00:04,  5.13it/s] 94%|█████████▍| 366/390 [01:16<00:04,  5.13it/s] 94%|█████████▍| 367/390 [01:16<00:04,  5.13it/s] 94%|█████████▍| 368/390 [01:16<00:04,  5.13it/s] 95%|█████████▍| 369/390 [01:17<00:04,  5.13it/s] 95%|█████████▍| 370/390 [01:17<00:03,  5.13it/s] 95%|█████████▌| 371/390 [01:17<00:03,  5.12it/s] 95%|█████████▌| 372/390 [01:17<00:03,  5.13it/s] 96%|█████████▌| 373/390 [01:17<00:03,  5.13it/s] 96%|█████████▌| 374/390 [01:18<00:03,  5.13it/s] 96%|█████████▌| 375/390 [01:18<00:02,  5.13it/s] 96%|█████████▋| 376/390 [01:18<00:02,  5.13it/s] 97%|█████████▋| 377/390 [01:18<00:02,  5.13it/s] 97%|█████████▋| 378/390 [01:18<00:02,  5.13it/s] 97%|█████████▋| 379/390 [01:19<00:02,  5.13it/s] 97%|█████████▋| 380/390 [01:19<00:01,  5.13it/s] 98%|█████████▊| 381/390 [01:19<00:01,  5.13it/s] 98%|█████████▊| 382/390 [01:19<00:01,  5.13it/s] 98%|█████████▊| 383/390 [01:19<00:01,  5.13it/s] 98%|█████████▊| 384/390 [01:20<00:01,  5.13it/s] 99%|█████████▊| 385/390 [01:20<00:00,  5.13it/s] 99%|█████████▉| 386/390 [01:20<00:00,  5.13it/s] 99%|█████████▉| 387/390 [01:20<00:00,  5.13it/s] 99%|█████████▉| 388/390 [01:20<00:00,  5.13it/s]100%|█████████▉| 389/390 [01:21<00:00,  5.13it/s]100%|██████████| 390/390 [01:21<00:00,  5.19it/s][INFO|trainer.py:1679] 2022-08-27 22:02:49,350 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


                                                 100%|██████████| 390/390 [01:21<00:00,  5.19it/s]100%|██████████| 390/390 [01:21<00:00,  4.80it/s]
[INFO|trainer.py:2409] 2022-08-27 22:02:49,351 >> Saving model checkpoint to tmp/baseline/RTE/bert-base-uncased/HPO_S0/1e-06/64/10
[INFO|configuration_utils.py:446] 2022-08-27 22:02:49,352 >> Configuration saved in tmp/baseline/RTE/bert-base-uncased/HPO_S0/1e-06/64/10/config.json
[INFO|modeling_utils.py:1602] 2022-08-27 22:02:50,165 >> Model weights saved in tmp/baseline/RTE/bert-base-uncased/HPO_S0/1e-06/64/10/pytorch_model.bin
[INFO|tokenization_utils_base.py:2108] 2022-08-27 22:02:50,165 >> tokenizer config file saved in tmp/baseline/RTE/bert-base-uncased/HPO_S0/1e-06/64/10/tokenizer_config.json
[INFO|tokenization_utils_base.py:2114] 2022-08-27 22:02:50,166 >> Special tokens file saved in tmp/baseline/RTE/bert-base-uncased/HPO_S0/1e-06/64/10/special_tokens_map.json
{'train_runtime': 81.2838, 'train_samples_per_second': 306.334, 'train_steps_per_second': 4.798, 'train_loss': 0.6987437712840545, 'epoch': 10.0}
***** train metrics *****
  epoch                    =       10.0
  train_loss               =     0.6987
  train_runtime            = 0:01:21.28
  train_samples            =       2490
  train_samples_per_second =    306.334
  train_steps_per_second   =      4.798
08/27/2022 22:02:50 - INFO - __main__ - *** Evaluate ***
[INFO|trainer.py:628] 2022-08-27 22:02:50,211 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1, idx, sentence2. If sentence1, idx, sentence2 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:2659] 2022-08-27 22:02:50,214 >> ***** Running Evaluation *****
[INFO|trainer.py:2661] 2022-08-27 22:02:50,214 >>   Num examples = 277
[INFO|trainer.py:2664] 2022-08-27 22:02:50,214 >>   Batch size = 16
  0%|          | 0/18 [00:00<?, ?it/s] 17%|█▋        | 3/18 [00:00<00:00, 29.59it/s] 33%|███▎      | 6/18 [00:00<00:00, 22.90it/s] 50%|█████     | 9/18 [00:00<00:00, 21.42it/s] 67%|██████▋   | 12/18 [00:00<00:00, 20.71it/s] 83%|████████▎ | 15/18 [00:00<00:00, 20.38it/s]100%|██████████| 18/18 [00:00<00:00, 20.16it/s]08/27/2022 22:02:51 - INFO - datasets.metric - Removing /home/ubuntu/.cache/huggingface/metrics/glue/rte/default_experiment-1-0.arrow
100%|██████████| 18/18 [00:00<00:00, 20.78it/s]
***** eval metrics *****
  epoch                   =       10.0
  eval_accuracy           =     0.4729
  eval_loss               =     0.6939
  eval_runtime            = 0:00:00.91
  eval_samples            =        277
  eval_samples_per_second =    301.148
  eval_steps_per_second   =     19.569
08/27/2022 22:02:54 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 2distributed training: False, 16-bits training: True
08/27/2022 22:02:54 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=2,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=IntervalStrategy.NO,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_min_num_params=0,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=1e-06,
length_column_name=length,
load_best_model_at_end=False,
local_rank=-1,
log_level=-1,
log_level_replica=-1,
log_on_each_node=True,
logging_dir=tmp/baseline/RTE/bert-base-uncased/HPO_S0/1e-06/64/30/runs/Aug27_22-02-54_ip-10-21-39-60,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
no_cuda=False,
num_train_epochs=30.0,
optim=OptimizerNames.ADAMW_HF,
output_dir=tmp/baseline/RTE/bert-base-uncased/HPO_S0/1e-06/64/30,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=tmp/baseline/RTE/bert-base-uncased/HPO_S0/1e-06/64/30,
save_on_each_node=False,
save_steps=500,
save_strategy=IntervalStrategy.STEPS,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tf32=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
xpu_backend=None,
)
08/27/2022 22:02:55 - INFO - datasets.info - Loading Dataset Infos from /home/ubuntu/.cache/huggingface/modules/datasets_modules/datasets/glue/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad
08/27/2022 22:02:55 - INFO - datasets.builder - Overwrite dataset info from restored data version.
08/27/2022 22:02:55 - INFO - datasets.info - Loading Dataset info from /home/ubuntu/.cache/huggingface/datasets/glue/rte/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad
08/27/2022 22:02:55 - WARNING - datasets.builder - Reusing dataset glue (/home/ubuntu/.cache/huggingface/datasets/glue/rte/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)
08/27/2022 22:02:55 - INFO - datasets.info - Loading Dataset info from /home/ubuntu/.cache/huggingface/datasets/glue/rte/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 529.25it/s]
[INFO|configuration_utils.py:657] 2022-08-27 22:02:55,061 >> loading configuration file /home/ubuntu/checkpoints/exp/RTE/config.json
[INFO|configuration_utils.py:708] 2022-08-27 22:02:55,062 >> Model config BertConfig {
  "_name_or_path": "/home/ubuntu/checkpoints/exp/RTE",
  "architectures": [
    "BertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "crypten": false,
  "finetuning_task": "rte",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "entailment",
    "1": "not_entailment"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "entailment": 0,
    "not_entailment": 1
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "softmax_act": "softmax",
  "torch_dtype": "float32",
  "transformers_version": "4.20.0.dev0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|tokenization_utils_base.py:1701] 2022-08-27 22:02:55,063 >> Didn't find file /home/ubuntu/checkpoints/exp/RTE/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1779] 2022-08-27 22:02:55,063 >> loading file /home/ubuntu/checkpoints/exp/RTE/vocab.txt
[INFO|tokenization_utils_base.py:1779] 2022-08-27 22:02:55,063 >> loading file /home/ubuntu/checkpoints/exp/RTE/tokenizer.json
[INFO|tokenization_utils_base.py:1779] 2022-08-27 22:02:55,063 >> loading file None
[INFO|tokenization_utils_base.py:1779] 2022-08-27 22:02:55,063 >> loading file /home/ubuntu/checkpoints/exp/RTE/special_tokens_map.json
[INFO|tokenization_utils_base.py:1779] 2022-08-27 22:02:55,063 >> loading file /home/ubuntu/checkpoints/exp/RTE/tokenizer_config.json
[INFO|modeling_utils.py:2047] 2022-08-27 22:02:55,106 >> loading weights file /home/ubuntu/checkpoints/exp/RTE/pytorch_model.bin
[INFO|modeling_bert.py:194] 2022-08-27 22:02:55,431 >> initializing embedding using nn.Embedding
[INFO|modeling_utils.py:2417] 2022-08-27 22:02:56,555 >> All model checkpoint weights were used when initializing BertForSequenceClassification.

[INFO|modeling_utils.py:2426] 2022-08-27 22:02:56,555 >> All the weights of BertForSequenceClassification were initialized from the model checkpoint at /home/ubuntu/checkpoints/exp/RTE.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.
using model config: BertConfig {
  "_name_or_path": "/home/ubuntu/checkpoints/exp/RTE",
  "architectures": [
    "BertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "crypten": false,
  "finetuning_task": "rte",
  "gradient_checkpointing": false,
  "hidden_act": "quad",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "entailment",
    "1": "not_entailment"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "entailment": 0,
    "not_entailment": 1
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "softmax_act": "2quad",
  "torch_dtype": "float32",
  "transformers_version": "4.20.0.dev0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

BertConfig {
  "_name_or_path": "/home/ubuntu/checkpoints/exp/RTE",
  "architectures": [
    "BertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "crypten": false,
  "finetuning_task": "rte",
  "gradient_checkpointing": false,
  "hidden_act": "quad",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "entailment",
    "1": "not_entailment"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "entailment": 0,
    "not_entailment": 1
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "softmax_act": "2quad",
  "torch_dtype": "float32",
  "transformers_version": "4.20.0.dev0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

model architecture: BertForSequenceClassification(
  (bert): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): QuadActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): QuadActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): QuadActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): QuadActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): QuadActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): QuadActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): QuadActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): QuadActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): QuadActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): QuadActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): QuadActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): QuadActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (dropout): Dropout(p=0.1, inplace=False)
  (classifier): Linear(in_features=768, out_features=2, bias=True)
)
08/27/2022 22:02:56 - WARNING - datasets.fingerprint - Parameter 'function'=<function main.<locals>.preprocess_function at 0x7fddf89cd488> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
Running tokenizer on dataset:   0%|          | 0/3 [00:00<?, ?ba/s]08/27/2022 22:02:56 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/ubuntu/.cache/huggingface/datasets/glue/rte/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-1c80317fa3b1799d.arrow
Running tokenizer on dataset:  33%|███▎      | 1/3 [00:00<00:00,  7.79ba/s]Running tokenizer on dataset:  67%|██████▋   | 2/3 [00:00<00:00,  5.46ba/s]Running tokenizer on dataset: 100%|██████████| 3/3 [00:00<00:00,  7.61ba/s]08/27/2022 22:02:57 - INFO - datasets.fingerprint - Parameter 'function'=<function main.<locals>.preprocess_function at 0x7fddf89c9950> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead.

Running tokenizer on dataset:   0%|          | 0/1 [00:00<?, ?ba/s]08/27/2022 22:02:57 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/ubuntu/.cache/huggingface/datasets/glue/rte/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-bdd640fb06671ad1.arrow
Running tokenizer on dataset: 100%|██████████| 1/1 [00:00<00:00, 35.19ba/s]08/27/2022 22:02:57 - INFO - datasets.fingerprint - Parameter 'function'=<function main.<locals>.preprocess_function at 0x7fddf89cd840> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead.

Running tokenizer on dataset:   0%|          | 0/3 [00:00<?, ?ba/s]08/27/2022 22:02:57 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/ubuntu/.cache/huggingface/datasets/glue/rte/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-3eb13b9046685257.arrow
Running tokenizer on dataset:  67%|██████▋   | 2/3 [00:00<00:00, 12.41ba/s]Running tokenizer on dataset: 100%|██████████| 3/3 [00:00<00:00, 11.40ba/s]128
08/27/2022 22:03:02 - INFO - __main__ - Sample 914 of the training set: {'sentence1': "Because of Reagan's economic strategy, the federal budget deficit ballooned.", 'sentence2': "Reagan's economic strategy led to huge federal budget deficits.", 'label': 0, 'idx': 914, 'input_ids': [101, 2138, 1997, 11531, 1005, 1055, 3171, 5656, 1010, 1996, 2976, 5166, 15074, 13212, 2098, 1012, 102, 11531, 1005, 1055, 3171, 5656, 2419, 2000, 4121, 2976, 5166, 15074, 2015, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.
128
08/27/2022 22:03:07 - INFO - __main__ - Sample 571 of the training set: {'sentence1': 'So far the British have preferred a policy of a voluntary approach to restricting advertising and high taxes on tobacco products.', 'sentence2': 'Sales have declined due to restrictions on advertising.', 'label': 1, 'idx': 571, 'input_ids': [101, 2061, 2521, 1996, 2329, 2031, 6871, 1037, 3343, 1997, 1037, 10758, 3921, 2000, 26996, 6475, 1998, 2152, 7773, 2006, 9098, 3688, 1012, 102, 4341, 2031, 6430, 2349, 2000, 9259, 2006, 6475, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.
128
08/27/2022 22:03:12 - INFO - __main__ - Sample 419 of the training set: {'sentence1': 'Protesters, many from organized pro-government groups but including many ordinary citizens, carried anti-American banners and chanted slogans attacking U.N. Secretary-General Kofi Annan for his close alignment with U.S. policy.', 'sentence2': 'Protesters confiscated anti-American banners and chanted slogans attacking U.N. Secretary-General Kofi Annan for his close alignment with U.S. policy.', 'label': 1, 'idx': 419, 'input_ids': [101, 13337, 1010, 2116, 2013, 4114, 4013, 1011, 2231, 2967, 2021, 2164, 2116, 6623, 4480, 1010, 3344, 3424, 1011, 2137, 23562, 1998, 16883, 2098, 14558, 2015, 7866, 1057, 1012, 1050, 1012, 3187, 1011, 2236, 12849, 8873, 4698, 2078, 2005, 2010, 2485, 12139, 2007, 1057, 1012, 1055, 1012, 3343, 1012, 102, 13337, 17182, 3424, 1011, 2137, 23562, 1998, 16883, 2098, 14558, 2015, 7866, 1057, 1012, 1050, 1012, 3187, 1011, 2236, 12849, 8873, 4698, 2078, 2005, 2010, 2485, 12139, 2007, 1057, 1012, 1055, 1012, 3343, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.

[INFO|trainer.py:506] 2022-08-27 22:03:16,923 >> Using amp half precision backend
[INFO|trainer.py:628] 2022-08-27 22:03:16,926 >> The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: idx, sentence2, sentence1. If idx, sentence2, sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.
/home/ubuntu/transformers_private/src/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  FutureWarning,
[INFO|trainer.py:1430] 2022-08-27 22:03:16,942 >> ***** Running training *****
[INFO|trainer.py:1431] 2022-08-27 22:03:16,942 >>   Num examples = 2490
[INFO|trainer.py:1432] 2022-08-27 22:03:16,942 >>   Num Epochs = 30
[INFO|trainer.py:1433] 2022-08-27 22:03:16,942 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1434] 2022-08-27 22:03:16,942 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1435] 2022-08-27 22:03:16,942 >>   Gradient Accumulation steps = 1
[INFO|trainer.py:1436] 2022-08-27 22:03:16,942 >>   Total optimization steps = 1170
using training arge: TrainingArguments(
_n_gpu=2,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=IntervalStrategy.NO,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_min_num_params=0,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=1e-06,
length_column_name=length,
load_best_model_at_end=False,
local_rank=-1,
log_level=-1,
log_level_replica=-1,
log_on_each_node=True,
logging_dir=tmp/baseline/RTE/bert-base-uncased/HPO_S0/1e-06/64/30/runs/Aug27_22-02-54_ip-10-21-39-60,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
no_cuda=False,
num_train_epochs=30.0,
optim=OptimizerNames.ADAMW_HF,
output_dir=tmp/baseline/RTE/bert-base-uncased/HPO_S0/1e-06/64/30,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=tmp/baseline/RTE/bert-base-uncased/HPO_S0/1e-06/64/30,
save_on_each_node=False,
save_steps=500,
save_strategy=IntervalStrategy.STEPS,
save_total_limit=0,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tf32=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
xpu_backend=None,
)
  0%|          | 0/1170 [00:00<?, ?it/s]/home/ubuntu/anaconda3/envs/transformers/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
  0%|          | 1/1170 [00:04<1:28:51,  4.56s/it]  0%|          | 2/1170 [00:04<39:17,  2.02s/it]    0%|          | 3/1170 [00:04<23:04,  1.19s/it]  0%|          | 4/1170 [00:05<15:27,  1.26it/s]  0%|          | 5/1170 [00:05<11:15,  1.73it/s]  1%|          | 6/1170 [00:05<08:43,  2.23it/s]  1%|          | 7/1170 [00:05<07:06,  2.73it/s]  1%|          | 8/1170 [00:05<06:03,  3.20it/s]  1%|          | 9/1170 [00:06<05:20,  3.62it/s]  1%|          | 10/1170 [00:06<04:51,  3.98it/s]  1%|          | 11/1170 [00:06<04:31,  4.26it/s]  1%|          | 12/1170 [00:06<04:19,  4.46it/s]  1%|          | 13/1170 [00:06<04:09,  4.63it/s]  1%|          | 14/1170 [00:07<04:02,  4.76it/s]  1%|▏         | 15/1170 [00:07<03:58,  4.85it/s]  1%|▏         | 16/1170 [00:07<03:54,  4.92it/s]  1%|▏         | 17/1170 [00:07<03:52,  4.97it/s]  2%|▏         | 18/1170 [00:07<03:50,  5.01it/s]  2%|▏         | 19/1170 [00:08<03:48,  5.03it/s]  2%|▏         | 20/1170 [00:08<03:48,  5.03it/s]  2%|▏         | 21/1170 [00:08<03:47,  5.04it/s]  2%|▏         | 22/1170 [00:08<03:46,  5.06it/s]  2%|▏         | 23/1170 [00:08<03:46,  5.07it/s]  2%|▏         | 24/1170 [00:09<03:46,  5.07it/s]  2%|▏         | 25/1170 [00:09<03:45,  5.07it/s]  2%|▏         | 26/1170 [00:09<03:45,  5.08it/s]  2%|▏         | 27/1170 [00:09<03:44,  5.08it/s]  2%|▏         | 28/1170 [00:09<03:45,  5.06it/s]  2%|▏         | 29/1170 [00:10<03:45,  5.07it/s]  3%|▎         | 30/1170 [00:10<03:44,  5.08it/s]  3%|▎         | 31/1170 [00:10<03:44,  5.08it/s]  3%|▎         | 32/1170 [00:10<03:43,  5.08it/s]  3%|▎         | 33/1170 [00:10<03:44,  5.06it/s]  3%|▎         | 34/1170 [00:11<03:44,  5.07it/s]  3%|▎         | 35/1170 [00:11<03:43,  5.07it/s]  3%|▎         | 36/1170 [00:11<03:43,  5.08it/s]  3%|▎         | 37/1170 [00:11<03:44,  5.05it/s]  3%|▎         | 38/1170 [00:11<03:43,  5.06it/s]  3%|▎         | 39/1170 [00:12<03:40,  5.13it/s]  3%|▎         | 40/1170 [00:12<03:41,  5.11it/s]  4%|▎         | 41/1170 [00:12<03:41,  5.10it/s]  4%|▎         | 42/1170 [00:12<03:42,  5.07it/s]  4%|▎         | 43/1170 [00:12<03:42,  5.07it/s]  4%|▍         | 44/1170 [00:13<03:41,  5.07it/s]  4%|▍         | 45/1170 [00:13<03:41,  5.08it/s]  4%|▍         | 46/1170 [00:13<03:41,  5.08it/s]  4%|▍         | 47/1170 [00:13<03:40,  5.08it/s]  4%|▍         | 48/1170 [00:13<03:40,  5.08it/s]  4%|▍         | 49/1170 [00:14<04:10,  4.47it/s]  4%|▍         | 50/1170 [00:14<04:01,  4.64it/s]  4%|▍         | 51/1170 [00:14<03:54,  4.77it/s]  4%|▍         | 52/1170 [00:14<03:50,  4.86it/s]  5%|▍         | 53/1170 [00:14<03:46,  4.92it/s]  5%|▍         | 54/1170 [00:15<03:44,  4.97it/s]  5%|▍         | 55/1170 [00:15<03:42,  5.01it/s]  5%|▍         | 56/1170 [00:15<03:41,  5.03it/s]  5%|▍         | 57/1170 [00:15<03:40,  5.05it/s]  5%|▍         | 58/1170 [00:15<03:39,  5.06it/s]  5%|▌         | 59/1170 [00:16<03:39,  5.05it/s]  5%|▌         | 60/1170 [00:16<03:39,  5.06it/s]  5%|▌         | 61/1170 [00:16<03:38,  5.07it/s]  5%|▌         | 62/1170 [00:16<03:38,  5.07it/s]  5%|▌         | 63/1170 [00:16<03:37,  5.08it/s]  5%|▌         | 64/1170 [00:17<03:37,  5.08it/s]  6%|▌         | 65/1170 [00:17<03:37,  5.08it/s]  6%|▌         | 66/1170 [00:17<03:37,  5.08it/s]  6%|▌         | 67/1170 [00:17<03:36,  5.09it/s]  6%|▌         | 68/1170 [00:17<03:36,  5.09it/s]  6%|▌         | 69/1170 [00:18<03:36,  5.09it/s]  6%|▌         | 70/1170 [00:18<03:36,  5.08it/s]  6%|▌         | 71/1170 [00:18<03:36,  5.08it/s]  6%|▌         | 72/1170 [00:18<03:36,  5.08it/s]  6%|▌         | 73/1170 [00:18<03:35,  5.08it/s]  6%|▋         | 74/1170 [00:19<03:35,  5.08it/s]  6%|▋         | 75/1170 [00:19<03:35,  5.09it/s]  6%|▋         | 76/1170 [00:19<03:35,  5.09it/s]  7%|▋         | 77/1170 [00:19<03:34,  5.08it/s]  7%|▋         | 78/1170 [00:19<03:32,  5.14it/s]  7%|▋         | 79/1170 [00:20<03:33,  5.12it/s]  7%|▋         | 80/1170 [00:20<03:33,  5.11it/s]  7%|▋         | 81/1170 [00:20<03:33,  5.10it/s]  7%|▋         | 82/1170 [00:20<03:34,  5.08it/s]  7%|▋         | 83/1170 [00:20<03:33,  5.08it/s]  7%|▋         | 84/1170 [00:21<03:33,  5.08it/s]  7%|▋         | 85/1170 [00:21<03:33,  5.08it/s]  7%|▋         | 86/1170 [00:21<03:33,  5.09it/s]  7%|▋         | 87/1170 [00:21<03:32,  5.09it/s]  8%|▊         | 88/1170 [00:21<03:32,  5.08it/s]  8%|▊         | 89/1170 [00:22<03:32,  5.08it/s]  8%|▊         | 90/1170 [00:22<03:32,  5.09it/s]  8%|▊         | 91/1170 [00:22<03:32,  5.09it/s]  8%|▊         | 92/1170 [00:22<03:31,  5.09it/s]  8%|▊         | 93/1170 [00:22<03:31,  5.09it/s]  8%|▊         | 94/1170 [00:22<03:31,  5.09it/s]  8%|▊         | 95/1170 [00:23<03:31,  5.09it/s]  8%|▊         | 96/1170 [00:23<03:30,  5.09it/s]  8%|▊         | 97/1170 [00:23<03:30,  5.09it/s]  8%|▊         | 98/1170 [00:23<03:30,  5.09it/s]  8%|▊         | 99/1170 [00:23<03:30,  5.09it/s]  9%|▊         | 100/1170 [00:24<03:30,  5.09it/s]  9%|▊         | 101/1170 [00:24<03:29,  5.09it/s]  9%|▊         | 102/1170 [00:24<03:53,  4.57it/s]  9%|▉         | 103/1170 [00:24<03:46,  4.71it/s]  9%|▉         | 104/1170 [00:25<03:41,  4.81it/s]  9%|▉         | 105/1170 [00:25<03:37,  4.89it/s]  9%|▉         | 106/1170 [00:25<03:34,  4.95it/s]  9%|▉         | 107/1170 [00:25<03:32,  4.99it/s]  9%|▉         | 108/1170 [00:25<03:31,  5.02it/s]  9%|▉         | 109/1170 [00:26<03:30,  5.04it/s]  9%|▉         | 110/1170 [00:26<03:29,  5.05it/s]  9%|▉         | 111/1170 [00:26<03:29,  5.06it/s] 10%|▉         | 112/1170 [00:26<03:28,  5.07it/s] 10%|▉         | 113/1170 [00:26<03:28,  5.07it/s] 10%|▉         | 114/1170 [00:26<03:27,  5.08it/s] 10%|▉         | 115/1170 [00:27<03:27,  5.08it/s] 10%|▉         | 116/1170 [00:27<03:27,  5.08it/s] 10%|█         | 117/1170 [00:27<03:24,  5.14it/s] 10%|█         | 118/1170 [00:27<03:25,  5.12it/s] 10%|█         | 119/1170 [00:27<03:25,  5.11it/s] 10%|█         | 120/1170 [00:28<03:25,  5.10it/s] 10%|█         | 121/1170 [00:28<03:25,  5.09it/s] 10%|█         | 122/1170 [00:28<03:25,  5.09it/s] 11%|█         | 123/1170 [00:28<03:25,  5.09it/s] 11%|█         | 124/1170 [00:28<03:25,  5.09it/s] 11%|█         | 125/1170 [00:29<03:25,  5.08it/s] 11%|█         | 126/1170 [00:29<03:25,  5.08it/s] 11%|█         | 127/1170 [00:29<03:25,  5.08it/s] 11%|█         | 128/1170 [00:29<03:24,  5.08it/s] 11%|█         | 129/1170 [00:29<03:24,  5.08it/s] 11%|█         | 130/1170 [00:30<03:24,  5.09it/s] 11%|█         | 131/1170 [00:30<03:25,  5.07it/s] 11%|█▏        | 132/1170 [00:30<03:24,  5.07it/s] 11%|█▏        | 133/1170 [00:30<03:24,  5.08it/s] 11%|█▏        | 134/1170 [00:30<03:23,  5.08it/s] 12%|█▏        | 135/1170 [00:31<03:23,  5.08it/s] 12%|█▏        | 136/1170 [00:31<03:24,  5.06it/s] 12%|█▏        | 137/1170 [00:31<03:24,  5.06it/s] 12%|█▏        | 138/1170 [00:31<03:23,  5.07it/s] 12%|█▏        | 139/1170 [00:31<03:23,  5.08it/s] 12%|█▏        | 140/1170 [00:32<03:22,  5.08it/s] 12%|█▏        | 141/1170 [00:32<03:22,  5.08it/s] 12%|█▏        | 142/1170 [00:32<03:22,  5.09it/s] 12%|█▏        | 143/1170 [00:32<03:22,  5.08it/s] 12%|█▏        | 144/1170 [00:32<03:21,  5.08it/s] 12%|█▏        | 145/1170 [00:33<03:21,  5.09it/s] 12%|█▏        | 146/1170 [00:33<03:21,  5.09it/s] 13%|█▎        | 147/1170 [00:33<03:20,  5.09it/s] 13%|█▎        | 148/1170 [00:33<03:20,  5.09it/s] 13%|█▎        | 149/1170 [00:33<03:20,  5.09it/s] 13%|█▎        | 150/1170 [00:34<03:20,  5.09it/s] 13%|█▎        | 151/1170 [00:34<03:20,  5.09it/s] 13%|█▎        | 152/1170 [00:34<03:19,  5.09it/s] 13%|█▎        | 153/1170 [00:34<03:19,  5.10it/s] 13%|█▎        | 154/1170 [00:34<03:19,  5.10it/s] 13%|█▎        | 155/1170 [00:35<03:19,  5.10it/s] 13%|█▎        | 156/1170 [00:35<03:16,  5.16it/s] 13%|█▎        | 157/1170 [00:35<03:17,  5.14it/s] 14%|█▎        | 158/1170 [00:35<03:17,  5.13it/s] 14%|█▎        | 159/1170 [00:35<03:17,  5.12it/s] 14%|█▎        | 160/1170 [00:36<03:17,  5.11it/s] 14%|█▍        | 161/1170 [00:36<03:17,  5.11it/s] 14%|█▍        | 162/1170 [00:36<03:17,  5.10it/s] 14%|█▍        | 163/1170 [00:36<03:17,  5.10it/s] 14%|█▍        | 164/1170 [00:36<03:17,  5.10it/s] 14%|█▍        | 165/1170 [00:37<03:17,  5.10it/s] 14%|█▍        | 166/1170 [00:37<03:16,  5.10it/s] 14%|█▍        | 167/1170 [00:37<03:16,  5.10it/s] 14%|█▍        | 168/1170 [00:37<03:16,  5.10it/s] 14%|█▍        | 169/1170 [00:37<03:16,  5.10it/s] 15%|█▍        | 170/1170 [00:37<03:16,  5.10it/s] 15%|█▍        | 171/1170 [00:38<03:15,  5.10it/s] 15%|█▍        | 172/1170 [00:38<03:15,  5.10it/s] 15%|█▍        | 173/1170 [00:38<03:15,  5.10it/s] 15%|█▍        | 174/1170 [00:38<03:15,  5.10it/s] 15%|█▍        | 175/1170 [00:38<03:15,  5.10it/s] 15%|█▌        | 176/1170 [00:39<03:14,  5.10it/s] 15%|█▌        | 177/1170 [00:39<03:14,  5.10it/s] 15%|█▌        | 178/1170 [00:39<03:14,  5.10it/s] 15%|█▌        | 179/1170 [00:39<03:14,  5.10it/s] 15%|█▌        | 180/1170 [00:39<03:14,  5.10it/s] 15%|█▌        | 181/1170 [00:40<03:13,  5.10it/s] 16%|█▌        | 182/1170 [00:40<03:13,  5.10it/s] 16%|█▌        | 183/1170 [00:40<03:13,  5.10it/s] 16%|█▌        | 184/1170 [00:40<03:13,  5.10it/s] 16%|█▌        | 185/1170 [00:40<03:13,  5.09it/s] 16%|█▌        | 186/1170 [00:41<03:13,  5.09it/s] 16%|█▌        | 187/1170 [00:41<03:12,  5.10it/s] 16%|█▌        | 188/1170 [00:41<03:12,  5.10it/s] 16%|█▌        | 189/1170 [00:41<03:12,  5.10it/s] 16%|█▌        | 190/1170 [00:41<03:12,  5.10it/s] 16%|█▋        | 191/1170 [00:42<03:11,  5.10it/s] 16%|█▋        | 192/1170 [00:42<03:11,  5.10it/s] 16%|█▋        | 193/1170 [00:42<03:11,  5.10it/s] 17%|█▋        | 194/1170 [00:42<03:11,  5.10it/s] 17%|█▋        | 195/1170 [00:42<03:08,  5.16it/s] 17%|█▋        | 196/1170 [00:43<03:09,  5.14it/s] 17%|█▋        | 197/1170 [00:43<03:09,  5.12it/s] 17%|█▋        | 198/1170 [00:43<03:09,  5.12it/s] 17%|█▋        | 199/1170 [00:43<03:33,  4.56it/s] 17%|█▋        | 200/1170 [00:43<03:26,  4.70it/s] 17%|█▋        | 201/1170 [00:44<03:21,  4.82it/s] 17%|█▋        | 202/1170 [00:44<03:17,  4.89it/s] 17%|█▋        | 203/1170 [00:44<03:15,  4.95it/s] 17%|█▋        | 204/1170 [00:44<03:13,  4.99it/s] 18%|█▊        | 205/1170 [00:44<03:12,  5.02it/s] 18%|█▊        | 206/1170 [00:45<03:11,  5.03it/s] 18%|█▊        | 207/1170 [00:45<03:10,  5.04it/s] 18%|█▊        | 208/1170 [00:45<03:11,  5.03it/s] 18%|█▊        | 209/1170 [00:45<03:10,  5.03it/s] 18%|█▊        | 210/1170 [00:45<03:10,  5.05it/s] 18%|█▊        | 211/1170 [00:46<03:09,  5.06it/s] 18%|█▊        | 212/1170 [00:46<03:08,  5.07it/s] 18%|█▊        | 213/1170 [00:46<03:08,  5.07it/s] 18%|█▊        | 214/1170 [00:46<03:08,  5.07it/s] 18%|█▊        | 215/1170 [00:46<03:08,  5.08it/s] 18%|█▊        | 216/1170 [00:47<03:07,  5.09it/s] 19%|█▊        | 217/1170 [00:47<03:07,  5.09it/s] 19%|█▊        | 218/1170 [00:47<03:06,  5.09it/s] 19%|█▊        | 219/1170 [00:47<03:06,  5.09it/s] 19%|█▉        | 220/1170 [00:47<03:06,  5.10it/s] 19%|█▉        | 221/1170 [00:48<03:06,  5.10it/s] 19%|█▉        | 222/1170 [00:48<03:05,  5.10it/s] 19%|█▉        | 223/1170 [00:48<03:05,  5.10it/s] 19%|█▉        | 224/1170 [00:48<03:05,  5.09it/s] 19%|█▉        | 225/1170 [00:48<03:05,  5.09it/s] 19%|█▉        | 226/1170 [00:49<03:05,  5.09it/s] 19%|█▉        | 227/1170 [00:49<03:05,  5.10it/s] 19%|█▉        | 228/1170 [00:49<03:04,  5.10it/s] 20%|█▉        | 229/1170 [00:49<03:04,  5.10it/s] 20%|█▉        | 230/1170 [00:49<03:04,  5.10it/s] 20%|█▉        | 231/1170 [00:50<03:04,  5.10it/s] 20%|█▉        | 232/1170 [00:50<03:04,  5.09it/s] 20%|█▉        | 233/1170 [00:50<03:04,  5.09it/s] 20%|██        | 234/1170 [00:50<03:01,  5.15it/s] 20%|██        | 235/1170 [00:50<03:02,  5.13it/s] 20%|██        | 236/1170 [00:51<03:02,  5.12it/s] 20%|██        | 237/1170 [00:51<03:02,  5.11it/s] 20%|██        | 238/1170 [00:51<03:02,  5.11it/s] 20%|██        | 239/1170 [00:51<03:02,  5.11it/s] 21%|██        | 240/1170 [00:51<03:02,  5.11it/s] 21%|██        | 241/1170 [00:51<03:01,  5.11it/s] 21%|██        | 242/1170 [00:52<03:01,  5.10it/s] 21%|██        | 243/1170 [00:52<03:01,  5.10it/s] 21%|██        | 244/1170 [00:52<03:01,  5.10it/s] 21%|██        | 245/1170 [00:52<03:01,  5.10it/s] 21%|██        | 246/1170 [00:52<03:01,  5.10it/s] 21%|██        | 247/1170 [00:53<03:00,  5.10it/s] 21%|██        | 248/1170 [00:53<03:00,  5.10it/s] 21%|██▏       | 249/1170 [00:53<03:00,  5.10it/s] 21%|██▏       | 250/1170 [00:53<03:00,  5.10it/s] 21%|██▏       | 251/1170 [00:53<03:00,  5.10it/s] 22%|██▏       | 252/1170 [00:54<03:21,  4.56it/s] 22%|██▏       | 253/1170 [00:54<03:14,  4.71it/s] 22%|██▏       | 254/1170 [00:54<03:10,  4.82it/s] 22%|██▏       | 255/1170 [00:54<03:06,  4.90it/s] 22%|██▏       | 256/1170 [00:55<03:04,  4.96it/s] 22%|██▏       | 257/1170 [00:55<03:02,  5.00it/s] 22%|██▏       | 258/1170 [00:55<03:01,  5.03it/s] 22%|██▏       | 259/1170 [00:55<03:01,  5.03it/s] 22%|██▏       | 260/1170 [00:55<03:00,  5.04it/s] 22%|██▏       | 261/1170 [00:55<03:00,  5.04it/s] 22%|██▏       | 262/1170 [00:56<03:00,  5.04it/s] 22%|██▏       | 263/1170 [00:56<02:59,  5.05it/s] 23%|██▎       | 264/1170 [00:56<03:00,  5.02it/s] 23%|██▎       | 265/1170 [00:56<03:00,  5.00it/s] 23%|██▎       | 266/1170 [00:56<03:00,  5.02it/s] 23%|██▎       | 267/1170 [00:57<02:59,  5.03it/s] 23%|██▎       | 268/1170 [00:57<02:59,  5.03it/s] 23%|██▎       | 269/1170 [00:57<02:59,  5.03it/s] 23%|██▎       | 270/1170 [00:57<02:58,  5.04it/s] 23%|██▎       | 271/1170 [00:57<02:58,  5.04it/s] 23%|██▎       | 272/1170 [00:58<02:57,  5.05it/s] 23%|██▎       | 273/1170 [00:58<02:56,  5.09it/s] 23%|██▎       | 274/1170 [00:58<02:56,  5.08it/s] 24%|██▎       | 275/1170 [00:58<02:57,  5.04it/s] 24%|██▎       | 276/1170 [00:58<02:57,  5.03it/s] 24%|██▎       | 277/1170 [00:59<02:58,  5.02it/s] 24%|██▍       | 278/1170 [00:59<02:57,  5.03it/s] 24%|██▍       | 279/1170 [00:59<02:57,  5.03it/s] 24%|██▍       | 280/1170 [00:59<02:56,  5.03it/s] 24%|██▍       | 281/1170 [00:59<02:56,  5.04it/s] 24%|██▍       | 282/1170 [01:00<02:56,  5.04it/s] 24%|██▍       | 283/1170 [01:00<02:55,  5.04it/s] 24%|██▍       | 284/1170 [01:00<02:55,  5.04it/s] 24%|██▍       | 285/1170 [01:00<02:54,  5.06it/s] 24%|██▍       | 286/1170 [01:00<02:55,  5.04it/s] 25%|██▍       | 287/1170 [01:01<02:57,  4.98it/s] 25%|██▍       | 288/1170 [01:01<02:56,  4.99it/s] 25%|██▍       | 289/1170 [01:01<02:56,  5.00it/s] 25%|██▍       | 290/1170 [01:01<02:55,  5.01it/s] 25%|██▍       | 291/1170 [01:01<02:55,  5.02it/s] 25%|██▍       | 292/1170 [01:02<02:55,  5.01it/s] 25%|██▌       | 293/1170 [01:02<02:54,  5.02it/s] 25%|██▌       | 294/1170 [01:02<02:54,  5.03it/s] 25%|██▌       | 295/1170 [01:02<02:53,  5.03it/s] 25%|██▌       | 296/1170 [01:02<02:53,  5.04it/s] 25%|██▌       | 297/1170 [01:03<02:53,  5.05it/s] 25%|██▌       | 298/1170 [01:03<02:54,  5.00it/s] 26%|██▌       | 299/1170 [01:03<02:53,  5.02it/s] 26%|██▌       | 300/1170 [01:03<02:53,  5.01it/s] 26%|██▌       | 301/1170 [01:03<02:53,  5.01it/s] 26%|██▌       | 302/1170 [01:04<02:53,  5.01it/s] 26%|██▌       | 303/1170 [01:04<02:52,  5.02it/s] 26%|██▌       | 304/1170 [01:04<02:52,  5.02it/s] 26%|██▌       | 305/1170 [01:04<02:51,  5.03it/s] 26%|██▌       | 306/1170 [01:04<02:51,  5.03it/s] 26%|██▌       | 307/1170 [01:05<02:51,  5.04it/s] 26%|██▋       | 308/1170 [01:05<02:51,  5.02it/s] 26%|██▋       | 309/1170 [01:05<02:51,  5.02it/s] 26%|██▋       | 310/1170 [01:05<02:51,  5.02it/s] 27%|██▋       | 311/1170 [01:05<02:51,  5.01it/s] 27%|██▋       | 312/1170 [01:06<02:49,  5.08it/s] 27%|██▋       | 313/1170 [01:06<02:49,  5.07it/s] 27%|██▋       | 314/1170 [01:06<02:49,  5.04it/s] 27%|██▋       | 315/1170 [01:06<02:49,  5.05it/s] 27%|██▋       | 316/1170 [01:06<02:50,  5.02it/s] 27%|██▋       | 317/1170 [01:07<02:49,  5.04it/s] 27%|██▋       | 318/1170 [01:07<02:48,  5.04it/s] 27%|██▋       | 319/1170 [01:07<02:49,  5.01it/s] 27%|██▋       | 320/1170 [01:07<02:50,  4.99it/s] 27%|██▋       | 321/1170 [01:07<02:49,  5.00it/s] 28%|██▊       | 322/1170 [01:08<02:49,  5.01it/s] 28%|██▊       | 323/1170 [01:08<02:48,  5.02it/s] 28%|██▊       | 324/1170 [01:08<02:48,  5.01it/s] 28%|██▊       | 325/1170 [01:08<02:49,  4.99it/s] 28%|██▊       | 326/1170 [01:08<02:48,  5.00it/s] 28%|██▊       | 327/1170 [01:09<02:49,  4.99it/s] 28%|██▊       | 328/1170 [01:09<02:48,  4.99it/s] 28%|██▊       | 329/1170 [01:09<02:48,  5.01it/s] 28%|██▊       | 330/1170 [01:09<02:47,  5.01it/s] 28%|██▊       | 331/1170 [01:09<02:48,  4.99it/s] 28%|██▊       | 332/1170 [01:10<02:48,  4.98it/s] 28%|██▊       | 333/1170 [01:10<02:47,  5.00it/s] 29%|██▊       | 334/1170 [01:10<02:46,  5.01it/s] 29%|██▊       | 335/1170 [01:10<02:47,  4.99it/s] 29%|██▊       | 336/1170 [01:10<02:46,  5.00it/s] 29%|██▉       | 337/1170 [01:11<02:46,  4.99it/s] 29%|██▉       | 338/1170 [01:11<02:46,  4.99it/s] 29%|██▉       | 339/1170 [01:11<02:46,  4.99it/s] 29%|██▉       | 340/1170 [01:11<02:46,  4.97it/s] 29%|██▉       | 341/1170 [01:11<02:46,  4.99it/s] 29%|██▉       | 342/1170 [01:12<02:45,  4.99it/s] 29%|██▉       | 343/1170 [01:12<02:45,  5.01it/s] 29%|██▉       | 344/1170 [01:12<02:45,  5.00it/s] 29%|██▉       | 345/1170 [01:12<02:44,  5.01it/s] 30%|██▉       | 346/1170 [01:12<02:44,  5.01it/s] 30%|██▉       | 347/1170 [01:13<02:44,  5.02it/s] 30%|██▉       | 348/1170 [01:13<02:43,  5.03it/s] 30%|██▉       | 349/1170 [01:13<03:03,  4.48it/s] 30%|██▉       | 350/1170 [01:13<02:56,  4.63it/s] 30%|███       | 351/1170 [01:13<02:50,  4.81it/s] 30%|███       | 352/1170 [01:14<02:48,  4.86it/s] 30%|███       | 353/1170 [01:14<02:46,  4.90it/s] 30%|███       | 354/1170 [01:14<02:45,  4.92it/s] 30%|███       | 355/1170 [01:14<02:44,  4.95it/s] 30%|███       | 356/1170 [01:14<02:43,  4.99it/s] 31%|███       | 357/1170 [01:15<02:42,  5.01it/s] 31%|███       | 358/1170 [01:15<02:41,  5.03it/s] 31%|███       | 359/1170 [01:15<02:40,  5.05it/s] 31%|███       | 360/1170 [01:15<02:39,  5.06it/s] 31%|███       | 361/1170 [01:15<02:39,  5.07it/s] 31%|███       | 362/1170 [01:16<02:38,  5.08it/s] 31%|███       | 363/1170 [01:16<02:38,  5.09it/s] 31%|███       | 364/1170 [01:16<02:38,  5.09it/s] 31%|███       | 365/1170 [01:16<02:38,  5.09it/s] 31%|███▏      | 366/1170 [01:16<02:37,  5.10it/s] 31%|███▏      | 367/1170 [01:17<02:37,  5.10it/s] 31%|███▏      | 368/1170 [01:17<02:37,  5.10it/s] 32%|███▏      | 369/1170 [01:17<02:37,  5.10it/s] 32%|███▏      | 370/1170 [01:17<02:36,  5.10it/s] 32%|███▏      | 371/1170 [01:17<02:36,  5.10it/s] 32%|███▏      | 372/1170 [01:18<02:36,  5.10it/s] 32%|███▏      | 373/1170 [01:18<02:36,  5.10it/s] 32%|███▏      | 374/1170 [01:18<02:36,  5.10it/s] 32%|███▏      | 375/1170 [01:18<02:35,  5.10it/s] 32%|███▏      | 376/1170 [01:18<02:35,  5.10it/s] 32%|███▏      | 377/1170 [01:19<02:35,  5.10it/s] 32%|███▏      | 378/1170 [01:19<02:35,  5.10it/s] 32%|███▏      | 379/1170 [01:19<02:35,  5.10it/s] 32%|███▏      | 380/1170 [01:19<02:34,  5.10it/s] 33%|███▎      | 381/1170 [01:19<02:34,  5.10it/s] 33%|███▎      | 382/1170 [01:20<02:34,  5.10it/s] 33%|███▎      | 383/1170 [01:20<02:34,  5.10it/s] 33%|███▎      | 384/1170 [01:20<02:34,  5.10it/s] 33%|███▎      | 385/1170 [01:20<02:33,  5.10it/s] 33%|███▎      | 386/1170 [01:20<02:33,  5.09it/s] 33%|███▎      | 387/1170 [01:21<02:34,  5.08it/s] 33%|███▎      | 388/1170 [01:21<02:34,  5.07it/s] 33%|███▎      | 389/1170 [01:21<02:34,  5.06it/s] 33%|███▎      | 390/1170 [01:21<02:32,  5.12it/s] 33%|███▎      | 391/1170 [01:21<02:32,  5.09it/s] 34%|███▎      | 392/1170 [01:22<02:33,  5.08it/s] 34%|███▎      | 393/1170 [01:22<02:33,  5.07it/s] 34%|███▎      | 394/1170 [01:22<02:33,  5.06it/s] 34%|███▍      | 395/1170 [01:22<02:33,  5.05it/s] 34%|███▍      | 396/1170 [01:22<02:33,  5.05it/s] 34%|███▍      | 397/1170 [01:23<02:33,  5.05it/s] 34%|███▍      | 398/1170 [01:23<02:32,  5.05it/s] 34%|███▍      | 399/1170 [01:23<02:32,  5.05it/s] 34%|███▍      | 400/1170 [01:23<02:32,  5.04it/s] 34%|███▍      | 401/1170 [01:23<02:32,  5.04it/s] 34%|███▍      | 402/1170 [01:24<02:47,  4.58it/s] 34%|███▍      | 403/1170 [01:24<02:43,  4.70it/s] 35%|███▍      | 404/1170 [01:24<02:39,  4.79it/s] 35%|███▍      | 405/1170 [01:24<02:37,  4.86it/s] 35%|███▍      | 406/1170 [01:24<02:35,  4.90it/s] 35%|███▍      | 407/1170 [01:25<02:34,  4.94it/s] 35%|███▍      | 408/1170 [01:25<02:33,  4.96it/s] 35%|███▍      | 409/1170 [01:25<02:32,  4.98it/s] 35%|███▌      | 410/1170 [01:25<02:32,  4.97it/s] 35%|███▌      | 411/1170 [01:25<02:32,  4.98it/s] 35%|███▌      | 412/1170 [01:26<02:31,  5.00it/s] 35%|███▌      | 413/1170 [01:26<02:31,  5.00it/s] 35%|███▌      | 414/1170 [01:26<02:30,  5.01it/s] 35%|███▌      | 415/1170 [01:26<02:30,  5.01it/s] 36%|███▌      | 416/1170 [01:26<02:30,  5.01it/s] 36%|███▌      | 417/1170 [01:27<02:30,  5.01it/s] 36%|███▌      | 418/1170 [01:27<02:30,  5.01it/s] 36%|███▌      | 419/1170 [01:27<02:29,  5.01it/s] 36%|███▌      | 420/1170 [01:27<02:30,  4.99it/s] 36%|███▌      | 421/1170 [01:27<02:29,  5.01it/s] 36%|███▌      | 422/1170 [01:28<02:29,  5.01it/s] 36%|███▌      | 423/1170 [01:28<02:28,  5.02it/s] 36%|███▌      | 424/1170 [01:28<02:28,  5.02it/s] 36%|███▋      | 425/1170 [01:28<02:28,  5.03it/s] 36%|███▋      | 426/1170 [01:28<02:27,  5.03it/s] 36%|███▋      | 427/1170 [01:29<02:27,  5.03it/s] 37%|███▋      | 428/1170 [01:29<02:27,  5.03it/s] 37%|███▋      | 429/1170 [01:29<02:25,  5.09it/s] 37%|███▋      | 430/1170 [01:29<02:26,  5.07it/s] 37%|███▋      | 431/1170 [01:29<02:26,  5.06it/s] 37%|███▋      | 432/1170 [01:30<02:26,  5.05it/s] 37%|███▋      | 433/1170 [01:30<02:25,  5.05it/s] 37%|███▋      | 434/1170 [01:30<02:25,  5.06it/s] 37%|███▋      | 435/1170 [01:30<02:25,  5.06it/s] 37%|███▋      | 436/1170 [01:30<02:24,  5.06it/s] 37%|███▋      | 437/1170 [01:31<02:24,  5.07it/s] 37%|███▋      | 438/1170 [01:31<02:24,  5.07it/s] 38%|███▊      | 439/1170 [01:31<02:24,  5.07it/s] 38%|███▊      | 440/1170 [01:31<02:23,  5.07it/s] 38%|███▊      | 441/1170 [01:31<02:23,  5.07it/s] 38%|███▊      | 442/1170 [01:32<02:23,  5.08it/s] 38%|███▊      | 443/1170 [01:32<02:23,  5.08it/s] 38%|███▊      | 444/1170 [01:32<02:22,  5.08it/s] 38%|███▊      | 445/1170 [01:32<02:22,  5.08it/s] 38%|███▊      | 446/1170 [01:32<02:22,  5.08it/s] 38%|███▊      | 447/1170 [01:33<02:22,  5.08it/s] 38%|███▊      | 448/1170 [01:33<02:22,  5.08it/s] 38%|███▊      | 449/1170 [01:33<02:21,  5.08it/s] 38%|███▊      | 450/1170 [01:33<02:21,  5.08it/s] 39%|███▊      | 451/1170 [01:33<02:21,  5.08it/s] 39%|███▊      | 452/1170 [01:34<02:21,  5.08it/s] 39%|███▊      | 453/1170 [01:34<02:21,  5.08it/s] 39%|███▉      | 454/1170 [01:34<02:21,  5.08it/s] 39%|███▉      | 455/1170 [01:34<02:20,  5.08it/s] 39%|███▉      | 456/1170 [01:34<02:20,  5.08it/s] 39%|███▉      | 457/1170 [01:35<02:20,  5.08it/s] 39%|███▉      | 458/1170 [01:35<02:20,  5.08it/s] 39%|███▉      | 459/1170 [01:35<02:19,  5.08it/s] 39%|███▉      | 460/1170 [01:35<02:19,  5.08it/s] 39%|███▉      | 461/1170 [01:35<02:19,  5.08it/s] 39%|███▉      | 462/1170 [01:35<02:19,  5.08it/s] 40%|███▉      | 463/1170 [01:36<02:19,  5.08it/s] 40%|███▉      | 464/1170 [01:36<02:18,  5.09it/s] 40%|███▉      | 465/1170 [01:36<02:18,  5.09it/s] 40%|███▉      | 466/1170 [01:36<02:18,  5.09it/s] 40%|███▉      | 467/1170 [01:36<02:17,  5.09it/s] 40%|████      | 468/1170 [01:37<02:16,  5.16it/s] 40%|████      | 469/1170 [01:37<02:16,  5.14it/s] 40%|████      | 470/1170 [01:37<02:16,  5.13it/s] 40%|████      | 471/1170 [01:37<02:16,  5.12it/s] 40%|████      | 472/1170 [01:37<02:16,  5.12it/s] 40%|████      | 473/1170 [01:38<02:16,  5.11it/s] 41%|████      | 474/1170 [01:38<02:16,  5.11it/s] 41%|████      | 475/1170 [01:38<02:16,  5.11it/s] 41%|████      | 476/1170 [01:38<02:15,  5.11it/s] 41%|████      | 477/1170 [01:38<02:15,  5.11it/s] 41%|████      | 478/1170 [01:39<02:15,  5.11it/s] 41%|████      | 479/1170 [01:39<02:15,  5.10it/s] 41%|████      | 480/1170 [01:39<02:15,  5.10it/s] 41%|████      | 481/1170 [01:39<02:15,  5.10it/s] 41%|████      | 482/1170 [01:39<02:14,  5.10it/s] 41%|████▏     | 483/1170 [01:40<02:14,  5.10it/s] 41%|████▏     | 484/1170 [01:40<02:14,  5.10it/s] 41%|████▏     | 485/1170 [01:40<02:14,  5.10it/s] 42%|████▏     | 486/1170 [01:40<02:14,  5.10it/s] 42%|████▏     | 487/1170 [01:40<02:13,  5.10it/s] 42%|████▏     | 488/1170 [01:41<02:13,  5.10it/s] 42%|████▏     | 489/1170 [01:41<02:13,  5.10it/s] 42%|████▏     | 490/1170 [01:41<02:13,  5.10it/s] 42%|████▏     | 491/1170 [01:41<02:13,  5.09it/s] 42%|████▏     | 492/1170 [01:41<02:13,  5.09it/s] 42%|████▏     | 493/1170 [01:42<02:12,  5.10it/s] 42%|████▏     | 494/1170 [01:42<02:12,  5.10it/s] 42%|████▏     | 495/1170 [01:42<02:12,  5.09it/s] 42%|████▏     | 496/1170 [01:42<02:12,  5.09it/s] 42%|████▏     | 497/1170 [01:42<02:12,  5.09it/s] 43%|████▎     | 498/1170 [01:43<02:11,  5.09it/s] 43%|████▎     | 499/1170 [01:43<02:27,  4.55it/s] 43%|████▎     | 500/1170 [01:43<02:22,  4.70it/s]                                                   43%|████▎     | 500/1170 [01:43<02:22,  4.70it/s][INFO|trainer.py:2409] 2022-08-27 22:05:00,463 >> Saving model checkpoint to tmp/baseline/RTE/bert-base-uncased/HPO_S0/1e-06/64/30/checkpoint-500
[INFO|configuration_utils.py:446] 2022-08-27 22:05:00,464 >> Configuration saved in tmp/baseline/RTE/bert-base-uncased/HPO_S0/1e-06/64/30/checkpoint-500/config.json
[INFO|modeling_utils.py:1602] 2022-08-27 22:05:01,290 >> Model weights saved in tmp/baseline/RTE/bert-base-uncased/HPO_S0/1e-06/64/30/checkpoint-500/pytorch_model.bin
[INFO|tokenization_utils_base.py:2108] 2022-08-27 22:05:01,291 >> tokenizer config file saved in tmp/baseline/RTE/bert-base-uncased/HPO_S0/1e-06/64/30/checkpoint-500/tokenizer_config.json
[INFO|tokenization_utils_base.py:2114] 2022-08-27 22:05:01,291 >> Special tokens file saved in tmp/baseline/RTE/bert-base-uncased/HPO_S0/1e-06/64/30/checkpoint-500/special_tokens_map.json
/home/ubuntu/anaconda3/envs/transformers/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 43%|████▎     | 501/1170 [01:46<10:38,  1.05it/s] 43%|████▎     | 502/1170 [01:46<08:05,  1.37it/s] 43%|████▎     | 503/1170 [01:46<06:19,  1.76it/s] 43%|████▎     | 504/1170 [01:46<05:04,  2.19it/s] 43%|████▎     | 505/1170 [01:46<04:11,  2.64it/s] 43%|████▎     | 506/1170 [01:47<03:35,  3.08it/s] 43%|████▎     | 507/1170 [01:47<03:08,  3.53it/s] 43%|████▎     | 508/1170 [01:47<02:50,  3.88it/s] 44%|████▎     | 509/1170 [01:47<02:38,  4.18it/s] 44%|████▎     | 510/1170 [01:47<02:29,  4.42it/s] 44%|████▎     | 511/1170 [01:48<02:23,  4.60it/s] 44%|████▍     | 512/1170 [01:48<02:18,  4.74it/s] 44%|████▍     | 513/1170 [01:48<02:15,  4.84it/s] 44%|████▍     | 514/1170 [01:48<02:13,  4.91it/s] 44%|████▍     | 515/1170 [01:48<02:12,  4.95it/s] 44%|████▍     | 516/1170 [01:49<02:11,  4.99it/s] 44%|████▍     | 517/1170 [01:49<02:09,  5.02it/s] 44%|████▍     | 518/1170 [01:49<02:09,  5.05it/s] 44%|████▍     | 519/1170 [01:49<02:08,  5.06it/s] 44%|████▍     | 520/1170 [01:49<02:08,  5.07it/s] 45%|████▍     | 521/1170 [01:50<02:07,  5.08it/s] 45%|████▍     | 522/1170 [01:50<02:07,  5.08it/s] 45%|████▍     | 523/1170 [01:50<02:07,  5.09it/s] 45%|████▍     | 524/1170 [01:50<02:06,  5.09it/s] 45%|████▍     | 525/1170 [01:50<02:06,  5.09it/s] 45%|████▍     | 526/1170 [01:51<02:06,  5.08it/s] 45%|████▌     | 527/1170 [01:51<02:06,  5.08it/s] 45%|████▌     | 528/1170 [01:51<02:06,  5.08it/s] 45%|████▌     | 529/1170 [01:51<02:06,  5.09it/s] 45%|████▌     | 530/1170 [01:51<02:05,  5.09it/s] 45%|████▌     | 531/1170 [01:52<02:05,  5.09it/s] 45%|████▌     | 532/1170 [01:52<02:05,  5.09it/s] 46%|████▌     | 533/1170 [01:52<02:05,  5.09it/s] 46%|████▌     | 534/1170 [01:52<02:04,  5.09it/s] 46%|████▌     | 535/1170 [01:52<02:04,  5.09it/s] 46%|████▌     | 536/1170 [01:53<02:04,  5.09it/s] 46%|████▌     | 537/1170 [01:53<02:04,  5.09it/s] 46%|████▌     | 538/1170 [01:53<02:04,  5.09it/s] 46%|████▌     | 539/1170 [01:53<02:03,  5.09it/s] 46%|████▌     | 540/1170 [01:53<02:03,  5.09it/s] 46%|████▌     | 541/1170 [01:54<02:03,  5.10it/s] 46%|████▋     | 542/1170 [01:54<02:03,  5.10it/s] 46%|████▋     | 543/1170 [01:54<02:02,  5.10it/s] 46%|████▋     | 544/1170 [01:54<02:02,  5.10it/s] 47%|████▋     | 545/1170 [01:54<02:02,  5.10it/s] 47%|████▋     | 546/1170 [01:55<02:00,  5.16it/s] 47%|████▋     | 547/1170 [01:55<02:01,  5.14it/s] 47%|████▋     | 548/1170 [01:55<02:01,  5.13it/s] 47%|████▋     | 549/1170 [01:55<02:01,  5.12it/s] 47%|████▋     | 550/1170 [01:55<02:01,  5.11it/s] 47%|████▋     | 551/1170 [01:56<02:01,  5.11it/s] 47%|████▋     | 552/1170 [01:56<02:01,  5.11it/s] 47%|████▋     | 553/1170 [01:56<02:00,  5.10it/s] 47%|████▋     | 554/1170 [01:56<02:00,  5.10it/s] 47%|████▋     | 555/1170 [01:56<02:00,  5.10it/s] 48%|████▊     | 556/1170 [01:56<02:00,  5.09it/s] 48%|████▊     | 557/1170 [01:57<02:00,  5.09it/s] 48%|████▊     | 558/1170 [01:57<02:00,  5.09it/s] 48%|████▊     | 559/1170 [01:57<02:00,  5.09it/s] 48%|████▊     | 560/1170 [01:57<01:59,  5.08it/s] 48%|████▊     | 561/1170 [01:57<01:59,  5.09it/s] 48%|████▊     | 562/1170 [01:58<01:59,  5.08it/s] 48%|████▊     | 563/1170 [01:58<01:59,  5.08it/s] 48%|████▊     | 564/1170 [01:58<01:59,  5.07it/s] 48%|████▊     | 565/1170 [01:58<02:13,  4.52it/s] 48%|████▊     | 566/1170 [01:59<02:09,  4.68it/s] 48%|████▊     | 567/1170 [01:59<02:05,  4.79it/s] 49%|████▊     | 568/1170 [01:59<02:03,  4.88it/s] 49%|████▊     | 569/1170 [01:59<02:01,  4.94it/s] 49%|████▊     | 570/1170 [01:59<02:00,  4.98it/s] 49%|████▉     | 571/1170 [02:00<01:59,  5.01it/s] 49%|████▉     | 572/1170 [02:00<01:58,  5.03it/s] 49%|████▉     | 573/1170 [02:00<01:58,  5.05it/s] 49%|████▉     | 574/1170 [02:00<01:57,  5.06it/s] 49%|████▉     | 575/1170 [02:00<01:57,  5.07it/s] 49%|████▉     | 576/1170 [02:01<01:57,  5.07it/s] 49%|████▉     | 577/1170 [02:01<01:56,  5.08it/s] 49%|████▉     | 578/1170 [02:01<01:56,  5.08it/s] 49%|████▉     | 579/1170 [02:01<01:56,  5.09it/s] 50%|████▉     | 580/1170 [02:01<01:55,  5.09it/s] 50%|████▉     | 581/1170 [02:01<01:55,  5.08it/s] 50%|████▉     | 582/1170 [02:02<01:55,  5.08it/s] 50%|████▉     | 583/1170 [02:02<01:55,  5.08it/s] 50%|████▉     | 584/1170 [02:02<01:55,  5.09it/s] 50%|█████     | 585/1170 [02:02<01:53,  5.15it/s] 50%|█████     | 586/1170 [02:02<01:54,  5.12it/s] 50%|█████     | 587/1170 [02:03<01:54,  5.11it/s] 50%|█████     | 588/1170 [02:03<01:54,  5.10it/s] 50%|█████     | 589/1170 [02:03<01:54,  5.10it/s] 50%|█████     | 590/1170 [02:03<01:53,  5.09it/s] 51%|█████     | 591/1170 [02:03<01:53,  5.09it/s] 51%|█████     | 592/1170 [02:04<01:53,  5.09it/s] 51%|█████     | 593/1170 [02:04<01:53,  5.08it/s] 51%|█████     | 594/1170 [02:04<01:53,  5.08it/s] 51%|█████     | 595/1170 [02:04<01:53,  5.08it/s] 51%|█████     | 596/1170 [02:04<01:52,  5.08it/s] 51%|█████     | 597/1170 [02:05<01:52,  5.09it/s] 51%|█████     | 598/1170 [02:05<01:52,  5.09it/s] 51%|█████     | 599/1170 [02:05<01:52,  5.09it/s] 51%|█████▏    | 600/1170 [02:05<01:51,  5.09it/s] 51%|█████▏    | 601/1170 [02:05<01:51,  5.09it/s] 51%|█████▏    | 602/1170 [02:06<01:51,  5.09it/s] 52%|█████▏    | 603/1170 [02:06<01:51,  5.09it/s] 52%|█████▏    | 604/1170 [02:06<01:51,  5.09it/s] 52%|█████▏    | 605/1170 [02:06<01:51,  5.09it/s] 52%|█████▏    | 606/1170 [02:06<01:50,  5.09it/s] 52%|█████▏    | 607/1170 [02:07<01:50,  5.09it/s] 52%|█████▏    | 608/1170 [02:07<01:50,  5.09it/s] 52%|█████▏    | 609/1170 [02:07<01:50,  5.09it/s] 52%|█████▏    | 610/1170 [02:07<01:50,  5.09it/s] 52%|█████▏    | 611/1170 [02:07<01:49,  5.09it/s] 52%|█████▏    | 612/1170 [02:08<01:49,  5.09it/s] 52%|█████▏    | 613/1170 [02:08<01:49,  5.09it/s] 52%|█████▏    | 614/1170 [02:08<01:49,  5.09it/s] 53%|█████▎    | 615/1170 [02:08<01:49,  5.09it/s] 53%|█████▎    | 616/1170 [02:08<01:48,  5.09it/s] 53%|█████▎    | 617/1170 [02:09<01:48,  5.09it/s] 53%|█████▎    | 618/1170 [02:09<01:48,  5.09it/s] 53%|█████▎    | 619/1170 [02:09<01:48,  5.09it/s] 53%|█████▎    | 620/1170 [02:09<01:47,  5.09it/s] 53%|█████▎    | 621/1170 [02:09<01:47,  5.09it/s] 53%|█████▎    | 622/1170 [02:10<01:47,  5.09it/s] 53%|█████▎    | 623/1170 [02:10<01:47,  5.09it/s] 53%|█████▎    | 624/1170 [02:10<01:46,  5.15it/s] 53%|█████▎    | 625/1170 [02:10<01:46,  5.13it/s] 54%|█████▎    | 626/1170 [02:10<01:46,  5.12it/s] 54%|█████▎    | 627/1170 [02:11<01:46,  5.11it/s] 54%|█████▎    | 628/1170 [02:11<01:46,  5.10it/s] 54%|█████▍    | 629/1170 [02:11<01:46,  5.10it/s] 54%|█████▍    | 630/1170 [02:11<01:45,  5.10it/s] 54%|█████▍    | 631/1170 [02:11<01:45,  5.09it/s] 54%|█████▍    | 632/1170 [02:12<01:45,  5.09it/s] 54%|█████▍    | 633/1170 [02:12<01:45,  5.09it/s] 54%|█████▍    | 634/1170 [02:12<01:45,  5.09it/s] 54%|█████▍    | 635/1170 [02:12<01:45,  5.09it/s] 54%|█████▍    | 636/1170 [02:12<01:44,  5.09it/s] 54%|█████▍    | 637/1170 [02:12<01:44,  5.09it/s] 55%|█████▍    | 638/1170 [02:13<01:44,  5.09it/s] 55%|█████▍    | 639/1170 [02:13<01:44,  5.09it/s] 55%|█████▍    | 640/1170 [02:13<01:44,  5.09it/s] 55%|█████▍    | 641/1170 [02:13<01:43,  5.09it/s] 55%|█████▍    | 642/1170 [02:13<01:43,  5.09it/s] 55%|█████▍    | 643/1170 [02:14<01:43,  5.09it/s] 55%|█████▌    | 644/1170 [02:14<01:43,  5.09it/s] 55%|█████▌    | 645/1170 [02:14<01:43,  5.09it/s] 55%|█████▌    | 646/1170 [02:14<01:43,  5.08it/s] 55%|█████▌    | 647/1170 [02:14<01:42,  5.09it/s] 55%|█████▌    | 648/1170 [02:15<01:42,  5.09it/s] 55%|█████▌    | 649/1170 [02:15<01:42,  5.09it/s] 56%|█████▌    | 650/1170 [02:15<01:42,  5.08it/s] 56%|█████▌    | 651/1170 [02:15<01:42,  5.09it/s] 56%|█████▌    | 652/1170 [02:15<01:41,  5.09it/s] 56%|█████▌    | 653/1170 [02:16<01:41,  5.09it/s] 56%|█████▌    | 654/1170 [02:16<01:41,  5.09it/s] 56%|█████▌    | 655/1170 [02:16<01:41,  5.09it/s] 56%|█████▌    | 656/1170 [02:16<01:40,  5.09it/s] 56%|█████▌    | 657/1170 [02:16<01:40,  5.09it/s] 56%|█████▌    | 658/1170 [02:17<01:40,  5.09it/s] 56%|█████▋    | 659/1170 [02:17<01:40,  5.09it/s] 56%|█████▋    | 660/1170 [02:17<01:40,  5.09it/s] 56%|█████▋    | 661/1170 [02:17<01:40,  5.07it/s] 57%|█████▋    | 662/1170 [02:17<01:40,  5.07it/s] 57%|█████▋    | 663/1170 [02:18<01:38,  5.14it/s] 57%|█████▋    | 664/1170 [02:18<01:38,  5.12it/s] 57%|█████▋    | 665/1170 [02:18<01:38,  5.11it/s] 57%|█████▋    | 666/1170 [02:18<01:38,  5.11it/s] 57%|█████▋    | 667/1170 [02:18<01:38,  5.10it/s] 57%|█████▋    | 668/1170 [02:19<01:38,  5.10it/s] 57%|█████▋    | 669/1170 [02:19<01:38,  5.10it/s] 57%|█████▋    | 670/1170 [02:19<01:38,  5.09it/s] 57%|█████▋    | 671/1170 [02:19<01:38,  5.09it/s] 57%|█████▋    | 672/1170 [02:19<01:37,  5.09it/s] 58%|█████▊    | 673/1170 [02:20<01:37,  5.09it/s] 58%|█████▊    | 674/1170 [02:20<01:37,  5.09it/s] 58%|█████▊    | 675/1170 [02:20<01:37,  5.07it/s] 58%|█████▊    | 676/1170 [02:20<01:37,  5.07it/s] 58%|█████▊    | 677/1170 [02:20<01:37,  5.08it/s] 58%|█████▊    | 678/1170 [02:21<01:36,  5.08it/s] 58%|█████▊    | 679/1170 [02:21<01:36,  5.08it/s] 58%|█████▊    | 680/1170 [02:21<01:36,  5.09it/s] 58%|█████▊    | 681/1170 [02:21<01:36,  5.09it/s] 58%|█████▊    | 682/1170 [02:21<01:35,  5.09it/s] 58%|█████▊    | 683/1170 [02:22<01:35,  5.09it/s] 58%|█████▊    | 684/1170 [02:22<01:35,  5.09it/s] 59%|█████▊    | 685/1170 [02:22<01:35,  5.09it/s] 59%|█████▊    | 686/1170 [02:22<01:35,  5.09it/s] 59%|█████▊    | 687/1170 [02:22<01:34,  5.09it/s] 59%|█████▉    | 688/1170 [02:23<01:34,  5.09it/s] 59%|█████▉    | 689/1170 [02:23<01:34,  5.09it/s] 59%|█████▉    | 690/1170 [02:23<01:34,  5.09it/s] 59%|█████▉    | 691/1170 [02:23<01:34,  5.08it/s] 59%|█████▉    | 692/1170 [02:23<01:34,  5.07it/s] 59%|█████▉    | 693/1170 [02:23<01:33,  5.08it/s] 59%|█████▉    | 694/1170 [02:24<01:33,  5.08it/s] 59%|█████▉    | 695/1170 [02:24<01:33,  5.08it/s] 59%|█████▉    | 696/1170 [02:24<01:33,  5.09it/s] 60%|█████▉    | 697/1170 [02:24<01:32,  5.09it/s] 60%|█████▉    | 698/1170 [02:24<01:32,  5.09it/s] 60%|█████▉    | 699/1170 [02:25<01:32,  5.09it/s] 60%|█████▉    | 700/1170 [02:25<01:32,  5.09it/s] 60%|█████▉    | 701/1170 [02:25<01:32,  5.09it/s] 60%|██████    | 702/1170 [02:25<01:30,  5.15it/s] 60%|██████    | 703/1170 [02:25<01:31,  5.13it/s] 60%|██████    | 704/1170 [02:26<01:31,  5.12it/s] 60%|██████    | 705/1170 [02:26<01:30,  5.11it/s] 60%|██████    | 706/1170 [02:26<01:30,  5.11it/s] 60%|██████    | 707/1170 [02:26<01:30,  5.10it/s] 61%|██████    | 708/1170 [02:26<01:30,  5.10it/s] 61%|██████    | 709/1170 [02:27<01:30,  5.09it/s] 61%|██████    | 710/1170 [02:27<01:30,  5.09it/s] 61%|██████    | 711/1170 [02:27<01:30,  5.09it/s] 61%|██████    | 712/1170 [02:27<01:29,  5.09it/s] 61%|██████    | 713/1170 [02:27<01:29,  5.09it/s] 61%|██████    | 714/1170 [02:28<01:29,  5.09it/s] 61%|██████    | 715/1170 [02:28<01:29,  5.09it/s] 61%|██████    | 716/1170 [02:28<01:29,  5.09it/s] 61%|██████▏   | 717/1170 [02:28<01:28,  5.09it/s] 61%|██████▏   | 718/1170 [02:28<01:28,  5.09it/s] 61%|██████▏   | 719/1170 [02:29<01:28,  5.09it/s] 62%|██████▏   | 720/1170 [02:29<01:28,  5.09it/s] 62%|██████▏   | 721/1170 [02:29<01:28,  5.09it/s] 62%|██████▏   | 722/1170 [02:29<01:28,  5.09it/s] 62%|██████▏   | 723/1170 [02:29<01:27,  5.09it/s] 62%|██████▏   | 724/1170 [02:30<01:27,  5.09it/s] 62%|██████▏   | 725/1170 [02:30<01:27,  5.09it/s] 62%|██████▏   | 726/1170 [02:30<01:27,  5.09it/s] 62%|██████▏   | 727/1170 [02:30<01:27,  5.09it/s] 62%|██████▏   | 728/1170 [02:30<01:26,  5.09it/s] 62%|██████▏   | 729/1170 [02:31<01:26,  5.09it/s] 62%|██████▏   | 730/1170 [02:31<01:26,  5.09it/s] 62%|██████▏   | 731/1170 [02:31<01:26,  5.09it/s] 63%|██████▎   | 732/1170 [02:31<01:25,  5.09it/s] 63%|██████▎   | 733/1170 [02:31<01:25,  5.09it/s] 63%|██████▎   | 734/1170 [02:32<01:25,  5.09it/s] 63%|██████▎   | 735/1170 [02:32<01:25,  5.09it/s] 63%|██████▎   | 736/1170 [02:32<01:25,  5.09it/s] 63%|██████▎   | 737/1170 [02:32<01:25,  5.09it/s] 63%|██████▎   | 738/1170 [02:32<01:24,  5.09it/s] 63%|██████▎   | 739/1170 [02:33<01:24,  5.09it/s] 63%|██████▎   | 740/1170 [02:33<01:24,  5.09it/s] 63%|██████▎   | 741/1170 [02:33<01:23,  5.15it/s] 63%|██████▎   | 742/1170 [02:33<01:23,  5.13it/s] 64%|██████▎   | 743/1170 [02:33<01:23,  5.12it/s] 64%|██████▎   | 744/1170 [02:33<01:23,  5.11it/s] 64%|██████▎   | 745/1170 [02:34<01:23,  5.10it/s] 64%|██████▍   | 746/1170 [02:34<01:23,  5.10it/s] 64%|██████▍   | 747/1170 [02:34<01:23,  5.10it/s] 64%|██████▍   | 748/1170 [02:34<01:22,  5.09it/s] 64%|██████▍   | 749/1170 [02:34<01:22,  5.09it/s] 64%|██████▍   | 750/1170 [02:35<01:32,  4.55it/s] 64%|██████▍   | 751/1170 [02:35<01:29,  4.70it/s] 64%|██████▍   | 752/1170 [02:35<01:26,  4.81it/s] 64%|██████▍   | 753/1170 [02:35<01:25,  4.89it/s] 64%|██████▍   | 754/1170 [02:36<01:24,  4.95it/s] 65%|██████▍   | 755/1170 [02:36<01:23,  4.99it/s] 65%|██████▍   | 756/1170 [02:36<01:22,  5.02it/s] 65%|██████▍   | 757/1170 [02:36<01:21,  5.04it/s] 65%|██████▍   | 758/1170 [02:36<01:21,  5.05it/s] 65%|██████▍   | 759/1170 [02:37<01:21,  5.06it/s] 65%|██████▍   | 760/1170 [02:37<01:20,  5.07it/s] 65%|██████▌   | 761/1170 [02:37<01:20,  5.07it/s] 65%|██████▌   | 762/1170 [02:37<01:20,  5.07it/s] 65%|██████▌   | 763/1170 [02:37<01:20,  5.07it/s] 65%|██████▌   | 764/1170 [02:38<01:19,  5.08it/s] 65%|██████▌   | 765/1170 [02:38<01:19,  5.08it/s] 65%|██████▌   | 766/1170 [02:38<01:19,  5.09it/s] 66%|██████▌   | 767/1170 [02:38<01:19,  5.09it/s] 66%|██████▌   | 768/1170 [02:38<01:18,  5.09it/s] 66%|██████▌   | 769/1170 [02:38<01:18,  5.09it/s] 66%|██████▌   | 770/1170 [02:39<01:18,  5.09it/s] 66%|██████▌   | 771/1170 [02:39<01:18,  5.09it/s] 66%|██████▌   | 772/1170 [02:39<01:18,  5.09it/s] 66%|██████▌   | 773/1170 [02:39<01:18,  5.09it/s] 66%|██████▌   | 774/1170 [02:39<01:17,  5.09it/s] 66%|██████▌   | 775/1170 [02:40<01:17,  5.09it/s] 66%|██████▋   | 776/1170 [02:40<01:17,  5.09it/s] 66%|██████▋   | 777/1170 [02:40<01:17,  5.09it/s] 66%|██████▋   | 778/1170 [02:40<01:17,  5.08it/s] 67%|██████▋   | 779/1170 [02:40<01:16,  5.08it/s] 67%|██████▋   | 780/1170 [02:41<01:15,  5.14it/s] 67%|██████▋   | 781/1170 [02:41<01:15,  5.12it/s] 67%|██████▋   | 782/1170 [02:41<01:15,  5.11it/s] 67%|██████▋   | 783/1170 [02:41<01:15,  5.10it/s] 67%|██████▋   | 784/1170 [02:41<01:15,  5.10it/s] 67%|██████▋   | 785/1170 [02:42<01:15,  5.10it/s] 67%|██████▋   | 786/1170 [02:42<01:15,  5.10it/s] 67%|██████▋   | 787/1170 [02:42<01:15,  5.10it/s] 67%|██████▋   | 788/1170 [02:42<01:14,  5.09it/s] 67%|██████▋   | 789/1170 [02:42<01:14,  5.08it/s] 68%|██████▊   | 790/1170 [02:43<01:14,  5.07it/s] 68%|██████▊   | 791/1170 [02:43<01:14,  5.07it/s] 68%|██████▊   | 792/1170 [02:43<01:14,  5.08it/s] 68%|██████▊   | 793/1170 [02:43<01:14,  5.08it/s] 68%|██████▊   | 794/1170 [02:43<01:13,  5.08it/s] 68%|██████▊   | 795/1170 [02:44<01:13,  5.08it/s] 68%|██████▊   | 796/1170 [02:44<01:13,  5.08it/s] 68%|██████▊   | 797/1170 [02:44<01:13,  5.07it/s] 68%|██████▊   | 798/1170 [02:44<01:13,  5.07it/s] 68%|██████▊   | 799/1170 [02:44<01:13,  5.07it/s] 68%|██████▊   | 800/1170 [02:45<01:12,  5.07it/s] 68%|██████▊   | 801/1170 [02:45<01:12,  5.08it/s] 69%|██████▊   | 802/1170 [02:45<01:12,  5.08it/s] 69%|██████▊   | 803/1170 [02:45<01:12,  5.08it/s] 69%|██████▊   | 804/1170 [02:45<01:11,  5.08it/s] 69%|██████▉   | 805/1170 [02:46<01:11,  5.08it/s] 69%|██████▉   | 806/1170 [02:46<01:11,  5.09it/s] 69%|██████▉   | 807/1170 [02:46<01:11,  5.09it/s] 69%|██████▉   | 808/1170 [02:46<01:11,  5.09it/s] 69%|██████▉   | 809/1170 [02:46<01:10,  5.09it/s] 69%|██████▉   | 810/1170 [02:47<01:10,  5.09it/s] 69%|██████▉   | 811/1170 [02:47<01:10,  5.09it/s] 69%|██████▉   | 812/1170 [02:47<01:10,  5.09it/s] 69%|██████▉   | 813/1170 [02:47<01:10,  5.09it/s] 70%|██████▉   | 814/1170 [02:47<01:14,  4.76it/s] 70%|██████▉   | 815/1170 [02:48<01:13,  4.85it/s] 70%|██████▉   | 816/1170 [02:48<01:11,  4.92it/s] 70%|██████▉   | 817/1170 [02:48<01:10,  4.97it/s] 70%|██████▉   | 818/1170 [02:48<01:10,  5.01it/s] 70%|███████   | 819/1170 [02:48<01:08,  5.09it/s] 70%|███████   | 820/1170 [02:49<01:08,  5.08it/s] 70%|███████   | 821/1170 [02:49<01:08,  5.09it/s] 70%|███████   | 822/1170 [02:49<01:08,  5.08it/s] 70%|███████   | 823/1170 [02:49<01:08,  5.08it/s] 70%|███████   | 824/1170 [02:49<01:08,  5.08it/s] 71%|███████   | 825/1170 [02:50<01:07,  5.08it/s] 71%|███████   | 826/1170 [02:50<01:07,  5.09it/s] 71%|███████   | 827/1170 [02:50<01:07,  5.09it/s] 71%|███████   | 828/1170 [02:50<01:07,  5.09it/s] 71%|███████   | 829/1170 [02:50<01:06,  5.09it/s] 71%|███████   | 830/1170 [02:51<01:06,  5.09it/s] 71%|███████   | 831/1170 [02:51<01:06,  5.09it/s] 71%|███████   | 832/1170 [02:51<01:06,  5.09it/s] 71%|███████   | 833/1170 [02:51<01:06,  5.09it/s] 71%|███████▏  | 834/1170 [02:51<01:06,  5.09it/s] 71%|███████▏  | 835/1170 [02:51<01:05,  5.09it/s] 71%|███████▏  | 836/1170 [02:52<01:05,  5.09it/s] 72%|███████▏  | 837/1170 [02:52<01:05,  5.07it/s] 72%|███████▏  | 838/1170 [02:52<01:05,  5.07it/s] 72%|███████▏  | 839/1170 [02:52<01:05,  5.07it/s] 72%|███████▏  | 840/1170 [02:52<01:04,  5.08it/s] 72%|███████▏  | 841/1170 [02:53<01:04,  5.08it/s] 72%|███████▏  | 842/1170 [02:53<01:04,  5.09it/s] 72%|███████▏  | 843/1170 [02:53<01:04,  5.09it/s] 72%|███████▏  | 844/1170 [02:53<01:04,  5.08it/s] 72%|███████▏  | 845/1170 [02:53<01:03,  5.09it/s] 72%|███████▏  | 846/1170 [02:54<01:03,  5.09it/s] 72%|███████▏  | 847/1170 [02:54<01:03,  5.09it/s] 72%|███████▏  | 848/1170 [02:54<01:03,  5.09it/s] 73%|███████▎  | 849/1170 [02:54<01:03,  5.09it/s] 73%|███████▎  | 850/1170 [02:54<01:02,  5.09it/s] 73%|███████▎  | 851/1170 [02:55<01:02,  5.09it/s] 73%|███████▎  | 852/1170 [02:55<01:02,  5.09it/s] 73%|███████▎  | 853/1170 [02:55<01:02,  5.09it/s] 73%|███████▎  | 854/1170 [02:55<01:02,  5.06it/s] 73%|███████▎  | 855/1170 [02:55<01:02,  5.07it/s] 73%|███████▎  | 856/1170 [02:56<01:01,  5.08it/s] 73%|███████▎  | 857/1170 [02:56<01:01,  5.09it/s] 73%|███████▎  | 858/1170 [02:56<01:00,  5.15it/s] 73%|███████▎  | 859/1170 [02:56<01:00,  5.13it/s] 74%|███████▎  | 860/1170 [02:56<01:00,  5.12it/s] 74%|███████▎  | 861/1170 [02:57<01:00,  5.11it/s] 74%|███████▎  | 862/1170 [02:57<01:00,  5.11it/s] 74%|███████▍  | 863/1170 [02:57<01:00,  5.11it/s] 74%|███████▍  | 864/1170 [02:57<00:59,  5.11it/s] 74%|███████▍  | 865/1170 [02:57<00:59,  5.11it/s] 74%|███████▍  | 866/1170 [02:58<00:59,  5.10it/s] 74%|███████▍  | 867/1170 [02:58<00:59,  5.10it/s] 74%|███████▍  | 868/1170 [02:58<00:59,  5.10it/s] 74%|███████▍  | 869/1170 [02:58<00:59,  5.10it/s] 74%|███████▍  | 870/1170 [02:58<00:58,  5.10it/s] 74%|███████▍  | 871/1170 [02:59<00:58,  5.10it/s] 75%|███████▍  | 872/1170 [02:59<00:58,  5.10it/s] 75%|███████▍  | 873/1170 [02:59<00:58,  5.09it/s] 75%|███████▍  | 874/1170 [02:59<00:58,  5.09it/s] 75%|███████▍  | 875/1170 [02:59<00:57,  5.09it/s] 75%|███████▍  | 876/1170 [03:00<00:57,  5.10it/s] 75%|███████▍  | 877/1170 [03:00<00:57,  5.10it/s] 75%|███████▌  | 878/1170 [03:00<00:57,  5.10it/s] 75%|███████▌  | 879/1170 [03:00<00:57,  5.10it/s] 75%|███████▌  | 880/1170 [03:00<00:56,  5.10it/s] 75%|███████▌  | 881/1170 [03:01<00:56,  5.10it/s] 75%|███████▌  | 882/1170 [03:01<00:56,  5.10it/s] 75%|███████▌  | 883/1170 [03:01<00:56,  5.10it/s] 76%|███████▌  | 884/1170 [03:01<00:56,  5.10it/s] 76%|███████▌  | 885/1170 [03:01<00:55,  5.10it/s] 76%|███████▌  | 886/1170 [03:01<00:55,  5.10it/s] 76%|███████▌  | 887/1170 [03:02<00:55,  5.10it/s] 76%|███████▌  | 888/1170 [03:02<00:55,  5.10it/s] 76%|███████▌  | 889/1170 [03:02<00:55,  5.10it/s] 76%|███████▌  | 890/1170 [03:02<00:54,  5.10it/s] 76%|███████▌  | 891/1170 [03:02<00:54,  5.10it/s] 76%|███████▌  | 892/1170 [03:03<00:54,  5.10it/s] 76%|███████▋  | 893/1170 [03:03<00:54,  5.10it/s] 76%|███████▋  | 894/1170 [03:03<00:54,  5.10it/s] 76%|███████▋  | 895/1170 [03:03<00:53,  5.10it/s] 77%|███████▋  | 896/1170 [03:03<00:53,  5.10it/s] 77%|███████▋  | 897/1170 [03:04<00:52,  5.16it/s] 77%|███████▋  | 898/1170 [03:04<00:52,  5.14it/s] 77%|███████▋  | 899/1170 [03:04<00:52,  5.13it/s] 77%|███████▋  | 900/1170 [03:04<00:52,  5.12it/s] 77%|███████▋  | 901/1170 [03:04<00:52,  5.12it/s] 77%|███████▋  | 902/1170 [03:05<00:52,  5.12it/s] 77%|███████▋  | 903/1170 [03:05<00:52,  5.11it/s] 77%|███████▋  | 904/1170 [03:05<00:52,  5.11it/s] 77%|███████▋  | 905/1170 [03:05<00:51,  5.11it/s] 77%|███████▋  | 906/1170 [03:05<00:51,  5.11it/s] 78%|███████▊  | 907/1170 [03:06<00:51,  5.11it/s] 78%|███████▊  | 908/1170 [03:06<00:51,  5.11it/s] 78%|███████▊  | 909/1170 [03:06<00:51,  5.11it/s] 78%|███████▊  | 910/1170 [03:06<00:50,  5.11it/s] 78%|███████▊  | 911/1170 [03:06<00:50,  5.10it/s] 78%|███████▊  | 912/1170 [03:07<00:50,  5.10it/s] 78%|███████▊  | 913/1170 [03:07<00:50,  5.10it/s] 78%|███████▊  | 914/1170 [03:07<00:50,  5.10it/s] 78%|███████▊  | 915/1170 [03:07<00:49,  5.10it/s] 78%|███████▊  | 916/1170 [03:07<00:49,  5.10it/s] 78%|███████▊  | 917/1170 [03:08<00:49,  5.10it/s] 78%|███████▊  | 918/1170 [03:08<00:49,  5.10it/s] 79%|███████▊  | 919/1170 [03:08<00:49,  5.10it/s] 79%|███████▊  | 920/1170 [03:08<00:49,  5.10it/s] 79%|███████▊  | 921/1170 [03:08<00:48,  5.10it/s] 79%|███████▉  | 922/1170 [03:09<00:48,  5.10it/s] 79%|███████▉  | 923/1170 [03:09<00:48,  5.10it/s] 79%|███████▉  | 924/1170 [03:09<00:48,  5.10it/s] 79%|███████▉  | 925/1170 [03:09<00:48,  5.10it/s] 79%|███████▉  | 926/1170 [03:09<00:47,  5.10it/s] 79%|███████▉  | 927/1170 [03:10<00:47,  5.10it/s] 79%|███████▉  | 928/1170 [03:10<00:47,  5.10it/s] 79%|███████▉  | 929/1170 [03:10<00:47,  5.11it/s] 79%|███████▉  | 930/1170 [03:10<00:47,  5.10it/s] 80%|███████▉  | 931/1170 [03:10<00:46,  5.10it/s] 80%|███████▉  | 932/1170 [03:11<00:46,  5.10it/s] 80%|███████▉  | 933/1170 [03:11<00:46,  5.10it/s] 80%|███████▉  | 934/1170 [03:11<00:46,  5.10it/s] 80%|███████▉  | 935/1170 [03:11<00:46,  5.10it/s] 80%|████████  | 936/1170 [03:11<00:45,  5.16it/s] 80%|████████  | 937/1170 [03:11<00:45,  5.14it/s] 80%|████████  | 938/1170 [03:12<00:45,  5.13it/s] 80%|████████  | 939/1170 [03:12<00:45,  5.11it/s] 80%|████████  | 940/1170 [03:12<00:45,  5.11it/s] 80%|████████  | 941/1170 [03:12<00:44,  5.10it/s] 81%|████████  | 942/1170 [03:12<00:44,  5.10it/s] 81%|████████  | 943/1170 [03:13<00:44,  5.09it/s] 81%|████████  | 944/1170 [03:13<00:44,  5.09it/s] 81%|████████  | 945/1170 [03:13<00:44,  5.09it/s] 81%|████████  | 946/1170 [03:13<00:43,  5.09it/s] 81%|████████  | 947/1170 [03:13<00:43,  5.09it/s] 81%|████████  | 948/1170 [03:14<00:43,  5.09it/s] 81%|████████  | 949/1170 [03:14<00:43,  5.09it/s] 81%|████████  | 950/1170 [03:14<00:43,  5.09it/s] 81%|████████▏ | 951/1170 [03:14<00:42,  5.10it/s] 81%|████████▏ | 952/1170 [03:14<00:42,  5.10it/s] 81%|████████▏ | 953/1170 [03:15<00:42,  5.10it/s] 82%|████████▏ | 954/1170 [03:15<00:42,  5.10it/s] 82%|████████▏ | 955/1170 [03:15<00:42,  5.10it/s] 82%|████████▏ | 956/1170 [03:15<00:41,  5.10it/s] 82%|████████▏ | 957/1170 [03:15<00:41,  5.10it/s] 82%|████████▏ | 958/1170 [03:16<00:41,  5.10it/s] 82%|████████▏ | 959/1170 [03:16<00:41,  5.10it/s] 82%|████████▏ | 960/1170 [03:16<00:41,  5.10it/s] 82%|████████▏ | 961/1170 [03:16<00:40,  5.10it/s] 82%|████████▏ | 962/1170 [03:16<00:40,  5.10it/s] 82%|████████▏ | 963/1170 [03:17<00:40,  5.10it/s] 82%|████████▏ | 964/1170 [03:17<00:40,  5.10it/s] 82%|████████▏ | 965/1170 [03:17<00:40,  5.10it/s] 83%|████████▎ | 966/1170 [03:17<00:40,  5.10it/s] 83%|████████▎ | 967/1170 [03:17<00:39,  5.10it/s] 83%|████████▎ | 968/1170 [03:18<00:39,  5.10it/s] 83%|████████▎ | 969/1170 [03:18<00:39,  5.10it/s] 83%|████████▎ | 970/1170 [03:18<00:39,  5.10it/s] 83%|████████▎ | 971/1170 [03:18<00:39,  5.10it/s] 83%|████████▎ | 972/1170 [03:18<00:38,  5.10it/s] 83%|████████▎ | 973/1170 [03:19<00:38,  5.10it/s] 83%|████████▎ | 974/1170 [03:19<00:38,  5.10it/s] 83%|████████▎ | 975/1170 [03:19<00:37,  5.16it/s] 83%|████████▎ | 976/1170 [03:19<00:37,  5.14it/s] 84%|████████▎ | 977/1170 [03:19<00:37,  5.13it/s] 84%|████████▎ | 978/1170 [03:20<00:37,  5.12it/s] 84%|████████▎ | 979/1170 [03:20<00:37,  5.12it/s] 84%|████████▍ | 980/1170 [03:20<00:37,  5.11it/s] 84%|████████▍ | 981/1170 [03:20<00:37,  5.11it/s] 84%|████████▍ | 982/1170 [03:20<00:36,  5.11it/s] 84%|████████▍ | 983/1170 [03:20<00:36,  5.10it/s] 84%|████████▍ | 984/1170 [03:21<00:36,  5.10it/s] 84%|████████▍ | 985/1170 [03:21<00:36,  5.10it/s] 84%|████████▍ | 986/1170 [03:21<00:36,  5.10it/s] 84%|████████▍ | 987/1170 [03:21<00:35,  5.10it/s] 84%|████████▍ | 988/1170 [03:21<00:35,  5.10it/s] 85%|████████▍ | 989/1170 [03:22<00:35,  5.10it/s] 85%|████████▍ | 990/1170 [03:22<00:35,  5.10it/s] 85%|████████▍ | 991/1170 [03:22<00:35,  5.10it/s] 85%|████████▍ | 992/1170 [03:22<00:34,  5.10it/s] 85%|████████▍ | 993/1170 [03:22<00:34,  5.10it/s] 85%|████████▍ | 994/1170 [03:23<00:34,  5.10it/s] 85%|████████▌ | 995/1170 [03:23<00:34,  5.10it/s] 85%|████████▌ | 996/1170 [03:23<00:34,  5.10it/s] 85%|████████▌ | 997/1170 [03:23<00:33,  5.10it/s] 85%|████████▌ | 998/1170 [03:23<00:33,  5.10it/s] 85%|████████▌ | 999/1170 [03:24<00:33,  5.10it/s] 85%|████████▌ | 1000/1170 [03:24<00:33,  5.10it/s]                                                    85%|████████▌ | 1000/1170 [03:24<00:33,  5.10it/s][INFO|trainer.py:2409] 2022-08-27 22:06:41,274 >> Saving model checkpoint to tmp/baseline/RTE/bert-base-uncased/HPO_S0/1e-06/64/30/checkpoint-1000
[INFO|configuration_utils.py:446] 2022-08-27 22:06:41,275 >> Configuration saved in tmp/baseline/RTE/bert-base-uncased/HPO_S0/1e-06/64/30/checkpoint-1000/config.json
[INFO|modeling_utils.py:1602] 2022-08-27 22:06:42,088 >> Model weights saved in tmp/baseline/RTE/bert-base-uncased/HPO_S0/1e-06/64/30/checkpoint-1000/pytorch_model.bin
[INFO|tokenization_utils_base.py:2108] 2022-08-27 22:06:42,089 >> tokenizer config file saved in tmp/baseline/RTE/bert-base-uncased/HPO_S0/1e-06/64/30/checkpoint-1000/tokenizer_config.json
[INFO|tokenization_utils_base.py:2114] 2022-08-27 22:06:42,089 >> Special tokens file saved in tmp/baseline/RTE/bert-base-uncased/HPO_S0/1e-06/64/30/checkpoint-1000/special_tokens_map.json
/home/ubuntu/anaconda3/envs/transformers/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 86%|████████▌ | 1001/1170 [03:27<02:42,  1.04it/s] 86%|████████▌ | 1002/1170 [03:27<02:02,  1.37it/s] 86%|████████▌ | 1003/1170 [03:27<01:35,  1.75it/s] 86%|████████▌ | 1004/1170 [03:27<01:16,  2.18it/s] 86%|████████▌ | 1005/1170 [03:27<01:02,  2.63it/s] 86%|████████▌ | 1006/1170 [03:28<00:53,  3.08it/s] 86%|████████▌ | 1007/1170 [03:28<00:46,  3.49it/s] 86%|████████▌ | 1008/1170 [03:28<00:41,  3.86it/s] 86%|████████▌ | 1009/1170 [03:28<00:38,  4.16it/s] 86%|████████▋ | 1010/1170 [03:28<00:36,  4.41it/s] 86%|████████▋ | 1011/1170 [03:29<00:34,  4.59it/s] 86%|████████▋ | 1012/1170 [03:29<00:33,  4.73it/s] 87%|████████▋ | 1013/1170 [03:29<00:32,  4.84it/s] 87%|████████▋ | 1014/1170 [03:29<00:31,  4.97it/s] 87%|████████▋ | 1015/1170 [03:29<00:30,  5.01it/s] 87%|████████▋ | 1016/1170 [03:30<00:30,  5.03it/s] 87%|████████▋ | 1017/1170 [03:30<00:30,  5.06it/s] 87%|████████▋ | 1018/1170 [03:30<00:29,  5.07it/s] 87%|████████▋ | 1019/1170 [03:30<00:29,  5.08it/s] 87%|████████▋ | 1020/1170 [03:30<00:29,  5.09it/s] 87%|████████▋ | 1021/1170 [03:30<00:29,  5.09it/s] 87%|████████▋ | 1022/1170 [03:31<00:29,  5.08it/s] 87%|████████▋ | 1023/1170 [03:31<00:28,  5.09it/s] 88%|████████▊ | 1024/1170 [03:31<00:28,  5.09it/s] 88%|████████▊ | 1025/1170 [03:31<00:28,  5.09it/s] 88%|████████▊ | 1026/1170 [03:31<00:28,  5.09it/s] 88%|████████▊ | 1027/1170 [03:32<00:28,  5.09it/s] 88%|████████▊ | 1028/1170 [03:32<00:27,  5.09it/s] 88%|████████▊ | 1029/1170 [03:32<00:27,  5.09it/s] 88%|████████▊ | 1030/1170 [03:32<00:27,  5.09it/s] 88%|████████▊ | 1031/1170 [03:32<00:27,  5.09it/s] 88%|████████▊ | 1032/1170 [03:33<00:27,  5.09it/s] 88%|████████▊ | 1033/1170 [03:33<00:26,  5.08it/s] 88%|████████▊ | 1034/1170 [03:33<00:26,  5.08it/s] 88%|████████▊ | 1035/1170 [03:33<00:26,  5.08it/s] 89%|████████▊ | 1036/1170 [03:33<00:26,  5.09it/s] 89%|████████▊ | 1037/1170 [03:34<00:26,  5.09it/s] 89%|████████▊ | 1038/1170 [03:34<00:25,  5.09it/s] 89%|████████▉ | 1039/1170 [03:34<00:25,  5.09it/s] 89%|████████▉ | 1040/1170 [03:34<00:25,  5.09it/s] 89%|████████▉ | 1041/1170 [03:34<00:25,  5.09it/s] 89%|████████▉ | 1042/1170 [03:35<00:25,  5.09it/s] 89%|████████▉ | 1043/1170 [03:35<00:24,  5.09it/s] 89%|████████▉ | 1044/1170 [03:35<00:24,  5.09it/s] 89%|████████▉ | 1045/1170 [03:35<00:24,  5.09it/s] 89%|████████▉ | 1046/1170 [03:35<00:24,  5.09it/s] 89%|████████▉ | 1047/1170 [03:36<00:24,  5.09it/s] 90%|████████▉ | 1048/1170 [03:36<00:23,  5.09it/s] 90%|████████▉ | 1049/1170 [03:36<00:23,  5.09it/s] 90%|████████▉ | 1050/1170 [03:36<00:23,  5.09it/s] 90%|████████▉ | 1051/1170 [03:36<00:23,  5.09it/s] 90%|████████▉ | 1052/1170 [03:37<00:23,  5.09it/s] 90%|█████████ | 1053/1170 [03:37<00:22,  5.15it/s] 90%|█████████ | 1054/1170 [03:37<00:22,  5.13it/s] 90%|█████████ | 1055/1170 [03:37<00:22,  5.12it/s] 90%|█████████ | 1056/1170 [03:37<00:22,  5.11it/s] 90%|█████████ | 1057/1170 [03:38<00:22,  5.10it/s] 90%|█████████ | 1058/1170 [03:38<00:21,  5.10it/s] 91%|█████████ | 1059/1170 [03:38<00:21,  5.09it/s] 91%|█████████ | 1060/1170 [03:38<00:21,  5.09it/s] 91%|█████████ | 1061/1170 [03:38<00:21,  5.09it/s] 91%|█████████ | 1062/1170 [03:39<00:21,  5.09it/s] 91%|█████████ | 1063/1170 [03:39<00:21,  5.09it/s] 91%|█████████ | 1064/1170 [03:39<00:20,  5.09it/s] 91%|█████████ | 1065/1170 [03:39<00:20,  5.09it/s] 91%|█████████ | 1066/1170 [03:39<00:20,  5.09it/s] 91%|█████████ | 1067/1170 [03:40<00:20,  5.09it/s] 91%|█████████▏| 1068/1170 [03:40<00:20,  5.09it/s] 91%|█████████▏| 1069/1170 [03:40<00:19,  5.08it/s] 91%|█████████▏| 1070/1170 [03:40<00:19,  5.08it/s] 92%|█████████▏| 1071/1170 [03:40<00:19,  5.08it/s] 92%|█████████▏| 1072/1170 [03:41<00:19,  5.09it/s] 92%|█████████▏| 1073/1170 [03:41<00:19,  5.09it/s] 92%|█████████▏| 1074/1170 [03:41<00:18,  5.09it/s] 92%|█████████▏| 1075/1170 [03:41<00:18,  5.10it/s] 92%|█████████▏| 1076/1170 [03:41<00:18,  5.10it/s] 92%|█████████▏| 1077/1170 [03:41<00:18,  5.10it/s] 92%|█████████▏| 1078/1170 [03:42<00:18,  5.10it/s] 92%|█████████▏| 1079/1170 [03:42<00:17,  5.10it/s] 92%|█████████▏| 1080/1170 [03:42<00:17,  5.10it/s] 92%|█████████▏| 1081/1170 [03:42<00:17,  5.10it/s] 92%|█████████▏| 1082/1170 [03:42<00:17,  5.11it/s] 93%|█████████▎| 1083/1170 [03:43<00:17,  5.10it/s] 93%|█████████▎| 1084/1170 [03:43<00:16,  5.10it/s] 93%|█████████▎| 1085/1170 [03:43<00:16,  5.10it/s] 93%|█████████▎| 1086/1170 [03:43<00:16,  5.10it/s] 93%|█████████▎| 1087/1170 [03:43<00:16,  5.10it/s] 93%|█████████▎| 1088/1170 [03:44<00:16,  5.10it/s] 93%|█████████▎| 1089/1170 [03:44<00:15,  5.10it/s] 93%|█████████▎| 1090/1170 [03:44<00:15,  5.09it/s] 93%|█████████▎| 1091/1170 [03:44<00:15,  5.09it/s] 93%|█████████▎| 1092/1170 [03:44<00:15,  5.15it/s] 93%|█████████▎| 1093/1170 [03:45<00:15,  5.12it/s] 94%|█████████▎| 1094/1170 [03:45<00:14,  5.11it/s] 94%|█████████▎| 1095/1170 [03:45<00:14,  5.11it/s] 94%|█████████▎| 1096/1170 [03:45<00:14,  5.11it/s] 94%|█████████▍| 1097/1170 [03:45<00:14,  5.11it/s] 94%|█████████▍| 1098/1170 [03:46<00:15,  4.57it/s] 94%|█████████▍| 1099/1170 [03:46<00:15,  4.71it/s] 94%|█████████▍| 1100/1170 [03:46<00:14,  4.82it/s] 94%|█████████▍| 1101/1170 [03:46<00:14,  4.90it/s] 94%|█████████▍| 1102/1170 [03:46<00:13,  4.96it/s] 94%|█████████▍| 1103/1170 [03:47<00:13,  5.00it/s] 94%|█████████▍| 1104/1170 [03:47<00:13,  5.03it/s] 94%|█████████▍| 1105/1170 [03:47<00:12,  5.05it/s] 95%|█████████▍| 1106/1170 [03:47<00:12,  5.07it/s] 95%|█████████▍| 1107/1170 [03:47<00:12,  5.07it/s] 95%|█████████▍| 1108/1170 [03:48<00:12,  5.08it/s] 95%|█████████▍| 1109/1170 [03:48<00:11,  5.09it/s] 95%|█████████▍| 1110/1170 [03:48<00:11,  5.09it/s] 95%|█████████▍| 1111/1170 [03:48<00:11,  5.09it/s] 95%|█████████▌| 1112/1170 [03:48<00:11,  5.09it/s] 95%|█████████▌| 1113/1170 [03:49<00:11,  5.10it/s] 95%|█████████▌| 1114/1170 [03:49<00:10,  5.10it/s] 95%|█████████▌| 1115/1170 [03:49<00:10,  5.10it/s] 95%|█████████▌| 1116/1170 [03:49<00:10,  5.10it/s] 95%|█████████▌| 1117/1170 [03:49<00:10,  5.10it/s] 96%|█████████▌| 1118/1170 [03:50<00:10,  5.10it/s] 96%|█████████▌| 1119/1170 [03:50<00:09,  5.10it/s] 96%|█████████▌| 1120/1170 [03:50<00:09,  5.10it/s] 96%|█████████▌| 1121/1170 [03:50<00:09,  5.10it/s] 96%|█████████▌| 1122/1170 [03:50<00:09,  5.10it/s] 96%|█████████▌| 1123/1170 [03:51<00:09,  5.10it/s] 96%|█████████▌| 1124/1170 [03:51<00:09,  5.10it/s] 96%|█████████▌| 1125/1170 [03:51<00:08,  5.10it/s] 96%|█████████▌| 1126/1170 [03:51<00:08,  5.10it/s] 96%|█████████▋| 1127/1170 [03:51<00:08,  5.10it/s] 96%|█████████▋| 1128/1170 [03:52<00:08,  5.10it/s] 96%|█████████▋| 1129/1170 [03:52<00:08,  5.10it/s] 97%|█████████▋| 1130/1170 [03:52<00:07,  5.10it/s] 97%|█████████▋| 1131/1170 [03:52<00:07,  5.16it/s] 97%|█████████▋| 1132/1170 [03:52<00:07,  5.14it/s] 97%|█████████▋| 1133/1170 [03:53<00:07,  5.13it/s] 97%|█████████▋| 1134/1170 [03:53<00:07,  5.12it/s] 97%|█████████▋| 1135/1170 [03:53<00:06,  5.12it/s] 97%|█████████▋| 1136/1170 [03:53<00:06,  5.11it/s] 97%|█████████▋| 1137/1170 [03:53<00:06,  5.11it/s] 97%|█████████▋| 1138/1170 [03:54<00:06,  5.11it/s] 97%|█████████▋| 1139/1170 [03:54<00:06,  5.11it/s] 97%|█████████▋| 1140/1170 [03:54<00:05,  5.10it/s] 98%|█████████▊| 1141/1170 [03:54<00:05,  5.10it/s] 98%|█████████▊| 1142/1170 [03:54<00:05,  5.10it/s] 98%|█████████▊| 1143/1170 [03:54<00:05,  5.10it/s] 98%|█████████▊| 1144/1170 [03:55<00:05,  5.10it/s] 98%|█████████▊| 1145/1170 [03:55<00:04,  5.11it/s] 98%|█████████▊| 1146/1170 [03:55<00:04,  5.10it/s] 98%|█████████▊| 1147/1170 [03:55<00:04,  5.10it/s] 98%|█████████▊| 1148/1170 [03:55<00:04,  5.10it/s] 98%|█████████▊| 1149/1170 [03:56<00:04,  5.10it/s] 98%|█████████▊| 1150/1170 [03:56<00:03,  5.10it/s] 98%|█████████▊| 1151/1170 [03:56<00:03,  4.78it/s] 98%|█████████▊| 1152/1170 [03:56<00:03,  4.87it/s] 99%|█████████▊| 1153/1170 [03:56<00:03,  4.93it/s] 99%|█████████▊| 1154/1170 [03:57<00:03,  4.98it/s] 99%|█████████▊| 1155/1170 [03:57<00:02,  5.01it/s] 99%|█████████▉| 1156/1170 [03:57<00:02,  5.04it/s] 99%|█████████▉| 1157/1170 [03:57<00:02,  5.06it/s] 99%|█████████▉| 1158/1170 [03:57<00:02,  5.07it/s] 99%|█████████▉| 1159/1170 [03:58<00:02,  5.08it/s] 99%|█████████▉| 1160/1170 [03:58<00:01,  5.09it/s] 99%|█████████▉| 1161/1170 [03:58<00:01,  5.09it/s] 99%|█████████▉| 1162/1170 [03:58<00:01,  5.10it/s] 99%|█████████▉| 1163/1170 [03:58<00:01,  5.10it/s] 99%|█████████▉| 1164/1170 [03:59<00:01,  5.10it/s]100%|█████████▉| 1165/1170 [03:59<00:00,  5.10it/s]100%|█████████▉| 1166/1170 [03:59<00:00,  5.10it/s]100%|█████████▉| 1167/1170 [03:59<00:00,  5.10it/s]100%|█████████▉| 1168/1170 [03:59<00:00,  5.11it/s]100%|█████████▉| 1169/1170 [04:00<00:00,  5.11it/s]100%|██████████| 1170/1170 [04:00<00:00,  5.16it/s][INFO|trainer.py:1679] 2022-08-27 22:07:17,263 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


                                                   100%|██████████| 1170/1170 [04:00<00:00,  5.16it/s]100%|██████████| 1170/1170 [04:00<00:00,  4.87it/s]
[INFO|trainer.py:2409] 2022-08-27 22:07:17,264 >> Saving model checkpoint to tmp/baseline/RTE/bert-base-uncased/HPO_S0/1e-06/64/30
[INFO|configuration_utils.py:446] 2022-08-27 22:07:17,265 >> Configuration saved in tmp/baseline/RTE/bert-base-uncased/HPO_S0/1e-06/64/30/config.json
[INFO|modeling_utils.py:1602] 2022-08-27 22:07:18,068 >> Model weights saved in tmp/baseline/RTE/bert-base-uncased/HPO_S0/1e-06/64/30/pytorch_model.bin
[INFO|tokenization_utils_base.py:2108] 2022-08-27 22:07:18,069 >> tokenizer config file saved in tmp/baseline/RTE/bert-base-uncased/HPO_S0/1e-06/64/30/tokenizer_config.json
[INFO|tokenization_utils_base.py:2114] 2022-08-27 22:07:18,069 >> Special tokens file saved in tmp/baseline/RTE/bert-base-uncased/HPO_S0/1e-06/64/30/special_tokens_map.json
{'loss': 0.6992, 'learning_rate': 7.158119658119658e-07, 'epoch': 12.82}
{'loss': 0.6962, 'learning_rate': 1.8162393162393162e-07, 'epoch': 25.64}
{'train_runtime': 240.3212, 'train_samples_per_second': 310.834, 'train_steps_per_second': 4.868, 'train_loss': 0.6976451254298545, 'epoch': 30.0}
***** train metrics *****
  epoch                    =       30.0
  train_loss               =     0.6976
  train_runtime            = 0:04:00.32
  train_samples            =       2490
  train_samples_per_second =    310.834
  train_steps_per_second   =      4.868
08/27/2022 22:07:18 - INFO - __main__ - *** Evaluate ***
[INFO|trainer.py:628] 2022-08-27 22:07:18,115 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: idx, sentence2, sentence1. If idx, sentence2, sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:2659] 2022-08-27 22:07:18,118 >> ***** Running Evaluation *****
[INFO|trainer.py:2661] 2022-08-27 22:07:18,118 >>   Num examples = 277
[INFO|trainer.py:2664] 2022-08-27 22:07:18,118 >>   Batch size = 16
  0%|          | 0/18 [00:00<?, ?it/s] 17%|█▋        | 3/18 [00:00<00:00, 29.16it/s] 33%|███▎      | 6/18 [00:00<00:00, 22.68it/s] 50%|█████     | 9/18 [00:00<00:00, 21.14it/s] 67%|██████▋   | 12/18 [00:00<00:00, 20.44it/s] 83%|████████▎ | 15/18 [00:00<00:00, 20.09it/s]100%|██████████| 18/18 [00:00<00:00, 20.03it/s]08/27/2022 22:07:19 - INFO - datasets.metric - Removing /home/ubuntu/.cache/huggingface/metrics/glue/rte/default_experiment-1-0.arrow
100%|██████████| 18/18 [00:00<00:00, 20.59it/s]
***** eval metrics *****
  epoch                   =       30.0
  eval_accuracy           =     0.4729
  eval_loss               =     0.6935
  eval_runtime            = 0:00:00.92
  eval_samples            =        277
  eval_samples_per_second =    298.457
  eval_steps_per_second   =     19.394
08/27/2022 22:07:22 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 2distributed training: False, 16-bits training: True
08/27/2022 22:07:22 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=2,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=IntervalStrategy.NO,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_min_num_params=0,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=1e-06,
length_column_name=length,
load_best_model_at_end=False,
local_rank=-1,
log_level=-1,
log_level_replica=-1,
log_on_each_node=True,
logging_dir=tmp/baseline/RTE/bert-base-uncased/HPO_S0/1e-06/64/100/runs/Aug27_22-07-22_ip-10-21-39-60,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
no_cuda=False,
num_train_epochs=100.0,
optim=OptimizerNames.ADAMW_HF,
output_dir=tmp/baseline/RTE/bert-base-uncased/HPO_S0/1e-06/64/100,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=tmp/baseline/RTE/bert-base-uncased/HPO_S0/1e-06/64/100,
save_on_each_node=False,
save_steps=500,
save_strategy=IntervalStrategy.STEPS,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tf32=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
xpu_backend=None,
)
08/27/2022 22:07:23 - INFO - datasets.info - Loading Dataset Infos from /home/ubuntu/.cache/huggingface/modules/datasets_modules/datasets/glue/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad
08/27/2022 22:07:23 - INFO - datasets.builder - Overwrite dataset info from restored data version.
08/27/2022 22:07:23 - INFO - datasets.info - Loading Dataset info from /home/ubuntu/.cache/huggingface/datasets/glue/rte/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad
08/27/2022 22:07:23 - WARNING - datasets.builder - Reusing dataset glue (/home/ubuntu/.cache/huggingface/datasets/glue/rte/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)
08/27/2022 22:07:23 - INFO - datasets.info - Loading Dataset info from /home/ubuntu/.cache/huggingface/datasets/glue/rte/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 759.79it/s]
[INFO|configuration_utils.py:657] 2022-08-27 22:07:23,099 >> loading configuration file /home/ubuntu/checkpoints/exp/RTE/config.json
[INFO|configuration_utils.py:708] 2022-08-27 22:07:23,100 >> Model config BertConfig {
  "_name_or_path": "/home/ubuntu/checkpoints/exp/RTE",
  "architectures": [
    "BertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "crypten": false,
  "finetuning_task": "rte",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "entailment",
    "1": "not_entailment"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "entailment": 0,
    "not_entailment": 1
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "softmax_act": "softmax",
  "torch_dtype": "float32",
  "transformers_version": "4.20.0.dev0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|tokenization_utils_base.py:1701] 2022-08-27 22:07:23,101 >> Didn't find file /home/ubuntu/checkpoints/exp/RTE/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1779] 2022-08-27 22:07:23,101 >> loading file /home/ubuntu/checkpoints/exp/RTE/vocab.txt
[INFO|tokenization_utils_base.py:1779] 2022-08-27 22:07:23,101 >> loading file /home/ubuntu/checkpoints/exp/RTE/tokenizer.json
[INFO|tokenization_utils_base.py:1779] 2022-08-27 22:07:23,101 >> loading file None
[INFO|tokenization_utils_base.py:1779] 2022-08-27 22:07:23,101 >> loading file /home/ubuntu/checkpoints/exp/RTE/special_tokens_map.json
[INFO|tokenization_utils_base.py:1779] 2022-08-27 22:07:23,101 >> loading file /home/ubuntu/checkpoints/exp/RTE/tokenizer_config.json
[INFO|modeling_utils.py:2047] 2022-08-27 22:07:23,140 >> loading weights file /home/ubuntu/checkpoints/exp/RTE/pytorch_model.bin
[INFO|modeling_bert.py:194] 2022-08-27 22:07:23,476 >> initializing embedding using nn.Embedding
[INFO|modeling_utils.py:2417] 2022-08-27 22:07:24,630 >> All model checkpoint weights were used when initializing BertForSequenceClassification.

[INFO|modeling_utils.py:2426] 2022-08-27 22:07:24,630 >> All the weights of BertForSequenceClassification were initialized from the model checkpoint at /home/ubuntu/checkpoints/exp/RTE.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.
using model config: BertConfig {
  "_name_or_path": "/home/ubuntu/checkpoints/exp/RTE",
  "architectures": [
    "BertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "crypten": false,
  "finetuning_task": "rte",
  "gradient_checkpointing": false,
  "hidden_act": "quad",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "entailment",
    "1": "not_entailment"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "entailment": 0,
    "not_entailment": 1
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "softmax_act": "2quad",
  "torch_dtype": "float32",
  "transformers_version": "4.20.0.dev0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

BertConfig {
  "_name_or_path": "/home/ubuntu/checkpoints/exp/RTE",
  "architectures": [
    "BertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "crypten": false,
  "finetuning_task": "rte",
  "gradient_checkpointing": false,
  "hidden_act": "quad",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "entailment",
    "1": "not_entailment"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "entailment": 0,
    "not_entailment": 1
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "softmax_act": "2quad",
  "torch_dtype": "float32",
  "transformers_version": "4.20.0.dev0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

model architecture: BertForSequenceClassification(
  (bert): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): QuadActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): QuadActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): QuadActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): QuadActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): QuadActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): QuadActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): QuadActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): QuadActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): QuadActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): QuadActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): QuadActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): QuadActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (dropout): Dropout(p=0.1, inplace=False)
  (classifier): Linear(in_features=768, out_features=2, bias=True)
)
08/27/2022 22:07:24 - WARNING - datasets.fingerprint - Parameter 'function'=<function main.<locals>.preprocess_function at 0x7fa60591e488> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
Running tokenizer on dataset:   0%|          | 0/3 [00:00<?, ?ba/s]08/27/2022 22:07:24 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/ubuntu/.cache/huggingface/datasets/glue/rte/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-1c80317fa3b1799d.arrow
Running tokenizer on dataset:  33%|███▎      | 1/3 [00:00<00:00,  8.03ba/s]Running tokenizer on dataset:  67%|██████▋   | 2/3 [00:00<00:00,  7.31ba/s]Running tokenizer on dataset: 100%|██████████| 3/3 [00:00<00:00,  9.51ba/s]08/27/2022 22:07:25 - INFO - datasets.fingerprint - Parameter 'function'=<function main.<locals>.preprocess_function at 0x7fa60591a950> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead.

Running tokenizer on dataset:   0%|          | 0/1 [00:00<?, ?ba/s]08/27/2022 22:07:25 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/ubuntu/.cache/huggingface/datasets/glue/rte/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-bdd640fb06671ad1.arrow
Running tokenizer on dataset: 100%|██████████| 1/1 [00:00<00:00, 35.37ba/s]08/27/2022 22:07:25 - INFO - datasets.fingerprint - Parameter 'function'=<function main.<locals>.preprocess_function at 0x7fa60591e840> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead.

Running tokenizer on dataset:   0%|          | 0/3 [00:00<?, ?ba/s]08/27/2022 22:07:25 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/ubuntu/.cache/huggingface/datasets/glue/rte/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-3eb13b9046685257.arrow
Running tokenizer on dataset:  67%|██████▋   | 2/3 [00:00<00:00, 12.68ba/s]Running tokenizer on dataset: 100%|██████████| 3/3 [00:00<00:00, 11.55ba/s]128
08/27/2022 22:07:30 - INFO - __main__ - Sample 914 of the training set: {'sentence1': "Because of Reagan's economic strategy, the federal budget deficit ballooned.", 'sentence2': "Reagan's economic strategy led to huge federal budget deficits.", 'label': 0, 'idx': 914, 'input_ids': [101, 2138, 1997, 11531, 1005, 1055, 3171, 5656, 1010, 1996, 2976, 5166, 15074, 13212, 2098, 1012, 102, 11531, 1005, 1055, 3171, 5656, 2419, 2000, 4121, 2976, 5166, 15074, 2015, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.
128
08/27/2022 22:07:35 - INFO - __main__ - Sample 571 of the training set: {'sentence1': 'So far the British have preferred a policy of a voluntary approach to restricting advertising and high taxes on tobacco products.', 'sentence2': 'Sales have declined due to restrictions on advertising.', 'label': 1, 'idx': 571, 'input_ids': [101, 2061, 2521, 1996, 2329, 2031, 6871, 1037, 3343, 1997, 1037, 10758, 3921, 2000, 26996, 6475, 1998, 2152, 7773, 2006, 9098, 3688, 1012, 102, 4341, 2031, 6430, 2349, 2000, 9259, 2006, 6475, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.
128
08/27/2022 22:07:40 - INFO - __main__ - Sample 419 of the training set: {'sentence1': 'Protesters, many from organized pro-government groups but including many ordinary citizens, carried anti-American banners and chanted slogans attacking U.N. Secretary-General Kofi Annan for his close alignment with U.S. policy.', 'sentence2': 'Protesters confiscated anti-American banners and chanted slogans attacking U.N. Secretary-General Kofi Annan for his close alignment with U.S. policy.', 'label': 1, 'idx': 419, 'input_ids': [101, 13337, 1010, 2116, 2013, 4114, 4013, 1011, 2231, 2967, 2021, 2164, 2116, 6623, 4480, 1010, 3344, 3424, 1011, 2137, 23562, 1998, 16883, 2098, 14558, 2015, 7866, 1057, 1012, 1050, 1012, 3187, 1011, 2236, 12849, 8873, 4698, 2078, 2005, 2010, 2485, 12139, 2007, 1057, 1012, 1055, 1012, 3343, 1012, 102, 13337, 17182, 3424, 1011, 2137, 23562, 1998, 16883, 2098, 14558, 2015, 7866, 1057, 1012, 1050, 1012, 3187, 1011, 2236, 12849, 8873, 4698, 2078, 2005, 2010, 2485, 12139, 2007, 1057, 1012, 1055, 1012, 3343, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.

[INFO|trainer.py:506] 2022-08-27 22:07:44,730 >> Using amp half precision backend
[INFO|trainer.py:628] 2022-08-27 22:07:44,731 >> The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1, idx, sentence2. If sentence1, idx, sentence2 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.
/home/ubuntu/transformers_private/src/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  FutureWarning,
[INFO|trainer.py:1430] 2022-08-27 22:07:44,743 >> ***** Running training *****
[INFO|trainer.py:1431] 2022-08-27 22:07:44,743 >>   Num examples = 2490
[INFO|trainer.py:1432] 2022-08-27 22:07:44,743 >>   Num Epochs = 100
[INFO|trainer.py:1433] 2022-08-27 22:07:44,743 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1434] 2022-08-27 22:07:44,743 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1435] 2022-08-27 22:07:44,743 >>   Gradient Accumulation steps = 1
[INFO|trainer.py:1436] 2022-08-27 22:07:44,743 >>   Total optimization steps = 3900
using training arge: TrainingArguments(
_n_gpu=2,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=IntervalStrategy.NO,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_min_num_params=0,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=1e-06,
length_column_name=length,
load_best_model_at_end=False,
local_rank=-1,
log_level=-1,
log_level_replica=-1,
log_on_each_node=True,
logging_dir=tmp/baseline/RTE/bert-base-uncased/HPO_S0/1e-06/64/100/runs/Aug27_22-07-22_ip-10-21-39-60,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
no_cuda=False,
num_train_epochs=100.0,
optim=OptimizerNames.ADAMW_HF,
output_dir=tmp/baseline/RTE/bert-base-uncased/HPO_S0/1e-06/64/100,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=tmp/baseline/RTE/bert-base-uncased/HPO_S0/1e-06/64/100,
save_on_each_node=False,
save_steps=500,
save_strategy=IntervalStrategy.STEPS,
save_total_limit=0,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tf32=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
xpu_backend=None,
)
  0%|          | 0/3900 [00:00<?, ?it/s]/home/ubuntu/anaconda3/envs/transformers/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
  0%|          | 1/3900 [00:04<4:59:50,  4.61s/it]  0%|          | 2/3900 [00:04<2:11:56,  2.03s/it]  0%|          | 3/3900 [00:05<1:17:32,  1.19s/it]  0%|          | 4/3900 [00:05<51:59,  1.25it/s]    0%|          | 5/3900 [00:05<37:51,  1.71it/s]  0%|          | 6/3900 [00:05<29:21,  2.21it/s]  0%|          | 7/3900 [00:05<24:00,  2.70it/s]  0%|          | 8/3900 [00:06<20:25,  3.18it/s]  0%|          | 9/3900 [00:06<18:00,  3.60it/s]  0%|          | 10/3900 [00:06<16:22,  3.96it/s]  0%|          | 11/3900 [00:06<15:14,  4.25it/s]  0%|          | 12/3900 [00:06<14:32,  4.46it/s]  0%|          | 13/3900 [00:07<13:58,  4.63it/s]  0%|          | 14/3900 [00:07<13:35,  4.76it/s]  0%|          | 15/3900 [00:07<13:19,  4.86it/s]  0%|          | 16/3900 [00:07<13:08,  4.93it/s]  0%|          | 17/3900 [00:07<13:00,  4.98it/s]  0%|          | 18/3900 [00:07<12:54,  5.01it/s]  0%|          | 19/3900 [00:08<12:50,  5.04it/s]  1%|          | 20/3900 [00:08<12:47,  5.06it/s]  1%|          | 21/3900 [00:08<12:45,  5.07it/s]  1%|          | 22/3900 [00:08<12:44,  5.07it/s]  1%|          | 23/3900 [00:08<12:43,  5.08it/s]  1%|          | 24/3900 [00:09<12:42,  5.08it/s]  1%|          | 25/3900 [00:09<12:41,  5.09it/s]  1%|          | 26/3900 [00:09<12:40,  5.09it/s]  1%|          | 27/3900 [00:09<12:39,  5.10it/s]  1%|          | 28/3900 [00:09<12:39,  5.10it/s]  1%|          | 29/3900 [00:10<12:39,  5.10it/s]  1%|          | 30/3900 [00:10<12:39,  5.10it/s]  1%|          | 31/3900 [00:10<12:38,  5.10it/s]  1%|          | 32/3900 [00:10<12:38,  5.10it/s]  1%|          | 33/3900 [00:10<12:38,  5.10it/s]  1%|          | 34/3900 [00:11<12:38,  5.10it/s]  1%|          | 35/3900 [00:11<12:38,  5.10it/s]  1%|          | 36/3900 [00:11<12:37,  5.10it/s]  1%|          | 37/3900 [00:11<12:36,  5.10it/s]  1%|          | 38/3900 [00:11<12:36,  5.10it/s]  1%|          | 39/3900 [00:12<12:28,  5.16it/s]  1%|          | 40/3900 [00:12<12:30,  5.14it/s]  1%|          | 41/3900 [00:12<12:31,  5.13it/s]  1%|          | 42/3900 [00:12<12:32,  5.13it/s]  1%|          | 43/3900 [00:12<12:32,  5.12it/s]  1%|          | 44/3900 [00:13<12:33,  5.12it/s]  1%|          | 45/3900 [00:13<12:33,  5.12it/s]  1%|          | 46/3900 [00:13<12:34,  5.11it/s]  1%|          | 47/3900 [00:13<12:35,  5.10it/s]  1%|          | 48/3900 [00:13<12:35,  5.10it/s]  1%|▏         | 49/3900 [00:14<14:01,  4.58it/s]  1%|▏         | 50/3900 [00:14<13:39,  4.70it/s]  1%|▏         | 51/3900 [00:14<13:19,  4.81it/s]  1%|▏         | 52/3900 [00:14<13:05,  4.90it/s]  1%|▏         | 53/3900 [00:14<12:55,  4.96it/s]  1%|▏         | 54/3900 [00:15<12:48,  5.00it/s]  1%|▏         | 55/3900 [00:15<12:44,  5.03it/s]  1%|▏         | 56/3900 [00:15<12:41,  5.05it/s]  1%|▏         | 57/3900 [00:15<12:38,  5.07it/s]  1%|▏         | 58/3900 [00:15<12:37,  5.08it/s]  2%|▏         | 59/3900 [00:16<12:36,  5.08it/s]  2%|▏         | 60/3900 [00:16<12:39,  5.05it/s]  2%|▏         | 61/3900 [00:16<12:39,  5.05it/s]  2%|▏         | 62/3900 [00:16<12:37,  5.07it/s]  2%|▏         | 63/3900 [00:16<12:35,  5.08it/s]  2%|▏         | 64/3900 [00:17<12:34,  5.08it/s]  2%|▏         | 65/3900 [00:17<12:33,  5.09it/s]  2%|▏         | 66/3900 [00:17<12:34,  5.08it/s]  2%|▏         | 67/3900 [00:17<12:34,  5.08it/s]  2%|▏         | 68/3900 [00:17<12:33,  5.08it/s]  2%|▏         | 69/3900 [00:18<12:33,  5.08it/s]  2%|▏         | 70/3900 [00:18<12:33,  5.08it/s]  2%|▏         | 71/3900 [00:18<12:32,  5.09it/s]  2%|▏         | 72/3900 [00:18<12:32,  5.09it/s]  2%|▏         | 73/3900 [00:18<12:31,  5.09it/s]  2%|▏         | 74/3900 [00:19<12:30,  5.10it/s]  2%|▏         | 75/3900 [00:19<12:30,  5.10it/s]  2%|▏         | 76/3900 [00:19<12:30,  5.10it/s]  2%|▏         | 77/3900 [00:19<12:29,  5.10it/s]  2%|▏         | 78/3900 [00:19<12:20,  5.16it/s]  2%|▏         | 79/3900 [00:20<12:23,  5.14it/s]  2%|▏         | 80/3900 [00:20<12:25,  5.13it/s]  2%|▏         | 81/3900 [00:20<12:25,  5.12it/s]  2%|▏         | 82/3900 [00:20<12:26,  5.12it/s]  2%|▏         | 83/3900 [00:20<12:26,  5.11it/s]  2%|▏         | 84/3900 [00:21<12:26,  5.11it/s]  2%|▏         | 85/3900 [00:21<12:26,  5.11it/s]  2%|▏         | 86/3900 [00:21<12:27,  5.11it/s]  2%|▏         | 87/3900 [00:21<12:27,  5.10it/s]  2%|▏         | 88/3900 [00:21<12:27,  5.10it/s]  2%|▏         | 89/3900 [00:21<12:26,  5.10it/s]  2%|▏         | 90/3900 [00:22<12:26,  5.10it/s]  2%|▏         | 91/3900 [00:22<12:26,  5.10it/s]  2%|▏         | 92/3900 [00:22<12:26,  5.10it/s]  2%|▏         | 93/3900 [00:22<12:26,  5.10it/s]  2%|▏         | 94/3900 [00:22<12:26,  5.10it/s]  2%|▏         | 95/3900 [00:23<12:25,  5.10it/s]  2%|▏         | 96/3900 [00:23<12:25,  5.10it/s]  2%|▏         | 97/3900 [00:23<12:25,  5.10it/s]  3%|▎         | 98/3900 [00:23<12:26,  5.09it/s]  3%|▎         | 99/3900 [00:23<12:26,  5.09it/s]  3%|▎         | 100/3900 [00:24<12:25,  5.09it/s]  3%|▎         | 101/3900 [00:24<13:50,  4.57it/s]  3%|▎         | 102/3900 [00:24<14:52,  4.26it/s]  3%|▎         | 103/3900 [00:24<14:08,  4.48it/s]  3%|▎         | 104/3900 [00:25<13:36,  4.65it/s]  3%|▎         | 105/3900 [00:25<13:14,  4.78it/s]  3%|▎         | 106/3900 [00:25<12:59,  4.87it/s]  3%|▎         | 107/3900 [00:25<12:48,  4.94it/s]  3%|▎         | 108/3900 [00:25<12:40,  4.99it/s]  3%|▎         | 109/3900 [00:26<12:34,  5.02it/s]  3%|▎         | 110/3900 [00:26<12:30,  5.05it/s]  3%|▎         | 111/3900 [00:26<12:28,  5.06it/s]  3%|▎         | 112/3900 [00:26<12:26,  5.08it/s]  3%|▎         | 113/3900 [00:26<12:24,  5.09it/s]  3%|▎         | 114/3900 [00:27<12:23,  5.09it/s]  3%|▎         | 115/3900 [00:27<12:22,  5.10it/s]  3%|▎         | 116/3900 [00:27<12:22,  5.10it/s]  3%|▎         | 117/3900 [00:27<12:14,  5.15it/s]  3%|▎         | 118/3900 [00:27<12:17,  5.13it/s]  3%|▎         | 119/3900 [00:28<12:19,  5.11it/s]  3%|▎         | 120/3900 [00:28<12:19,  5.11it/s]  3%|▎         | 121/3900 [00:28<12:20,  5.11it/s]  3%|▎         | 122/3900 [00:28<12:20,  5.10it/s]  3%|▎         | 123/3900 [00:28<12:20,  5.10it/s]  3%|▎         | 124/3900 [00:28<12:20,  5.10it/s]  3%|▎         | 125/3900 [00:29<12:22,  5.08it/s]  3%|▎         | 126/3900 [00:29<12:22,  5.08it/s]  3%|▎         | 127/3900 [00:29<12:22,  5.08it/s]  3%|▎         | 128/3900 [00:29<12:21,  5.08it/s]  3%|▎         | 129/3900 [00:29<12:21,  5.09it/s]  3%|▎         | 130/3900 [00:30<12:20,  5.09it/s]  3%|▎         | 131/3900 [00:30<12:20,  5.09it/s]  3%|▎         | 132/3900 [00:30<12:22,  5.07it/s]  3%|▎         | 133/3900 [00:30<12:21,  5.08it/s]  3%|▎         | 134/3900 [00:30<12:20,  5.08it/s]  3%|▎         | 135/3900 [00:31<12:20,  5.09it/s]  3%|▎         | 136/3900 [00:31<12:19,  5.09it/s]  4%|▎         | 137/3900 [00:31<12:18,  5.10it/s]  4%|▎         | 138/3900 [00:31<12:18,  5.10it/s]  4%|▎         | 139/3900 [00:31<12:19,  5.09it/s]  4%|▎         | 140/3900 [00:32<12:18,  5.09it/s]  4%|▎         | 141/3900 [00:32<12:17,  5.10it/s]  4%|▎         | 142/3900 [00:32<12:16,  5.10it/s]  4%|▎         | 143/3900 [00:32<12:16,  5.10it/s]  4%|▎         | 144/3900 [00:32<12:15,  5.11it/s]  4%|▎         | 145/3900 [00:33<12:14,  5.11it/s]  4%|▎         | 146/3900 [00:33<12:14,  5.11it/s]  4%|▍         | 147/3900 [00:33<12:14,  5.11it/s]  4%|▍         | 148/3900 [00:33<12:14,  5.11it/s]  4%|▍         | 149/3900 [00:33<12:14,  5.11it/s]  4%|▍         | 150/3900 [00:34<12:13,  5.11it/s]  4%|▍         | 151/3900 [00:34<12:14,  5.11it/s]  4%|▍         | 152/3900 [00:34<12:14,  5.11it/s]  4%|▍         | 153/3900 [00:34<12:13,  5.11it/s]  4%|▍         | 154/3900 [00:34<12:13,  5.11it/s]  4%|▍         | 155/3900 [00:35<12:12,  5.11it/s]  4%|▍         | 156/3900 [00:35<12:04,  5.16it/s]  4%|▍         | 157/3900 [00:35<12:08,  5.14it/s]  4%|▍         | 158/3900 [00:35<12:09,  5.13it/s]  4%|▍         | 159/3900 [00:35<12:10,  5.12it/s]  4%|▍         | 160/3900 [00:36<12:10,  5.12it/s]  4%|▍         | 161/3900 [00:36<12:10,  5.12it/s]  4%|▍         | 162/3900 [00:36<12:11,  5.11it/s]  4%|▍         | 163/3900 [00:36<12:11,  5.11it/s]  4%|▍         | 164/3900 [00:36<12:10,  5.11it/s]  4%|▍         | 165/3900 [00:37<12:11,  5.11it/s]  4%|▍         | 166/3900 [00:37<12:10,  5.11it/s]  4%|▍         | 167/3900 [00:37<12:11,  5.10it/s]  4%|▍         | 168/3900 [00:37<12:10,  5.11it/s]  4%|▍         | 169/3900 [00:37<12:10,  5.11it/s]  4%|▍         | 170/3900 [00:38<12:10,  5.11it/s]  4%|▍         | 171/3900 [00:38<12:10,  5.10it/s]  4%|▍         | 172/3900 [00:38<12:09,  5.11it/s]  4%|▍         | 173/3900 [00:38<12:09,  5.11it/s]  4%|▍         | 174/3900 [00:38<12:09,  5.10it/s]  4%|▍         | 175/3900 [00:38<12:09,  5.10it/s]  5%|▍         | 176/3900 [00:39<12:09,  5.10it/s]  5%|▍         | 177/3900 [00:39<12:10,  5.10it/s]  5%|▍         | 178/3900 [00:39<12:10,  5.09it/s]  5%|▍         | 179/3900 [00:39<12:11,  5.09it/s]  5%|▍         | 180/3900 [00:39<12:09,  5.10it/s]  5%|▍         | 181/3900 [00:40<12:09,  5.10it/s]  5%|▍         | 182/3900 [00:40<12:09,  5.10it/s]  5%|▍         | 183/3900 [00:40<12:09,  5.10it/s]  5%|▍         | 184/3900 [00:40<12:08,  5.10it/s]  5%|▍         | 185/3900 [00:40<12:08,  5.10it/s]  5%|▍         | 186/3900 [00:41<12:08,  5.10it/s]  5%|▍         | 187/3900 [00:41<12:08,  5.10it/s]  5%|▍         | 188/3900 [00:41<12:08,  5.10it/s]  5%|▍         | 189/3900 [00:41<12:08,  5.09it/s]  5%|▍         | 190/3900 [00:41<12:08,  5.09it/s]  5%|▍         | 191/3900 [00:42<12:07,  5.10it/s]  5%|▍         | 192/3900 [00:42<12:07,  5.10it/s]  5%|▍         | 193/3900 [00:42<12:07,  5.10it/s]  5%|▍         | 194/3900 [00:42<12:06,  5.10it/s]  5%|▌         | 195/3900 [00:42<11:58,  5.16it/s]  5%|▌         | 196/3900 [00:43<12:01,  5.14it/s]  5%|▌         | 197/3900 [00:43<12:02,  5.12it/s]  5%|▌         | 198/3900 [00:43<12:03,  5.12it/s]  5%|▌         | 199/3900 [00:43<13:31,  4.56it/s]  5%|▌         | 200/3900 [00:43<13:05,  4.71it/s]  5%|▌         | 201/3900 [00:44<12:48,  4.81it/s]  5%|▌         | 202/3900 [00:44<12:35,  4.89it/s]  5%|▌         | 203/3900 [00:44<12:26,  4.95it/s]  5%|▌         | 204/3900 [00:44<12:19,  5.00it/s]  5%|▌         | 205/3900 [00:44<12:15,  5.02it/s]  5%|▌         | 206/3900 [00:45<12:12,  5.04it/s]  5%|▌         | 207/3900 [00:45<12:10,  5.06it/s]  5%|▌         | 208/3900 [00:45<12:08,  5.07it/s]  5%|▌         | 209/3900 [00:45<12:11,  5.05it/s]  5%|▌         | 210/3900 [00:45<12:10,  5.05it/s]  5%|▌         | 211/3900 [00:46<12:08,  5.06it/s]  5%|▌         | 212/3900 [00:46<12:07,  5.07it/s]  5%|▌         | 213/3900 [00:46<12:06,  5.07it/s]  5%|▌         | 214/3900 [00:46<12:06,  5.08it/s]  6%|▌         | 215/3900 [00:46<12:05,  5.08it/s]  6%|▌         | 216/3900 [00:47<12:04,  5.09it/s]  6%|▌         | 217/3900 [00:47<12:06,  5.07it/s]  6%|▌         | 218/3900 [00:47<12:06,  5.07it/s]  6%|▌         | 219/3900 [00:47<12:04,  5.08it/s]  6%|▌         | 220/3900 [00:47<12:03,  5.09it/s]  6%|▌         | 221/3900 [00:48<12:03,  5.09it/s]  6%|▌         | 222/3900 [00:48<12:02,  5.09it/s]  6%|▌         | 223/3900 [00:48<12:01,  5.09it/s]  6%|▌         | 224/3900 [00:48<12:01,  5.09it/s]  6%|▌         | 225/3900 [00:48<12:01,  5.10it/s]  6%|▌         | 226/3900 [00:49<12:00,  5.10it/s]  6%|▌         | 227/3900 [00:49<11:59,  5.10it/s]  6%|▌         | 228/3900 [00:49<11:59,  5.10it/s]  6%|▌         | 229/3900 [00:49<11:59,  5.10it/s]  6%|▌         | 230/3900 [00:49<11:59,  5.10it/s]  6%|▌         | 231/3900 [00:50<11:59,  5.10it/s]  6%|▌         | 232/3900 [00:50<11:58,  5.10it/s]  6%|▌         | 233/3900 [00:50<11:58,  5.10it/s]  6%|▌         | 234/3900 [00:50<11:50,  5.16it/s]  6%|▌         | 235/3900 [00:50<11:53,  5.14it/s]  6%|▌         | 236/3900 [00:51<11:54,  5.13it/s]  6%|▌         | 237/3900 [00:51<11:55,  5.12it/s]  6%|▌         | 238/3900 [00:51<11:55,  5.12it/s]  6%|▌         | 239/3900 [00:51<11:55,  5.12it/s]  6%|▌         | 240/3900 [00:51<11:56,  5.11it/s]  6%|▌         | 241/3900 [00:52<11:55,  5.11it/s]  6%|▌         | 242/3900 [00:52<11:55,  5.11it/s]  6%|▌         | 243/3900 [00:52<11:55,  5.11it/s]  6%|▋         | 244/3900 [00:52<11:55,  5.11it/s]  6%|▋         | 245/3900 [00:52<11:55,  5.11it/s]  6%|▋         | 246/3900 [00:52<11:56,  5.10it/s]  6%|▋         | 247/3900 [00:53<11:56,  5.10it/s]  6%|▋         | 248/3900 [00:53<11:55,  5.10it/s]  6%|▋         | 249/3900 [00:53<11:55,  5.10it/s]  6%|▋         | 250/3900 [00:53<11:55,  5.10it/s]  6%|▋         | 251/3900 [00:53<11:54,  5.10it/s]  6%|▋         | 252/3900 [00:54<13:18,  4.57it/s]  6%|▋         | 253/3900 [00:54<12:53,  4.71it/s]  7%|▋         | 254/3900 [00:54<12:36,  4.82it/s]  7%|▋         | 255/3900 [00:54<12:23,  4.90it/s]  7%|▋         | 256/3900 [00:55<12:14,  4.96it/s]  7%|▋         | 257/3900 [00:55<12:08,  5.00it/s]  7%|▋         | 258/3900 [00:55<12:04,  5.03it/s]  7%|▋         | 259/3900 [00:55<12:00,  5.05it/s]  7%|▋         | 260/3900 [00:55<11:58,  5.07it/s]  7%|▋         | 261/3900 [00:56<11:56,  5.08it/s]  7%|▋         | 262/3900 [00:56<11:55,  5.08it/s]  7%|▋         | 263/3900 [00:56<11:54,  5.09it/s]  7%|▋         | 264/3900 [00:56<11:55,  5.09it/s]  7%|▋         | 265/3900 [00:56<11:54,  5.09it/s]  7%|▋         | 266/3900 [00:56<11:54,  5.09it/s]  7%|▋         | 267/3900 [00:57<11:53,  5.09it/s]  7%|▋         | 268/3900 [00:57<11:53,  5.09it/s]  7%|▋         | 269/3900 [00:57<11:57,  5.06it/s]  7%|▋         | 270/3900 [00:57<11:55,  5.07it/s]  7%|▋         | 271/3900 [00:57<11:53,  5.09it/s]  7%|▋         | 272/3900 [00:58<11:51,  5.10it/s]  7%|▋         | 273/3900 [00:58<11:43,  5.16it/s]  7%|▋         | 274/3900 [00:58<11:45,  5.14it/s]  7%|▋         | 275/3900 [00:58<11:45,  5.14it/s]  7%|▋         | 276/3900 [00:58<11:46,  5.13it/s]  7%|▋         | 277/3900 [00:59<11:46,  5.13it/s]  7%|▋         | 278/3900 [00:59<11:47,  5.12it/s]  7%|▋         | 279/3900 [00:59<11:47,  5.12it/s]  7%|▋         | 280/3900 [00:59<11:47,  5.12it/s]  7%|▋         | 281/3900 [00:59<11:47,  5.12it/s]  7%|▋         | 282/3900 [01:00<11:46,  5.12it/s]  7%|▋         | 283/3900 [01:00<11:46,  5.12it/s]  7%|▋         | 284/3900 [01:00<11:46,  5.12it/s]  7%|▋         | 285/3900 [01:00<11:46,  5.12it/s]  7%|▋         | 286/3900 [01:00<11:46,  5.12it/s]  7%|▋         | 287/3900 [01:01<11:45,  5.12it/s]  7%|▋         | 288/3900 [01:01<11:46,  5.12it/s]  7%|▋         | 289/3900 [01:01<11:46,  5.11it/s]  7%|▋         | 290/3900 [01:01<11:45,  5.11it/s]  7%|▋         | 291/3900 [01:01<11:45,  5.12it/s]  7%|▋         | 292/3900 [01:02<11:45,  5.12it/s]  8%|▊         | 293/3900 [01:02<11:45,  5.12it/s]  8%|▊         | 294/3900 [01:02<11:45,  5.11it/s]  8%|▊         | 295/3900 [01:02<11:44,  5.12it/s]  8%|▊         | 296/3900 [01:02<11:44,  5.12it/s]  8%|▊         | 297/3900 [01:03<11:44,  5.12it/s]  8%|▊         | 298/3900 [01:03<11:43,  5.12it/s]  8%|▊         | 299/3900 [01:03<11:43,  5.12it/s]  8%|▊         | 300/3900 [01:03<11:43,  5.12it/s]  8%|▊         | 301/3900 [01:03<11:43,  5.11it/s]  8%|▊         | 302/3900 [01:04<11:44,  5.11it/s]  8%|▊         | 303/3900 [01:04<11:44,  5.11it/s]  8%|▊         | 304/3900 [01:04<11:43,  5.11it/s]  8%|▊         | 305/3900 [01:04<11:43,  5.11it/s]  8%|▊         | 306/3900 [01:04<11:42,  5.11it/s]  8%|▊         | 307/3900 [01:05<11:42,  5.11it/s]  8%|▊         | 308/3900 [01:05<11:42,  5.11it/s]  8%|▊         | 309/3900 [01:05<11:42,  5.11it/s]  8%|▊         | 310/3900 [01:05<11:42,  5.11it/s]  8%|▊         | 311/3900 [01:05<11:44,  5.10it/s]  8%|▊         | 312/3900 [01:05<11:36,  5.15it/s]  8%|▊         | 313/3900 [01:06<11:38,  5.14it/s]  8%|▊         | 314/3900 [01:06<11:39,  5.13it/s]  8%|▊         | 315/3900 [01:06<11:39,  5.13it/s]  8%|▊         | 316/3900 [01:06<11:39,  5.12it/s]  8%|▊         | 317/3900 [01:06<11:39,  5.12it/s]  8%|▊         | 318/3900 [01:07<11:39,  5.12it/s]  8%|▊         | 319/3900 [01:07<11:39,  5.12it/s]  8%|▊         | 320/3900 [01:07<11:39,  5.12it/s]  8%|▊         | 321/3900 [01:07<11:40,  5.11it/s]  8%|▊         | 322/3900 [01:07<11:40,  5.11it/s]  8%|▊         | 323/3900 [01:08<11:40,  5.11it/s]  8%|▊         | 324/3900 [01:08<11:39,  5.11it/s]  8%|▊         | 325/3900 [01:08<11:39,  5.11it/s]  8%|▊         | 326/3900 [01:08<11:39,  5.11it/s]  8%|▊         | 327/3900 [01:08<11:38,  5.11it/s]  8%|▊         | 328/3900 [01:09<11:38,  5.11it/s]  8%|▊         | 329/3900 [01:09<11:38,  5.11it/s]  8%|▊         | 330/3900 [01:09<11:38,  5.11it/s]  8%|▊         | 331/3900 [01:09<11:37,  5.11it/s]  9%|▊         | 332/3900 [01:09<11:37,  5.12it/s]  9%|▊         | 333/3900 [01:10<11:37,  5.12it/s]  9%|▊         | 334/3900 [01:10<11:36,  5.12it/s]  9%|▊         | 335/3900 [01:10<11:37,  5.11it/s]  9%|▊         | 336/3900 [01:10<11:37,  5.11it/s]  9%|▊         | 337/3900 [01:10<11:37,  5.11it/s]  9%|▊         | 338/3900 [01:11<11:36,  5.11it/s]  9%|▊         | 339/3900 [01:11<11:36,  5.11it/s]  9%|▊         | 340/3900 [01:11<11:35,  5.12it/s]  9%|▊         | 341/3900 [01:11<11:35,  5.12it/s]  9%|▉         | 342/3900 [01:11<11:35,  5.12it/s]  9%|▉         | 343/3900 [01:12<11:34,  5.12it/s]  9%|▉         | 344/3900 [01:12<11:34,  5.12it/s]  9%|▉         | 345/3900 [01:12<11:34,  5.12it/s]  9%|▉         | 346/3900 [01:12<11:34,  5.12it/s]  9%|▉         | 347/3900 [01:12<11:34,  5.12it/s]  9%|▉         | 348/3900 [01:13<11:34,  5.12it/s]  9%|▉         | 349/3900 [01:13<12:54,  4.58it/s]  9%|▉         | 350/3900 [01:13<12:30,  4.73it/s]  9%|▉         | 351/3900 [01:13<12:05,  4.89it/s]  9%|▉         | 352/3900 [01:13<11:56,  4.95it/s]  9%|▉         | 353/3900 [01:14<11:49,  5.00it/s]  9%|▉         | 354/3900 [01:14<11:44,  5.03it/s]  9%|▉         | 355/3900 [01:14<11:40,  5.06it/s]  9%|▉         | 356/3900 [01:14<11:37,  5.08it/s]  9%|▉         | 357/3900 [01:14<11:36,  5.09it/s]  9%|▉         | 358/3900 [01:15<11:35,  5.10it/s]  9%|▉         | 359/3900 [01:15<11:34,  5.10it/s]  9%|▉         | 360/3900 [01:15<11:33,  5.11it/s]  9%|▉         | 361/3900 [01:15<11:32,  5.11it/s]  9%|▉         | 362/3900 [01:15<11:32,  5.11it/s]  9%|▉         | 363/3900 [01:16<11:31,  5.11it/s]  9%|▉         | 364/3900 [01:16<11:31,  5.11it/s]  9%|▉         | 365/3900 [01:16<11:31,  5.11it/s]  9%|▉         | 366/3900 [01:16<11:32,  5.11it/s]  9%|▉         | 367/3900 [01:16<11:32,  5.10it/s]  9%|▉         | 368/3900 [01:16<11:32,  5.10it/s]  9%|▉         | 369/3900 [01:17<11:32,  5.10it/s]  9%|▉         | 370/3900 [01:17<11:32,  5.10it/s] 10%|▉         | 371/3900 [01:17<11:32,  5.10it/s] 10%|▉         | 372/3900 [01:17<11:30,  5.11it/s] 10%|▉         | 373/3900 [01:17<11:30,  5.11it/s] 10%|▉         | 374/3900 [01:18<11:29,  5.12it/s] 10%|▉         | 375/3900 [01:18<11:28,  5.12it/s] 10%|▉         | 376/3900 [01:18<11:28,  5.12it/s] 10%|▉         | 377/3900 [01:18<11:28,  5.12it/s] 10%|▉         | 378/3900 [01:18<11:28,  5.12it/s] 10%|▉         | 379/3900 [01:19<11:29,  5.11it/s] 10%|▉         | 380/3900 [01:19<11:30,  5.10it/s] 10%|▉         | 381/3900 [01:19<11:32,  5.08it/s] 10%|▉         | 382/3900 [01:19<11:32,  5.08it/s] 10%|▉         | 383/3900 [01:19<11:33,  5.07it/s] 10%|▉         | 384/3900 [01:20<11:33,  5.07it/s] 10%|▉         | 385/3900 [01:20<11:34,  5.06it/s] 10%|▉         | 386/3900 [01:20<11:34,  5.06it/s] 10%|▉         | 387/3900 [01:20<11:33,  5.06it/s] 10%|▉         | 388/3900 [01:20<11:33,  5.06it/s] 10%|▉         | 389/3900 [01:21<11:33,  5.06it/s] 10%|█         | 390/3900 [01:21<11:24,  5.13it/s] 10%|█         | 391/3900 [01:21<11:26,  5.11it/s] 10%|█         | 392/3900 [01:21<11:28,  5.10it/s] 10%|█         | 393/3900 [01:21<11:28,  5.09it/s] 10%|█         | 394/3900 [01:22<11:29,  5.08it/s] 10%|█         | 395/3900 [01:22<11:30,  5.08it/s] 10%|█         | 396/3900 [01:22<11:30,  5.08it/s] 10%|█         | 397/3900 [01:22<11:31,  5.07it/s] 10%|█         | 398/3900 [01:22<11:31,  5.06it/s] 10%|█         | 399/3900 [01:23<11:31,  5.06it/s] 10%|█         | 400/3900 [01:23<11:31,  5.06it/s] 10%|█         | 401/3900 [01:23<11:31,  5.06it/s] 10%|█         | 402/3900 [01:23<12:31,  4.66it/s] 10%|█         | 403/3900 [01:23<12:12,  4.77it/s] 10%|█         | 404/3900 [01:24<11:59,  4.86it/s] 10%|█         | 405/3900 [01:24<11:50,  4.92it/s] 10%|█         | 406/3900 [01:24<11:44,  4.96it/s] 10%|█         | 407/3900 [01:24<11:40,  4.99it/s] 10%|█         | 408/3900 [01:24<11:37,  5.01it/s] 10%|█         | 409/3900 [01:25<11:35,  5.02it/s] 11%|█         | 410/3900 [01:25<11:33,  5.04it/s] 11%|█         | 411/3900 [01:25<11:31,  5.04it/s] 11%|█         | 412/3900 [01:25<11:31,  5.05it/s] 11%|█         | 413/3900 [01:25<11:29,  5.06it/s] 11%|█         | 414/3900 [01:26<11:28,  5.06it/s] 11%|█         | 415/3900 [01:26<11:27,  5.07it/s] 11%|█         | 416/3900 [01:26<11:25,  5.08it/s] 11%|█         | 417/3900 [01:26<11:24,  5.09it/s] 11%|█         | 418/3900 [01:26<11:23,  5.10it/s] 11%|█         | 419/3900 [01:27<11:22,  5.10it/s] 11%|█         | 420/3900 [01:27<11:22,  5.10it/s] 11%|█         | 421/3900 [01:27<11:21,  5.11it/s] 11%|█         | 422/3900 [01:27<11:20,  5.11it/s] 11%|█         | 423/3900 [01:27<11:20,  5.11it/s] 11%|█         | 424/3900 [01:28<11:20,  5.10it/s] 11%|█         | 425/3900 [01:28<11:20,  5.11it/s] 11%|█         | 426/3900 [01:28<11:20,  5.11it/s] 11%|█         | 427/3900 [01:28<11:20,  5.11it/s] 11%|█         | 428/3900 [01:28<11:19,  5.11it/s] 11%|█         | 429/3900 [01:29<11:11,  5.17it/s] 11%|█         | 430/3900 [01:29<11:13,  5.15it/s] 11%|█         | 431/3900 [01:29<11:14,  5.14it/s] 11%|█         | 432/3900 [01:29<11:15,  5.13it/s] 11%|█         | 433/3900 [01:29<11:16,  5.13it/s] 11%|█         | 434/3900 [01:30<11:16,  5.13it/s] 11%|█         | 435/3900 [01:30<11:16,  5.12it/s] 11%|█         | 436/3900 [01:30<11:16,  5.12it/s] 11%|█         | 437/3900 [01:30<11:16,  5.12it/s] 11%|█         | 438/3900 [01:30<11:16,  5.12it/s] 11%|█▏        | 439/3900 [01:30<11:16,  5.12it/s] 11%|█▏        | 440/3900 [01:31<11:16,  5.12it/s] 11%|█▏        | 441/3900 [01:31<11:16,  5.12it/s] 11%|█▏        | 442/3900 [01:31<11:15,  5.12it/s] 11%|█▏        | 443/3900 [01:31<11:15,  5.12it/s] 11%|█▏        | 444/3900 [01:31<11:14,  5.12it/s] 11%|█▏        | 445/3900 [01:32<11:16,  5.10it/s] 11%|█▏        | 446/3900 [01:32<11:16,  5.10it/s] 11%|█▏        | 447/3900 [01:32<11:16,  5.11it/s] 11%|█▏        | 448/3900 [01:32<11:15,  5.11it/s] 12%|█▏        | 449/3900 [01:32<11:15,  5.11it/s] 12%|█▏        | 450/3900 [01:33<11:14,  5.12it/s] 12%|█▏        | 451/3900 [01:33<11:14,  5.12it/s] 12%|█▏        | 452/3900 [01:33<11:13,  5.12it/s] 12%|█▏        | 453/3900 [01:33<11:13,  5.12it/s] 12%|█▏        | 454/3900 [01:33<11:14,  5.11it/s] 12%|█▏        | 455/3900 [01:34<11:13,  5.12it/s] 12%|█▏        | 456/3900 [01:34<11:13,  5.12it/s] 12%|█▏        | 457/3900 [01:34<11:12,  5.12it/s] 12%|█▏        | 458/3900 [01:34<11:12,  5.12it/s] 12%|█▏        | 459/3900 [01:34<11:12,  5.12it/s] 12%|█▏        | 460/3900 [01:35<11:12,  5.12it/s] 12%|█▏        | 461/3900 [01:35<11:12,  5.12it/s] 12%|█▏        | 462/3900 [01:35<11:12,  5.11it/s] 12%|█▏        | 463/3900 [01:35<11:14,  5.10it/s] 12%|█▏        | 464/3900 [01:35<11:14,  5.10it/s] 12%|█▏        | 465/3900 [01:36<11:14,  5.10it/s] 12%|█▏        | 466/3900 [01:36<11:13,  5.10it/s] 12%|█▏        | 467/3900 [01:36<11:12,  5.10it/s] 12%|█▏        | 468/3900 [01:36<11:04,  5.17it/s] 12%|█▏        | 469/3900 [01:36<11:06,  5.15it/s] 12%|█▏        | 470/3900 [01:37<11:07,  5.14it/s] 12%|█▏        | 471/3900 [01:37<11:07,  5.13it/s] 12%|█▏        | 472/3900 [01:37<11:08,  5.13it/s] 12%|█▏        | 473/3900 [01:37<11:08,  5.13it/s] 12%|█▏        | 474/3900 [01:37<11:08,  5.12it/s] 12%|█▏        | 475/3900 [01:38<11:08,  5.12it/s] 12%|█▏        | 476/3900 [01:38<11:08,  5.12it/s] 12%|█▏        | 477/3900 [01:38<11:09,  5.12it/s] 12%|█▏        | 478/3900 [01:38<11:08,  5.12it/s] 12%|█▏        | 479/3900 [01:38<11:08,  5.12it/s] 12%|█▏        | 480/3900 [01:39<11:08,  5.12it/s] 12%|█▏        | 481/3900 [01:39<11:07,  5.12it/s] 12%|█▏        | 482/3900 [01:39<11:07,  5.12it/s] 12%|█▏        | 483/3900 [01:39<11:07,  5.12it/s] 12%|█▏        | 484/3900 [01:39<11:07,  5.12it/s] 12%|█▏        | 485/3900 [01:39<11:06,  5.12it/s] 12%|█▏        | 486/3900 [01:40<11:06,  5.12it/s] 12%|█▏        | 487/3900 [01:40<11:07,  5.12it/s] 13%|█▎        | 488/3900 [01:40<11:06,  5.12it/s] 13%|█▎        | 489/3900 [01:40<11:06,  5.12it/s] 13%|█▎        | 490/3900 [01:40<11:06,  5.12it/s] 13%|█▎        | 491/3900 [01:41<11:06,  5.12it/s] 13%|█▎        | 492/3900 [01:41<11:05,  5.12it/s] 13%|█▎        | 493/3900 [01:41<11:05,  5.12it/s] 13%|█▎        | 494/3900 [01:41<11:05,  5.12it/s] 13%|█▎        | 495/3900 [01:41<11:05,  5.12it/s] 13%|█▎        | 496/3900 [01:42<11:04,  5.12it/s] 13%|█▎        | 497/3900 [01:42<11:04,  5.12it/s] 13%|█▎        | 498/3900 [01:42<11:04,  5.12it/s] 13%|█▎        | 499/3900 [01:42<12:23,  4.57it/s] 13%|█▎        | 500/3900 [01:42<12:00,  4.72it/s]                                                   13%|█▎        | 500/3900 [01:42<12:00,  4.72it/s][INFO|trainer.py:2409] 2022-08-27 22:09:27,743 >> Saving model checkpoint to tmp/baseline/RTE/bert-base-uncased/HPO_S0/1e-06/64/100/checkpoint-500
[INFO|configuration_utils.py:446] 2022-08-27 22:09:27,744 >> Configuration saved in tmp/baseline/RTE/bert-base-uncased/HPO_S0/1e-06/64/100/checkpoint-500/config.json
[INFO|modeling_utils.py:1602] 2022-08-27 22:09:28,570 >> Model weights saved in tmp/baseline/RTE/bert-base-uncased/HPO_S0/1e-06/64/100/checkpoint-500/pytorch_model.bin
[INFO|tokenization_utils_base.py:2108] 2022-08-27 22:09:28,570 >> tokenizer config file saved in tmp/baseline/RTE/bert-base-uncased/HPO_S0/1e-06/64/100/checkpoint-500/tokenizer_config.json
[INFO|tokenization_utils_base.py:2114] 2022-08-27 22:09:28,571 >> Special tokens file saved in tmp/baseline/RTE/bert-base-uncased/HPO_S0/1e-06/64/100/checkpoint-500/special_tokens_map.json
/home/ubuntu/anaconda3/envs/transformers/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 13%|█▎        | 501/3900 [01:45<54:33,  1.04it/s] 13%|█▎        | 502/3900 [01:45<41:30,  1.36it/s] 13%|█▎        | 503/3900 [01:46<32:21,  1.75it/s] 13%|█▎        | 504/3900 [01:46<25:58,  2.18it/s] 13%|█▎        | 505/3900 [01:46<21:29,  2.63it/s] 13%|█▎        | 506/3900 [01:46<18:21,  3.08it/s] 13%|█▎        | 507/3900 [01:46<16:02,  3.52it/s] 13%|█▎        | 508/3900 [01:47<14:32,  3.89it/s] 13%|█▎        | 509/3900 [01:47<13:29,  4.19it/s] 13%|█▎        | 510/3900 [01:47<12:45,  4.43it/s] 13%|█▎        | 511/3900 [01:47<12:14,  4.62it/s] 13%|█▎        | 512/3900 [01:47<11:52,  4.76it/s] 13%|█▎        | 513/3900 [01:48<11:37,  4.86it/s] 13%|█▎        | 514/3900 [01:48<11:26,  4.93it/s] 13%|█▎        | 515/3900 [01:48<11:18,  4.99it/s] 13%|█▎        | 516/3900 [01:48<11:13,  5.03it/s] 13%|█▎        | 517/3900 [01:48<11:09,  5.05it/s] 13%|█▎        | 518/3900 [01:49<11:06,  5.07it/s] 13%|█▎        | 519/3900 [01:49<11:04,  5.08it/s] 13%|█▎        | 520/3900 [01:49<11:03,  5.09it/s] 13%|█▎        | 521/3900 [01:49<11:02,  5.10it/s] 13%|█▎        | 522/3900 [01:49<11:01,  5.10it/s] 13%|█▎        | 523/3900 [01:50<11:01,  5.11it/s] 13%|█▎        | 524/3900 [01:50<11:00,  5.11it/s] 13%|█▎        | 525/3900 [01:50<11:00,  5.11it/s] 13%|█▎        | 526/3900 [01:50<11:00,  5.11it/s] 14%|█▎        | 527/3900 [01:50<10:59,  5.11it/s] 14%|█▎        | 528/3900 [01:50<10:59,  5.11it/s] 14%|█▎        | 529/3900 [01:51<10:59,  5.11it/s] 14%|█▎        | 530/3900 [01:51<10:58,  5.11it/s] 14%|█▎        | 531/3900 [01:51<10:58,  5.11it/s] 14%|█▎        | 532/3900 [01:51<10:58,  5.11it/s] 14%|█▎        | 533/3900 [01:51<10:58,  5.11it/s] 14%|█▎        | 534/3900 [01:52<10:58,  5.11it/s] 14%|█▎        | 535/3900 [01:52<10:58,  5.11it/s] 14%|█▎        | 536/3900 [01:52<10:57,  5.11it/s] 14%|█▍        | 537/3900 [01:52<10:58,  5.10it/s] 14%|█▍        | 538/3900 [01:52<11:00,  5.09it/s] 14%|█▍        | 539/3900 [01:53<11:01,  5.08it/s] 14%|█▍        | 540/3900 [01:53<11:02,  5.07it/s] 14%|█▍        | 541/3900 [01:53<11:02,  5.07it/s] 14%|█▍        | 542/3900 [01:53<11:02,  5.07it/s] 14%|█▍        | 543/3900 [01:53<11:03,  5.06it/s] 14%|█▍        | 544/3900 [01:54<11:04,  5.05it/s] 14%|█▍        | 545/3900 [01:54<11:04,  5.05it/s] 14%|█▍        | 546/3900 [01:54<10:56,  5.11it/s] 14%|█▍        | 547/3900 [01:54<10:58,  5.09it/s] 14%|█▍        | 548/3900 [01:54<10:58,  5.09it/s] 14%|█▍        | 549/3900 [01:55<10:59,  5.08it/s] 14%|█▍        | 550/3900 [01:55<10:59,  5.08it/s] 14%|█▍        | 551/3900 [01:55<11:00,  5.07it/s] 14%|█▍        | 552/3900 [01:55<11:01,  5.06it/s] 14%|█▍        | 553/3900 [01:55<11:01,  5.06it/s] 14%|█▍        | 554/3900 [01:56<11:01,  5.06it/s] 14%|█▍        | 555/3900 [01:56<11:01,  5.06it/s] 14%|█▍        | 556/3900 [01:56<11:00,  5.06it/s] 14%|█▍        | 557/3900 [01:56<11:04,  5.03it/s] 14%|█▍        | 558/3900 [01:56<11:03,  5.03it/s] 14%|█▍        | 559/3900 [01:57<11:03,  5.03it/s] 14%|█▍        | 560/3900 [01:57<11:02,  5.04it/s] 14%|█▍        | 561/3900 [01:57<11:01,  5.05it/s] 14%|█▍        | 562/3900 [01:57<11:01,  5.05it/s] 14%|█▍        | 563/3900 [01:57<11:00,  5.05it/s] 14%|█▍        | 564/3900 [01:58<11:00,  5.05it/s] 14%|█▍        | 565/3900 [01:58<11:58,  4.64it/s] 15%|█▍        | 566/3900 [01:58<11:40,  4.76it/s] 15%|█▍        | 567/3900 [01:58<11:28,  4.84it/s] 15%|█▍        | 568/3900 [01:58<11:19,  4.91it/s] 15%|█▍        | 569/3900 [01:59<11:13,  4.95it/s] 15%|█▍        | 570/3900 [01:59<11:08,  4.98it/s] 15%|█▍        | 571/3900 [01:59<11:04,  5.01it/s] 15%|█▍        | 572/3900 [01:59<11:01,  5.03it/s] 15%|█▍        | 573/3900 [01:59<10:59,  5.05it/s] 15%|█▍        | 574/3900 [02:00<10:56,  5.06it/s] 15%|█▍        | 575/3900 [02:00<10:54,  5.08it/s] 15%|█▍        | 576/3900 [02:00<10:53,  5.09it/s] 15%|█▍        | 577/3900 [02:00<10:52,  5.09it/s] 15%|█▍        | 578/3900 [02:00<10:52,  5.09it/s] 15%|█▍        | 579/3900 [02:01<10:50,  5.10it/s] 15%|█▍        | 580/3900 [02:01<10:50,  5.10it/s] 15%|█▍        | 581/3900 [02:01<10:50,  5.10it/s] 15%|█▍        | 582/3900 [02:01<10:50,  5.10it/s] 15%|█▍        | 583/3900 [02:01<10:49,  5.11it/s] 15%|█▍        | 584/3900 [02:02<10:48,  5.11it/s] 15%|█▌        | 585/3900 [02:02<10:41,  5.17it/s] 15%|█▌        | 586/3900 [02:02<10:44,  5.15it/s] 15%|█▌        | 587/3900 [02:02<10:45,  5.14it/s] 15%|█▌        | 588/3900 [02:02<10:45,  5.13it/s] 15%|█▌        | 589/3900 [02:03<10:46,  5.12it/s] 15%|█▌        | 590/3900 [02:03<10:46,  5.12it/s] 15%|█▌        | 591/3900 [02:03<10:46,  5.11it/s] 15%|█▌        | 592/3900 [02:03<10:46,  5.12it/s] 15%|█▌        | 593/3900 [02:03<10:46,  5.12it/s] 15%|█▌        | 594/3900 [02:04<10:46,  5.11it/s] 15%|█▌        | 595/3900 [02:04<10:46,  5.11it/s] 15%|█▌        | 596/3900 [02:04<10:45,  5.12it/s] 15%|█▌        | 597/3900 [02:04<10:45,  5.12it/s] 15%|█▌        | 598/3900 [02:04<10:45,  5.12it/s] 15%|█▌        | 599/3900 [02:04<10:45,  5.12it/s] 15%|█▌        | 600/3900 [02:05<10:45,  5.12it/s] 15%|█▌        | 601/3900 [02:05<10:44,  5.12it/s] 15%|█▌        | 602/3900 [02:05<10:44,  5.11it/s] 15%|█▌        | 603/3900 [02:05<10:45,  5.11it/s] 15%|█▌        | 604/3900 [02:05<10:44,  5.11it/s] 16%|█▌        | 605/3900 [02:06<10:44,  5.12it/s] 16%|█▌        | 606/3900 [02:06<10:43,  5.12it/s] 16%|█▌        | 607/3900 [02:06<10:43,  5.12it/s] 16%|█▌        | 608/3900 [02:06<10:43,  5.12it/s] 16%|█▌        | 609/3900 [02:06<10:43,  5.12it/s] 16%|█▌        | 610/3900 [02:07<10:43,  5.12it/s] 16%|█▌        | 611/3900 [02:07<10:43,  5.11it/s] 16%|█▌        | 612/3900 [02:07<10:42,  5.11it/s] 16%|█▌        | 613/3900 [02:07<10:42,  5.11it/s] 16%|█▌        | 614/3900 [02:07<10:42,  5.11it/s] 16%|█▌        | 615/3900 [02:08<10:42,  5.11it/s] 16%|█▌        | 616/3900 [02:08<10:42,  5.11it/s] 16%|█▌        | 617/3900 [02:08<10:41,  5.11it/s] 16%|█▌        | 618/3900 [02:08<10:41,  5.11it/s] 16%|█▌        | 619/3900 [02:08<10:41,  5.12it/s] 16%|█▌        | 620/3900 [02:09<10:41,  5.11it/s] 16%|█▌        | 621/3900 [02:09<10:41,  5.12it/s] 16%|█▌        | 622/3900 [02:09<10:40,  5.11it/s] 16%|█▌        | 623/3900 [02:09<10:40,  5.11it/s] 16%|█▌        | 624/3900 [02:09<10:33,  5.18it/s] 16%|█▌        | 625/3900 [02:10<10:35,  5.16it/s] 16%|█▌        | 626/3900 [02:10<10:36,  5.14it/s] 16%|█▌        | 627/3900 [02:10<10:37,  5.14it/s] 16%|█▌        | 628/3900 [02:10<10:37,  5.13it/s] 16%|█▌        | 629/3900 [02:10<10:38,  5.12it/s] 16%|█▌        | 630/3900 [02:11<10:38,  5.12it/s] 16%|█▌        | 631/3900 [02:11<10:38,  5.12it/s] 16%|█▌        | 632/3900 [02:11<10:38,  5.12it/s] 16%|█▌        | 633/3900 [02:11<10:38,  5.12it/s] 16%|█▋        | 634/3900 [02:11<10:38,  5.11it/s] 16%|█▋        | 635/3900 [02:12<10:38,  5.11it/s] 16%|█▋        | 636/3900 [02:12<10:37,  5.12it/s] 16%|█▋        | 637/3900 [02:12<10:37,  5.12it/s] 16%|█▋        | 638/3900 [02:12<10:37,  5.12it/s] 16%|█▋        | 639/3900 [02:12<10:37,  5.11it/s] 16%|█▋        | 640/3900 [02:13<10:37,  5.12it/s] 16%|█▋        | 641/3900 [02:13<10:37,  5.12it/s] 16%|█▋        | 642/3900 [02:13<10:37,  5.11it/s] 16%|█▋        | 643/3900 [02:13<10:37,  5.11it/s] 17%|█▋        | 644/3900 [02:13<10:37,  5.11it/s] 17%|█▋        | 645/3900 [02:13<10:36,  5.11it/s] 17%|█▋        | 646/3900 [02:14<10:36,  5.12it/s] 17%|█▋        | 647/3900 [02:14<10:36,  5.11it/s] 17%|█▋        | 648/3900 [02:14<10:35,  5.12it/s] 17%|█▋        | 649/3900 [02:14<10:35,  5.12it/s] 17%|█▋        | 650/3900 [02:14<10:35,  5.12it/s] 17%|█▋        | 651/3900 [02:15<10:34,  5.12it/s] 17%|█▋        | 652/3900 [02:15<10:34,  5.12it/s] 17%|█▋        | 653/3900 [02:15<10:35,  5.11it/s] 17%|█▋        | 654/3900 [02:15<10:35,  5.11it/s] 17%|█▋        | 655/3900 [02:15<10:35,  5.11it/s] 17%|█▋        | 656/3900 [02:16<10:34,  5.11it/s] 17%|█▋        | 657/3900 [02:16<10:34,  5.11it/s] 17%|█▋        | 658/3900 [02:16<10:34,  5.11it/s] 17%|█▋        | 659/3900 [02:16<10:35,  5.10it/s] 17%|█▋        | 660/3900 [02:16<10:36,  5.09it/s] 17%|█▋        | 661/3900 [02:17<10:36,  5.09it/s] 17%|█▋        | 662/3900 [02:17<10:35,  5.09it/s] 17%|█▋        | 663/3900 [02:17<10:28,  5.15it/s] 17%|█▋        | 664/3900 [02:17<10:30,  5.13it/s] 17%|█▋        | 665/3900 [02:17<10:31,  5.13it/s] 17%|█▋        | 666/3900 [02:18<10:31,  5.12it/s] 17%|█▋        | 667/3900 [02:18<10:31,  5.12it/s] 17%|█▋        | 668/3900 [02:18<10:31,  5.12it/s] 17%|█▋        | 669/3900 [02:18<10:31,  5.12it/s] 17%|█▋        | 670/3900 [02:18<10:31,  5.12it/s] 17%|█▋        | 671/3900 [02:19<10:31,  5.11it/s] 17%|█▋        | 672/3900 [02:19<10:31,  5.11it/s] 17%|█▋        | 673/3900 [02:19<10:31,  5.11it/s] 17%|█▋        | 674/3900 [02:19<10:32,  5.10it/s] 17%|█▋        | 675/3900 [02:19<10:31,  5.11it/s] 17%|█▋        | 676/3900 [02:20<10:31,  5.11it/s] 17%|█▋        | 677/3900 [02:20<10:30,  5.11it/s] 17%|█▋        | 678/3900 [02:20<10:30,  5.11it/s] 17%|█▋        | 679/3900 [02:20<10:30,  5.11it/s] 17%|█▋        | 680/3900 [02:20<10:29,  5.11it/s] 17%|█▋        | 681/3900 [02:21<10:29,  5.12it/s] 17%|█▋        | 682/3900 [02:21<10:29,  5.11it/s] 18%|█▊        | 683/3900 [02:21<10:29,  5.11it/s] 18%|█▊        | 684/3900 [02:21<10:28,  5.11it/s] 18%|█▊        | 685/3900 [02:21<10:28,  5.11it/s] 18%|█▊        | 686/3900 [02:22<10:28,  5.11it/s] 18%|█▊        | 687/3900 [02:22<10:28,  5.12it/s] 18%|█▊        | 688/3900 [02:22<10:27,  5.12it/s] 18%|█▊        | 689/3900 [02:22<10:27,  5.12it/s] 18%|█▊        | 690/3900 [02:22<10:27,  5.11it/s] 18%|█▊        | 691/3900 [02:22<10:27,  5.11it/s] 18%|█▊        | 692/3900 [02:23<10:27,  5.11it/s] 18%|█▊        | 693/3900 [02:23<10:27,  5.11it/s] 18%|█▊        | 694/3900 [02:23<10:27,  5.11it/s] 18%|█▊        | 695/3900 [02:23<10:26,  5.11it/s] 18%|█▊        | 696/3900 [02:23<10:26,  5.11it/s] 18%|█▊        | 697/3900 [02:24<10:26,  5.12it/s] 18%|█▊        | 698/3900 [02:24<10:25,  5.12it/s] 18%|█▊        | 699/3900 [02:24<10:26,  5.11it/s] 18%|█▊        | 700/3900 [02:24<10:25,  5.11it/s] 18%|█▊        | 701/3900 [02:24<10:25,  5.12it/s] 18%|█▊        | 702/3900 [02:25<10:17,  5.18it/s] 18%|█▊        | 703/3900 [02:25<10:20,  5.15it/s] 18%|█▊        | 704/3900 [02:25<10:21,  5.14it/s] 18%|█▊        | 705/3900 [02:25<10:22,  5.13it/s] 18%|█▊        | 706/3900 [02:25<10:23,  5.12it/s] 18%|█▊        | 707/3900 [02:26<10:23,  5.12it/s] 18%|█▊        | 708/3900 [02:26<10:23,  5.12it/s] 18%|█▊        | 709/3900 [02:26<10:23,  5.12it/s] 18%|█▊        | 710/3900 [02:26<10:23,  5.11it/s] 18%|█▊        | 711/3900 [02:26<10:23,  5.11it/s] 18%|█▊        | 712/3900 [02:27<10:23,  5.11it/s] 18%|█▊        | 713/3900 [02:27<10:23,  5.11it/s] 18%|█▊        | 714/3900 [02:27<10:23,  5.11it/s] 18%|█▊        | 715/3900 [02:27<10:23,  5.11it/s] 18%|█▊        | 716/3900 [02:27<10:22,  5.11it/s] 18%|█▊        | 717/3900 [02:28<10:22,  5.11it/s] 18%|█▊        | 718/3900 [02:28<10:21,  5.12it/s] 18%|█▊        | 719/3900 [02:28<10:22,  5.11it/s] 18%|█▊        | 720/3900 [02:28<10:22,  5.11it/s] 18%|█▊        | 721/3900 [02:28<10:22,  5.11it/s] 19%|█▊        | 722/3900 [02:29<10:22,  5.10it/s] 19%|█▊        | 723/3900 [02:29<10:22,  5.11it/s] 19%|█▊        | 724/3900 [02:29<10:21,  5.11it/s] 19%|█▊        | 725/3900 [02:29<10:21,  5.11it/s] 19%|█▊        | 726/3900 [02:29<10:21,  5.11it/s] 19%|█▊        | 727/3900 [02:30<10:20,  5.11it/s] 19%|█▊        | 728/3900 [02:30<10:20,  5.11it/s] 19%|█▊        | 729/3900 [02:30<10:20,  5.11it/s] 19%|█▊        | 730/3900 [02:30<10:19,  5.12it/s] 19%|█▊        | 731/3900 [02:30<10:19,  5.12it/s] 19%|█▉        | 732/3900 [02:30<10:18,  5.12it/s] 19%|█▉        | 733/3900 [02:31<10:18,  5.12it/s] 19%|█▉        | 734/3900 [02:31<10:19,  5.11it/s] 19%|█▉        | 735/3900 [02:31<10:19,  5.11it/s] 19%|█▉        | 736/3900 [02:31<10:18,  5.11it/s] 19%|█▉        | 737/3900 [02:31<10:18,  5.11it/s] 19%|█▉        | 738/3900 [02:32<10:18,  5.11it/s] 19%|█▉        | 739/3900 [02:32<10:18,  5.11it/s] 19%|█▉        | 740/3900 [02:32<10:17,  5.12it/s] 19%|█▉        | 741/3900 [02:32<10:10,  5.17it/s] 19%|█▉        | 742/3900 [02:32<10:12,  5.15it/s] 19%|█▉        | 743/3900 [02:33<10:14,  5.14it/s] 19%|█▉        | 744/3900 [02:33<10:14,  5.14it/s] 19%|█▉        | 745/3900 [02:33<10:14,  5.13it/s] 19%|█▉        | 746/3900 [02:33<10:15,  5.12it/s] 19%|█▉        | 747/3900 [02:33<10:15,  5.12it/s] 19%|█▉        | 748/3900 [02:34<10:15,  5.12it/s] 19%|█▉        | 749/3900 [02:34<10:15,  5.12it/s] 19%|█▉        | 750/3900 [02:34<11:26,  4.59it/s] 19%|█▉        | 751/3900 [02:34<11:07,  4.72it/s] 19%|█▉        | 752/3900 [02:34<10:52,  4.82it/s] 19%|█▉        | 753/3900 [02:35<10:41,  4.90it/s] 19%|█▉        | 754/3900 [02:35<10:33,  4.97it/s] 19%|█▉        | 755/3900 [02:35<10:27,  5.01it/s] 19%|█▉        | 756/3900 [02:35<10:23,  5.04it/s] 19%|█▉        | 757/3900 [02:35<10:20,  5.06it/s] 19%|█▉        | 758/3900 [02:36<10:18,  5.08it/s] 19%|█▉        | 759/3900 [02:36<10:16,  5.09it/s] 19%|█▉        | 760/3900 [02:36<10:15,  5.10it/s] 20%|█▉        | 761/3900 [02:36<10:15,  5.10it/s] 20%|█▉        | 762/3900 [02:36<10:14,  5.11it/s] 20%|█▉        | 763/3900 [02:37<10:13,  5.11it/s] 20%|█▉        | 764/3900 [02:37<10:13,  5.11it/s] 20%|█▉        | 765/3900 [02:37<10:12,  5.12it/s] 20%|█▉        | 766/3900 [02:37<10:12,  5.12it/s] 20%|█▉        | 767/3900 [02:37<10:12,  5.12it/s] 20%|█▉        | 768/3900 [02:38<10:11,  5.12it/s] 20%|█▉        | 769/3900 [02:38<10:11,  5.12it/s] 20%|█▉        | 770/3900 [02:38<10:11,  5.12it/s] 20%|█▉        | 771/3900 [02:38<10:12,  5.11it/s] 20%|█▉        | 772/3900 [02:38<10:12,  5.11it/s] 20%|█▉        | 773/3900 [02:39<10:11,  5.11it/s] 20%|█▉        | 774/3900 [02:39<10:10,  5.12it/s] 20%|█▉        | 775/3900 [02:39<10:10,  5.12it/s] 20%|█▉        | 776/3900 [02:39<10:10,  5.12it/s] 20%|█▉        | 777/3900 [02:39<10:10,  5.12it/s] 20%|█▉        | 778/3900 [02:40<10:10,  5.12it/s] 20%|█▉        | 779/3900 [02:40<10:09,  5.12it/s] 20%|██        | 780/3900 [02:40<10:02,  5.18it/s] 20%|██        | 781/3900 [02:40<10:05,  5.15it/s] 20%|██        | 782/3900 [02:40<10:06,  5.14it/s] 20%|██        | 783/3900 [02:41<10:06,  5.14it/s] 20%|██        | 784/3900 [02:41<10:07,  5.13it/s] 20%|██        | 785/3900 [02:41<10:07,  5.13it/s] 20%|██        | 786/3900 [02:41<10:07,  5.12it/s] 20%|██        | 787/3900 [02:41<10:07,  5.12it/s] 20%|██        | 788/3900 [02:42<10:07,  5.12it/s] 20%|██        | 789/3900 [02:42<10:07,  5.12it/s] 20%|██        | 790/3900 [02:42<10:07,  5.12it/s] 20%|██        | 791/3900 [02:42<10:07,  5.12it/s] 20%|██        | 792/3900 [02:42<10:07,  5.12it/s] 20%|██        | 793/3900 [02:42<10:06,  5.12it/s] 20%|██        | 794/3900 [02:43<10:06,  5.12it/s] 20%|██        | 795/3900 [02:43<10:06,  5.12it/s] 20%|██        | 796/3900 [02:43<10:06,  5.12it/s] 20%|██        | 797/3900 [02:43<10:06,  5.12it/s] 20%|██        | 798/3900 [02:43<10:05,  5.12it/s] 20%|██        | 799/3900 [02:44<10:05,  5.12it/s] 21%|██        | 800/3900 [02:44<10:05,  5.12it/s] 21%|██        | 801/3900 [02:44<10:05,  5.12it/s] 21%|██        | 802/3900 [02:44<10:05,  5.12it/s] 21%|██        | 803/3900 [02:44<10:05,  5.12it/s] 21%|██        | 804/3900 [02:45<10:04,  5.12it/s] 21%|██        | 805/3900 [02:45<10:04,  5.12it/s] 21%|██        | 806/3900 [02:45<10:04,  5.12it/s] 21%|██        | 807/3900 [02:45<10:04,  5.11it/s] 21%|██        | 808/3900 [02:45<10:04,  5.11it/s] 21%|██        | 809/3900 [02:46<10:04,  5.12it/s] 21%|██        | 810/3900 [02:46<10:03,  5.12it/s] 21%|██        | 811/3900 [02:46<10:03,  5.11it/s] 21%|██        | 812/3900 [02:46<10:03,  5.11it/s] 21%|██        | 813/3900 [02:46<10:03,  5.12it/s] 21%|██        | 814/3900 [02:47<11:07,  4.62it/s] 21%|██        | 815/3900 [02:47<10:48,  4.76it/s] 21%|██        | 816/3900 [02:47<10:34,  4.86it/s] 21%|██        | 817/3900 [02:47<10:24,  4.93it/s] 21%|██        | 818/3900 [02:47<10:18,  4.99it/s] 21%|██        | 819/3900 [02:48<10:06,  5.08it/s] 21%|██        | 820/3900 [02:48<10:05,  5.09it/s] 21%|██        | 821/3900 [02:48<10:04,  5.10it/s] 21%|██        | 822/3900 [02:48<10:03,  5.10it/s] 21%|██        | 823/3900 [02:48<10:02,  5.10it/s] 21%|██        | 824/3900 [02:49<10:02,  5.11it/s] 21%|██        | 825/3900 [02:49<10:01,  5.11it/s] 21%|██        | 826/3900 [02:49<10:01,  5.11it/s] 21%|██        | 827/3900 [02:49<10:01,  5.11it/s] 21%|██        | 828/3900 [02:49<10:01,  5.11it/s] 21%|██▏       | 829/3900 [02:50<10:00,  5.11it/s] 21%|██▏       | 830/3900 [02:50<10:00,  5.11it/s] 21%|██▏       | 831/3900 [02:50<09:59,  5.12it/s] 21%|██▏       | 832/3900 [02:50<09:59,  5.12it/s] 21%|██▏       | 833/3900 [02:50<09:59,  5.12it/s] 21%|██▏       | 834/3900 [02:51<09:58,  5.12it/s] 21%|██▏       | 835/3900 [02:51<09:58,  5.12it/s] 21%|██▏       | 836/3900 [02:51<09:58,  5.12it/s] 21%|██▏       | 837/3900 [02:51<09:58,  5.12it/s] 21%|██▏       | 838/3900 [02:51<09:58,  5.12it/s] 22%|██▏       | 839/3900 [02:52<09:58,  5.12it/s] 22%|██▏       | 840/3900 [02:52<09:58,  5.12it/s] 22%|██▏       | 841/3900 [02:52<09:57,  5.12it/s] 22%|██▏       | 842/3900 [02:52<09:57,  5.12it/s] 22%|██▏       | 843/3900 [02:52<09:57,  5.12it/s] 22%|██▏       | 844/3900 [02:53<09:57,  5.12it/s] 22%|██▏       | 845/3900 [02:53<09:56,  5.12it/s] 22%|██▏       | 846/3900 [02:53<09:56,  5.12it/s] 22%|██▏       | 847/3900 [02:53<09:56,  5.12it/s] 22%|██▏       | 848/3900 [02:53<09:56,  5.12it/s] 22%|██▏       | 849/3900 [02:53<09:56,  5.12it/s] 22%|██▏       | 850/3900 [02:54<09:56,  5.12it/s] 22%|██▏       | 851/3900 [02:54<09:55,  5.12it/s] 22%|██▏       | 852/3900 [02:54<09:55,  5.12it/s] 22%|██▏       | 853/3900 [02:54<09:55,  5.12it/s] 22%|██▏       | 854/3900 [02:54<09:55,  5.12it/s] 22%|██▏       | 855/3900 [02:55<09:54,  5.12it/s] 22%|██▏       | 856/3900 [02:55<09:54,  5.12it/s] 22%|██▏       | 857/3900 [02:55<09:54,  5.12it/s] 22%|██▏       | 858/3900 [02:55<09:47,  5.18it/s] 22%|██▏       | 859/3900 [02:55<09:50,  5.15it/s] 22%|██▏       | 860/3900 [02:56<09:51,  5.14it/s] 22%|██▏       | 861/3900 [02:56<09:52,  5.13it/s] 22%|██▏       | 862/3900 [02:56<09:52,  5.13it/s] 22%|██▏       | 863/3900 [02:56<09:52,  5.12it/s] 22%|██▏       | 864/3900 [02:56<09:52,  5.12it/s] 22%|██▏       | 865/3900 [02:57<09:52,  5.12it/s] 22%|██▏       | 866/3900 [02:57<09:52,  5.12it/s] 22%|██▏       | 867/3900 [02:57<09:52,  5.12it/s] 22%|██▏       | 868/3900 [02:57<09:52,  5.12it/s] 22%|██▏       | 869/3900 [02:57<09:51,  5.12it/s] 22%|██▏       | 870/3900 [02:58<09:51,  5.12it/s] 22%|██▏       | 871/3900 [02:58<09:52,  5.11it/s] 22%|██▏       | 872/3900 [02:58<09:52,  5.11it/s] 22%|██▏       | 873/3900 [02:58<09:51,  5.12it/s] 22%|██▏       | 874/3900 [02:58<09:51,  5.12it/s] 22%|██▏       | 875/3900 [02:59<09:51,  5.12it/s] 22%|██▏       | 876/3900 [02:59<09:50,  5.12it/s] 22%|██▏       | 877/3900 [02:59<09:50,  5.12it/s] 23%|██▎       | 878/3900 [02:59<09:50,  5.12it/s] 23%|██▎       | 879/3900 [02:59<09:49,  5.12it/s] 23%|██▎       | 880/3900 [03:00<09:49,  5.12it/s] 23%|██▎       | 881/3900 [03:00<09:49,  5.12it/s] 23%|██▎       | 882/3900 [03:00<09:49,  5.12it/s] 23%|██▎       | 883/3900 [03:00<09:49,  5.12it/s] 23%|██▎       | 884/3900 [03:00<09:49,  5.12it/s] 23%|██▎       | 885/3900 [03:01<09:48,  5.12it/s] 23%|██▎       | 886/3900 [03:01<09:48,  5.12it/s] 23%|██▎       | 887/3900 [03:01<09:48,  5.12it/s] 23%|██▎       | 888/3900 [03:01<09:48,  5.12it/s] 23%|██▎       | 889/3900 [03:01<09:47,  5.12it/s] 23%|██▎       | 890/3900 [03:01<09:47,  5.12it/s] 23%|██▎       | 891/3900 [03:02<09:47,  5.12it/s] 23%|██▎       | 892/3900 [03:02<09:47,  5.12it/s] 23%|██▎       | 893/3900 [03:02<09:47,  5.12it/s] 23%|██▎       | 894/3900 [03:02<09:47,  5.12it/s] 23%|██▎       | 895/3900 [03:02<09:46,  5.12it/s] 23%|██▎       | 896/3900 [03:03<09:46,  5.12it/s] 23%|██▎       | 897/3900 [03:03<09:39,  5.18it/s] 23%|██▎       | 898/3900 [03:03<09:41,  5.16it/s] 23%|██▎       | 899/3900 [03:03<09:43,  5.15it/s] 23%|██▎       | 900/3900 [03:03<09:43,  5.14it/s] 23%|██▎       | 901/3900 [03:04<09:44,  5.13it/s] 23%|██▎       | 902/3900 [03:04<09:44,  5.13it/s] 23%|██▎       | 903/3900 [03:04<09:44,  5.13it/s] 23%|██▎       | 904/3900 [03:04<09:44,  5.12it/s] 23%|██▎       | 905/3900 [03:04<09:44,  5.13it/s] 23%|██▎       | 906/3900 [03:05<09:44,  5.12it/s] 23%|██▎       | 907/3900 [03:05<09:45,  5.11it/s] 23%|██▎       | 908/3900 [03:05<09:45,  5.11it/s] 23%|██▎       | 909/3900 [03:05<09:45,  5.11it/s] 23%|██▎       | 910/3900 [03:05<09:45,  5.11it/s] 23%|██▎       | 911/3900 [03:06<09:45,  5.11it/s] 23%|██▎       | 912/3900 [03:06<09:44,  5.11it/s] 23%|██▎       | 913/3900 [03:06<09:44,  5.11it/s] 23%|██▎       | 914/3900 [03:06<09:43,  5.11it/s] 23%|██▎       | 915/3900 [03:06<09:44,  5.11it/s] 23%|██▎       | 916/3900 [03:07<09:43,  5.11it/s] 24%|██▎       | 917/3900 [03:07<09:43,  5.11it/s] 24%|██▎       | 918/3900 [03:07<09:42,  5.12it/s] 24%|██▎       | 919/3900 [03:07<09:44,  5.10it/s] 24%|██▎       | 920/3900 [03:07<09:46,  5.08it/s] 24%|██▎       | 921/3900 [03:08<09:47,  5.07it/s] 24%|██▎       | 922/3900 [03:08<09:47,  5.07it/s] 24%|██▎       | 923/3900 [03:08<09:48,  5.06it/s] 24%|██▎       | 924/3900 [03:08<09:48,  5.06it/s] 24%|██▎       | 925/3900 [03:08<09:48,  5.05it/s] 24%|██▎       | 926/3900 [03:09<09:48,  5.05it/s] 24%|██▍       | 927/3900 [03:09<09:48,  5.05it/s] 24%|██▍       | 928/3900 [03:09<09:48,  5.05it/s] 24%|██▍       | 929/3900 [03:09<09:48,  5.05it/s] 24%|██▍       | 930/3900 [03:09<09:48,  5.05it/s] 24%|██▍       | 931/3900 [03:10<09:47,  5.05it/s] 24%|██▍       | 932/3900 [03:10<09:47,  5.05it/s] 24%|██▍       | 933/3900 [03:10<09:47,  5.05it/s] 24%|██▍       | 934/3900 [03:10<09:47,  5.05it/s] 24%|██▍       | 935/3900 [03:10<09:47,  5.05it/s] 24%|██▍       | 936/3900 [03:11<09:40,  5.10it/s] 24%|██▍       | 937/3900 [03:11<09:42,  5.08it/s] 24%|██▍       | 938/3900 [03:11<09:43,  5.07it/s] 24%|██▍       | 939/3900 [03:11<09:44,  5.07it/s] 24%|██▍       | 940/3900 [03:11<09:44,  5.06it/s] 24%|██▍       | 941/3900 [03:12<09:44,  5.06it/s] 24%|██▍       | 942/3900 [03:12<09:45,  5.06it/s] 24%|██▍       | 943/3900 [03:12<09:45,  5.05it/s] 24%|██▍       | 944/3900 [03:12<09:45,  5.05it/s] 24%|██▍       | 945/3900 [03:12<09:45,  5.05it/s] 24%|██▍       | 946/3900 [03:13<09:45,  5.05it/s] 24%|██▍       | 947/3900 [03:13<09:44,  5.05it/s] 24%|██▍       | 948/3900 [03:13<09:46,  5.03it/s] 24%|██▍       | 949/3900 [03:13<09:47,  5.03it/s] 24%|██▍       | 950/3900 [03:13<09:46,  5.03it/s] 24%|██▍       | 951/3900 [03:13<09:45,  5.04it/s] 24%|██▍       | 952/3900 [03:14<09:44,  5.04it/s] 24%|██▍       | 953/3900 [03:14<09:44,  5.04it/s] 24%|██▍       | 954/3900 [03:14<09:44,  5.04it/s] 24%|██▍       | 955/3900 [03:14<09:44,  5.04it/s] 25%|██▍       | 956/3900 [03:14<09:44,  5.04it/s] 25%|██▍       | 957/3900 [03:15<09:44,  5.03it/s] 25%|██▍       | 958/3900 [03:15<09:44,  5.04it/s] 25%|██▍       | 959/3900 [03:15<09:43,  5.04it/s] 25%|██▍       | 960/3900 [03:15<09:43,  5.04it/s] 25%|██▍       | 961/3900 [03:15<09:42,  5.04it/s] 25%|██▍       | 962/3900 [03:16<09:42,  5.04it/s] 25%|██▍       | 963/3900 [03:16<09:42,  5.04it/s] 25%|██▍       | 964/3900 [03:16<09:42,  5.04it/s] 25%|██▍       | 965/3900 [03:16<09:42,  5.04it/s] 25%|██▍       | 966/3900 [03:16<09:41,  5.04it/s] 25%|██▍       | 967/3900 [03:17<09:41,  5.04it/s] 25%|██▍       | 968/3900 [03:17<09:45,  5.01it/s] 25%|██▍       | 969/3900 [03:17<09:44,  5.01it/s] 25%|██▍       | 970/3900 [03:17<09:43,  5.02it/s] 25%|██▍       | 971/3900 [03:17<09:43,  5.02it/s] 25%|██▍       | 972/3900 [03:18<09:42,  5.02it/s] 25%|██▍       | 973/3900 [03:18<09:42,  5.03it/s] 25%|██▍       | 974/3900 [03:18<09:41,  5.03it/s] 25%|██▌       | 975/3900 [03:18<09:34,  5.09it/s] 25%|██▌       | 976/3900 [03:18<09:36,  5.07it/s] 25%|██▌       | 977/3900 [03:19<09:37,  5.07it/s] 25%|██▌       | 978/3900 [03:19<09:38,  5.05it/s] 25%|██▌       | 979/3900 [03:19<09:38,  5.05it/s] 25%|██▌       | 980/3900 [03:19<09:38,  5.05it/s] 25%|██▌       | 981/3900 [03:19<09:38,  5.05it/s] 25%|██▌       | 982/3900 [03:20<09:39,  5.04it/s] 25%|██▌       | 983/3900 [03:20<09:39,  5.04it/s] 25%|██▌       | 984/3900 [03:20<09:38,  5.04it/s] 25%|██▌       | 985/3900 [03:20<09:38,  5.04it/s] 25%|██▌       | 986/3900 [03:20<09:37,  5.04it/s] 25%|██▌       | 987/3900 [03:21<09:37,  5.04it/s] 25%|██▌       | 988/3900 [03:21<09:37,  5.04it/s] 25%|██▌       | 989/3900 [03:21<09:36,  5.05it/s] 25%|██▌       | 990/3900 [03:21<09:36,  5.05it/s] 25%|██▌       | 991/3900 [03:21<09:36,  5.05it/s] 25%|██▌       | 992/3900 [03:22<09:35,  5.05it/s] 25%|██▌       | 993/3900 [03:22<09:35,  5.05it/s] 25%|██▌       | 994/3900 [03:22<09:35,  5.05it/s] 26%|██▌       | 995/3900 [03:22<09:34,  5.05it/s] 26%|██▌       | 996/3900 [03:22<09:34,  5.05it/s] 26%|██▌       | 997/3900 [03:23<09:34,  5.05it/s] 26%|██▌       | 998/3900 [03:23<09:35,  5.05it/s] 26%|██▌       | 999/3900 [03:23<09:34,  5.05it/s] 26%|██▌       | 1000/3900 [03:23<09:34,  5.05it/s]                                                    26%|██▌       | 1000/3900 [03:23<09:34,  5.05it/s][INFO|trainer.py:2409] 2022-08-27 22:11:08,459 >> Saving model checkpoint to tmp/baseline/RTE/bert-base-uncased/HPO_S0/1e-06/64/100/checkpoint-1000
[INFO|configuration_utils.py:446] 2022-08-27 22:11:08,460 >> Configuration saved in tmp/baseline/RTE/bert-base-uncased/HPO_S0/1e-06/64/100/checkpoint-1000/config.json
[INFO|modeling_utils.py:1602] 2022-08-27 22:11:09,317 >> Model weights saved in tmp/baseline/RTE/bert-base-uncased/HPO_S0/1e-06/64/100/checkpoint-1000/pytorch_model.bin
[INFO|tokenization_utils_base.py:2108] 2022-08-27 22:11:09,318 >> tokenizer config file saved in tmp/baseline/RTE/bert-base-uncased/HPO_S0/1e-06/64/100/checkpoint-1000/tokenizer_config.json
[INFO|tokenization_utils_base.py:2114] 2022-08-27 22:11:09,318 >> Special tokens file saved in tmp/baseline/RTE/bert-base-uncased/HPO_S0/1e-06/64/100/checkpoint-1000/special_tokens_map.json
/home/ubuntu/anaconda3/envs/transformers/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 26%|██▌       | 1001/3900 [03:26<48:43,  1.01s/it] 26%|██▌       | 1002/3900 [03:26<37:00,  1.31it/s] 26%|██▌       | 1003/3900 [03:27<28:45,  1.68it/s] 26%|██▌       | 1004/3900 [03:27<22:59,  2.10it/s] 26%|██▌       | 1005/3900 [03:27<18:57,  2.55it/s] 26%|██▌       | 1006/3900 [03:27<16:07,  2.99it/s] 26%|██▌       | 1007/3900 [03:27<14:09,  3.41it/s] 26%|██▌       | 1008/3900 [03:27<12:46,  3.77it/s] 26%|██▌       | 1009/3900 [03:28<11:47,  4.08it/s] 26%|██▌       | 1010/3900 [03:28<11:07,  4.33it/s] 26%|██▌       | 1011/3900 [03:28<10:38,  4.52it/s] 26%|██▌       | 1012/3900 [03:28<10:18,  4.67it/s] 26%|██▌       | 1013/3900 [03:28<10:04,  4.77it/s] 26%|██▌       | 1014/3900 [03:29<09:48,  4.90it/s] 26%|██▌       | 1015/3900 [03:29<09:44,  4.94it/s] 26%|██▌       | 1016/3900 [03:29<09:40,  4.97it/s] 26%|██▌       | 1017/3900 [03:29<09:37,  4.99it/s] 26%|██▌       | 1018/3900 [03:29<09:35,  5.01it/s] 26%|██▌       | 1019/3900 [03:30<09:33,  5.02it/s] 26%|██▌       | 1020/3900 [03:30<09:32,  5.03it/s] 26%|██▌       | 1021/3900 [03:30<09:31,  5.03it/s] 26%|██▌       | 1022/3900 [03:30<09:31,  5.04it/s] 26%|██▌       | 1023/3900 [03:30<09:31,  5.04it/s] 26%|██▋       | 1024/3900 [03:31<09:30,  5.04it/s] 26%|██▋       | 1025/3900 [03:31<09:29,  5.05it/s] 26%|██▋       | 1026/3900 [03:31<09:29,  5.04it/s] 26%|██▋       | 1027/3900 [03:31<09:29,  5.05it/s] 26%|██▋       | 1028/3900 [03:31<09:29,  5.05it/s] 26%|██▋       | 1029/3900 [03:32<09:28,  5.05it/s] 26%|██▋       | 1030/3900 [03:32<09:28,  5.05it/s] 26%|██▋       | 1031/3900 [03:32<09:28,  5.05it/s] 26%|██▋       | 1032/3900 [03:32<09:28,  5.05it/s] 26%|██▋       | 1033/3900 [03:32<09:27,  5.05it/s] 27%|██▋       | 1034/3900 [03:33<09:27,  5.05it/s] 27%|██▋       | 1035/3900 [03:33<09:27,  5.05it/s] 27%|██▋       | 1036/3900 [03:33<09:27,  5.05it/s] 27%|██▋       | 1037/3900 [03:33<09:26,  5.05it/s] 27%|██▋       | 1038/3900 [03:33<09:27,  5.05it/s] 27%|██▋       | 1039/3900 [03:34<09:26,  5.05it/s] 27%|██▋       | 1040/3900 [03:34<09:27,  5.04it/s] 27%|██▋       | 1041/3900 [03:34<09:28,  5.03it/s] 27%|██▋       | 1042/3900 [03:34<09:27,  5.03it/s] 27%|██▋       | 1043/3900 [03:34<09:27,  5.04it/s] 27%|██▋       | 1044/3900 [03:35<09:27,  5.03it/s] 27%|██▋       | 1045/3900 [03:35<09:26,  5.04it/s] 27%|██▋       | 1046/3900 [03:35<09:26,  5.04it/s] 27%|██▋       | 1047/3900 [03:35<09:26,  5.04it/s] 27%|██▋       | 1048/3900 [03:35<09:26,  5.03it/s] 27%|██▋       | 1049/3900 [03:36<09:26,  5.04it/s] 27%|██▋       | 1050/3900 [03:36<09:25,  5.04it/s] 27%|██▋       | 1051/3900 [03:36<09:25,  5.04it/s] 27%|██▋       | 1052/3900 [03:36<09:25,  5.04it/s] 27%|██▋       | 1053/3900 [03:36<09:18,  5.09it/s] 27%|██▋       | 1054/3900 [03:37<09:21,  5.07it/s] 27%|██▋       | 1055/3900 [03:37<09:21,  5.07it/s] 27%|██▋       | 1056/3900 [03:37<09:22,  5.06it/s] 27%|██▋       | 1057/3900 [03:37<09:22,  5.05it/s] 27%|██▋       | 1058/3900 [03:37<09:22,  5.05it/s] 27%|██▋       | 1059/3900 [03:38<09:23,  5.04it/s] 27%|██▋       | 1060/3900 [03:38<09:23,  5.04it/s] 27%|██▋       | 1061/3900 [03:38<09:23,  5.04it/s] 27%|██▋       | 1062/3900 [03:38<09:23,  5.03it/s] 27%|██▋       | 1063/3900 [03:38<09:23,  5.03it/s] 27%|██▋       | 1064/3900 [03:39<09:23,  5.03it/s] 27%|██▋       | 1065/3900 [03:39<09:23,  5.03it/s] 27%|██▋       | 1066/3900 [03:39<09:22,  5.04it/s] 27%|██▋       | 1067/3900 [03:39<09:22,  5.04it/s] 27%|██▋       | 1068/3900 [03:39<09:22,  5.03it/s] 27%|██▋       | 1069/3900 [03:40<09:22,  5.03it/s] 27%|██▋       | 1070/3900 [03:40<09:21,  5.04it/s] 27%|██▋       | 1071/3900 [03:40<09:21,  5.04it/s] 27%|██▋       | 1072/3900 [03:40<09:21,  5.04it/s] 28%|██▊       | 1073/3900 [03:40<09:21,  5.04it/s] 28%|██▊       | 1074/3900 [03:41<09:21,  5.03it/s] 28%|██▊       | 1075/3900 [03:41<09:21,  5.03it/s] 28%|██▊       | 1076/3900 [03:41<09:21,  5.03it/s] 28%|██▊       | 1077/3900 [03:41<09:21,  5.03it/s] 28%|██▊       | 1078/3900 [03:41<09:20,  5.03it/s] 28%|██▊       | 1079/3900 [03:42<09:20,  5.04it/s] 28%|██▊       | 1080/3900 [03:42<09:20,  5.03it/s] 28%|██▊       | 1081/3900 [03:42<09:20,  5.03it/s] 28%|██▊       | 1082/3900 [03:42<09:19,  5.04it/s] 28%|██▊       | 1083/3900 [03:42<09:19,  5.04it/s] 28%|██▊       | 1084/3900 [03:43<09:18,  5.04it/s] 28%|██▊       | 1085/3900 [03:43<09:18,  5.04it/s] 28%|██▊       | 1086/3900 [03:43<09:17,  5.05it/s] 28%|██▊       | 1087/3900 [03:43<09:17,  5.05it/s] 28%|██▊       | 1088/3900 [03:43<09:17,  5.05it/s] 28%|██▊       | 1089/3900 [03:44<09:17,  5.05it/s] 28%|██▊       | 1090/3900 [03:44<09:16,  5.05it/s] 28%|██▊       | 1091/3900 [03:44<09:16,  5.05it/s] 28%|██▊       | 1092/3900 [03:44<09:10,  5.10it/s] 28%|██▊       | 1093/3900 [03:44<09:12,  5.08it/s] 28%|██▊       | 1094/3900 [03:45<09:13,  5.07it/s] 28%|██▊       | 1095/3900 [03:45<09:13,  5.06it/s] 28%|██▊       | 1096/3900 [03:45<09:13,  5.06it/s] 28%|██▊       | 1097/3900 [03:45<09:14,  5.05it/s] 28%|██▊       | 1098/3900 [03:45<09:52,  4.73it/s] 28%|██▊       | 1099/3900 [03:46<09:41,  4.82it/s] 28%|██▊       | 1100/3900 [03:46<09:33,  4.88it/s] 28%|██▊       | 1101/3900 [03:46<09:27,  4.93it/s] 28%|██▊       | 1102/3900 [03:46<09:24,  4.96it/s] 28%|██▊       | 1103/3900 [03:46<09:21,  4.98it/s] 28%|██▊       | 1104/3900 [03:47<09:19,  5.00it/s] 28%|██▊       | 1105/3900 [03:47<09:17,  5.01it/s] 28%|██▊       | 1106/3900 [03:47<09:15,  5.03it/s] 28%|██▊       | 1107/3900 [03:47<09:14,  5.03it/s] 28%|██▊       | 1108/3900 [03:47<09:14,  5.04it/s] 28%|██▊       | 1109/3900 [03:48<09:13,  5.04it/s] 28%|██▊       | 1110/3900 [03:48<09:13,  5.04it/s] 28%|██▊       | 1111/3900 [03:48<09:13,  5.04it/s] 29%|██▊       | 1112/3900 [03:48<09:12,  5.05it/s] 29%|██▊       | 1113/3900 [03:48<09:12,  5.05it/s] 29%|██▊       | 1114/3900 [03:49<09:11,  5.05it/s] 29%|██▊       | 1115/3900 [03:49<09:12,  5.04it/s] 29%|██▊       | 1116/3900 [03:49<09:12,  5.04it/s] 29%|██▊       | 1117/3900 [03:49<09:11,  5.04it/s] 29%|██▊       | 1118/3900 [03:49<09:11,  5.05it/s] 29%|██▊       | 1119/3900 [03:50<09:10,  5.05it/s] 29%|██▊       | 1120/3900 [03:50<09:10,  5.05it/s] 29%|██▊       | 1121/3900 [03:50<09:10,  5.05it/s] 29%|██▉       | 1122/3900 [03:50<09:10,  5.05it/s] 29%|██▉       | 1123/3900 [03:50<09:09,  5.05it/s] 29%|██▉       | 1124/3900 [03:51<09:09,  5.05it/s] 29%|██▉       | 1125/3900 [03:51<09:09,  5.05it/s] 29%|██▉       | 1126/3900 [03:51<09:09,  5.05it/s] 29%|██▉       | 1127/3900 [03:51<09:09,  5.05it/s] 29%|██▉       | 1128/3900 [03:51<09:08,  5.05it/s] 29%|██▉       | 1129/3900 [03:52<09:09,  5.05it/s] 29%|██▉       | 1130/3900 [03:52<09:08,  5.05it/s] 29%|██▉       | 1131/3900 [03:52<09:02,  5.11it/s] 29%|██▉       | 1132/3900 [03:52<09:04,  5.09it/s] 29%|██▉       | 1133/3900 [03:52<09:05,  5.08it/s] 29%|██▉       | 1134/3900 [03:53<09:05,  5.07it/s] 29%|██▉       | 1135/3900 [03:53<09:06,  5.06it/s] 29%|██▉       | 1136/3900 [03:53<09:06,  5.06it/s] 29%|██▉       | 1137/3900 [03:53<09:06,  5.05it/s] 29%|██▉       | 1138/3900 [03:53<09:06,  5.05it/s] 29%|██▉       | 1139/3900 [03:53<09:06,  5.05it/s] 29%|██▉       | 1140/3900 [03:54<09:06,  5.05it/s] 29%|██▉       | 1141/3900 [03:54<09:06,  5.05it/s] 29%|██▉       | 1142/3900 [03:54<09:06,  5.05it/s] 29%|██▉       | 1143/3900 [03:54<09:06,  5.05it/s] 29%|██▉       | 1144/3900 [03:54<09:05,  5.05it/s] 29%|██▉       | 1145/3900 [03:55<09:05,  5.05it/s] 29%|██▉       | 1146/3900 [03:55<09:05,  5.05it/s] 29%|██▉       | 1147/3900 [03:55<09:05,  5.05it/s] 29%|██▉       | 1148/3900 [03:55<09:04,  5.05it/s] 29%|██▉       | 1149/3900 [03:55<09:04,  5.05it/s] 29%|██▉       | 1150/3900 [03:56<09:04,  5.05it/s] 30%|██▉       | 1151/3900 [03:56<09:41,  4.72it/s] 30%|██▉       | 1152/3900 [03:56<09:31,  4.81it/s] 30%|██▉       | 1153/3900 [03:56<09:22,  4.88it/s] 30%|██▉       | 1154/3900 [03:57<09:16,  4.93it/s] 30%|██▉       | 1155/3900 [03:57<09:12,  4.97it/s] 30%|██▉       | 1156/3900 [03:57<09:09,  4.99it/s] 30%|██▉       | 1157/3900 [03:57<09:07,  5.01it/s] 30%|██▉       | 1158/3900 [03:57<09:06,  5.02it/s] 30%|██▉       | 1159/3900 [03:58<09:05,  5.03it/s] 30%|██▉       | 1160/3900 [03:58<09:04,  5.04it/s] 30%|██▉       | 1161/3900 [03:58<09:03,  5.04it/s] 30%|██▉       | 1162/3900 [03:58<09:03,  5.04it/s] 30%|██▉       | 1163/3900 [03:58<09:02,  5.04it/s] 30%|██▉       | 1164/3900 [03:58<09:02,  5.04it/s] 30%|██▉       | 1165/3900 [03:59<09:02,  5.04it/s] 30%|██▉       | 1166/3900 [03:59<09:01,  5.05it/s] 30%|██▉       | 1167/3900 [03:59<09:01,  5.04it/s] 30%|██▉       | 1168/3900 [03:59<09:01,  5.05it/s] 30%|██▉       | 1169/3900 [03:59<09:01,  5.05it/s] 30%|███       | 1170/3900 [04:00<08:54,  5.11it/s] 30%|███       | 1171/3900 [04:00<08:56,  5.09it/s] 30%|███       | 1172/3900 [04:00<08:57,  5.08it/s] 30%|███       | 1173/3900 [04:00<08:58,  5.06it/s] 30%|███       | 1174/3900 [04:00<08:59,  5.06it/s] 30%|███       | 1175/3900 [04:01<08:59,  5.05it/s] 30%|███       | 1176/3900 [04:01<09:00,  5.04it/s] 30%|███       | 1177/3900 [04:01<09:00,  5.04it/s] 30%|███       | 1178/3900 [04:01<09:00,  5.03it/s] 30%|███       | 1179/3900 [04:01<09:00,  5.03it/s] 30%|███       | 1180/3900 [04:02<09:00,  5.03it/s] 30%|███       | 1181/3900 [04:02<09:00,  5.03it/s] 30%|███       | 1182/3900 [04:02<08:59,  5.04it/s] 30%|███       | 1183/3900 [04:02<08:59,  5.04it/s] 30%|███       | 1184/3900 [04:02<08:58,  5.04it/s] 30%|███       | 1185/3900 [04:03<08:59,  5.03it/s] 30%|███       | 1186/3900 [04:03<08:59,  5.03it/s] 30%|███       | 1187/3900 [04:03<08:59,  5.03it/s] 30%|███       | 1188/3900 [04:03<08:58,  5.03it/s] 30%|███       | 1189/3900 [04:03<08:58,  5.03it/s] 31%|███       | 1190/3900 [04:04<08:58,  5.03it/s] 31%|███       | 1191/3900 [04:04<08:58,  5.03it/s] 31%|███       | 1192/3900 [04:04<08:58,  5.03it/s] 31%|███       | 1193/3900 [04:04<08:58,  5.03it/s] 31%|███       | 1194/3900 [04:04<08:57,  5.04it/s] 31%|███       | 1195/3900 [04:05<08:56,  5.04it/s] 31%|███       | 1196/3900 [04:05<08:56,  5.04it/s] 31%|███       | 1197/3900 [04:05<08:56,  5.04it/s] 31%|███       | 1198/3900 [04:05<08:56,  5.04it/s] 31%|███       | 1199/3900 [04:05<08:56,  5.03it/s] 31%|███       | 1200/3900 [04:06<08:56,  5.04it/s] 31%|███       | 1201/3900 [04:06<08:55,  5.04it/s] 31%|███       | 1202/3900 [04:06<08:55,  5.04it/s] 31%|███       | 1203/3900 [04:06<08:55,  5.04it/s] 31%|███       | 1204/3900 [04:06<08:55,  5.04it/s] 31%|███       | 1205/3900 [04:07<08:54,  5.04it/s] 31%|███       | 1206/3900 [04:07<08:54,  5.04it/s] 31%|███       | 1207/3900 [04:07<08:54,  5.04it/s] 31%|███       | 1208/3900 [04:07<08:54,  5.04it/s] 31%|███       | 1209/3900 [04:07<08:48,  5.09it/s] 31%|███       | 1210/3900 [04:08<08:50,  5.07it/s] 31%|███       | 1211/3900 [04:08<08:51,  5.06it/s] 31%|███       | 1212/3900 [04:08<08:51,  5.05it/s] 31%|███       | 1213/3900 [04:08<08:52,  5.05it/s] 31%|███       | 1214/3900 [04:08<08:52,  5.04it/s] 31%|███       | 1215/3900 [04:09<08:52,  5.04it/s] 31%|███       | 1216/3900 [04:09<08:52,  5.04it/s] 31%|███       | 1217/3900 [04:09<08:51,  5.04it/s] 31%|███       | 1218/3900 [04:09<08:51,  5.04it/s] 31%|███▏      | 1219/3900 [04:09<08:51,  5.04it/s] 31%|███▏      | 1220/3900 [04:10<08:51,  5.04it/s] 31%|███▏      | 1221/3900 [04:10<08:51,  5.04it/s] 31%|███▏      | 1222/3900 [04:10<08:51,  5.04it/s] 31%|███▏      | 1223/3900 [04:10<08:51,  5.04it/s] 31%|███▏      | 1224/3900 [04:10<08:50,  5.04it/s] 31%|███▏      | 1225/3900 [04:11<08:50,  5.04it/s] 31%|███▏      | 1226/3900 [04:11<08:50,  5.04it/s] 31%|███▏      | 1227/3900 [04:11<08:50,  5.04it/s] 31%|███▏      | 1228/3900 [04:11<08:50,  5.04it/s] 32%|███▏      | 1229/3900 [04:11<08:49,  5.04it/s] 32%|███▏      | 1230/3900 [04:12<08:49,  5.04it/s] 32%|███▏      | 1231/3900 [04:12<08:49,  5.04it/s] 32%|███▏      | 1232/3900 [04:12<08:49,  5.04it/s] 32%|███▏      | 1233/3900 [04:12<08:49,  5.04it/s] 32%|███▏      | 1234/3900 [04:12<08:48,  5.04it/s] 32%|███▏      | 1235/3900 [04:13<08:48,  5.04it/s] 32%|███▏      | 1236/3900 [04:13<08:48,  5.04it/s] 32%|███▏      | 1237/3900 [04:13<08:48,  5.04it/s] 32%|███▏      | 1238/3900 [04:13<08:48,  5.04it/s] 32%|███▏      | 1239/3900 [04:13<08:48,  5.04it/s] 32%|███▏      | 1240/3900 [04:14<08:47,  5.04it/s] 32%|███▏      | 1241/3900 [04:14<08:47,  5.04it/s] 32%|███▏      | 1242/3900 [04:14<08:47,  5.04it/s] 32%|███▏      | 1243/3900 [04:14<08:47,  5.04it/s] 32%|███▏      | 1244/3900 [04:14<08:46,  5.04it/s] 32%|███▏      | 1245/3900 [04:15<08:46,  5.04it/s] 32%|███▏      | 1246/3900 [04:15<08:46,  5.04it/s] 32%|███▏      | 1247/3900 [04:15<08:46,  5.04it/s] 32%|███▏      | 1248/3900 [04:15<09:44,  4.53it/s] 32%|███▏      | 1249/3900 [04:15<09:27,  4.67it/s] 32%|███▏      | 1250/3900 [04:16<09:15,  4.77it/s] 32%|███▏      | 1251/3900 [04:16<09:06,  4.85it/s] 32%|███▏      | 1252/3900 [04:16<09:00,  4.90it/s] 32%|███▏      | 1253/3900 [04:16<08:56,  4.94it/s] 32%|███▏      | 1254/3900 [04:16<08:52,  4.97it/s] 32%|███▏      | 1255/3900 [04:17<08:50,  4.99it/s] 32%|███▏      | 1256/3900 [04:17<08:48,  5.00it/s] 32%|███▏      | 1257/3900 [04:17<08:48,  5.00it/s] 32%|███▏      | 1258/3900 [04:17<08:47,  5.01it/s] 32%|███▏      | 1259/3900 [04:17<08:46,  5.01it/s] 32%|███▏      | 1260/3900 [04:18<08:45,  5.02it/s] 32%|███▏      | 1261/3900 [04:18<08:45,  5.03it/s] 32%|███▏      | 1262/3900 [04:18<08:44,  5.03it/s] 32%|███▏      | 1263/3900 [04:18<08:44,  5.03it/s] 32%|███▏      | 1264/3900 [04:18<08:43,  5.03it/s] 32%|███▏      | 1265/3900 [04:19<08:43,  5.03it/s] 32%|███▏      | 1266/3900 [04:19<08:43,  5.03it/s] 32%|███▏      | 1267/3900 [04:19<08:43,  5.03it/s] 33%|███▎      | 1268/3900 [04:19<08:43,  5.03it/s] 33%|███▎      | 1269/3900 [04:19<08:42,  5.03it/s] 33%|███▎      | 1270/3900 [04:20<08:42,  5.04it/s] 33%|███▎      | 1271/3900 [04:20<08:41,  5.04it/s] 33%|███▎      | 1272/3900 [04:20<08:41,  5.04it/s] 33%|███▎      | 1273/3900 [04:20<08:41,  5.04it/s] 33%|███▎      | 1274/3900 [04:20<08:40,  5.04it/s] 33%|███▎      | 1275/3900 [04:21<08:40,  5.04it/s] 33%|███▎      | 1276/3900 [04:21<08:40,  5.04it/s] 33%|███▎      | 1277/3900 [04:21<08:40,  5.04it/s] 33%|███▎      | 1278/3900 [04:21<08:40,  5.04it/s] 33%|███▎      | 1279/3900 [04:21<08:39,  5.04it/s] 33%|███▎      | 1280/3900 [04:22<08:39,  5.04it/s] 33%|███▎      | 1281/3900 [04:22<08:39,  5.04it/s] 33%|███▎      | 1282/3900 [04:22<08:39,  5.04it/s] 33%|███▎      | 1283/3900 [04:22<08:39,  5.04it/s] 33%|███▎      | 1284/3900 [04:22<08:39,  5.04it/s] 33%|███▎      | 1285/3900 [04:23<08:38,  5.04it/s] 33%|███▎      | 1286/3900 [04:23<08:38,  5.04it/s] 33%|███▎      | 1287/3900 [04:23<08:32,  5.10it/s] 33%|███▎      | 1288/3900 [04:23<08:34,  5.08it/s] 33%|███▎      | 1289/3900 [04:23<08:35,  5.06it/s] 33%|███▎      | 1290/3900 [04:24<08:36,  5.06it/s] 33%|███▎      | 1291/3900 [04:24<08:36,  5.05it/s] 33%|███▎      | 1292/3900 [04:24<08:36,  5.05it/s] 33%|███▎      | 1293/3900 [04:24<08:36,  5.05it/s] 33%|███▎      | 1294/3900 [04:24<08:36,  5.04it/s] 33%|███▎      | 1295/3900 [04:25<08:36,  5.04it/s] 33%|███▎      | 1296/3900 [04:25<08:36,  5.04it/s] 33%|███▎      | 1297/3900 [04:25<08:36,  5.04it/s] 33%|███▎      | 1298/3900 [04:25<08:36,  5.04it/s] 33%|███▎      | 1299/3900 [04:25<08:36,  5.04it/s] 33%|███▎      | 1300/3900 [04:26<08:35,  5.04it/s] 33%|███▎      | 1301/3900 [04:26<09:12,  4.71it/s] 33%|███▎      | 1302/3900 [04:26<09:01,  4.80it/s] 33%|███▎      | 1303/3900 [04:26<08:54,  4.86it/s] 33%|███▎      | 1304/3900 [04:26<08:50,  4.89it/s] 33%|███▎      | 1305/3900 [04:27<08:47,  4.92it/s] 33%|███▎      | 1306/3900 [04:27<08:45,  4.93it/s] 34%|███▎      | 1307/3900 [04:27<08:43,  4.95it/s] 34%|███▎      | 1308/3900 [04:27<08:47,  4.92it/s] 34%|███▎      | 1309/3900 [04:27<08:45,  4.93it/s] 34%|███▎      | 1310/3900 [04:28<08:45,  4.93it/s] 34%|███▎      | 1311/3900 [04:28<08:43,  4.95it/s] 34%|███▎      | 1312/3900 [04:28<08:41,  4.96it/s] 34%|███▎      | 1313/3900 [04:28<08:43,  4.94it/s] 34%|███▎      | 1314/3900 [04:28<08:40,  4.97it/s] 34%|███▎      | 1315/3900 [04:29<08:42,  4.95it/s] 34%|███▎      | 1316/3900 [04:29<08:42,  4.94it/s] 34%|███▍      | 1317/3900 [04:29<08:42,  4.94it/s] 34%|███▍      | 1318/3900 [04:29<08:42,  4.94it/s] 34%|███▍      | 1319/3900 [04:29<08:41,  4.95it/s] 34%|███▍      | 1320/3900 [04:30<08:39,  4.97it/s] 34%|███▍      | 1321/3900 [04:30<08:38,  4.98it/s] 34%|███▍      | 1322/3900 [04:30<08:36,  4.99it/s] 34%|███▍      | 1323/3900 [04:30<08:36,  4.99it/s] 34%|███▍      | 1324/3900 [04:30<08:38,  4.97it/s] 34%|███▍      | 1325/3900 [04:31<08:38,  4.96it/s] 34%|███▍      | 1326/3900 [04:31<08:32,  5.02it/s] 34%|███▍      | 1327/3900 [04:31<08:34,  5.00it/s] 34%|███▍      | 1328/3900 [04:31<08:35,  4.99it/s] 34%|███▍      | 1329/3900 [04:31<08:39,  4.95it/s] 34%|███▍      | 1330/3900 [04:32<08:37,  4.97it/s] 34%|███▍      | 1331/3900 [04:32<08:36,  4.97it/s] 34%|███▍      | 1332/3900 [04:32<08:36,  4.97it/s] 34%|███▍      | 1333/3900 [04:32<08:37,  4.96it/s] 34%|███▍      | 1334/3900 [04:32<08:41,  4.92it/s] 34%|███▍      | 1335/3900 [04:33<08:38,  4.94it/s] 34%|███▍      | 1336/3900 [04:33<08:38,  4.95it/s] 34%|███▍      | 1337/3900 [04:33<08:38,  4.94it/s] 34%|███▍      | 1338/3900 [04:33<08:36,  4.96it/s] 34%|███▍      | 1339/3900 [04:33<08:37,  4.95it/s] 34%|███▍      | 1340/3900 [04:34<08:36,  4.95it/s] 34%|███▍      | 1341/3900 [04:34<08:38,  4.94it/s] 34%|███▍      | 1342/3900 [04:34<08:36,  4.96it/s] 34%|███▍      | 1343/3900 [04:34<08:34,  4.97it/s] 34%|███▍      | 1344/3900 [04:34<08:37,  4.94it/s] 34%|███▍      | 1345/3900 [04:35<08:37,  4.94it/s] 35%|███▍      | 1346/3900 [04:35<08:35,  4.96it/s] 35%|███▍      | 1347/3900 [04:35<08:33,  4.97it/s] 35%|███▍      | 1348/3900 [04:35<08:32,  4.98it/s] 35%|███▍      | 1349/3900 [04:35<08:35,  4.94it/s] 35%|███▍      | 1350/3900 [04:36<08:34,  4.95it/s] 35%|███▍      | 1351/3900 [04:36<08:33,  4.96it/s] 35%|███▍      | 1352/3900 [04:36<08:33,  4.97it/s] 35%|███▍      | 1353/3900 [04:36<08:33,  4.96it/s] 35%|███▍      | 1354/3900 [04:36<08:34,  4.95it/s] 35%|███▍      | 1355/3900 [04:37<08:34,  4.94it/s] 35%|███▍      | 1356/3900 [04:37<08:35,  4.94it/s] 35%|███▍      | 1357/3900 [04:37<08:35,  4.94it/s] 35%|███▍      | 1358/3900 [04:37<08:33,  4.95it/s] 35%|███▍      | 1359/3900 [04:37<08:35,  4.93it/s] 35%|███▍      | 1360/3900 [04:38<08:35,  4.93it/s] 35%|███▍      | 1361/3900 [04:38<08:37,  4.91it/s] 35%|███▍      | 1362/3900 [04:38<08:34,  4.93it/s] 35%|███▍      | 1363/3900 [04:38<08:31,  4.96it/s] 35%|███▍      | 1364/3900 [04:38<08:34,  4.93it/s] 35%|███▌      | 1365/3900 [04:39<08:26,  5.00it/s] 35%|███▌      | 1366/3900 [04:39<08:28,  4.99it/s] 35%|███▌      | 1367/3900 [04:39<08:29,  4.97it/s] 35%|███▌      | 1368/3900 [04:39<08:34,  4.92it/s] 35%|███▌      | 1369/3900 [04:40<08:35,  4.91it/s] 35%|███▌      | 1370/3900 [04:40<08:35,  4.90it/s] 35%|███▌      | 1371/3900 [04:40<08:31,  4.94it/s] 35%|███▌      | 1372/3900 [04:40<08:31,  4.94it/s] 35%|███▌      | 1373/3900 [04:40<08:30,  4.95it/s] 35%|███▌      | 1374/3900 [04:41<08:30,  4.95it/s] 35%|███▌      | 1375/3900 [04:41<08:29,  4.96it/s] 35%|███▌      | 1376/3900 [04:41<08:29,  4.95it/s] 35%|███▌      | 1377/3900 [04:41<08:28,  4.96it/s] 35%|███▌      | 1378/3900 [04:41<08:26,  4.98it/s] 35%|███▌      | 1379/3900 [04:42<08:28,  4.96it/s] 35%|███▌      | 1380/3900 [04:42<08:29,  4.95it/s] 35%|███▌      | 1381/3900 [04:42<08:27,  4.97it/s] 35%|███▌      | 1382/3900 [04:42<08:26,  4.98it/s] 35%|███▌      | 1383/3900 [04:42<08:26,  4.97it/s] 35%|███▌      | 1384/3900 [04:43<08:28,  4.95it/s] 36%|███▌      | 1385/3900 [04:43<08:28,  4.95it/s] 36%|███▌      | 1386/3900 [04:43<08:26,  4.96it/s] 36%|███▌      | 1387/3900 [04:43<08:25,  4.98it/s] 36%|███▌      | 1388/3900 [04:43<08:25,  4.97it/s] 36%|███▌      | 1389/3900 [04:44<08:26,  4.96it/s] 36%|███▌      | 1390/3900 [04:44<08:26,  4.95it/s] 36%|███▌      | 1391/3900 [04:44<08:24,  4.97it/s] 36%|███▌      | 1392/3900 [04:44<08:23,  4.98it/s] 36%|███▌      | 1393/3900 [04:44<08:24,  4.97it/s] 36%|███▌      | 1394/3900 [04:45<08:25,  4.96it/s] 36%|███▌      | 1395/3900 [04:45<08:25,  4.96it/s] 36%|███▌      | 1396/3900 [04:45<08:25,  4.95it/s] 36%|███▌      | 1397/3900 [04:45<08:24,  4.96it/s] 36%|███▌      | 1398/3900 [04:45<09:00,  4.63it/s] 36%|███▌      | 1399/3900 [04:46<08:49,  4.72it/s] 36%|███▌      | 1400/3900 [04:46<08:42,  4.78it/s] 36%|███▌      | 1401/3900 [04:46<08:36,  4.84it/s] 36%|███▌      | 1402/3900 [04:46<08:31,  4.88it/s] 36%|███▌      | 1403/3900 [04:46<08:30,  4.89it/s] 36%|███▌      | 1404/3900 [04:47<08:22,  4.97it/s] 36%|███▌      | 1405/3900 [04:47<08:22,  4.97it/s] 36%|███▌      | 1406/3900 [04:47<08:21,  4.97it/s] 36%|███▌      | 1407/3900 [04:47<08:21,  4.97it/s] 36%|███▌      | 1408/3900 [04:47<08:23,  4.95it/s] 36%|███▌      | 1409/3900 [04:48<08:21,  4.96it/s] 36%|███▌      | 1410/3900 [04:48<08:22,  4.95it/s] 36%|███▌      | 1411/3900 [04:48<08:23,  4.94it/s] 36%|███▌      | 1412/3900 [04:48<08:23,  4.95it/s] 36%|███▌      | 1413/3900 [04:48<08:27,  4.90it/s] 36%|███▋      | 1414/3900 [04:49<08:25,  4.92it/s] 36%|███▋      | 1415/3900 [04:49<08:23,  4.94it/s] 36%|███▋      | 1416/3900 [04:49<08:21,  4.95it/s] 36%|███▋      | 1417/3900 [04:49<08:21,  4.95it/s] 36%|███▋      | 1418/3900 [04:49<08:22,  4.94it/s] 36%|███▋      | 1419/3900 [04:50<08:22,  4.94it/s] 36%|███▋      | 1420/3900 [04:50<08:22,  4.94it/s] 36%|███▋      | 1421/3900 [04:50<08:21,  4.95it/s] 36%|███▋      | 1422/3900 [04:50<08:20,  4.95it/s] 36%|███▋      | 1423/3900 [04:50<08:24,  4.91it/s] 37%|███▋      | 1424/3900 [04:51<08:23,  4.92it/s] 37%|███▋      | 1425/3900 [04:51<08:21,  4.94it/s] 37%|███▋      | 1426/3900 [04:51<08:20,  4.95it/s] 37%|███▋      | 1427/3900 [04:51<08:19,  4.96it/s] 37%|███▋      | 1428/3900 [04:51<08:20,  4.93it/s] 37%|███▋      | 1429/3900 [04:52<08:20,  4.94it/s] 37%|███▋      | 1430/3900 [04:52<08:19,  4.94it/s] 37%|███▋      | 1431/3900 [04:52<08:17,  4.96it/s] 37%|███▋      | 1432/3900 [04:52<08:16,  4.97it/s] 37%|███▋      | 1433/3900 [04:52<08:16,  4.97it/s] 37%|███▋      | 1434/3900 [04:53<08:14,  4.98it/s] 37%|███▋      | 1435/3900 [04:53<08:14,  4.98it/s] 37%|███▋      | 1436/3900 [04:53<08:14,  4.98it/s] 37%|███▋      | 1437/3900 [04:53<08:13,  4.99it/s] 37%|███▋      | 1438/3900 [04:53<08:14,  4.98it/s] 37%|███▋      | 1439/3900 [04:54<08:14,  4.98it/s] 37%|███▋      | 1440/3900 [04:54<08:15,  4.96it/s] 37%|███▋      | 1441/3900 [04:54<08:14,  4.97it/s] 37%|███▋      | 1442/3900 [04:54<08:14,  4.97it/s] 37%|███▋      | 1443/3900 [04:54<08:14,  4.97it/s] 37%|███▋      | 1444/3900 [04:55<08:15,  4.96it/s] 37%|███▋      | 1445/3900 [04:55<08:14,  4.96it/s] 37%|███▋      | 1446/3900 [04:55<08:14,  4.97it/s] 37%|███▋      | 1447/3900 [04:55<08:13,  4.97it/s] 37%|███▋      | 1448/3900 [04:55<08:16,  4.94it/s] 37%|███▋      | 1449/3900 [04:56<08:15,  4.94it/s] 37%|███▋      | 1450/3900 [04:56<08:17,  4.93it/s] 37%|███▋      | 1451/3900 [04:56<08:52,  4.60it/s] 37%|███▋      | 1452/3900 [04:56<08:41,  4.69it/s] 37%|███▋      | 1453/3900 [04:57<08:32,  4.78it/s] 37%|███▋      | 1454/3900 [04:57<08:26,  4.83it/s] 37%|███▋      | 1455/3900 [04:57<08:24,  4.84it/s] 37%|███▋      | 1456/3900 [04:57<08:24,  4.85it/s] 37%|███▋      | 1457/3900 [04:57<08:20,  4.88it/s] 37%|███▋      | 1458/3900 [04:58<08:17,  4.91it/s] 37%|███▋      | 1459/3900 [04:58<08:15,  4.93it/s] 37%|███▋      | 1460/3900 [04:58<08:13,  4.94it/s] 37%|███▋      | 1461/3900 [04:58<08:15,  4.92it/s] 37%|███▋      | 1462/3900 [04:58<08:13,  4.94it/s] 38%|███▊      | 1463/3900 [04:59<08:12,  4.95it/s] 38%|███▊      | 1464/3900 [04:59<08:10,  4.96it/s] 38%|███▊      | 1465/3900 [04:59<08:10,  4.96it/s] 38%|███▊      | 1466/3900 [04:59<08:12,  4.95it/s] 38%|███▊      | 1467/3900 [04:59<08:11,  4.95it/s] 38%|███▊      | 1468/3900 [05:00<08:09,  4.97it/s] 38%|███▊      | 1469/3900 [05:00<08:09,  4.97it/s] 38%|███▊      | 1470/3900 [05:00<08:10,  4.95it/s] 38%|███▊      | 1471/3900 [05:00<08:11,  4.94it/s] 38%|███▊      | 1472/3900 [05:00<08:11,  4.94it/s] 38%|███▊      | 1473/3900 [05:01<08:09,  4.96it/s] 38%|███▊      | 1474/3900 [05:01<08:08,  4.97it/s] 38%|███▊      | 1475/3900 [05:01<08:07,  4.97it/s] 38%|███▊      | 1476/3900 [05:01<08:12,  4.92it/s] 38%|███▊      | 1477/3900 [05:01<08:10,  4.94it/s] 38%|███▊      | 1478/3900 [05:02<08:08,  4.95it/s] 38%|███▊      | 1479/3900 [05:02<08:06,  4.97it/s] 38%|███▊      | 1480/3900 [05:02<08:04,  4.99it/s] 38%|███▊      | 1481/3900 [05:02<08:07,  4.97it/s] 38%|███▊      | 1482/3900 [05:02<08:00,  5.03it/s] 38%|███▊      | 1483/3900 [05:03<08:02,  5.01it/s] 38%|███▊      | 1484/3900 [05:03<08:02,  5.01it/s] 38%|███▊      | 1485/3900 [05:03<08:01,  5.01it/s] 38%|███▊      | 1486/3900 [05:03<08:04,  4.99it/s] 38%|███▊      | 1487/3900 [05:03<08:05,  4.97it/s] 38%|███▊      | 1488/3900 [05:04<08:05,  4.96it/s] 38%|███▊      | 1489/3900 [05:04<08:04,  4.97it/s] 38%|███▊      | 1490/3900 [05:04<08:04,  4.98it/s] 38%|███▊      | 1491/3900 [05:04<08:04,  4.97it/s] 38%|███▊      | 1492/3900 [05:04<08:04,  4.97it/s] 38%|███▊      | 1493/3900 [05:05<08:03,  4.98it/s] 38%|███▊      | 1494/3900 [05:05<08:02,  4.98it/s] 38%|███▊      | 1495/3900 [05:05<08:02,  4.98it/s] 38%|███▊      | 1496/3900 [05:05<08:01,  4.99it/s] 38%|███▊      | 1497/3900 [05:05<08:03,  4.97it/s] 38%|███▊      | 1498/3900 [05:06<08:03,  4.97it/s] 38%|███▊      | 1499/3900 [05:06<08:02,  4.98it/s] 38%|███▊      | 1500/3900 [05:06<08:01,  4.98it/s]                                                    38%|███▊      | 1500/3900 [05:06<08:01,  4.98it/s][INFO|trainer.py:2409] 2022-08-27 22:12:51,261 >> Saving model checkpoint to tmp/baseline/RTE/bert-base-uncased/HPO_S0/1e-06/64/100/checkpoint-1500
[INFO|configuration_utils.py:446] 2022-08-27 22:12:51,262 >> Configuration saved in tmp/baseline/RTE/bert-base-uncased/HPO_S0/1e-06/64/100/checkpoint-1500/config.json
[INFO|modeling_utils.py:1602] 2022-08-27 22:12:52,173 >> Model weights saved in tmp/baseline/RTE/bert-base-uncased/HPO_S0/1e-06/64/100/checkpoint-1500/pytorch_model.bin
[INFO|tokenization_utils_base.py:2108] 2022-08-27 22:12:52,173 >> tokenizer config file saved in tmp/baseline/RTE/bert-base-uncased/HPO_S0/1e-06/64/100/checkpoint-1500/tokenizer_config.json
[INFO|tokenization_utils_base.py:2114] 2022-08-27 22:12:52,173 >> Special tokens file saved in tmp/baseline/RTE/bert-base-uncased/HPO_S0/1e-06/64/100/checkpoint-1500/special_tokens_map.json
/home/ubuntu/anaconda3/envs/transformers/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 38%|███▊      | 1501/3900 [05:09<41:18,  1.03s/it] 39%|███▊      | 1502/3900 [05:09<31:18,  1.28it/s] 39%|███▊      | 1503/3900 [05:09<24:18,  1.64it/s] 39%|███▊      | 1504/3900 [05:10<19:27,  2.05it/s] 39%|███▊      | 1505/3900 [05:10<16:03,  2.49it/s] 39%|███▊      | 1506/3900 [05:10<13:40,  2.92it/s] 39%|███▊      | 1507/3900 [05:10<11:57,  3.33it/s] 39%|███▊      | 1508/3900 [05:10<10:45,  3.71it/s] 39%|███▊      | 1509/3900 [05:11<09:55,  4.01it/s] 39%|███▊      | 1510/3900 [05:11<09:19,  4.27it/s] 39%|███▊      | 1511/3900 [05:11<08:55,  4.46it/s] 39%|███▉      | 1512/3900 [05:11<08:39,  4.59it/s] 39%|███▉      | 1513/3900 [05:11<08:27,  4.70it/s] 39%|███▉      | 1514/3900 [05:12<08:20,  4.77it/s] 39%|███▉      | 1515/3900 [05:12<08:16,  4.80it/s] 39%|███▉      | 1516/3900 [05:12<08:10,  4.87it/s] 39%|███▉      | 1517/3900 [05:12<08:05,  4.91it/s] 39%|███▉      | 1518/3900 [05:12<08:02,  4.93it/s] 39%|███▉      | 1519/3900 [05:13<08:00,  4.96it/s] 39%|███▉      | 1520/3900 [05:13<08:01,  4.94it/s] 39%|███▉      | 1521/3900 [05:13<07:55,  5.01it/s] 39%|███▉      | 1522/3900 [05:13<07:55,  5.00it/s] 39%|███▉      | 1523/3900 [05:13<07:55,  4.99it/s] 39%|███▉      | 1524/3900 [05:14<07:55,  5.00it/s] 39%|███▉      | 1525/3900 [05:14<08:00,  4.95it/s] 39%|███▉      | 1526/3900 [05:14<07:59,  4.95it/s] 39%|███▉      | 1527/3900 [05:14<07:58,  4.96it/s] 39%|███▉      | 1528/3900 [05:14<07:57,  4.97it/s] 39%|███▉      | 1529/3900 [05:15<07:56,  4.97it/s] 39%|███▉      | 1530/3900 [05:15<07:56,  4.97it/s] 39%|███▉      | 1531/3900 [05:15<07:55,  4.98it/s] 39%|███▉      | 1532/3900 [05:15<07:55,  4.98it/s] 39%|███▉      | 1533/3900 [05:15<07:55,  4.98it/s] 39%|███▉      | 1534/3900 [05:16<07:53,  4.99it/s] 39%|███▉      | 1535/3900 [05:16<07:54,  4.98it/s] 39%|███▉      | 1536/3900 [05:16<07:54,  4.98it/s] 39%|███▉      | 1537/3900 [05:16<07:53,  4.99it/s] 39%|███▉      | 1538/3900 [05:16<07:52,  5.00it/s] 39%|███▉      | 1539/3900 [05:17<07:52,  5.00it/s] 39%|███▉      | 1540/3900 [05:17<07:57,  4.94it/s] 40%|███▉      | 1541/3900 [05:17<07:57,  4.94it/s] 40%|███▉      | 1542/3900 [05:17<07:56,  4.95it/s] 40%|███▉      | 1543/3900 [05:17<07:56,  4.95it/s] 40%|███▉      | 1544/3900 [05:18<07:54,  4.96it/s] 40%|███▉      | 1545/3900 [05:18<07:57,  4.93it/s] 40%|███▉      | 1546/3900 [05:18<07:55,  4.95it/s] 40%|███▉      | 1547/3900 [05:18<07:53,  4.97it/s] 40%|███▉      | 1548/3900 [05:18<07:52,  4.98it/s] 40%|███▉      | 1549/3900 [05:19<07:51,  4.99it/s] 40%|███▉      | 1550/3900 [05:19<07:52,  4.98it/s] 40%|███▉      | 1551/3900 [05:19<07:53,  4.96it/s] 40%|███▉      | 1552/3900 [05:19<07:52,  4.97it/s] 40%|███▉      | 1553/3900 [05:19<07:52,  4.96it/s] 40%|███▉      | 1554/3900 [05:20<07:53,  4.95it/s] 40%|███▉      | 1555/3900 [05:20<07:53,  4.95it/s] 40%|███▉      | 1556/3900 [05:20<07:53,  4.95it/s] 40%|███▉      | 1557/3900 [05:20<07:53,  4.95it/s] 40%|███▉      | 1558/3900 [05:20<07:51,  4.97it/s] 40%|███▉      | 1559/3900 [05:21<07:50,  4.97it/s] 40%|████      | 1560/3900 [05:21<07:47,  5.01it/s] 40%|████      | 1561/3900 [05:21<07:51,  4.96it/s] 40%|████      | 1562/3900 [05:21<07:50,  4.97it/s] 40%|████      | 1563/3900 [05:21<07:49,  4.98it/s] 40%|████      | 1564/3900 [05:22<07:49,  4.98it/s] 40%|████      | 1565/3900 [05:22<07:49,  4.97it/s] 40%|████      | 1566/3900 [05:22<07:51,  4.96it/s] 40%|████      | 1567/3900 [05:22<07:49,  4.97it/s] 40%|████      | 1568/3900 [05:22<07:49,  4.96it/s] 40%|████      | 1569/3900 [05:23<07:51,  4.94it/s] 40%|████      | 1570/3900 [05:23<07:54,  4.91it/s] 40%|████      | 1571/3900 [05:23<07:55,  4.90it/s] 40%|████      | 1572/3900 [05:23<07:55,  4.90it/s] 40%|████      | 1573/3900 [05:23<07:53,  4.92it/s] 40%|████      | 1574/3900 [05:24<07:51,  4.94it/s] 40%|████      | 1575/3900 [05:24<07:53,  4.91it/s] 40%|████      | 1576/3900 [05:24<07:50,  4.93it/s] 40%|████      | 1577/3900 [05:24<07:50,  4.94it/s] 40%|████      | 1578/3900 [05:25<07:48,  4.95it/s] 40%|████      | 1579/3900 [05:25<07:47,  4.97it/s] 41%|████      | 1580/3900 [05:25<07:48,  4.95it/s] 41%|████      | 1581/3900 [05:25<07:46,  4.97it/s] 41%|████      | 1582/3900 [05:25<07:44,  4.99it/s] 41%|████      | 1583/3900 [05:26<07:44,  4.99it/s] 41%|████      | 1584/3900 [05:26<07:46,  4.97it/s] 41%|████      | 1585/3900 [05:26<07:47,  4.95it/s] 41%|████      | 1586/3900 [05:26<07:47,  4.95it/s] 41%|████      | 1587/3900 [05:26<07:45,  4.96it/s] 41%|████      | 1588/3900 [05:27<07:45,  4.96it/s] 41%|████      | 1589/3900 [05:27<07:45,  4.96it/s] 41%|████      | 1590/3900 [05:27<07:44,  4.97it/s] 41%|████      | 1591/3900 [05:27<07:44,  4.98it/s] 41%|████      | 1592/3900 [05:27<08:45,  4.39it/s] 41%|████      | 1593/3900 [05:28<08:30,  4.52it/s] 41%|████      | 1594/3900 [05:28<08:17,  4.63it/s] 41%|████      | 1595/3900 [05:28<08:07,  4.72it/s] 41%|████      | 1596/3900 [05:28<07:59,  4.81it/s] 41%|████      | 1597/3900 [05:28<07:53,  4.87it/s] 41%|████      | 1598/3900 [05:29<07:53,  4.87it/s] 41%|████      | 1599/3900 [05:29<07:45,  4.95it/s] 41%|████      | 1600/3900 [05:29<07:44,  4.95it/s] 41%|████      | 1601/3900 [05:29<07:43,  4.96it/s] 41%|████      | 1602/3900 [05:29<07:42,  4.97it/s] 41%|████      | 1603/3900 [05:30<07:43,  4.95it/s] 41%|████      | 1604/3900 [05:30<07:43,  4.95it/s] 41%|████      | 1605/3900 [05:30<07:42,  4.97it/s] 41%|████      | 1606/3900 [05:30<07:42,  4.96it/s] 41%|████      | 1607/3900 [05:30<07:40,  4.98it/s] 41%|████      | 1608/3900 [05:31<07:39,  4.98it/s] 41%|████▏     | 1609/3900 [05:31<07:38,  4.99it/s] 41%|████▏     | 1610/3900 [05:31<07:39,  4.99it/s] 41%|████▏     | 1611/3900 [05:31<07:38,  4.99it/s] 41%|████▏     | 1612/3900 [05:31<07:38,  4.99it/s] 41%|████▏     | 1613/3900 [05:32<07:38,  4.99it/s] 41%|████▏     | 1614/3900 [05:32<07:43,  4.94it/s] 41%|████▏     | 1615/3900 [05:32<07:41,  4.95it/s] 41%|████▏     | 1616/3900 [05:32<07:41,  4.95it/s] 41%|████▏     | 1617/3900 [05:32<07:40,  4.96it/s] 41%|████▏     | 1618/3900 [05:33<07:39,  4.96it/s] 42%|████▏     | 1619/3900 [05:33<07:45,  4.90it/s] 42%|████▏     | 1620/3900 [05:33<07:45,  4.90it/s] 42%|████▏     | 1621/3900 [05:33<07:42,  4.93it/s] 42%|████▏     | 1622/3900 [05:33<07:42,  4.92it/s] 42%|████▏     | 1623/3900 [05:34<07:41,  4.93it/s] 42%|████▏     | 1624/3900 [05:34<07:43,  4.91it/s] 42%|████▏     | 1625/3900 [05:34<07:41,  4.93it/s] 42%|████▏     | 1626/3900 [05:34<07:39,  4.94it/s] 42%|████▏     | 1627/3900 [05:34<07:39,  4.95it/s] 42%|████▏     | 1628/3900 [05:35<07:40,  4.93it/s] 42%|████▏     | 1629/3900 [05:35<07:42,  4.91it/s] 42%|████▏     | 1630/3900 [05:35<07:41,  4.92it/s] 42%|████▏     | 1631/3900 [05:35<07:42,  4.91it/s] 42%|████▏     | 1632/3900 [05:35<07:39,  4.93it/s] 42%|████▏     | 1633/3900 [05:36<07:41,  4.91it/s] 42%|████▏     | 1634/3900 [05:36<07:42,  4.90it/s] 42%|████▏     | 1635/3900 [05:36<07:41,  4.91it/s] 42%|████▏     | 1636/3900 [05:36<07:40,  4.92it/s] 42%|████▏     | 1637/3900 [05:37<07:39,  4.93it/s] 42%|████▏     | 1638/3900 [05:37<07:33,  4.99it/s] 42%|████▏     | 1639/3900 [05:37<07:37,  4.95it/s] 42%|████▏     | 1640/3900 [05:37<07:37,  4.94it/s] 42%|████▏     | 1641/3900 [05:37<07:36,  4.95it/s] 42%|████▏     | 1642/3900 [05:38<07:35,  4.95it/s] 42%|████▏     | 1643/3900 [05:38<07:36,  4.94it/s] 42%|████▏     | 1644/3900 [05:38<07:38,  4.92it/s] 42%|████▏     | 1645/3900 [05:38<07:38,  4.91it/s] 42%|████▏     | 1646/3900 [05:38<07:37,  4.93it/s] 42%|████▏     | 1647/3900 [05:39<07:37,  4.93it/s] 42%|████▏     | 1648/3900 [05:39<07:38,  4.91it/s] 42%|████▏     | 1649/3900 [05:39<07:37,  4.92it/s] 42%|████▏     | 1650/3900 [05:39<07:35,  4.94it/s] 42%|████▏     | 1651/3900 [05:39<07:35,  4.93it/s] 42%|████▏     | 1652/3900 [05:40<07:34,  4.94it/s] 42%|████▏     | 1653/3900 [05:40<07:32,  4.96it/s] 42%|████▏     | 1654/3900 [05:40<07:37,  4.91it/s] 42%|████▏     | 1655/3900 [05:40<07:35,  4.93it/s] 42%|████▏     | 1656/3900 [05:40<08:07,  4.61it/s] 42%|████▏     | 1657/3900 [05:41<07:58,  4.68it/s] 43%|████▎     | 1658/3900 [05:41<07:51,  4.76it/s] 43%|████▎     | 1659/3900 [05:41<07:45,  4.81it/s] 43%|████▎     | 1660/3900 [05:41<07:41,  4.85it/s] 43%|████▎     | 1661/3900 [05:41<07:38,  4.89it/s] 43%|████▎     | 1662/3900 [05:42<07:35,  4.91it/s] 43%|████▎     | 1663/3900 [05:42<07:35,  4.91it/s] 43%|████▎     | 1664/3900 [05:42<07:34,  4.92it/s] 43%|████▎     | 1665/3900 [05:42<07:32,  4.94it/s] 43%|████▎     | 1666/3900 [05:42<07:30,  4.96it/s] 43%|████▎     | 1667/3900 [05:43<07:29,  4.97it/s] 43%|████▎     | 1668/3900 [05:43<07:29,  4.96it/s] 43%|████▎     | 1669/3900 [05:43<07:28,  4.98it/s] 43%|████▎     | 1670/3900 [05:43<07:27,  4.98it/s] 43%|████▎     | 1671/3900 [05:43<07:27,  4.98it/s] 43%|████▎     | 1672/3900 [05:44<07:26,  4.99it/s] 43%|████▎     | 1673/3900 [05:44<07:27,  4.98it/s] 43%|████▎     | 1674/3900 [05:44<07:26,  4.98it/s] 43%|████▎     | 1675/3900 [05:44<07:27,  4.98it/s] 43%|████▎     | 1676/3900 [05:44<07:28,  4.96it/s] 43%|████▎     | 1677/3900 [05:45<07:23,  5.01it/s] 43%|████▎     | 1678/3900 [05:45<07:26,  4.98it/s] 43%|████▎     | 1679/3900 [05:45<07:26,  4.98it/s] 43%|████▎     | 1680/3900 [05:45<07:27,  4.96it/s] 43%|████▎     | 1681/3900 [05:45<07:25,  4.98it/s] 43%|████▎     | 1682/3900 [05:46<07:25,  4.98it/s] 43%|████▎     | 1683/3900 [05:46<07:25,  4.97it/s] 43%|████▎     | 1684/3900 [05:46<07:27,  4.95it/s] 43%|████▎     | 1685/3900 [05:46<07:26,  4.96it/s] 43%|████▎     | 1686/3900 [05:46<07:25,  4.98it/s] 43%|████▎     | 1687/3900 [05:47<07:23,  4.99it/s] 43%|████▎     | 1688/3900 [05:47<07:26,  4.96it/s] 43%|████▎     | 1689/3900 [05:47<07:26,  4.95it/s] 43%|████▎     | 1690/3900 [05:47<07:26,  4.95it/s] 43%|████▎     | 1691/3900 [05:47<07:25,  4.96it/s] 43%|████▎     | 1692/3900 [05:48<07:23,  4.98it/s] 43%|████▎     | 1693/3900 [05:48<07:24,  4.97it/s] 43%|████▎     | 1694/3900 [05:48<07:27,  4.93it/s] 43%|████▎     | 1695/3900 [05:48<07:27,  4.93it/s] 43%|████▎     | 1696/3900 [05:48<07:25,  4.95it/s] 44%|████▎     | 1697/3900 [05:49<07:23,  4.97it/s] 44%|████▎     | 1698/3900 [05:49<07:22,  4.98it/s] 44%|████▎     | 1699/3900 [05:49<07:26,  4.92it/s] 44%|████▎     | 1700/3900 [05:49<07:24,  4.94it/s] 44%|████▎     | 1701/3900 [05:49<07:22,  4.96it/s] 44%|████▎     | 1702/3900 [05:50<07:22,  4.97it/s] 44%|████▎     | 1703/3900 [05:50<07:21,  4.97it/s] 44%|████▎     | 1704/3900 [05:50<07:20,  4.99it/s] 44%|████▎     | 1705/3900 [05:50<07:21,  4.97it/s] 44%|████▎     | 1706/3900 [05:50<07:21,  4.97it/s] 44%|████▍     | 1707/3900 [05:51<07:21,  4.97it/s] 44%|████▍     | 1708/3900 [05:51<07:20,  4.98it/s] 44%|████▍     | 1709/3900 [05:51<07:21,  4.96it/s] 44%|████▍     | 1710/3900 [05:51<07:23,  4.94it/s] 44%|████▍     | 1711/3900 [05:51<07:23,  4.94it/s] 44%|████▍     | 1712/3900 [05:52<07:20,  4.96it/s] 44%|████▍     | 1713/3900 [05:52<07:19,  4.98it/s] 44%|████▍     | 1714/3900 [05:52<07:21,  4.95it/s] 44%|████▍     | 1715/3900 [05:52<07:20,  4.96it/s] 44%|████▍     | 1716/3900 [05:52<07:14,  5.02it/s] 44%|████▍     | 1717/3900 [05:53<07:15,  5.01it/s] 44%|████▍     | 1718/3900 [05:53<07:18,  4.98it/s] 44%|████▍     | 1719/3900 [05:53<07:23,  4.92it/s] 44%|████▍     | 1720/3900 [05:53<07:21,  4.94it/s] 44%|████▍     | 1721/3900 [05:53<07:19,  4.96it/s] 44%|████▍     | 1722/3900 [05:54<07:19,  4.95it/s] 44%|████▍     | 1723/3900 [05:54<07:19,  4.96it/s] 44%|████▍     | 1724/3900 [05:54<07:22,  4.92it/s] 44%|████▍     | 1725/3900 [05:54<07:20,  4.94it/s] 44%|████▍     | 1726/3900 [05:55<07:21,  4.93it/s] 44%|████▍     | 1727/3900 [05:55<07:20,  4.94it/s] 44%|████▍     | 1728/3900 [05:55<07:18,  4.95it/s] 44%|████▍     | 1729/3900 [05:55<07:19,  4.94it/s] 44%|████▍     | 1730/3900 [05:55<07:18,  4.95it/s] 44%|████▍     | 1731/3900 [05:56<07:16,  4.97it/s] 44%|████▍     | 1732/3900 [05:56<07:14,  4.99it/s] 44%|████▍     | 1733/3900 [05:56<07:14,  4.99it/s] 44%|████▍     | 1734/3900 [05:56<07:17,  4.95it/s] 44%|████▍     | 1735/3900 [05:56<07:17,  4.95it/s] 45%|████▍     | 1736/3900 [05:57<07:14,  4.98it/s] 45%|████▍     | 1737/3900 [05:57<07:13,  4.99it/s] 45%|████▍     | 1738/3900 [05:57<07:15,  4.96it/s] 45%|████▍     | 1739/3900 [05:57<07:17,  4.94it/s] 45%|████▍     | 1740/3900 [05:57<07:16,  4.94it/s] 45%|████▍     | 1741/3900 [05:58<07:14,  4.97it/s] 45%|████▍     | 1742/3900 [05:58<07:14,  4.96it/s] 45%|████▍     | 1743/3900 [05:58<07:13,  4.98it/s] 45%|████▍     | 1744/3900 [05:58<07:18,  4.92it/s] 45%|████▍     | 1745/3900 [05:58<07:17,  4.93it/s] 45%|████▍     | 1746/3900 [05:59<07:16,  4.94it/s] 45%|████▍     | 1747/3900 [05:59<07:14,  4.95it/s] 45%|████▍     | 1748/3900 [05:59<07:12,  4.97it/s] 45%|████▍     | 1749/3900 [05:59<07:16,  4.93it/s] 45%|████▍     | 1750/3900 [05:59<07:15,  4.93it/s] 45%|████▍     | 1751/3900 [06:00<07:14,  4.95it/s] 45%|████▍     | 1752/3900 [06:00<07:13,  4.96it/s] 45%|████▍     | 1753/3900 [06:00<07:13,  4.96it/s] 45%|████▍     | 1754/3900 [06:00<07:14,  4.94it/s] 45%|████▌     | 1755/3900 [06:00<07:07,  5.02it/s] 45%|████▌     | 1756/3900 [06:01<07:09,  5.00it/s] 45%|████▌     | 1757/3900 [06:01<07:09,  4.99it/s] 45%|████▌     | 1758/3900 [06:01<07:09,  4.98it/s] 45%|████▌     | 1759/3900 [06:01<07:11,  4.96it/s] 45%|████▌     | 1760/3900 [06:01<07:10,  4.97it/s] 45%|████▌     | 1761/3900 [06:02<07:10,  4.97it/s] 45%|████▌     | 1762/3900 [06:02<07:10,  4.97it/s] 45%|████▌     | 1763/3900 [06:02<07:10,  4.97it/s] 45%|████▌     | 1764/3900 [06:02<07:10,  4.96it/s] 45%|████▌     | 1765/3900 [06:02<07:09,  4.97it/s] 45%|████▌     | 1766/3900 [06:03<07:09,  4.97it/s] 45%|████▌     | 1767/3900 [06:03<07:08,  4.98it/s] 45%|████▌     | 1768/3900 [06:03<07:10,  4.96it/s] 45%|████▌     | 1769/3900 [06:03<07:10,  4.94it/s] 45%|████▌     | 1770/3900 [06:03<07:09,  4.96it/s] 45%|████▌     | 1771/3900 [06:04<07:08,  4.97it/s] 45%|████▌     | 1772/3900 [06:04<07:06,  4.98it/s] 45%|████▌     | 1773/3900 [06:04<07:06,  4.99it/s] 45%|████▌     | 1774/3900 [06:04<07:07,  4.97it/s] 46%|████▌     | 1775/3900 [06:04<07:07,  4.97it/s] 46%|████▌     | 1776/3900 [06:05<07:06,  4.98it/s] 46%|████▌     | 1777/3900 [06:05<07:07,  4.97it/s] 46%|████▌     | 1778/3900 [06:05<07:06,  4.97it/s] 46%|████▌     | 1779/3900 [06:05<07:07,  4.96it/s] 46%|████▌     | 1780/3900 [06:05<07:07,  4.96it/s] 46%|████▌     | 1781/3900 [06:06<07:06,  4.97it/s] 46%|████▌     | 1782/3900 [06:06<07:06,  4.97it/s] 46%|████▌     | 1783/3900 [06:06<07:05,  4.98it/s] 46%|████▌     | 1784/3900 [06:06<07:07,  4.94it/s] 46%|████▌     | 1785/3900 [06:06<07:08,  4.94it/s] 46%|████▌     | 1786/3900 [06:07<07:06,  4.95it/s] 46%|████▌     | 1787/3900 [06:07<07:05,  4.97it/s] 46%|████▌     | 1788/3900 [06:07<07:05,  4.96it/s] 46%|████▌     | 1789/3900 [06:07<07:06,  4.95it/s] 46%|████▌     | 1790/3900 [06:07<07:07,  4.94it/s] 46%|████▌     | 1791/3900 [06:08<07:05,  4.96it/s] 46%|████▌     | 1792/3900 [06:08<07:05,  4.96it/s] 46%|████▌     | 1793/3900 [06:08<07:04,  4.96it/s] 46%|████▌     | 1794/3900 [06:08<07:00,  5.01it/s] 46%|████▌     | 1795/3900 [06:08<07:02,  4.98it/s] 46%|████▌     | 1796/3900 [06:09<07:00,  5.00it/s] 46%|████▌     | 1797/3900 [06:09<06:59,  5.01it/s] 46%|████▌     | 1798/3900 [06:09<07:00,  5.00it/s] 46%|████▌     | 1799/3900 [06:09<07:02,  4.97it/s] 46%|████▌     | 1800/3900 [06:09<07:04,  4.94it/s] 46%|████▌     | 1801/3900 [06:10<07:03,  4.96it/s] 46%|████▌     | 1802/3900 [06:10<07:03,  4.96it/s] 46%|████▌     | 1803/3900 [06:10<07:01,  4.97it/s] 46%|████▋     | 1804/3900 [06:10<07:01,  4.97it/s] 46%|████▋     | 1805/3900 [06:10<07:03,  4.95it/s] 46%|████▋     | 1806/3900 [06:11<07:02,  4.95it/s] 46%|████▋     | 1807/3900 [06:11<07:01,  4.96it/s] 46%|████▋     | 1808/3900 [06:11<06:59,  4.98it/s] 46%|████▋     | 1809/3900 [06:11<06:59,  4.99it/s] 46%|████▋     | 1810/3900 [06:11<07:01,  4.96it/s] 46%|████▋     | 1811/3900 [06:12<07:01,  4.96it/s] 46%|████▋     | 1812/3900 [06:12<06:58,  4.99it/s] 46%|████▋     | 1813/3900 [06:12<06:58,  4.99it/s] 47%|████▋     | 1814/3900 [06:12<06:57,  5.00it/s] 47%|████▋     | 1815/3900 [06:12<07:00,  4.95it/s] 47%|████▋     | 1816/3900 [06:13<06:59,  4.96it/s] 47%|████▋     | 1817/3900 [06:13<06:59,  4.97it/s] 47%|████▋     | 1818/3900 [06:13<07:00,  4.95it/s] 47%|████▋     | 1819/3900 [06:13<07:01,  4.94it/s] 47%|████▋     | 1820/3900 [06:13<07:00,  4.95it/s] 47%|████▋     | 1821/3900 [06:14<07:00,  4.95it/s] 47%|████▋     | 1822/3900 [06:14<07:00,  4.94it/s] 47%|████▋     | 1823/3900 [06:14<06:58,  4.96it/s] 47%|████▋     | 1824/3900 [06:14<06:58,  4.96it/s] 47%|████▋     | 1825/3900 [06:14<07:00,  4.93it/s] 47%|████▋     | 1826/3900 [06:15<06:58,  4.96it/s] 47%|████▋     | 1827/3900 [06:15<06:57,  4.97it/s] 47%|████▋     | 1828/3900 [06:15<06:57,  4.96it/s] 47%|████▋     | 1829/3900 [06:15<06:56,  4.97it/s] 47%|████▋     | 1830/3900 [06:15<06:57,  4.96it/s] 47%|████▋     | 1831/3900 [06:16<06:57,  4.95it/s] 47%|████▋     | 1832/3900 [06:16<06:55,  4.98it/s] 47%|████▋     | 1833/3900 [06:16<06:51,  5.02it/s] 47%|████▋     | 1834/3900 [06:16<06:54,  4.98it/s] 47%|████▋     | 1835/3900 [06:16<06:58,  4.93it/s] 47%|████▋     | 1836/3900 [06:17<06:56,  4.95it/s] 47%|████▋     | 1837/3900 [06:17<06:57,  4.94it/s] 47%|████▋     | 1838/3900 [06:17<06:57,  4.94it/s] 47%|████▋     | 1839/3900 [06:17<06:56,  4.95it/s] 47%|████▋     | 1840/3900 [06:17<06:58,  4.92it/s] 47%|████▋     | 1841/3900 [06:18<07:46,  4.41it/s] 47%|████▋     | 1842/3900 [06:18<07:32,  4.55it/s] 47%|████▋     | 1843/3900 [06:18<07:20,  4.67it/s] 47%|████▋     | 1844/3900 [06:18<07:12,  4.75it/s] 47%|████▋     | 1845/3900 [06:19<07:06,  4.81it/s] 47%|████▋     | 1846/3900 [06:19<07:02,  4.87it/s] 47%|████▋     | 1847/3900 [06:19<06:59,  4.89it/s] 47%|████▋     | 1848/3900 [06:19<06:56,  4.92it/s] 47%|████▋     | 1849/3900 [06:19<06:57,  4.92it/s] 47%|████▋     | 1850/3900 [06:20<06:56,  4.93it/s] 47%|████▋     | 1851/3900 [06:20<06:53,  4.95it/s] 47%|████▋     | 1852/3900 [06:20<06:57,  4.91it/s] 48%|████▊     | 1853/3900 [06:20<06:55,  4.93it/s] 48%|████▊     | 1854/3900 [06:20<06:53,  4.95it/s] 48%|████▊     | 1855/3900 [06:21<06:51,  4.97it/s] 48%|████▊     | 1856/3900 [06:21<06:52,  4.96it/s] 48%|████▊     | 1857/3900 [06:21<06:54,  4.93it/s] 48%|████▊     | 1858/3900 [06:21<06:53,  4.94it/s] 48%|████▊     | 1859/3900 [06:21<06:51,  4.96it/s] 48%|████▊     | 1860/3900 [06:22<06:50,  4.97it/s] 48%|████▊     | 1861/3900 [06:22<06:48,  4.99it/s] 48%|████▊     | 1862/3900 [06:22<06:50,  4.96it/s] 48%|████▊     | 1863/3900 [06:22<06:50,  4.96it/s] 48%|████▊     | 1864/3900 [06:22<06:49,  4.97it/s] 48%|████▊     | 1865/3900 [06:23<06:49,  4.97it/s] 48%|████▊     | 1866/3900 [06:23<06:48,  4.98it/s] 48%|████▊     | 1867/3900 [06:23<06:48,  4.97it/s] 48%|████▊     | 1868/3900 [06:23<06:48,  4.97it/s] 48%|████▊     | 1869/3900 [06:23<06:48,  4.97it/s] 48%|████▊     | 1870/3900 [06:24<06:47,  4.98it/s] 48%|████▊     | 1871/3900 [06:24<06:48,  4.97it/s] 48%|████▊     | 1872/3900 [06:24<06:45,  5.00it/s] 48%|████▊     | 1873/3900 [06:24<06:48,  4.96it/s] 48%|████▊     | 1874/3900 [06:24<06:46,  4.98it/s] 48%|████▊     | 1875/3900 [06:25<06:46,  4.99it/s] 48%|████▊     | 1876/3900 [06:25<06:46,  4.98it/s] 48%|████▊     | 1877/3900 [06:25<06:46,  4.98it/s] 48%|████▊     | 1878/3900 [06:25<06:46,  4.98it/s] 48%|████▊     | 1879/3900 [06:25<06:45,  4.98it/s] 48%|████▊     | 1880/3900 [06:26<06:45,  4.98it/s] 48%|████▊     | 1881/3900 [06:26<06:45,  4.98it/s] 48%|████▊     | 1882/3900 [06:26<06:45,  4.97it/s] 48%|████▊     | 1883/3900 [06:26<06:45,  4.97it/s] 48%|████▊     | 1884/3900 [06:26<06:45,  4.97it/s] 48%|████▊     | 1885/3900 [06:27<06:47,  4.94it/s] 48%|████▊     | 1886/3900 [06:27<06:50,  4.91it/s] 48%|████▊     | 1887/3900 [06:27<06:49,  4.92it/s] 48%|████▊     | 1888/3900 [06:27<06:48,  4.92it/s] 48%|████▊     | 1889/3900 [06:27<06:48,  4.93it/s] 48%|████▊     | 1890/3900 [06:28<06:47,  4.94it/s] 48%|████▊     | 1891/3900 [06:28<06:46,  4.95it/s] 49%|████▊     | 1892/3900 [06:28<06:48,  4.91it/s] 49%|████▊     | 1893/3900 [06:28<06:46,  4.94it/s] 49%|████▊     | 1894/3900 [06:28<06:44,  4.96it/s] 49%|████▊     | 1895/3900 [06:29<06:43,  4.97it/s] 49%|████▊     | 1896/3900 [06:29<06:43,  4.96it/s] 49%|████▊     | 1897/3900 [06:29<06:46,  4.93it/s] 49%|████▊     | 1898/3900 [06:29<06:45,  4.94it/s] 49%|████▊     | 1899/3900 [06:29<06:43,  4.96it/s] 49%|████▊     | 1900/3900 [06:30<06:43,  4.96it/s] 49%|████▊     | 1901/3900 [06:30<06:42,  4.97it/s] 49%|████▉     | 1902/3900 [06:30<06:45,  4.93it/s] 49%|████▉     | 1903/3900 [06:30<06:44,  4.94it/s] 49%|████▉     | 1904/3900 [06:30<06:45,  4.92it/s] 49%|████▉     | 1905/3900 [06:31<07:15,  4.58it/s] 49%|████▉     | 1906/3900 [06:31<07:07,  4.66it/s] 49%|████▉     | 1907/3900 [06:31<07:01,  4.73it/s] 49%|████▉     | 1908/3900 [06:31<06:56,  4.79it/s] 49%|████▉     | 1909/3900 [06:32<06:52,  4.82it/s] 49%|████▉     | 1910/3900 [06:32<06:51,  4.84it/s] 49%|████▉     | 1911/3900 [06:32<06:44,  4.91it/s] 49%|████▉     | 1912/3900 [06:32<06:44,  4.91it/s] 49%|████▉     | 1913/3900 [06:32<06:43,  4.93it/s] 49%|████▉     | 1914/3900 [06:33<06:42,  4.94it/s] 49%|████▉     | 1915/3900 [06:33<06:43,  4.92it/s] 49%|████▉     | 1916/3900 [06:33<06:42,  4.93it/s] 49%|████▉     | 1917/3900 [06:33<06:42,  4.93it/s] 49%|████▉     | 1918/3900 [06:33<06:40,  4.95it/s] 49%|████▉     | 1919/3900 [06:34<06:39,  4.96it/s] 49%|████▉     | 1920/3900 [06:34<06:40,  4.94it/s] 49%|████▉     | 1921/3900 [06:34<06:39,  4.95it/s] 49%|████▉     | 1922/3900 [06:34<06:40,  4.94it/s] 49%|████▉     | 1923/3900 [06:34<06:40,  4.94it/s] 49%|████▉     | 1924/3900 [06:35<06:38,  4.95it/s] 49%|████▉     | 1925/3900 [06:35<06:38,  4.96it/s] 49%|████▉     | 1926/3900 [06:35<06:36,  4.98it/s] 49%|████▉     | 1927/3900 [06:35<06:35,  4.99it/s] 49%|████▉     | 1928/3900 [06:35<06:35,  4.99it/s] 49%|████▉     | 1929/3900 [06:36<06:37,  4.96it/s] 49%|████▉     | 1930/3900 [06:36<06:37,  4.95it/s] 50%|████▉     | 1931/3900 [06:36<06:37,  4.96it/s] 50%|████▉     | 1932/3900 [06:36<06:36,  4.97it/s] 50%|████▉     | 1933/3900 [06:36<06:36,  4.97it/s] 50%|████▉     | 1934/3900 [06:37<06:35,  4.97it/s] 50%|████▉     | 1935/3900 [06:37<06:38,  4.94it/s] 50%|████▉     | 1936/3900 [06:37<06:39,  4.92it/s] 50%|████▉     | 1937/3900 [06:37<06:38,  4.92it/s] 50%|████▉     | 1938/3900 [06:37<06:38,  4.92it/s] 50%|████▉     | 1939/3900 [06:38<06:37,  4.93it/s] 50%|████▉     | 1940/3900 [06:38<06:39,  4.91it/s] 50%|████▉     | 1941/3900 [06:38<06:39,  4.91it/s] 50%|████▉     | 1942/3900 [06:38<06:41,  4.88it/s] 50%|████▉     | 1943/3900 [06:38<06:39,  4.90it/s] 50%|████▉     | 1944/3900 [06:39<06:38,  4.91it/s] 50%|████▉     | 1945/3900 [06:39<06:40,  4.88it/s] 50%|████▉     | 1946/3900 [06:39<06:38,  4.90it/s] 50%|████▉     | 1947/3900 [06:39<06:38,  4.91it/s] 50%|████▉     | 1948/3900 [06:39<06:38,  4.90it/s] 50%|████▉     | 1949/3900 [06:40<06:38,  4.90it/s] 50%|█████     | 1950/3900 [06:40<06:33,  4.95it/s] 50%|█████     | 1951/3900 [06:40<06:35,  4.92it/s] 50%|█████     | 1952/3900 [06:40<06:38,  4.89it/s] 50%|█████     | 1953/3900 [06:40<06:38,  4.89it/s] 50%|█████     | 1954/3900 [06:41<06:40,  4.86it/s] 50%|█████     | 1955/3900 [06:41<06:38,  4.88it/s] 50%|█████     | 1956/3900 [06:41<06:36,  4.90it/s] 50%|█████     | 1957/3900 [06:41<06:35,  4.91it/s] 50%|█████     | 1958/3900 [06:41<06:35,  4.91it/s] 50%|█████     | 1959/3900 [06:42<06:37,  4.88it/s] 50%|█████     | 1960/3900 [06:42<06:35,  4.90it/s] 50%|█████     | 1961/3900 [06:42<06:35,  4.90it/s] 50%|█████     | 1962/3900 [06:42<06:35,  4.90it/s] 50%|█████     | 1963/3900 [06:43<06:36,  4.89it/s] 50%|█████     | 1964/3900 [06:43<06:37,  4.87it/s] 50%|█████     | 1965/3900 [06:43<06:35,  4.89it/s] 50%|█████     | 1966/3900 [06:43<06:36,  4.88it/s] 50%|█████     | 1967/3900 [06:43<06:35,  4.89it/s] 50%|█████     | 1968/3900 [06:44<06:34,  4.90it/s] 50%|█████     | 1969/3900 [06:44<06:34,  4.89it/s] 51%|█████     | 1970/3900 [06:44<06:33,  4.91it/s] 51%|█████     | 1971/3900 [06:44<06:33,  4.91it/s] 51%|█████     | 1972/3900 [06:44<06:32,  4.92it/s] 51%|█████     | 1973/3900 [06:45<06:31,  4.92it/s] 51%|█████     | 1974/3900 [06:45<06:35,  4.87it/s] 51%|█████     | 1975/3900 [06:45<06:33,  4.89it/s] 51%|█████     | 1976/3900 [06:45<06:30,  4.92it/s] 51%|█████     | 1977/3900 [06:45<06:28,  4.95it/s] 51%|█████     | 1978/3900 [06:46<06:28,  4.94it/s] 51%|█████     | 1979/3900 [06:46<06:29,  4.93it/s] 51%|█████     | 1980/3900 [06:46<06:28,  4.94it/s] 51%|█████     | 1981/3900 [06:46<06:28,  4.94it/s] 51%|█████     | 1982/3900 [06:46<06:27,  4.95it/s] 51%|█████     | 1983/3900 [06:47<06:26,  4.96it/s] 51%|█████     | 1984/3900 [06:47<06:27,  4.94it/s] 51%|█████     | 1985/3900 [06:47<06:26,  4.95it/s] 51%|█████     | 1986/3900 [06:47<06:25,  4.97it/s] 51%|█████     | 1987/3900 [06:47<06:25,  4.96it/s] 51%|█████     | 1988/3900 [06:48<06:25,  4.96it/s] 51%|█████     | 1989/3900 [06:48<06:23,  4.98it/s] 51%|█████     | 1990/3900 [06:48<06:23,  4.98it/s] 51%|█████     | 1991/3900 [06:48<06:23,  4.98it/s] 51%|█████     | 1992/3900 [06:48<06:23,  4.97it/s] 51%|█████     | 1993/3900 [06:49<06:24,  4.96it/s] 51%|█████     | 1994/3900 [06:49<06:26,  4.93it/s] 51%|█████     | 1995/3900 [06:49<06:24,  4.95it/s] 51%|█████     | 1996/3900 [06:49<06:24,  4.96it/s] 51%|█████     | 1997/3900 [06:49<06:22,  4.97it/s] 51%|█████     | 1998/3900 [06:50<06:22,  4.97it/s] 51%|█████▏    | 1999/3900 [06:50<06:22,  4.98it/s] 51%|█████▏    | 2000/3900 [06:50<06:22,  4.96it/s]                                                    51%|█████▏    | 2000/3900 [06:50<06:22,  4.96it/s][INFO|trainer.py:2409] 2022-08-27 22:14:35,244 >> Saving model checkpoint to tmp/baseline/RTE/bert-base-uncased/HPO_S0/1e-06/64/100/checkpoint-2000
[INFO|configuration_utils.py:446] 2022-08-27 22:14:35,245 >> Configuration saved in tmp/baseline/RTE/bert-base-uncased/HPO_S0/1e-06/64/100/checkpoint-2000/config.json
[INFO|modeling_utils.py:1602] 2022-08-27 22:14:36,136 >> Model weights saved in tmp/baseline/RTE/bert-base-uncased/HPO_S0/1e-06/64/100/checkpoint-2000/pytorch_model.bin
[INFO|tokenization_utils_base.py:2108] 2022-08-27 22:14:36,136 >> tokenizer config file saved in tmp/baseline/RTE/bert-base-uncased/HPO_S0/1e-06/64/100/checkpoint-2000/tokenizer_config.json
[INFO|tokenization_utils_base.py:2114] 2022-08-27 22:14:36,137 >> Special tokens file saved in tmp/baseline/RTE/bert-base-uncased/HPO_S0/1e-06/64/100/checkpoint-2000/special_tokens_map.json
/home/ubuntu/anaconda3/envs/transformers/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 51%|█████▏    | 2001/3900 [06:53<32:01,  1.01s/it] 51%|█████▏    | 2002/3900 [06:53<24:18,  1.30it/s] 51%|█████▏    | 2003/3900 [06:53<18:53,  1.67it/s] 51%|█████▏    | 2004/3900 [06:53<15:05,  2.09it/s] 51%|█████▏    | 2005/3900 [06:54<12:26,  2.54it/s] 51%|█████▏    | 2006/3900 [06:54<10:34,  2.99it/s] 51%|█████▏    | 2007/3900 [06:54<09:16,  3.40it/s] 51%|█████▏    | 2008/3900 [06:54<08:21,  3.77it/s] 52%|█████▏    | 2009/3900 [06:54<07:43,  4.08it/s] 52%|█████▏    | 2010/3900 [06:55<07:16,  4.33it/s] 52%|█████▏    | 2011/3900 [06:55<06:57,  4.53it/s] 52%|█████▏    | 2012/3900 [06:55<06:44,  4.67it/s] 52%|█████▏    | 2013/3900 [06:55<06:34,  4.78it/s] 52%|█████▏    | 2014/3900 [06:55<06:28,  4.86it/s] 52%|█████▏    | 2015/3900 [06:56<06:23,  4.91it/s] 52%|█████▏    | 2016/3900 [06:56<06:20,  4.95it/s] 52%|█████▏    | 2017/3900 [06:56<06:18,  4.98it/s] 52%|█████▏    | 2018/3900 [06:56<06:16,  5.00it/s] 52%|█████▏    | 2019/3900 [06:56<06:15,  5.01it/s] 52%|█████▏    | 2020/3900 [06:57<06:15,  5.01it/s] 52%|█████▏    | 2021/3900 [06:57<06:14,  5.02it/s] 52%|█████▏    | 2022/3900 [06:57<06:13,  5.02it/s] 52%|█████▏    | 2023/3900 [06:57<06:39,  4.70it/s] 52%|█████▏    | 2024/3900 [06:58<06:31,  4.79it/s] 52%|█████▏    | 2025/3900 [06:58<06:25,  4.86it/s] 52%|█████▏    | 2026/3900 [06:58<06:21,  4.91it/s] 52%|█████▏    | 2027/3900 [06:58<06:18,  4.95it/s] 52%|█████▏    | 2028/3900 [06:58<06:12,  5.03it/s] 52%|█████▏    | 2029/3900 [06:58<06:11,  5.03it/s] 52%|█████▏    | 2030/3900 [06:59<06:11,  5.03it/s] 52%|█████▏    | 2031/3900 [06:59<06:11,  5.03it/s] 52%|█████▏    | 2032/3900 [06:59<06:11,  5.03it/s] 52%|█████▏    | 2033/3900 [06:59<06:10,  5.04it/s] 52%|█████▏    | 2034/3900 [06:59<06:10,  5.04it/s] 52%|█████▏    | 2035/3900 [07:00<06:10,  5.04it/s] 52%|█████▏    | 2036/3900 [07:00<06:10,  5.04it/s] 52%|█████▏    | 2037/3900 [07:00<06:10,  5.03it/s] 52%|█████▏    | 2038/3900 [07:00<06:10,  5.03it/s] 52%|█████▏    | 2039/3900 [07:00<06:09,  5.03it/s] 52%|█████▏    | 2040/3900 [07:01<06:09,  5.03it/s] 52%|█████▏    | 2041/3900 [07:01<06:09,  5.03it/s] 52%|█████▏    | 2042/3900 [07:01<06:09,  5.03it/s] 52%|█████▏    | 2043/3900 [07:01<06:08,  5.03it/s] 52%|█████▏    | 2044/3900 [07:01<06:08,  5.04it/s] 52%|█████▏    | 2045/3900 [07:02<06:08,  5.04it/s] 52%|█████▏    | 2046/3900 [07:02<06:08,  5.04it/s] 52%|█████▏    | 2047/3900 [07:02<06:07,  5.04it/s] 53%|█████▎    | 2048/3900 [07:02<06:07,  5.04it/s] 53%|█████▎    | 2049/3900 [07:02<06:07,  5.04it/s] 53%|█████▎    | 2050/3900 [07:03<06:07,  5.04it/s] 53%|█████▎    | 2051/3900 [07:03<06:06,  5.04it/s] 53%|█████▎    | 2052/3900 [07:03<06:06,  5.04it/s] 53%|█████▎    | 2053/3900 [07:03<06:06,  5.04it/s] 53%|█████▎    | 2054/3900 [07:03<06:06,  5.04it/s] 53%|█████▎    | 2055/3900 [07:04<06:06,  5.04it/s] 53%|█████▎    | 2056/3900 [07:04<06:05,  5.04it/s] 53%|█████▎    | 2057/3900 [07:04<06:05,  5.04it/s] 53%|█████▎    | 2058/3900 [07:04<06:05,  5.04it/s] 53%|█████▎    | 2059/3900 [07:04<06:05,  5.04it/s] 53%|█████▎    | 2060/3900 [07:05<06:05,  5.04it/s] 53%|█████▎    | 2061/3900 [07:05<06:04,  5.04it/s] 53%|█████▎    | 2062/3900 [07:05<06:04,  5.04it/s] 53%|█████▎    | 2063/3900 [07:05<06:04,  5.04it/s] 53%|█████▎    | 2064/3900 [07:05<06:04,  5.03it/s] 53%|█████▎    | 2065/3900 [07:06<06:04,  5.03it/s] 53%|█████▎    | 2066/3900 [07:06<06:04,  5.03it/s] 53%|█████▎    | 2067/3900 [07:06<06:00,  5.09it/s] 53%|█████▎    | 2068/3900 [07:06<06:01,  5.07it/s] 53%|█████▎    | 2069/3900 [07:06<06:01,  5.06it/s] 53%|█████▎    | 2070/3900 [07:07<06:02,  5.05it/s] 53%|█████▎    | 2071/3900 [07:07<06:02,  5.05it/s] 53%|█████▎    | 2072/3900 [07:07<06:02,  5.05it/s] 53%|█████▎    | 2073/3900 [07:07<06:02,  5.05it/s] 53%|█████▎    | 2074/3900 [07:07<06:02,  5.04it/s] 53%|█████▎    | 2075/3900 [07:08<06:01,  5.04it/s] 53%|█████▎    | 2076/3900 [07:08<06:01,  5.04it/s] 53%|█████▎    | 2077/3900 [07:08<06:02,  5.03it/s] 53%|█████▎    | 2078/3900 [07:08<06:02,  5.03it/s] 53%|█████▎    | 2079/3900 [07:08<06:01,  5.03it/s] 53%|█████▎    | 2080/3900 [07:09<06:01,  5.04it/s] 53%|█████▎    | 2081/3900 [07:09<06:01,  5.04it/s] 53%|█████▎    | 2082/3900 [07:09<06:01,  5.04it/s] 53%|█████▎    | 2083/3900 [07:09<06:00,  5.03it/s] 53%|█████▎    | 2084/3900 [07:09<06:00,  5.04it/s] 53%|█████▎    | 2085/3900 [07:10<06:00,  5.04it/s] 53%|█████▎    | 2086/3900 [07:10<06:00,  5.04it/s] 54%|█████▎    | 2087/3900 [07:10<05:59,  5.04it/s] 54%|█████▎    | 2088/3900 [07:10<05:59,  5.03it/s] 54%|█████▎    | 2089/3900 [07:10<05:59,  5.04it/s] 54%|█████▎    | 2090/3900 [07:11<05:59,  5.04it/s] 54%|█████▎    | 2091/3900 [07:11<05:59,  5.04it/s] 54%|█████▎    | 2092/3900 [07:11<05:58,  5.04it/s] 54%|█████▎    | 2093/3900 [07:11<05:58,  5.04it/s] 54%|█████▎    | 2094/3900 [07:11<05:58,  5.04it/s] 54%|█████▎    | 2095/3900 [07:12<05:58,  5.04it/s] 54%|█████▎    | 2096/3900 [07:12<05:57,  5.04it/s] 54%|█████▍    | 2097/3900 [07:12<05:57,  5.04it/s] 54%|█████▍    | 2098/3900 [07:12<05:57,  5.04it/s] 54%|█████▍    | 2099/3900 [07:12<05:57,  5.04it/s] 54%|█████▍    | 2100/3900 [07:13<05:58,  5.03it/s] 54%|█████▍    | 2101/3900 [07:13<05:58,  5.02it/s] 54%|█████▍    | 2102/3900 [07:13<05:57,  5.02it/s] 54%|█████▍    | 2103/3900 [07:13<05:57,  5.03it/s] 54%|█████▍    | 2104/3900 [07:13<05:57,  5.03it/s] 54%|█████▍    | 2105/3900 [07:14<05:56,  5.03it/s] 54%|█████▍    | 2106/3900 [07:14<05:52,  5.09it/s] 54%|█████▍    | 2107/3900 [07:14<05:53,  5.07it/s] 54%|█████▍    | 2108/3900 [07:14<05:53,  5.06it/s] 54%|█████▍    | 2109/3900 [07:14<05:54,  5.06it/s] 54%|█████▍    | 2110/3900 [07:15<05:54,  5.05it/s] 54%|█████▍    | 2111/3900 [07:15<05:54,  5.05it/s] 54%|█████▍    | 2112/3900 [07:15<05:54,  5.04it/s] 54%|█████▍    | 2113/3900 [07:15<05:54,  5.04it/s] 54%|█████▍    | 2114/3900 [07:15<05:54,  5.04it/s] 54%|█████▍    | 2115/3900 [07:16<05:54,  5.04it/s] 54%|█████▍    | 2116/3900 [07:16<05:53,  5.04it/s] 54%|█████▍    | 2117/3900 [07:16<05:53,  5.04it/s] 54%|█████▍    | 2118/3900 [07:16<05:53,  5.04it/s] 54%|█████▍    | 2119/3900 [07:16<05:53,  5.04it/s] 54%|█████▍    | 2120/3900 [07:17<06:20,  4.68it/s] 54%|█████▍    | 2121/3900 [07:17<06:12,  4.77it/s] 54%|█████▍    | 2122/3900 [07:17<06:07,  4.84it/s] 54%|█████▍    | 2123/3900 [07:17<06:03,  4.89it/s] 54%|█████▍    | 2124/3900 [07:17<06:00,  4.93it/s] 54%|█████▍    | 2125/3900 [07:18<05:58,  4.96it/s] 55%|█████▍    | 2126/3900 [07:18<05:56,  4.98it/s] 55%|█████▍    | 2127/3900 [07:18<05:55,  4.99it/s] 55%|█████▍    | 2128/3900 [07:18<05:54,  5.00it/s] 55%|█████▍    | 2129/3900 [07:18<05:53,  5.01it/s] 55%|█████▍    | 2130/3900 [07:19<05:53,  5.01it/s] 55%|█████▍    | 2131/3900 [07:19<05:52,  5.02it/s] 55%|█████▍    | 2132/3900 [07:19<05:51,  5.02it/s] 55%|█████▍    | 2133/3900 [07:19<05:51,  5.03it/s] 55%|█████▍    | 2134/3900 [07:19<05:51,  5.03it/s] 55%|█████▍    | 2135/3900 [07:20<05:50,  5.03it/s] 55%|█████▍    | 2136/3900 [07:20<05:50,  5.03it/s] 55%|█████▍    | 2137/3900 [07:20<05:50,  5.03it/s] 55%|█████▍    | 2138/3900 [07:20<05:50,  5.03it/s] 55%|█████▍    | 2139/3900 [07:20<05:49,  5.04it/s] 55%|█████▍    | 2140/3900 [07:21<05:49,  5.04it/s] 55%|█████▍    | 2141/3900 [07:21<05:49,  5.04it/s] 55%|█████▍    | 2142/3900 [07:21<05:48,  5.04it/s] 55%|█████▍    | 2143/3900 [07:21<05:48,  5.04it/s] 55%|█████▍    | 2144/3900 [07:21<05:48,  5.03it/s] 55%|█████▌    | 2145/3900 [07:22<05:44,  5.09it/s] 55%|█████▌    | 2146/3900 [07:22<05:45,  5.07it/s] 55%|█████▌    | 2147/3900 [07:22<05:46,  5.06it/s] 55%|█████▌    | 2148/3900 [07:22<05:46,  5.05it/s] 55%|█████▌    | 2149/3900 [07:22<05:47,  5.04it/s] 55%|█████▌    | 2150/3900 [07:23<05:47,  5.04it/s] 55%|█████▌    | 2151/3900 [07:23<05:47,  5.04it/s] 55%|█████▌    | 2152/3900 [07:23<05:47,  5.04it/s] 55%|█████▌    | 2153/3900 [07:23<05:46,  5.04it/s] 55%|█████▌    | 2154/3900 [07:23<05:46,  5.04it/s] 55%|█████▌    | 2155/3900 [07:24<05:46,  5.04it/s] 55%|█████▌    | 2156/3900 [07:24<05:46,  5.04it/s] 55%|█████▌    | 2157/3900 [07:24<05:45,  5.04it/s] 55%|█████▌    | 2158/3900 [07:24<05:45,  5.04it/s] 55%|█████▌    | 2159/3900 [07:24<05:45,  5.04it/s] 55%|█████▌    | 2160/3900 [07:25<05:45,  5.04it/s] 55%|█████▌    | 2161/3900 [07:25<05:45,  5.04it/s] 55%|█████▌    | 2162/3900 [07:25<05:44,  5.04it/s] 55%|█████▌    | 2163/3900 [07:25<05:45,  5.03it/s] 55%|█████▌    | 2164/3900 [07:25<05:44,  5.03it/s] 56%|█████▌    | 2165/3900 [07:26<05:44,  5.04it/s] 56%|█████▌    | 2166/3900 [07:26<05:44,  5.04it/s] 56%|█████▌    | 2167/3900 [07:26<05:44,  5.04it/s] 56%|█████▌    | 2168/3900 [07:26<05:43,  5.04it/s] 56%|█████▌    | 2169/3900 [07:26<05:43,  5.04it/s] 56%|█████▌    | 2170/3900 [07:27<05:43,  5.04it/s] 56%|█████▌    | 2171/3900 [07:27<05:43,  5.04it/s] 56%|█████▌    | 2172/3900 [07:27<05:42,  5.04it/s] 56%|█████▌    | 2173/3900 [07:27<06:15,  4.60it/s] 56%|█████▌    | 2174/3900 [07:27<06:05,  4.72it/s] 56%|█████▌    | 2175/3900 [07:28<05:58,  4.81it/s] 56%|█████▌    | 2176/3900 [07:28<05:55,  4.85it/s] 56%|█████▌    | 2177/3900 [07:28<05:52,  4.88it/s] 56%|█████▌    | 2178/3900 [07:28<05:50,  4.91it/s] 56%|█████▌    | 2179/3900 [07:28<05:48,  4.94it/s] 56%|█████▌    | 2180/3900 [07:29<05:47,  4.95it/s] 56%|█████▌    | 2181/3900 [07:29<05:49,  4.92it/s] 56%|█████▌    | 2182/3900 [07:29<05:48,  4.93it/s] 56%|█████▌    | 2183/3900 [07:29<05:47,  4.94it/s] 56%|█████▌    | 2184/3900 [07:29<05:43,  4.99it/s] 56%|█████▌    | 2185/3900 [07:30<05:43,  4.99it/s] 56%|█████▌    | 2186/3900 [07:30<05:46,  4.95it/s] 56%|█████▌    | 2187/3900 [07:30<05:45,  4.96it/s] 56%|█████▌    | 2188/3900 [07:30<05:45,  4.95it/s] 56%|█████▌    | 2189/3900 [07:30<05:45,  4.95it/s] 56%|█████▌    | 2190/3900 [07:31<05:44,  4.96it/s] 56%|█████▌    | 2191/3900 [07:31<05:45,  4.95it/s] 56%|█████▌    | 2192/3900 [07:31<05:44,  4.96it/s] 56%|█████▌    | 2193/3900 [07:31<05:43,  4.97it/s] 56%|█████▋    | 2194/3900 [07:31<05:43,  4.97it/s] 56%|█████▋    | 2195/3900 [07:32<05:42,  4.98it/s] 56%|█████▋    | 2196/3900 [07:32<05:43,  4.96it/s] 56%|█████▋    | 2197/3900 [07:32<05:42,  4.97it/s] 56%|█████▋    | 2198/3900 [07:32<05:42,  4.97it/s] 56%|█████▋    | 2199/3900 [07:32<05:42,  4.97it/s] 56%|█████▋    | 2200/3900 [07:33<05:41,  4.98it/s] 56%|█████▋    | 2201/3900 [07:33<05:43,  4.95it/s] 56%|█████▋    | 2202/3900 [07:33<05:42,  4.95it/s] 56%|█████▋    | 2203/3900 [07:33<05:42,  4.95it/s] 57%|█████▋    | 2204/3900 [07:33<05:41,  4.96it/s] 57%|█████▋    | 2205/3900 [07:34<05:40,  4.97it/s] 57%|█████▋    | 2206/3900 [07:34<05:42,  4.95it/s] 57%|█████▋    | 2207/3900 [07:34<05:41,  4.95it/s] 57%|█████▋    | 2208/3900 [07:34<05:40,  4.96it/s] 57%|█████▋    | 2209/3900 [07:34<05:40,  4.97it/s] 57%|█████▋    | 2210/3900 [07:35<05:40,  4.96it/s] 57%|█████▋    | 2211/3900 [07:35<05:40,  4.95it/s] 57%|█████▋    | 2212/3900 [07:35<05:39,  4.97it/s] 57%|█████▋    | 2213/3900 [07:35<05:39,  4.97it/s] 57%|█████▋    | 2214/3900 [07:35<05:38,  4.98it/s] 57%|█████▋    | 2215/3900 [07:36<05:37,  4.99it/s] 57%|█████▋    | 2216/3900 [07:36<05:38,  4.97it/s] 57%|█████▋    | 2217/3900 [07:36<05:40,  4.95it/s] 57%|█████▋    | 2218/3900 [07:36<05:40,  4.94it/s] 57%|█████▋    | 2219/3900 [07:36<05:39,  4.95it/s] 57%|█████▋    | 2220/3900 [07:37<05:39,  4.95it/s] 57%|█████▋    | 2221/3900 [07:37<05:39,  4.95it/s] 57%|█████▋    | 2222/3900 [07:37<05:39,  4.94it/s] 57%|█████▋    | 2223/3900 [07:37<05:37,  4.97it/s] 57%|█████▋    | 2224/3900 [07:37<05:39,  4.94it/s] 57%|█████▋    | 2225/3900 [07:38<05:37,  4.96it/s] 57%|█████▋    | 2226/3900 [07:38<05:38,  4.95it/s] 57%|█████▋    | 2227/3900 [07:38<05:39,  4.93it/s] 57%|█████▋    | 2228/3900 [07:38<05:37,  4.95it/s] 57%|█████▋    | 2229/3900 [07:38<05:37,  4.95it/s] 57%|█████▋    | 2230/3900 [07:39<05:38,  4.94it/s] 57%|█████▋    | 2231/3900 [07:39<05:38,  4.93it/s] 57%|█████▋    | 2232/3900 [07:39<05:38,  4.92it/s] 57%|█████▋    | 2233/3900 [07:39<05:39,  4.90it/s] 57%|█████▋    | 2234/3900 [07:39<05:39,  4.91it/s] 57%|█████▋    | 2235/3900 [07:40<05:37,  4.93it/s] 57%|█████▋    | 2236/3900 [07:40<05:37,  4.93it/s] 57%|█████▋    | 2237/3900 [07:40<05:37,  4.93it/s] 57%|█████▋    | 2238/3900 [07:40<05:35,  4.96it/s] 57%|█████▋    | 2239/3900 [07:40<05:34,  4.96it/s] 57%|█████▋    | 2240/3900 [07:41<05:34,  4.97it/s] 57%|█████▋    | 2241/3900 [07:41<05:34,  4.96it/s] 57%|█████▋    | 2242/3900 [07:41<05:33,  4.96it/s] 58%|█████▊    | 2243/3900 [07:41<05:33,  4.97it/s] 58%|█████▊    | 2244/3900 [07:42<05:32,  4.98it/s] 58%|█████▊    | 2245/3900 [07:42<05:33,  4.97it/s] 58%|█████▊    | 2246/3900 [07:42<05:35,  4.93it/s] 58%|█████▊    | 2247/3900 [07:42<05:35,  4.93it/s] 58%|█████▊    | 2248/3900 [07:42<05:35,  4.93it/s] 58%|█████▊    | 2249/3900 [07:43<05:34,  4.93it/s] 58%|█████▊    | 2250/3900 [07:43<05:34,  4.93it/s] 58%|█████▊    | 2251/3900 [07:43<05:34,  4.93it/s] 58%|█████▊    | 2252/3900 [07:43<05:33,  4.94it/s] 58%|█████▊    | 2253/3900 [07:43<05:34,  4.93it/s] 58%|█████▊    | 2254/3900 [07:44<05:33,  4.94it/s] 58%|█████▊    | 2255/3900 [07:44<05:33,  4.94it/s] 58%|█████▊    | 2256/3900 [07:44<05:34,  4.92it/s] 58%|█████▊    | 2257/3900 [07:44<05:33,  4.93it/s] 58%|█████▊    | 2258/3900 [07:44<05:32,  4.94it/s] 58%|█████▊    | 2259/3900 [07:45<05:31,  4.95it/s] 58%|█████▊    | 2260/3900 [07:45<05:30,  4.96it/s] 58%|█████▊    | 2261/3900 [07:45<05:33,  4.92it/s] 58%|█████▊    | 2262/3900 [07:45<05:28,  4.98it/s] 58%|█████▊    | 2263/3900 [07:45<05:29,  4.97it/s] 58%|█████▊    | 2264/3900 [07:46<05:29,  4.97it/s] 58%|█████▊    | 2265/3900 [07:46<05:29,  4.97it/s] 58%|█████▊    | 2266/3900 [07:46<05:30,  4.94it/s] 58%|█████▊    | 2267/3900 [07:46<05:30,  4.94it/s] 58%|█████▊    | 2268/3900 [07:46<05:29,  4.96it/s] 58%|█████▊    | 2269/3900 [07:47<05:29,  4.95it/s] 58%|█████▊    | 2270/3900 [07:47<05:59,  4.53it/s] 58%|█████▊    | 2271/3900 [07:47<05:49,  4.66it/s] 58%|█████▊    | 2272/3900 [07:47<05:44,  4.73it/s] 58%|█████▊    | 2273/3900 [07:47<05:38,  4.80it/s] 58%|█████▊    | 2274/3900 [07:48<05:34,  4.86it/s] 58%|█████▊    | 2275/3900 [07:48<05:33,  4.87it/s] 58%|█████▊    | 2276/3900 [07:48<05:31,  4.90it/s] 58%|█████▊    | 2277/3900 [07:48<05:30,  4.92it/s] 58%|█████▊    | 2278/3900 [07:48<05:28,  4.93it/s] 58%|█████▊    | 2279/3900 [07:49<05:28,  4.94it/s] 58%|█████▊    | 2280/3900 [07:49<05:28,  4.93it/s] 58%|█████▊    | 2281/3900 [07:49<05:27,  4.94it/s] 59%|█████▊    | 2282/3900 [07:49<05:27,  4.94it/s] 59%|█████▊    | 2283/3900 [07:49<05:26,  4.95it/s] 59%|█████▊    | 2284/3900 [07:50<05:25,  4.97it/s] 59%|█████▊    | 2285/3900 [07:50<05:28,  4.92it/s] 59%|█████▊    | 2286/3900 [07:50<05:27,  4.93it/s] 59%|█████▊    | 2287/3900 [07:50<05:26,  4.95it/s] 59%|█████▊    | 2288/3900 [07:50<05:25,  4.96it/s] 59%|█████▊    | 2289/3900 [07:51<05:25,  4.95it/s] 59%|█████▊    | 2290/3900 [07:51<05:25,  4.94it/s] 59%|█████▊    | 2291/3900 [07:51<05:25,  4.95it/s] 59%|█████▉    | 2292/3900 [07:51<05:25,  4.95it/s] 59%|█████▉    | 2293/3900 [07:51<05:25,  4.94it/s] 59%|█████▉    | 2294/3900 [07:52<05:24,  4.95it/s] 59%|█████▉    | 2295/3900 [07:52<05:24,  4.94it/s] 59%|█████▉    | 2296/3900 [07:52<05:24,  4.94it/s] 59%|█████▉    | 2297/3900 [07:52<05:24,  4.94it/s] 59%|█████▉    | 2298/3900 [07:52<05:23,  4.95it/s] 59%|█████▉    | 2299/3900 [07:53<05:22,  4.96it/s] 59%|█████▉    | 2300/3900 [07:53<05:24,  4.93it/s] 59%|█████▉    | 2301/3900 [07:53<05:20,  4.99it/s] 59%|█████▉    | 2302/3900 [07:53<05:20,  4.98it/s] 59%|█████▉    | 2303/3900 [07:53<05:20,  4.98it/s] 59%|█████▉    | 2304/3900 [07:54<05:20,  4.98it/s] 59%|█████▉    | 2305/3900 [07:54<05:21,  4.95it/s] 59%|█████▉    | 2306/3900 [07:54<05:22,  4.94it/s] 59%|█████▉    | 2307/3900 [07:54<05:23,  4.92it/s] 59%|█████▉    | 2308/3900 [07:55<05:23,  4.92it/s] 59%|█████▉    | 2309/3900 [07:55<05:22,  4.93it/s] 59%|█████▉    | 2310/3900 [07:55<05:22,  4.92it/s] 59%|█████▉    | 2311/3900 [07:55<05:21,  4.95it/s] 59%|█████▉    | 2312/3900 [07:55<05:21,  4.94it/s] 59%|█████▉    | 2313/3900 [07:56<05:21,  4.94it/s] 59%|█████▉    | 2314/3900 [07:56<05:18,  4.98it/s] 59%|█████▉    | 2315/3900 [07:56<05:15,  5.02it/s] 59%|█████▉    | 2316/3900 [07:56<05:13,  5.05it/s] 59%|█████▉    | 2317/3900 [07:56<05:11,  5.07it/s] 59%|█████▉    | 2318/3900 [07:56<05:11,  5.08it/s] 59%|█████▉    | 2319/3900 [07:57<05:10,  5.09it/s] 59%|█████▉    | 2320/3900 [07:57<05:09,  5.10it/s] 60%|█████▉    | 2321/3900 [07:57<05:09,  5.11it/s] 60%|█████▉    | 2322/3900 [07:57<05:08,  5.11it/s] 60%|█████▉    | 2323/3900 [07:58<05:28,  4.80it/s] 60%|█████▉    | 2324/3900 [07:58<05:22,  4.89it/s] 60%|█████▉    | 2325/3900 [07:58<05:17,  4.96it/s] 60%|█████▉    | 2326/3900 [07:58<05:14,  5.00it/s] 60%|█████▉    | 2327/3900 [07:58<05:12,  5.03it/s] 60%|█████▉    | 2328/3900 [07:58<05:10,  5.06it/s] 60%|█████▉    | 2329/3900 [07:59<05:09,  5.07it/s] 60%|█████▉    | 2330/3900 [07:59<05:08,  5.09it/s] 60%|█████▉    | 2331/3900 [07:59<05:07,  5.10it/s] 60%|█████▉    | 2332/3900 [07:59<05:07,  5.10it/s] 60%|█████▉    | 2333/3900 [07:59<05:06,  5.11it/s] 60%|█████▉    | 2334/3900 [08:00<05:06,  5.11it/s] 60%|█████▉    | 2335/3900 [08:00<05:06,  5.11it/s] 60%|█████▉    | 2336/3900 [08:00<05:05,  5.12it/s] 60%|█████▉    | 2337/3900 [08:00<05:05,  5.12it/s] 60%|█████▉    | 2338/3900 [08:00<05:05,  5.12it/s] 60%|█████▉    | 2339/3900 [08:01<05:05,  5.12it/s] 60%|██████    | 2340/3900 [08:01<05:01,  5.18it/s] 60%|██████    | 2341/3900 [08:01<05:02,  5.16it/s] 60%|██████    | 2342/3900 [08:01<05:02,  5.14it/s] 60%|██████    | 2343/3900 [08:01<05:03,  5.14it/s] 60%|██████    | 2344/3900 [08:02<05:03,  5.13it/s] 60%|██████    | 2345/3900 [08:02<05:03,  5.13it/s] 60%|██████    | 2346/3900 [08:02<05:03,  5.12it/s] 60%|██████    | 2347/3900 [08:02<05:03,  5.12it/s] 60%|██████    | 2348/3900 [08:02<05:03,  5.12it/s] 60%|██████    | 2349/3900 [08:03<05:02,  5.12it/s] 60%|██████    | 2350/3900 [08:03<05:02,  5.12it/s] 60%|██████    | 2351/3900 [08:03<05:02,  5.12it/s] 60%|██████    | 2352/3900 [08:03<05:02,  5.12it/s] 60%|██████    | 2353/3900 [08:03<05:02,  5.12it/s] 60%|██████    | 2354/3900 [08:04<05:02,  5.12it/s] 60%|██████    | 2355/3900 [08:04<05:01,  5.12it/s] 60%|██████    | 2356/3900 [08:04<05:01,  5.12it/s] 60%|██████    | 2357/3900 [08:04<05:01,  5.12it/s] 60%|██████    | 2358/3900 [08:04<05:01,  5.12it/s] 60%|██████    | 2359/3900 [08:05<05:01,  5.12it/s] 61%|██████    | 2360/3900 [08:05<05:00,  5.12it/s] 61%|██████    | 2361/3900 [08:05<05:01,  5.11it/s] 61%|██████    | 2362/3900 [08:05<05:00,  5.11it/s] 61%|██████    | 2363/3900 [08:05<05:00,  5.12it/s] 61%|██████    | 2364/3900 [08:06<05:00,  5.11it/s] 61%|██████    | 2365/3900 [08:06<05:00,  5.11it/s] 61%|██████    | 2366/3900 [08:06<05:00,  5.10it/s] 61%|██████    | 2367/3900 [08:06<05:01,  5.09it/s] 61%|██████    | 2368/3900 [08:06<05:01,  5.07it/s] 61%|██████    | 2369/3900 [08:07<05:02,  5.07it/s] 61%|██████    | 2370/3900 [08:07<05:01,  5.07it/s] 61%|██████    | 2371/3900 [08:07<05:01,  5.07it/s] 61%|██████    | 2372/3900 [08:07<05:01,  5.06it/s] 61%|██████    | 2373/3900 [08:07<05:01,  5.07it/s] 61%|██████    | 2374/3900 [08:07<05:01,  5.06it/s] 61%|██████    | 2375/3900 [08:08<05:01,  5.06it/s] 61%|██████    | 2376/3900 [08:08<05:01,  5.06it/s] 61%|██████    | 2377/3900 [08:08<05:00,  5.06it/s] 61%|██████    | 2378/3900 [08:08<05:00,  5.06it/s] 61%|██████    | 2379/3900 [08:08<04:57,  5.12it/s] 61%|██████    | 2380/3900 [08:09<04:58,  5.10it/s] 61%|██████    | 2381/3900 [08:09<04:58,  5.09it/s] 61%|██████    | 2382/3900 [08:09<04:58,  5.08it/s] 61%|██████    | 2383/3900 [08:09<04:58,  5.07it/s] 61%|██████    | 2384/3900 [08:09<04:58,  5.07it/s] 61%|██████    | 2385/3900 [08:10<04:58,  5.07it/s] 61%|██████    | 2386/3900 [08:10<04:58,  5.07it/s] 61%|██████    | 2387/3900 [08:10<04:58,  5.07it/s] 61%|██████    | 2388/3900 [08:10<04:58,  5.06it/s] 61%|██████▏   | 2389/3900 [08:10<04:58,  5.06it/s] 61%|██████▏   | 2390/3900 [08:11<04:58,  5.06it/s] 61%|██████▏   | 2391/3900 [08:11<04:58,  5.06it/s] 61%|██████▏   | 2392/3900 [08:11<04:57,  5.06it/s] 61%|██████▏   | 2393/3900 [08:11<04:57,  5.06it/s] 61%|██████▏   | 2394/3900 [08:11<04:57,  5.06it/s] 61%|██████▏   | 2395/3900 [08:12<04:57,  5.06it/s] 61%|██████▏   | 2396/3900 [08:12<04:56,  5.06it/s] 61%|██████▏   | 2397/3900 [08:12<04:57,  5.06it/s] 61%|██████▏   | 2398/3900 [08:12<04:57,  5.06it/s] 62%|██████▏   | 2399/3900 [08:12<04:56,  5.06it/s] 62%|██████▏   | 2400/3900 [08:13<04:55,  5.07it/s] 62%|██████▏   | 2401/3900 [08:13<04:55,  5.07it/s] 62%|██████▏   | 2402/3900 [08:13<04:54,  5.08it/s] 62%|██████▏   | 2403/3900 [08:13<04:54,  5.08it/s] 62%|██████▏   | 2404/3900 [08:13<04:53,  5.09it/s] 62%|██████▏   | 2405/3900 [08:14<04:53,  5.10it/s] 62%|██████▏   | 2406/3900 [08:14<04:52,  5.10it/s] 62%|██████▏   | 2407/3900 [08:14<04:52,  5.11it/s] 62%|██████▏   | 2408/3900 [08:14<04:51,  5.11it/s] 62%|██████▏   | 2409/3900 [08:14<04:51,  5.11it/s] 62%|██████▏   | 2410/3900 [08:15<04:51,  5.12it/s] 62%|██████▏   | 2411/3900 [08:15<04:51,  5.12it/s] 62%|██████▏   | 2412/3900 [08:15<04:50,  5.12it/s] 62%|██████▏   | 2413/3900 [08:15<04:50,  5.11it/s] 62%|██████▏   | 2414/3900 [08:15<04:50,  5.11it/s] 62%|██████▏   | 2415/3900 [08:16<04:50,  5.12it/s] 62%|██████▏   | 2416/3900 [08:16<04:49,  5.12it/s] 62%|██████▏   | 2417/3900 [08:16<04:49,  5.12it/s] 62%|██████▏   | 2418/3900 [08:16<04:46,  5.18it/s] 62%|██████▏   | 2419/3900 [08:16<04:47,  5.15it/s] 62%|██████▏   | 2420/3900 [08:17<05:10,  4.77it/s] 62%|██████▏   | 2421/3900 [08:17<05:04,  4.86it/s] 62%|██████▏   | 2422/3900 [08:17<04:59,  4.93it/s] 62%|██████▏   | 2423/3900 [08:17<04:57,  4.96it/s] 62%|██████▏   | 2424/3900 [08:17<04:56,  4.98it/s] 62%|██████▏   | 2425/3900 [08:18<04:55,  5.00it/s] 62%|██████▏   | 2426/3900 [08:18<04:54,  5.01it/s] 62%|██████▏   | 2427/3900 [08:18<04:53,  5.02it/s] 62%|██████▏   | 2428/3900 [08:18<04:52,  5.02it/s] 62%|██████▏   | 2429/3900 [08:18<04:52,  5.03it/s] 62%|██████▏   | 2430/3900 [08:19<04:52,  5.03it/s] 62%|██████▏   | 2431/3900 [08:19<04:51,  5.03it/s] 62%|██████▏   | 2432/3900 [08:19<04:51,  5.04it/s] 62%|██████▏   | 2433/3900 [08:19<04:51,  5.04it/s] 62%|██████▏   | 2434/3900 [08:19<04:51,  5.04it/s] 62%|██████▏   | 2435/3900 [08:20<04:50,  5.04it/s] 62%|██████▏   | 2436/3900 [08:20<04:50,  5.03it/s] 62%|██████▏   | 2437/3900 [08:20<04:50,  5.04it/s] 63%|██████▎   | 2438/3900 [08:20<04:50,  5.04it/s] 63%|██████▎   | 2439/3900 [08:20<04:50,  5.04it/s] 63%|██████▎   | 2440/3900 [08:21<04:49,  5.04it/s] 63%|██████▎   | 2441/3900 [08:21<04:49,  5.04it/s] 63%|██████▎   | 2442/3900 [08:21<04:49,  5.03it/s] 63%|██████▎   | 2443/3900 [08:21<04:49,  5.03it/s] 63%|██████▎   | 2444/3900 [08:21<04:49,  5.04it/s] 63%|██████▎   | 2445/3900 [08:22<04:49,  5.03it/s] 63%|██████▎   | 2446/3900 [08:22<04:49,  5.03it/s] 63%|██████▎   | 2447/3900 [08:22<04:48,  5.03it/s] 63%|██████▎   | 2448/3900 [08:22<04:48,  5.03it/s] 63%|██████▎   | 2449/3900 [08:22<04:48,  5.03it/s] 63%|██████▎   | 2450/3900 [08:23<04:48,  5.03it/s] 63%|██████▎   | 2451/3900 [08:23<04:47,  5.04it/s] 63%|██████▎   | 2452/3900 [08:23<04:47,  5.03it/s] 63%|██████▎   | 2453/3900 [08:23<04:47,  5.03it/s] 63%|██████▎   | 2454/3900 [08:23<04:47,  5.03it/s] 63%|██████▎   | 2455/3900 [08:24<04:46,  5.04it/s] 63%|██████▎   | 2456/3900 [08:24<04:46,  5.04it/s] 63%|██████▎   | 2457/3900 [08:24<04:43,  5.09it/s] 63%|██████▎   | 2458/3900 [08:24<04:44,  5.07it/s] 63%|██████▎   | 2459/3900 [08:24<04:44,  5.06it/s] 63%|██████▎   | 2460/3900 [08:25<04:44,  5.06it/s] 63%|██████▎   | 2461/3900 [08:25<04:44,  5.05it/s] 63%|██████▎   | 2462/3900 [08:25<04:44,  5.05it/s] 63%|██████▎   | 2463/3900 [08:25<04:45,  5.04it/s] 63%|██████▎   | 2464/3900 [08:25<04:44,  5.04it/s] 63%|██████▎   | 2465/3900 [08:26<04:44,  5.04it/s] 63%|██████▎   | 2466/3900 [08:26<04:44,  5.04it/s] 63%|██████▎   | 2467/3900 [08:26<04:44,  5.04it/s] 63%|██████▎   | 2468/3900 [08:26<04:44,  5.04it/s] 63%|██████▎   | 2469/3900 [08:26<04:44,  5.03it/s] 63%|██████▎   | 2470/3900 [08:26<04:44,  5.03it/s] 63%|██████▎   | 2471/3900 [08:27<04:44,  5.02it/s] 63%|██████▎   | 2472/3900 [08:27<04:44,  5.03it/s] 63%|██████▎   | 2473/3900 [08:27<05:03,  4.69it/s] 63%|██████▎   | 2474/3900 [08:27<04:58,  4.78it/s] 63%|██████▎   | 2475/3900 [08:28<04:53,  4.85it/s] 63%|██████▎   | 2476/3900 [08:28<04:50,  4.89it/s] 64%|██████▎   | 2477/3900 [08:28<04:49,  4.92it/s] 64%|██████▎   | 2478/3900 [08:28<04:48,  4.94it/s] 64%|██████▎   | 2479/3900 [08:28<04:47,  4.94it/s] 64%|██████▎   | 2480/3900 [08:29<04:46,  4.95it/s] 64%|██████▎   | 2481/3900 [08:29<04:46,  4.96it/s] 64%|██████▎   | 2482/3900 [08:29<04:45,  4.96it/s] 64%|██████▎   | 2483/3900 [08:29<04:45,  4.97it/s] 64%|██████▎   | 2484/3900 [08:29<04:45,  4.97it/s] 64%|██████▎   | 2485/3900 [08:30<04:44,  4.97it/s] 64%|██████▎   | 2486/3900 [08:30<04:44,  4.97it/s] 64%|██████▍   | 2487/3900 [08:30<04:43,  4.98it/s] 64%|██████▍   | 2488/3900 [08:30<04:43,  4.98it/s] 64%|██████▍   | 2489/3900 [08:30<04:43,  4.98it/s] 64%|██████▍   | 2490/3900 [08:31<04:43,  4.98it/s] 64%|██████▍   | 2491/3900 [08:31<04:42,  4.98it/s] 64%|██████▍   | 2492/3900 [08:31<04:42,  4.98it/s] 64%|██████▍   | 2493/3900 [08:31<04:42,  4.98it/s] 64%|██████▍   | 2494/3900 [08:31<04:42,  4.98it/s] 64%|██████▍   | 2495/3900 [08:32<04:41,  4.98it/s] 64%|██████▍   | 2496/3900 [08:32<04:38,  5.04it/s] 64%|██████▍   | 2497/3900 [08:32<04:39,  5.01it/s] 64%|██████▍   | 2498/3900 [08:32<04:40,  5.00it/s] 64%|██████▍   | 2499/3900 [08:32<04:40,  4.99it/s] 64%|██████▍   | 2500/3900 [08:33<04:40,  4.99it/s]                                                    64%|██████▍   | 2500/3900 [08:33<04:40,  4.99it/s][INFO|trainer.py:2409] 2022-08-27 22:16:17,804 >> Saving model checkpoint to tmp/baseline/RTE/bert-base-uncased/HPO_S0/1e-06/64/100/checkpoint-2500
[INFO|configuration_utils.py:446] 2022-08-27 22:16:17,805 >> Configuration saved in tmp/baseline/RTE/bert-base-uncased/HPO_S0/1e-06/64/100/checkpoint-2500/config.json
[INFO|modeling_utils.py:1602] 2022-08-27 22:16:18,667 >> Model weights saved in tmp/baseline/RTE/bert-base-uncased/HPO_S0/1e-06/64/100/checkpoint-2500/pytorch_model.bin
[INFO|tokenization_utils_base.py:2108] 2022-08-27 22:16:18,668 >> tokenizer config file saved in tmp/baseline/RTE/bert-base-uncased/HPO_S0/1e-06/64/100/checkpoint-2500/tokenizer_config.json
[INFO|tokenization_utils_base.py:2114] 2022-08-27 22:16:18,668 >> Special tokens file saved in tmp/baseline/RTE/bert-base-uncased/HPO_S0/1e-06/64/100/checkpoint-2500/special_tokens_map.json
/home/ubuntu/anaconda3/envs/transformers/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 64%|██████▍   | 2501/3900 [08:35<22:56,  1.02it/s] 64%|██████▍   | 2502/3900 [08:36<17:27,  1.33it/s] 64%|██████▍   | 2503/3900 [08:36<13:36,  1.71it/s] 64%|██████▍   | 2504/3900 [08:36<10:54,  2.13it/s] 64%|██████▍   | 2505/3900 [08:36<09:01,  2.58it/s] 64%|██████▍   | 2506/3900 [08:36<07:41,  3.02it/s] 64%|██████▍   | 2507/3900 [08:37<06:45,  3.43it/s] 64%|██████▍   | 2508/3900 [08:37<06:08,  3.78it/s] 64%|██████▍   | 2509/3900 [08:37<05:40,  4.08it/s] 64%|██████▍   | 2510/3900 [08:37<05:21,  4.33it/s] 64%|██████▍   | 2511/3900 [08:37<05:07,  4.52it/s] 64%|██████▍   | 2512/3900 [08:38<04:57,  4.66it/s] 64%|██████▍   | 2513/3900 [08:38<04:51,  4.76it/s] 64%|██████▍   | 2514/3900 [08:38<04:46,  4.84it/s] 64%|██████▍   | 2515/3900 [08:38<04:42,  4.90it/s] 65%|██████▍   | 2516/3900 [08:38<04:40,  4.94it/s] 65%|██████▍   | 2517/3900 [08:39<04:38,  4.96it/s] 65%|██████▍   | 2518/3900 [08:39<04:37,  4.98it/s] 65%|██████▍   | 2519/3900 [08:39<04:36,  5.00it/s] 65%|██████▍   | 2520/3900 [08:39<04:35,  5.00it/s] 65%|██████▍   | 2521/3900 [08:39<04:35,  5.01it/s] 65%|██████▍   | 2522/3900 [08:40<04:34,  5.02it/s] 65%|██████▍   | 2523/3900 [08:40<04:34,  5.02it/s] 65%|██████▍   | 2524/3900 [08:40<04:34,  5.02it/s] 65%|██████▍   | 2525/3900 [08:40<04:33,  5.02it/s] 65%|██████▍   | 2526/3900 [08:40<04:33,  5.02it/s] 65%|██████▍   | 2527/3900 [08:41<04:33,  5.02it/s] 65%|██████▍   | 2528/3900 [08:41<04:33,  5.02it/s] 65%|██████▍   | 2529/3900 [08:41<04:33,  5.02it/s] 65%|██████▍   | 2530/3900 [08:41<04:33,  5.02it/s] 65%|██████▍   | 2531/3900 [08:41<04:32,  5.02it/s] 65%|██████▍   | 2532/3900 [08:42<04:32,  5.02it/s] 65%|██████▍   | 2533/3900 [08:42<04:32,  5.02it/s] 65%|██████▍   | 2534/3900 [08:42<04:32,  5.02it/s] 65%|██████▌   | 2535/3900 [08:42<04:29,  5.07it/s] 65%|██████▌   | 2536/3900 [08:42<04:29,  5.06it/s] 65%|██████▌   | 2537/3900 [08:43<04:30,  5.05it/s] 65%|██████▌   | 2538/3900 [08:43<04:30,  5.04it/s] 65%|██████▌   | 2539/3900 [08:43<04:30,  5.03it/s] 65%|██████▌   | 2540/3900 [08:43<04:30,  5.03it/s] 65%|██████▌   | 2541/3900 [08:43<04:30,  5.03it/s] 65%|██████▌   | 2542/3900 [08:44<04:29,  5.03it/s] 65%|██████▌   | 2543/3900 [08:44<04:29,  5.03it/s] 65%|██████▌   | 2544/3900 [08:44<04:29,  5.03it/s] 65%|██████▌   | 2545/3900 [08:44<04:29,  5.03it/s] 65%|██████▌   | 2546/3900 [08:44<04:29,  5.03it/s] 65%|██████▌   | 2547/3900 [08:45<04:29,  5.03it/s] 65%|██████▌   | 2548/3900 [08:45<04:28,  5.03it/s] 65%|██████▌   | 2549/3900 [08:45<04:28,  5.03it/s] 65%|██████▌   | 2550/3900 [08:45<04:28,  5.03it/s] 65%|██████▌   | 2551/3900 [08:45<04:28,  5.02it/s] 65%|██████▌   | 2552/3900 [08:46<04:28,  5.01it/s] 65%|██████▌   | 2553/3900 [08:46<04:28,  5.02it/s] 65%|██████▌   | 2554/3900 [08:46<04:28,  5.02it/s] 66%|██████▌   | 2555/3900 [08:46<04:28,  5.02it/s] 66%|██████▌   | 2556/3900 [08:46<04:27,  5.02it/s] 66%|██████▌   | 2557/3900 [08:47<04:27,  5.02it/s] 66%|██████▌   | 2558/3900 [08:47<04:27,  5.02it/s] 66%|██████▌   | 2559/3900 [08:47<04:26,  5.02it/s] 66%|██████▌   | 2560/3900 [08:47<04:26,  5.03it/s] 66%|██████▌   | 2561/3900 [08:47<04:26,  5.03it/s] 66%|██████▌   | 2562/3900 [08:48<04:25,  5.03it/s] 66%|██████▌   | 2563/3900 [08:48<04:25,  5.03it/s] 66%|██████▌   | 2564/3900 [08:48<04:25,  5.03it/s] 66%|██████▌   | 2565/3900 [08:48<04:25,  5.03it/s] 66%|██████▌   | 2566/3900 [08:48<04:25,  5.03it/s] 66%|██████▌   | 2567/3900 [08:49<04:25,  5.03it/s] 66%|██████▌   | 2568/3900 [08:49<04:24,  5.03it/s] 66%|██████▌   | 2569/3900 [08:49<04:24,  5.03it/s] 66%|██████▌   | 2570/3900 [08:49<04:24,  5.02it/s] 66%|██████▌   | 2571/3900 [08:49<04:24,  5.03it/s] 66%|██████▌   | 2572/3900 [08:49<04:24,  5.03it/s] 66%|██████▌   | 2573/3900 [08:50<04:23,  5.03it/s] 66%|██████▌   | 2574/3900 [08:50<04:20,  5.09it/s] 66%|██████▌   | 2575/3900 [08:50<04:21,  5.07it/s] 66%|██████▌   | 2576/3900 [08:50<04:21,  5.06it/s] 66%|██████▌   | 2577/3900 [08:50<04:22,  5.05it/s] 66%|██████▌   | 2578/3900 [08:51<04:22,  5.04it/s] 66%|██████▌   | 2579/3900 [08:51<04:22,  5.04it/s] 66%|██████▌   | 2580/3900 [08:51<04:22,  5.04it/s] 66%|██████▌   | 2581/3900 [08:51<04:21,  5.03it/s] 66%|██████▌   | 2582/3900 [08:51<04:21,  5.03it/s] 66%|██████▌   | 2583/3900 [08:52<04:21,  5.03it/s] 66%|██████▋   | 2584/3900 [08:52<04:21,  5.03it/s] 66%|██████▋   | 2585/3900 [08:52<04:21,  5.03it/s] 66%|██████▋   | 2586/3900 [08:52<04:21,  5.03it/s] 66%|██████▋   | 2587/3900 [08:52<04:20,  5.03it/s] 66%|██████▋   | 2588/3900 [08:53<04:20,  5.03it/s] 66%|██████▋   | 2589/3900 [08:53<04:20,  5.03it/s] 66%|██████▋   | 2590/3900 [08:53<04:20,  5.03it/s] 66%|██████▋   | 2591/3900 [08:53<04:20,  5.03it/s] 66%|██████▋   | 2592/3900 [08:53<04:20,  5.03it/s] 66%|██████▋   | 2593/3900 [08:54<04:19,  5.03it/s] 67%|██████▋   | 2594/3900 [08:54<04:19,  5.03it/s] 67%|██████▋   | 2595/3900 [08:54<04:19,  5.03it/s] 67%|██████▋   | 2596/3900 [08:54<04:19,  5.03it/s] 67%|██████▋   | 2597/3900 [08:54<04:19,  5.03it/s] 67%|██████▋   | 2598/3900 [08:55<04:18,  5.03it/s] 67%|██████▋   | 2599/3900 [08:55<04:18,  5.03it/s] 67%|██████▋   | 2600/3900 [08:55<04:18,  5.03it/s] 67%|██████▋   | 2601/3900 [08:55<04:18,  5.03it/s] 67%|██████▋   | 2602/3900 [08:55<04:17,  5.04it/s] 67%|██████▋   | 2603/3900 [08:56<04:18,  5.02it/s] 67%|██████▋   | 2604/3900 [08:56<04:18,  5.02it/s] 67%|██████▋   | 2605/3900 [08:56<04:17,  5.03it/s] 67%|██████▋   | 2606/3900 [08:56<04:17,  5.03it/s] 67%|██████▋   | 2607/3900 [08:56<04:16,  5.03it/s] 67%|██████▋   | 2608/3900 [08:57<04:16,  5.03it/s] 67%|██████▋   | 2609/3900 [08:57<04:16,  5.04it/s] 67%|██████▋   | 2610/3900 [08:57<04:16,  5.04it/s] 67%|██████▋   | 2611/3900 [08:57<04:15,  5.04it/s] 67%|██████▋   | 2612/3900 [08:57<04:15,  5.04it/s] 67%|██████▋   | 2613/3900 [08:58<04:12,  5.10it/s] 67%|██████▋   | 2614/3900 [08:58<04:13,  5.08it/s] 67%|██████▋   | 2615/3900 [08:58<04:13,  5.07it/s] 67%|██████▋   | 2616/3900 [08:58<04:13,  5.06it/s] 67%|██████▋   | 2617/3900 [08:58<04:14,  5.05it/s] 67%|██████▋   | 2618/3900 [08:59<04:14,  5.05it/s] 67%|██████▋   | 2619/3900 [08:59<04:13,  5.05it/s] 67%|██████▋   | 2620/3900 [08:59<04:13,  5.04it/s] 67%|██████▋   | 2621/3900 [08:59<04:13,  5.04it/s] 67%|██████▋   | 2622/3900 [08:59<04:13,  5.04it/s] 67%|██████▋   | 2623/3900 [09:00<04:13,  5.04it/s] 67%|██████▋   | 2624/3900 [09:00<04:13,  5.04it/s] 67%|██████▋   | 2625/3900 [09:00<04:13,  5.04it/s] 67%|██████▋   | 2626/3900 [09:00<04:13,  5.03it/s] 67%|██████▋   | 2627/3900 [09:00<04:13,  5.03it/s] 67%|██████▋   | 2628/3900 [09:01<04:12,  5.03it/s] 67%|██████▋   | 2629/3900 [09:01<04:12,  5.03it/s] 67%|██████▋   | 2630/3900 [09:01<04:12,  5.03it/s] 67%|██████▋   | 2631/3900 [09:01<04:12,  5.03it/s] 67%|██████▋   | 2632/3900 [09:01<04:12,  5.03it/s] 68%|██████▊   | 2633/3900 [09:02<04:12,  5.03it/s] 68%|██████▊   | 2634/3900 [09:02<04:11,  5.03it/s] 68%|██████▊   | 2635/3900 [09:02<04:41,  4.49it/s] 68%|██████▊   | 2636/3900 [09:02<04:32,  4.64it/s] 68%|██████▊   | 2637/3900 [09:02<04:26,  4.74it/s] 68%|██████▊   | 2638/3900 [09:03<04:21,  4.83it/s] 68%|██████▊   | 2639/3900 [09:03<04:18,  4.88it/s] 68%|██████▊   | 2640/3900 [09:03<04:15,  4.93it/s] 68%|██████▊   | 2641/3900 [09:03<04:13,  4.96it/s] 68%|██████▊   | 2642/3900 [09:03<04:12,  4.98it/s] 68%|██████▊   | 2643/3900 [09:04<04:11,  5.00it/s] 68%|██████▊   | 2644/3900 [09:04<04:10,  5.01it/s] 68%|██████▊   | 2645/3900 [09:04<04:10,  5.01it/s] 68%|██████▊   | 2646/3900 [09:04<04:10,  5.02it/s] 68%|██████▊   | 2647/3900 [09:04<04:09,  5.02it/s] 68%|██████▊   | 2648/3900 [09:05<04:09,  5.02it/s] 68%|██████▊   | 2649/3900 [09:05<04:09,  5.02it/s] 68%|██████▊   | 2650/3900 [09:05<04:08,  5.03it/s] 68%|██████▊   | 2651/3900 [09:05<04:08,  5.03it/s] 68%|██████▊   | 2652/3900 [09:05<04:05,  5.08it/s] 68%|██████▊   | 2653/3900 [09:06<04:06,  5.06it/s] 68%|██████▊   | 2654/3900 [09:06<04:06,  5.05it/s] 68%|██████▊   | 2655/3900 [09:06<04:06,  5.05it/s] 68%|██████▊   | 2656/3900 [09:06<04:06,  5.04it/s] 68%|██████▊   | 2657/3900 [09:06<04:06,  5.04it/s] 68%|██████▊   | 2658/3900 [09:07<04:06,  5.04it/s] 68%|██████▊   | 2659/3900 [09:07<04:06,  5.03it/s] 68%|██████▊   | 2660/3900 [09:07<04:06,  5.03it/s] 68%|██████▊   | 2661/3900 [09:07<04:06,  5.03it/s] 68%|██████▊   | 2662/3900 [09:07<04:05,  5.03it/s] 68%|██████▊   | 2663/3900 [09:08<04:05,  5.03it/s] 68%|██████▊   | 2664/3900 [09:08<04:05,  5.03it/s] 68%|██████▊   | 2665/3900 [09:08<04:05,  5.03it/s] 68%|██████▊   | 2666/3900 [09:08<04:05,  5.03it/s] 68%|██████▊   | 2667/3900 [09:08<04:05,  5.03it/s] 68%|██████▊   | 2668/3900 [09:09<04:04,  5.03it/s] 68%|██████▊   | 2669/3900 [09:09<04:04,  5.03it/s] 68%|██████▊   | 2670/3900 [09:09<04:04,  5.03it/s] 68%|██████▊   | 2671/3900 [09:09<04:04,  5.03it/s] 69%|██████▊   | 2672/3900 [09:09<04:04,  5.03it/s] 69%|██████▊   | 2673/3900 [09:10<04:03,  5.03it/s] 69%|██████▊   | 2674/3900 [09:10<04:03,  5.03it/s] 69%|██████▊   | 2675/3900 [09:10<04:03,  5.03it/s] 69%|██████▊   | 2676/3900 [09:10<04:03,  5.03it/s] 69%|██████▊   | 2677/3900 [09:10<04:03,  5.03it/s] 69%|██████▊   | 2678/3900 [09:11<04:02,  5.03it/s] 69%|██████▊   | 2679/3900 [09:11<04:02,  5.03it/s] 69%|██████▊   | 2680/3900 [09:11<04:02,  5.03it/s] 69%|██████▊   | 2681/3900 [09:11<04:02,  5.03it/s] 69%|██████▉   | 2682/3900 [09:11<04:02,  5.03it/s] 69%|██████▉   | 2683/3900 [09:12<04:02,  5.02it/s] 69%|██████▉   | 2684/3900 [09:12<04:02,  5.02it/s] 69%|██████▉   | 2685/3900 [09:12<04:02,  5.02it/s] 69%|██████▉   | 2686/3900 [09:12<04:01,  5.02it/s] 69%|██████▉   | 2687/3900 [09:12<04:01,  5.02it/s] 69%|██████▉   | 2688/3900 [09:13<04:01,  5.02it/s] 69%|██████▉   | 2689/3900 [09:13<04:01,  5.02it/s] 69%|██████▉   | 2690/3900 [09:13<04:00,  5.02it/s] 69%|██████▉   | 2691/3900 [09:13<03:58,  5.08it/s] 69%|██████▉   | 2692/3900 [09:13<03:58,  5.06it/s] 69%|██████▉   | 2693/3900 [09:14<03:59,  5.05it/s] 69%|██████▉   | 2694/3900 [09:14<03:58,  5.05it/s] 69%|██████▉   | 2695/3900 [09:14<03:59,  5.04it/s] 69%|██████▉   | 2696/3900 [09:14<03:58,  5.04it/s] 69%|██████▉   | 2697/3900 [09:14<03:58,  5.03it/s] 69%|██████▉   | 2698/3900 [09:15<03:58,  5.03it/s] 69%|██████▉   | 2699/3900 [09:15<04:15,  4.69it/s] 69%|██████▉   | 2700/3900 [09:15<04:10,  4.79it/s] 69%|██████▉   | 2701/3900 [09:15<04:07,  4.85it/s] 69%|██████▉   | 2702/3900 [09:15<04:04,  4.90it/s] 69%|██████▉   | 2703/3900 [09:16<04:02,  4.94it/s] 69%|██████▉   | 2704/3900 [09:16<04:01,  4.96it/s] 69%|██████▉   | 2705/3900 [09:16<04:00,  4.98it/s] 69%|██████▉   | 2706/3900 [09:16<03:59,  4.99it/s] 69%|██████▉   | 2707/3900 [09:16<03:59,  4.99it/s] 69%|██████▉   | 2708/3900 [09:17<03:58,  5.00it/s] 69%|██████▉   | 2709/3900 [09:17<03:57,  5.01it/s] 69%|██████▉   | 2710/3900 [09:17<03:57,  5.01it/s] 70%|██████▉   | 2711/3900 [09:17<03:57,  5.01it/s] 70%|██████▉   | 2712/3900 [09:17<03:56,  5.02it/s] 70%|██████▉   | 2713/3900 [09:18<03:56,  5.02it/s] 70%|██████▉   | 2714/3900 [09:18<03:56,  5.02it/s] 70%|██████▉   | 2715/3900 [09:18<03:55,  5.02it/s] 70%|██████▉   | 2716/3900 [09:18<03:55,  5.02it/s] 70%|██████▉   | 2717/3900 [09:18<03:55,  5.02it/s] 70%|██████▉   | 2718/3900 [09:19<03:55,  5.02it/s] 70%|██████▉   | 2719/3900 [09:19<03:55,  5.02it/s] 70%|██████▉   | 2720/3900 [09:19<03:54,  5.02it/s] 70%|██████▉   | 2721/3900 [09:19<03:54,  5.03it/s] 70%|██████▉   | 2722/3900 [09:19<03:54,  5.03it/s] 70%|██████▉   | 2723/3900 [09:20<03:54,  5.03it/s] 70%|██████▉   | 2724/3900 [09:20<03:54,  5.03it/s] 70%|██████▉   | 2725/3900 [09:20<03:53,  5.03it/s] 70%|██████▉   | 2726/3900 [09:20<03:53,  5.03it/s] 70%|██████▉   | 2727/3900 [09:20<03:53,  5.02it/s] 70%|██████▉   | 2728/3900 [09:21<03:53,  5.03it/s] 70%|██████▉   | 2729/3900 [09:21<03:52,  5.03it/s] 70%|███████   | 2730/3900 [09:21<03:50,  5.08it/s] 70%|███████   | 2731/3900 [09:21<03:50,  5.06it/s] 70%|███████   | 2732/3900 [09:21<03:51,  5.05it/s] 70%|███████   | 2733/3900 [09:22<03:51,  5.05it/s] 70%|███████   | 2734/3900 [09:22<03:51,  5.04it/s] 70%|███████   | 2735/3900 [09:22<03:51,  5.04it/s] 70%|███████   | 2736/3900 [09:22<03:51,  5.04it/s] 70%|███████   | 2737/3900 [09:22<03:51,  5.03it/s] 70%|███████   | 2738/3900 [09:23<03:50,  5.03it/s] 70%|███████   | 2739/3900 [09:23<03:50,  5.03it/s] 70%|███████   | 2740/3900 [09:23<03:50,  5.03it/s] 70%|███████   | 2741/3900 [09:23<03:50,  5.03it/s] 70%|███████   | 2742/3900 [09:23<03:50,  5.03it/s] 70%|███████   | 2743/3900 [09:24<03:50,  5.03it/s] 70%|███████   | 2744/3900 [09:24<03:49,  5.03it/s] 70%|███████   | 2745/3900 [09:24<03:49,  5.03it/s] 70%|███████   | 2746/3900 [09:24<03:49,  5.03it/s] 70%|███████   | 2747/3900 [09:24<03:49,  5.02it/s] 70%|███████   | 2748/3900 [09:25<03:49,  5.03it/s] 70%|███████   | 2749/3900 [09:25<03:49,  5.02it/s] 71%|███████   | 2750/3900 [09:25<03:49,  5.02it/s] 71%|███████   | 2751/3900 [09:25<03:49,  5.01it/s] 71%|███████   | 2752/3900 [09:25<03:48,  5.02it/s] 71%|███████   | 2753/3900 [09:26<03:48,  5.01it/s] 71%|███████   | 2754/3900 [09:26<03:48,  5.01it/s] 71%|███████   | 2755/3900 [09:26<03:48,  5.01it/s] 71%|███████   | 2756/3900 [09:26<03:48,  5.01it/s] 71%|███████   | 2757/3900 [09:26<03:47,  5.01it/s] 71%|███████   | 2758/3900 [09:27<03:47,  5.01it/s] 71%|███████   | 2759/3900 [09:27<03:47,  5.01it/s] 71%|███████   | 2760/3900 [09:27<03:47,  5.01it/s] 71%|███████   | 2761/3900 [09:27<03:47,  5.01it/s] 71%|███████   | 2762/3900 [09:27<03:47,  5.01it/s] 71%|███████   | 2763/3900 [09:28<03:46,  5.01it/s] 71%|███████   | 2764/3900 [09:28<03:46,  5.01it/s] 71%|███████   | 2765/3900 [09:28<03:46,  5.01it/s] 71%|███████   | 2766/3900 [09:28<03:46,  5.01it/s] 71%|███████   | 2767/3900 [09:28<03:46,  5.01it/s] 71%|███████   | 2768/3900 [09:29<03:45,  5.01it/s] 71%|███████   | 2769/3900 [09:29<03:43,  5.07it/s] 71%|███████   | 2770/3900 [09:29<03:43,  5.05it/s] 71%|███████   | 2771/3900 [09:29<03:44,  5.04it/s] 71%|███████   | 2772/3900 [09:29<03:44,  5.03it/s] 71%|███████   | 2773/3900 [09:30<03:44,  5.03it/s] 71%|███████   | 2774/3900 [09:30<03:43,  5.03it/s] 71%|███████   | 2775/3900 [09:30<03:43,  5.03it/s] 71%|███████   | 2776/3900 [09:30<03:43,  5.03it/s] 71%|███████   | 2777/3900 [09:30<03:43,  5.02it/s] 71%|███████   | 2778/3900 [09:31<03:43,  5.03it/s] 71%|███████▏  | 2779/3900 [09:31<03:42,  5.03it/s] 71%|███████▏  | 2780/3900 [09:31<03:42,  5.03it/s] 71%|███████▏  | 2781/3900 [09:31<03:42,  5.03it/s] 71%|███████▏  | 2782/3900 [09:31<03:42,  5.03it/s] 71%|███████▏  | 2783/3900 [09:32<03:42,  5.02it/s] 71%|███████▏  | 2784/3900 [09:32<03:42,  5.02it/s] 71%|███████▏  | 2785/3900 [09:32<03:42,  5.02it/s] 71%|███████▏  | 2786/3900 [09:32<03:41,  5.02it/s] 71%|███████▏  | 2787/3900 [09:32<03:41,  5.02it/s] 71%|███████▏  | 2788/3900 [09:33<03:41,  5.02it/s] 72%|███████▏  | 2789/3900 [09:33<03:41,  5.02it/s] 72%|███████▏  | 2790/3900 [09:33<03:40,  5.02it/s] 72%|███████▏  | 2791/3900 [09:33<03:40,  5.03it/s] 72%|███████▏  | 2792/3900 [09:33<03:40,  5.03it/s] 72%|███████▏  | 2793/3900 [09:34<03:40,  5.03it/s] 72%|███████▏  | 2794/3900 [09:34<03:39,  5.03it/s] 72%|███████▏  | 2795/3900 [09:34<03:39,  5.03it/s] 72%|███████▏  | 2796/3900 [09:34<03:39,  5.03it/s] 72%|███████▏  | 2797/3900 [09:34<03:39,  5.03it/s] 72%|███████▏  | 2798/3900 [09:35<03:39,  5.03it/s] 72%|███████▏  | 2799/3900 [09:35<03:38,  5.03it/s] 72%|███████▏  | 2800/3900 [09:35<03:38,  5.03it/s] 72%|███████▏  | 2801/3900 [09:35<03:38,  5.03it/s] 72%|███████▏  | 2802/3900 [09:35<03:38,  5.03it/s] 72%|███████▏  | 2803/3900 [09:36<03:38,  5.02it/s] 72%|███████▏  | 2804/3900 [09:36<03:38,  5.02it/s] 72%|███████▏  | 2805/3900 [09:36<03:38,  5.02it/s] 72%|███████▏  | 2806/3900 [09:36<03:37,  5.02it/s] 72%|███████▏  | 2807/3900 [09:36<03:37,  5.02it/s] 72%|███████▏  | 2808/3900 [09:37<03:35,  5.07it/s] 72%|███████▏  | 2809/3900 [09:37<03:35,  5.05it/s] 72%|███████▏  | 2810/3900 [09:37<03:36,  5.04it/s] 72%|███████▏  | 2811/3900 [09:37<03:36,  5.03it/s] 72%|███████▏  | 2812/3900 [09:37<03:36,  5.03it/s] 72%|███████▏  | 2813/3900 [09:38<03:36,  5.03it/s] 72%|███████▏  | 2814/3900 [09:38<03:36,  5.02it/s] 72%|███████▏  | 2815/3900 [09:38<03:36,  5.02it/s] 72%|███████▏  | 2816/3900 [09:38<03:35,  5.02it/s] 72%|███████▏  | 2817/3900 [09:38<03:35,  5.02it/s] 72%|███████▏  | 2818/3900 [09:39<03:35,  5.02it/s] 72%|███████▏  | 2819/3900 [09:39<03:35,  5.02it/s] 72%|███████▏  | 2820/3900 [09:39<03:35,  5.02it/s] 72%|███████▏  | 2821/3900 [09:39<03:34,  5.02it/s] 72%|███████▏  | 2822/3900 [09:39<03:34,  5.02it/s] 72%|███████▏  | 2823/3900 [09:40<03:34,  5.02it/s] 72%|███████▏  | 2824/3900 [09:40<03:34,  5.02it/s] 72%|███████▏  | 2825/3900 [09:40<03:34,  5.02it/s] 72%|███████▏  | 2826/3900 [09:40<03:33,  5.02it/s] 72%|███████▏  | 2827/3900 [09:40<03:33,  5.03it/s] 73%|███████▎  | 2828/3900 [09:41<03:33,  5.03it/s] 73%|███████▎  | 2829/3900 [09:41<03:33,  5.02it/s] 73%|███████▎  | 2830/3900 [09:41<03:33,  5.02it/s] 73%|███████▎  | 2831/3900 [09:41<03:33,  5.02it/s] 73%|███████▎  | 2832/3900 [09:41<03:32,  5.02it/s] 73%|███████▎  | 2833/3900 [09:42<03:32,  5.03it/s] 73%|███████▎  | 2834/3900 [09:42<03:32,  5.03it/s] 73%|███████▎  | 2835/3900 [09:42<03:32,  5.02it/s] 73%|███████▎  | 2836/3900 [09:42<03:31,  5.02it/s] 73%|███████▎  | 2837/3900 [09:42<03:31,  5.02it/s] 73%|███████▎  | 2838/3900 [09:43<03:31,  5.02it/s] 73%|███████▎  | 2839/3900 [09:43<03:31,  5.02it/s] 73%|███████▎  | 2840/3900 [09:43<03:31,  5.02it/s] 73%|███████▎  | 2841/3900 [09:43<03:31,  5.02it/s] 73%|███████▎  | 2842/3900 [09:43<03:30,  5.02it/s] 73%|███████▎  | 2843/3900 [09:43<03:30,  5.02it/s] 73%|███████▎  | 2844/3900 [09:44<03:30,  5.02it/s] 73%|███████▎  | 2845/3900 [09:44<03:29,  5.03it/s] 73%|███████▎  | 2846/3900 [09:44<03:29,  5.03it/s] 73%|███████▎  | 2847/3900 [09:44<03:26,  5.09it/s] 73%|███████▎  | 2848/3900 [09:44<03:27,  5.06it/s] 73%|███████▎  | 2849/3900 [09:45<03:27,  5.05it/s] 73%|███████▎  | 2850/3900 [09:45<03:28,  5.05it/s] 73%|███████▎  | 2851/3900 [09:45<03:28,  5.04it/s] 73%|███████▎  | 2852/3900 [09:45<03:28,  5.03it/s] 73%|███████▎  | 2853/3900 [09:45<03:28,  5.03it/s] 73%|███████▎  | 2854/3900 [09:46<03:27,  5.04it/s] 73%|███████▎  | 2855/3900 [09:46<03:27,  5.04it/s] 73%|███████▎  | 2856/3900 [09:46<03:27,  5.03it/s] 73%|███████▎  | 2857/3900 [09:46<03:27,  5.03it/s] 73%|███████▎  | 2858/3900 [09:46<03:27,  5.03it/s] 73%|███████▎  | 2859/3900 [09:47<03:26,  5.03it/s] 73%|███████▎  | 2860/3900 [09:47<03:26,  5.03it/s] 73%|███████▎  | 2861/3900 [09:47<03:26,  5.04it/s] 73%|███████▎  | 2862/3900 [09:47<03:26,  5.04it/s] 73%|███████▎  | 2863/3900 [09:47<03:25,  5.04it/s] 73%|███████▎  | 2864/3900 [09:48<03:25,  5.04it/s] 73%|███████▎  | 2865/3900 [09:48<03:25,  5.04it/s] 73%|███████▎  | 2866/3900 [09:48<03:25,  5.04it/s] 74%|███████▎  | 2867/3900 [09:48<03:24,  5.04it/s] 74%|███████▎  | 2868/3900 [09:48<03:24,  5.04it/s] 74%|███████▎  | 2869/3900 [09:49<03:24,  5.04it/s] 74%|███████▎  | 2870/3900 [09:49<03:24,  5.04it/s] 74%|███████▎  | 2871/3900 [09:49<03:24,  5.04it/s] 74%|███████▎  | 2872/3900 [09:49<03:23,  5.04it/s] 74%|███████▎  | 2873/3900 [09:49<03:23,  5.04it/s] 74%|███████▎  | 2874/3900 [09:50<03:23,  5.04it/s] 74%|███████▎  | 2875/3900 [09:50<03:23,  5.04it/s] 74%|███████▎  | 2876/3900 [09:50<03:23,  5.04it/s] 74%|███████▍  | 2877/3900 [09:50<03:22,  5.04it/s] 74%|███████▍  | 2878/3900 [09:50<03:22,  5.04it/s] 74%|███████▍  | 2879/3900 [09:51<03:22,  5.04it/s] 74%|███████▍  | 2880/3900 [09:51<03:22,  5.04it/s] 74%|███████▍  | 2881/3900 [09:51<03:22,  5.04it/s] 74%|███████▍  | 2882/3900 [09:51<03:22,  5.04it/s] 74%|███████▍  | 2883/3900 [09:51<03:21,  5.04it/s] 74%|███████▍  | 2884/3900 [09:52<03:21,  5.04it/s] 74%|███████▍  | 2885/3900 [09:52<03:21,  5.04it/s] 74%|███████▍  | 2886/3900 [09:52<03:18,  5.10it/s] 74%|███████▍  | 2887/3900 [09:52<03:32,  4.76it/s] 74%|███████▍  | 2888/3900 [09:52<03:29,  4.83it/s] 74%|███████▍  | 2889/3900 [09:53<03:26,  4.89it/s] 74%|███████▍  | 2890/3900 [09:53<03:24,  4.94it/s] 74%|███████▍  | 2891/3900 [09:53<03:23,  4.97it/s] 74%|███████▍  | 2892/3900 [09:53<03:22,  4.99it/s] 74%|███████▍  | 2893/3900 [09:53<03:21,  5.00it/s] 74%|███████▍  | 2894/3900 [09:54<03:20,  5.01it/s] 74%|███████▍  | 2895/3900 [09:54<03:20,  5.02it/s] 74%|███████▍  | 2896/3900 [09:54<03:19,  5.03it/s] 74%|███████▍  | 2897/3900 [09:54<03:19,  5.03it/s] 74%|███████▍  | 2898/3900 [09:54<03:19,  5.03it/s] 74%|███████▍  | 2899/3900 [09:55<03:19,  5.03it/s] 74%|███████▍  | 2900/3900 [09:55<03:18,  5.03it/s] 74%|███████▍  | 2901/3900 [09:55<03:18,  5.03it/s] 74%|███████▍  | 2902/3900 [09:55<03:18,  5.03it/s] 74%|███████▍  | 2903/3900 [09:55<03:17,  5.04it/s] 74%|███████▍  | 2904/3900 [09:56<03:17,  5.04it/s] 74%|███████▍  | 2905/3900 [09:56<03:17,  5.04it/s] 75%|███████▍  | 2906/3900 [09:56<03:17,  5.04it/s] 75%|███████▍  | 2907/3900 [09:56<03:16,  5.04it/s] 75%|███████▍  | 2908/3900 [09:56<03:16,  5.04it/s] 75%|███████▍  | 2909/3900 [09:57<03:16,  5.04it/s] 75%|███████▍  | 2910/3900 [09:57<03:16,  5.04it/s] 75%|███████▍  | 2911/3900 [09:57<03:16,  5.04it/s] 75%|███████▍  | 2912/3900 [09:57<03:15,  5.04it/s] 75%|███████▍  | 2913/3900 [09:57<03:15,  5.04it/s] 75%|███████▍  | 2914/3900 [09:58<03:15,  5.04it/s] 75%|███████▍  | 2915/3900 [09:58<03:15,  5.04it/s] 75%|███████▍  | 2916/3900 [09:58<03:15,  5.04it/s] 75%|███████▍  | 2917/3900 [09:58<03:15,  5.04it/s] 75%|███████▍  | 2918/3900 [09:58<03:14,  5.04it/s] 75%|███████▍  | 2919/3900 [09:59<03:14,  5.04it/s] 75%|███████▍  | 2920/3900 [09:59<03:14,  5.04it/s] 75%|███████▍  | 2921/3900 [09:59<03:14,  5.04it/s] 75%|███████▍  | 2922/3900 [09:59<03:14,  5.04it/s] 75%|███████▍  | 2923/3900 [09:59<03:13,  5.04it/s] 75%|███████▍  | 2924/3900 [10:00<03:13,  5.04it/s] 75%|███████▌  | 2925/3900 [10:00<03:11,  5.10it/s] 75%|███████▌  | 2926/3900 [10:00<03:11,  5.08it/s] 75%|███████▌  | 2927/3900 [10:00<03:12,  5.07it/s] 75%|███████▌  | 2928/3900 [10:00<03:12,  5.06it/s] 75%|███████▌  | 2929/3900 [10:01<03:12,  5.05it/s] 75%|███████▌  | 2930/3900 [10:01<03:12,  5.05it/s] 75%|███████▌  | 2931/3900 [10:01<03:12,  5.05it/s] 75%|███████▌  | 2932/3900 [10:01<03:11,  5.04it/s] 75%|███████▌  | 2933/3900 [10:01<03:11,  5.04it/s] 75%|███████▌  | 2934/3900 [10:02<03:11,  5.04it/s] 75%|███████▌  | 2935/3900 [10:02<03:11,  5.04it/s] 75%|███████▌  | 2936/3900 [10:02<03:11,  5.04it/s] 75%|███████▌  | 2937/3900 [10:02<03:11,  5.04it/s] 75%|███████▌  | 2938/3900 [10:02<03:10,  5.04it/s] 75%|███████▌  | 2939/3900 [10:03<03:10,  5.04it/s] 75%|███████▌  | 2940/3900 [10:03<03:10,  5.04it/s] 75%|███████▌  | 2941/3900 [10:03<03:10,  5.04it/s] 75%|███████▌  | 2942/3900 [10:03<03:10,  5.04it/s] 75%|███████▌  | 2943/3900 [10:03<03:09,  5.04it/s] 75%|███████▌  | 2944/3900 [10:04<03:09,  5.04it/s] 76%|███████▌  | 2945/3900 [10:04<03:09,  5.04it/s] 76%|███████▌  | 2946/3900 [10:04<03:09,  5.04it/s] 76%|███████▌  | 2947/3900 [10:04<03:09,  5.04it/s] 76%|███████▌  | 2948/3900 [10:04<03:08,  5.04it/s] 76%|███████▌  | 2949/3900 [10:05<03:08,  5.04it/s] 76%|███████▌  | 2950/3900 [10:05<03:08,  5.04it/s] 76%|███████▌  | 2951/3900 [10:05<03:21,  4.72it/s] 76%|███████▌  | 2952/3900 [10:05<03:17,  4.81it/s] 76%|███████▌  | 2953/3900 [10:05<03:14,  4.88it/s] 76%|███████▌  | 2954/3900 [10:06<03:12,  4.92it/s] 76%|███████▌  | 2955/3900 [10:06<03:10,  4.96it/s] 76%|███████▌  | 2956/3900 [10:06<03:09,  4.98it/s] 76%|███████▌  | 2957/3900 [10:06<03:08,  5.00it/s] 76%|███████▌  | 2958/3900 [10:06<03:07,  5.01it/s] 76%|███████▌  | 2959/3900 [10:07<03:07,  5.02it/s] 76%|███████▌  | 2960/3900 [10:07<03:07,  5.02it/s] 76%|███████▌  | 2961/3900 [10:07<03:06,  5.03it/s] 76%|███████▌  | 2962/3900 [10:07<03:06,  5.03it/s] 76%|███████▌  | 2963/3900 [10:07<03:06,  5.03it/s] 76%|███████▌  | 2964/3900 [10:08<03:03,  5.10it/s] 76%|███████▌  | 2965/3900 [10:08<03:04,  5.07it/s] 76%|███████▌  | 2966/3900 [10:08<03:04,  5.06it/s] 76%|███████▌  | 2967/3900 [10:08<03:04,  5.05it/s] 76%|███████▌  | 2968/3900 [10:08<03:04,  5.04it/s] 76%|███████▌  | 2969/3900 [10:09<03:04,  5.04it/s] 76%|███████▌  | 2970/3900 [10:09<03:04,  5.04it/s] 76%|███████▌  | 2971/3900 [10:09<03:04,  5.04it/s] 76%|███████▌  | 2972/3900 [10:09<03:04,  5.04it/s] 76%|███████▌  | 2973/3900 [10:09<03:03,  5.04it/s] 76%|███████▋  | 2974/3900 [10:10<03:03,  5.04it/s] 76%|███████▋  | 2975/3900 [10:10<03:03,  5.04it/s] 76%|███████▋  | 2976/3900 [10:10<03:03,  5.04it/s] 76%|███████▋  | 2977/3900 [10:10<03:03,  5.04it/s] 76%|███████▋  | 2978/3900 [10:10<03:03,  5.04it/s] 76%|███████▋  | 2979/3900 [10:11<03:02,  5.04it/s] 76%|███████▋  | 2980/3900 [10:11<03:02,  5.04it/s] 76%|███████▋  | 2981/3900 [10:11<03:02,  5.04it/s] 76%|███████▋  | 2982/3900 [10:11<03:02,  5.04it/s] 76%|███████▋  | 2983/3900 [10:11<03:02,  5.04it/s] 77%|███████▋  | 2984/3900 [10:12<03:01,  5.04it/s] 77%|███████▋  | 2985/3900 [10:12<03:01,  5.04it/s] 77%|███████▋  | 2986/3900 [10:12<03:01,  5.04it/s] 77%|███████▋  | 2987/3900 [10:12<03:01,  5.04it/s] 77%|███████▋  | 2988/3900 [10:12<03:01,  5.03it/s] 77%|███████▋  | 2989/3900 [10:13<03:01,  5.03it/s] 77%|███████▋  | 2990/3900 [10:13<03:01,  5.02it/s] 77%|███████▋  | 2991/3900 [10:13<03:00,  5.02it/s] 77%|███████▋  | 2992/3900 [10:13<03:00,  5.03it/s] 77%|███████▋  | 2993/3900 [10:13<03:00,  5.03it/s] 77%|███████▋  | 2994/3900 [10:14<02:59,  5.04it/s] 77%|███████▋  | 2995/3900 [10:14<02:59,  5.04it/s] 77%|███████▋  | 2996/3900 [10:14<02:59,  5.04it/s] 77%|███████▋  | 2997/3900 [10:14<02:59,  5.04it/s] 77%|███████▋  | 2998/3900 [10:14<02:59,  5.03it/s] 77%|███████▋  | 2999/3900 [10:15<02:58,  5.04it/s] 77%|███████▋  | 3000/3900 [10:15<02:58,  5.03it/s]                                                    77%|███████▋  | 3000/3900 [10:15<02:58,  5.03it/s][INFO|trainer.py:2409] 2022-08-27 22:17:59,975 >> Saving model checkpoint to tmp/baseline/RTE/bert-base-uncased/HPO_S0/1e-06/64/100/checkpoint-3000
[INFO|configuration_utils.py:446] 2022-08-27 22:17:59,976 >> Configuration saved in tmp/baseline/RTE/bert-base-uncased/HPO_S0/1e-06/64/100/checkpoint-3000/config.json
[INFO|modeling_utils.py:1602] 2022-08-27 22:18:00,860 >> Model weights saved in tmp/baseline/RTE/bert-base-uncased/HPO_S0/1e-06/64/100/checkpoint-3000/pytorch_model.bin
[INFO|tokenization_utils_base.py:2108] 2022-08-27 22:18:00,861 >> tokenizer config file saved in tmp/baseline/RTE/bert-base-uncased/HPO_S0/1e-06/64/100/checkpoint-3000/tokenizer_config.json
[INFO|tokenization_utils_base.py:2114] 2022-08-27 22:18:00,861 >> Special tokens file saved in tmp/baseline/RTE/bert-base-uncased/HPO_S0/1e-06/64/100/checkpoint-3000/special_tokens_map.json
/home/ubuntu/anaconda3/envs/transformers/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 77%|███████▋  | 3001/3900 [10:18<15:00,  1.00s/it] 77%|███████▋  | 3002/3900 [10:18<11:24,  1.31it/s] 77%|███████▋  | 3003/3900 [10:18<08:50,  1.69it/s] 77%|███████▋  | 3004/3900 [10:18<07:04,  2.11it/s] 77%|███████▋  | 3005/3900 [10:18<05:50,  2.56it/s] 77%|███████▋  | 3006/3900 [10:19<04:58,  3.00it/s] 77%|███████▋  | 3007/3900 [10:19<04:21,  3.41it/s] 77%|███████▋  | 3008/3900 [10:19<03:56,  3.78it/s] 77%|███████▋  | 3009/3900 [10:19<03:38,  4.08it/s] 77%|███████▋  | 3010/3900 [10:19<03:25,  4.33it/s] 77%|███████▋  | 3011/3900 [10:20<03:16,  4.52it/s] 77%|███████▋  | 3012/3900 [10:20<03:10,  4.66it/s] 77%|███████▋  | 3013/3900 [10:20<03:05,  4.77it/s] 77%|███████▋  | 3014/3900 [10:20<03:02,  4.85it/s] 77%|███████▋  | 3015/3900 [10:20<03:00,  4.90it/s] 77%|███████▋  | 3016/3900 [10:21<02:59,  4.93it/s] 77%|███████▋  | 3017/3900 [10:21<02:58,  4.95it/s] 77%|███████▋  | 3018/3900 [10:21<02:57,  4.98it/s] 77%|███████▋  | 3019/3900 [10:21<02:56,  4.99it/s] 77%|███████▋  | 3020/3900 [10:21<02:55,  5.00it/s] 77%|███████▋  | 3021/3900 [10:22<02:55,  5.01it/s] 77%|███████▋  | 3022/3900 [10:22<02:55,  5.01it/s] 78%|███████▊  | 3023/3900 [10:22<02:54,  5.02it/s] 78%|███████▊  | 3024/3900 [10:22<02:54,  5.02it/s] 78%|███████▊  | 3025/3900 [10:22<02:54,  5.02it/s] 78%|███████▊  | 3026/3900 [10:23<02:53,  5.02it/s] 78%|███████▊  | 3027/3900 [10:23<02:53,  5.03it/s] 78%|███████▊  | 3028/3900 [10:23<02:53,  5.02it/s] 78%|███████▊  | 3029/3900 [10:23<02:53,  5.03it/s] 78%|███████▊  | 3030/3900 [10:23<02:53,  5.03it/s] 78%|███████▊  | 3031/3900 [10:24<02:52,  5.03it/s] 78%|███████▊  | 3032/3900 [10:24<02:52,  5.03it/s] 78%|███████▊  | 3033/3900 [10:24<02:52,  5.03it/s] 78%|███████▊  | 3034/3900 [10:24<02:52,  5.03it/s] 78%|███████▊  | 3035/3900 [10:24<02:51,  5.03it/s] 78%|███████▊  | 3036/3900 [10:25<03:10,  4.55it/s] 78%|███████▊  | 3037/3900 [10:25<03:04,  4.68it/s] 78%|███████▊  | 3038/3900 [10:25<03:00,  4.77it/s] 78%|███████▊  | 3039/3900 [10:25<02:57,  4.84it/s] 78%|███████▊  | 3040/3900 [10:25<02:55,  4.89it/s] 78%|███████▊  | 3041/3900 [10:26<02:54,  4.92it/s] 78%|███████▊  | 3042/3900 [10:26<02:51,  5.01it/s] 78%|███████▊  | 3043/3900 [10:26<02:51,  5.01it/s] 78%|███████▊  | 3044/3900 [10:26<02:50,  5.01it/s] 78%|███████▊  | 3045/3900 [10:26<02:50,  5.02it/s] 78%|███████▊  | 3046/3900 [10:27<02:50,  5.01it/s] 78%|███████▊  | 3047/3900 [10:27<02:50,  5.02it/s] 78%|███████▊  | 3048/3900 [10:27<02:49,  5.01it/s] 78%|███████▊  | 3049/3900 [10:27<02:49,  5.02it/s] 78%|███████▊  | 3050/3900 [10:27<02:49,  5.02it/s] 78%|███████▊  | 3051/3900 [10:28<02:48,  5.02it/s] 78%|███████▊  | 3052/3900 [10:28<02:48,  5.03it/s] 78%|███████▊  | 3053/3900 [10:28<02:48,  5.03it/s] 78%|███████▊  | 3054/3900 [10:28<02:48,  5.03it/s] 78%|███████▊  | 3055/3900 [10:28<02:48,  5.03it/s] 78%|███████▊  | 3056/3900 [10:29<02:47,  5.03it/s] 78%|███████▊  | 3057/3900 [10:29<02:47,  5.03it/s] 78%|███████▊  | 3058/3900 [10:29<02:47,  5.03it/s] 78%|███████▊  | 3059/3900 [10:29<02:47,  5.02it/s] 78%|███████▊  | 3060/3900 [10:29<02:47,  5.02it/s] 78%|███████▊  | 3061/3900 [10:30<02:47,  5.02it/s] 79%|███████▊  | 3062/3900 [10:30<02:46,  5.02it/s] 79%|███████▊  | 3063/3900 [10:30<02:46,  5.02it/s] 79%|███████▊  | 3064/3900 [10:30<02:46,  5.03it/s] 79%|███████▊  | 3065/3900 [10:30<02:46,  5.03it/s] 79%|███████▊  | 3066/3900 [10:31<02:45,  5.03it/s] 79%|███████▊  | 3067/3900 [10:31<02:45,  5.03it/s] 79%|███████▊  | 3068/3900 [10:31<02:45,  5.03it/s] 79%|███████▊  | 3069/3900 [10:31<02:45,  5.03it/s] 79%|███████▊  | 3070/3900 [10:31<02:45,  5.03it/s] 79%|███████▊  | 3071/3900 [10:32<02:44,  5.03it/s] 79%|███████▉  | 3072/3900 [10:32<02:44,  5.03it/s] 79%|███████▉  | 3073/3900 [10:32<02:44,  5.03it/s] 79%|███████▉  | 3074/3900 [10:32<02:44,  5.03it/s] 79%|███████▉  | 3075/3900 [10:32<02:44,  5.03it/s] 79%|███████▉  | 3076/3900 [10:33<02:43,  5.03it/s] 79%|███████▉  | 3077/3900 [10:33<02:43,  5.03it/s] 79%|███████▉  | 3078/3900 [10:33<02:43,  5.03it/s] 79%|███████▉  | 3079/3900 [10:33<02:43,  5.03it/s] 79%|███████▉  | 3080/3900 [10:33<02:43,  5.02it/s] 79%|███████▉  | 3081/3900 [10:34<02:41,  5.08it/s] 79%|███████▉  | 3082/3900 [10:34<02:41,  5.06it/s] 79%|███████▉  | 3083/3900 [10:34<02:41,  5.05it/s] 79%|███████▉  | 3084/3900 [10:34<02:41,  5.04it/s] 79%|███████▉  | 3085/3900 [10:34<02:41,  5.04it/s] 79%|███████▉  | 3086/3900 [10:35<02:41,  5.03it/s] 79%|███████▉  | 3087/3900 [10:35<02:41,  5.03it/s] 79%|███████▉  | 3088/3900 [10:35<02:41,  5.02it/s] 79%|███████▉  | 3089/3900 [10:35<02:41,  5.02it/s] 79%|███████▉  | 3090/3900 [10:35<02:41,  5.01it/s] 79%|███████▉  | 3091/3900 [10:36<02:41,  5.01it/s] 79%|███████▉  | 3092/3900 [10:36<02:41,  5.00it/s] 79%|███████▉  | 3093/3900 [10:36<02:41,  5.00it/s] 79%|███████▉  | 3094/3900 [10:36<02:41,  5.00it/s] 79%|███████▉  | 3095/3900 [10:36<02:40,  5.00it/s] 79%|███████▉  | 3096/3900 [10:37<02:40,  5.00it/s] 79%|███████▉  | 3097/3900 [10:37<02:40,  5.00it/s] 79%|███████▉  | 3098/3900 [10:37<02:40,  5.01it/s] 79%|███████▉  | 3099/3900 [10:37<02:39,  5.01it/s] 79%|███████▉  | 3100/3900 [10:37<02:39,  5.01it/s] 80%|███████▉  | 3101/3900 [10:38<02:39,  5.01it/s] 80%|███████▉  | 3102/3900 [10:38<02:39,  5.01it/s] 80%|███████▉  | 3103/3900 [10:38<02:39,  5.01it/s] 80%|███████▉  | 3104/3900 [10:38<02:38,  5.01it/s] 80%|███████▉  | 3105/3900 [10:38<02:38,  5.02it/s] 80%|███████▉  | 3106/3900 [10:39<02:38,  5.02it/s] 80%|███████▉  | 3107/3900 [10:39<02:37,  5.02it/s] 80%|███████▉  | 3108/3900 [10:39<02:37,  5.03it/s] 80%|███████▉  | 3109/3900 [10:39<02:37,  5.03it/s] 80%|███████▉  | 3110/3900 [10:39<02:37,  5.03it/s] 80%|███████▉  | 3111/3900 [10:40<02:37,  5.02it/s] 80%|███████▉  | 3112/3900 [10:40<02:36,  5.02it/s] 80%|███████▉  | 3113/3900 [10:40<02:36,  5.02it/s] 80%|███████▉  | 3114/3900 [10:40<02:36,  5.03it/s] 80%|███████▉  | 3115/3900 [10:40<02:36,  5.03it/s] 80%|███████▉  | 3116/3900 [10:41<02:35,  5.03it/s] 80%|███████▉  | 3117/3900 [10:41<02:35,  5.03it/s] 80%|███████▉  | 3118/3900 [10:41<02:35,  5.03it/s] 80%|███████▉  | 3119/3900 [10:41<02:35,  5.03it/s] 80%|████████  | 3120/3900 [10:41<02:33,  5.09it/s] 80%|████████  | 3121/3900 [10:42<02:33,  5.06it/s] 80%|████████  | 3122/3900 [10:42<02:34,  5.05it/s] 80%|████████  | 3123/3900 [10:42<02:34,  5.05it/s] 80%|████████  | 3124/3900 [10:42<02:33,  5.04it/s] 80%|████████  | 3125/3900 [10:42<02:33,  5.03it/s] 80%|████████  | 3126/3900 [10:43<02:33,  5.03it/s] 80%|████████  | 3127/3900 [10:43<02:33,  5.03it/s] 80%|████████  | 3128/3900 [10:43<02:33,  5.03it/s] 80%|████████  | 3129/3900 [10:43<02:33,  5.03it/s] 80%|████████  | 3130/3900 [10:43<02:33,  5.03it/s] 80%|████████  | 3131/3900 [10:44<02:32,  5.03it/s] 80%|████████  | 3132/3900 [10:44<02:32,  5.03it/s] 80%|████████  | 3133/3900 [10:44<02:45,  4.63it/s] 80%|████████  | 3134/3900 [10:44<02:41,  4.74it/s] 80%|████████  | 3135/3900 [10:44<02:38,  4.82it/s] 80%|████████  | 3136/3900 [10:45<02:36,  4.88it/s] 80%|████████  | 3137/3900 [10:45<02:34,  4.92it/s] 80%|████████  | 3138/3900 [10:45<02:33,  4.95it/s] 80%|████████  | 3139/3900 [10:45<02:32,  4.98it/s] 81%|████████  | 3140/3900 [10:45<02:32,  4.99it/s] 81%|████████  | 3141/3900 [10:46<02:31,  5.00it/s] 81%|████████  | 3142/3900 [10:46<02:31,  5.01it/s] 81%|████████  | 3143/3900 [10:46<02:31,  5.01it/s] 81%|████████  | 3144/3900 [10:46<02:30,  5.01it/s] 81%|████████  | 3145/3900 [10:46<02:30,  5.01it/s] 81%|████████  | 3146/3900 [10:47<02:30,  5.02it/s] 81%|████████  | 3147/3900 [10:47<02:30,  5.02it/s] 81%|████████  | 3148/3900 [10:47<02:29,  5.02it/s] 81%|████████  | 3149/3900 [10:47<02:29,  5.02it/s] 81%|████████  | 3150/3900 [10:47<02:29,  5.02it/s] 81%|████████  | 3151/3900 [10:48<02:28,  5.03it/s] 81%|████████  | 3152/3900 [10:48<02:28,  5.03it/s] 81%|████████  | 3153/3900 [10:48<02:28,  5.03it/s] 81%|████████  | 3154/3900 [10:48<02:28,  5.03it/s] 81%|████████  | 3155/3900 [10:48<02:28,  5.03it/s] 81%|████████  | 3156/3900 [10:49<02:27,  5.03it/s] 81%|████████  | 3157/3900 [10:49<02:27,  5.03it/s] 81%|████████  | 3158/3900 [10:49<02:27,  5.03it/s] 81%|████████  | 3159/3900 [10:49<02:25,  5.09it/s] 81%|████████  | 3160/3900 [10:49<02:26,  5.06it/s] 81%|████████  | 3161/3900 [10:50<02:26,  5.05it/s] 81%|████████  | 3162/3900 [10:50<02:26,  5.04it/s] 81%|████████  | 3163/3900 [10:50<02:26,  5.04it/s] 81%|████████  | 3164/3900 [10:50<02:26,  5.04it/s] 81%|████████  | 3165/3900 [10:50<02:26,  5.03it/s] 81%|████████  | 3166/3900 [10:51<02:25,  5.03it/s] 81%|████████  | 3167/3900 [10:51<02:25,  5.03it/s] 81%|████████  | 3168/3900 [10:51<02:25,  5.03it/s] 81%|████████▏ | 3169/3900 [10:51<02:25,  5.03it/s] 81%|████████▏ | 3170/3900 [10:51<02:25,  5.03it/s] 81%|████████▏ | 3171/3900 [10:52<02:25,  5.03it/s] 81%|████████▏ | 3172/3900 [10:52<02:24,  5.03it/s] 81%|████████▏ | 3173/3900 [10:52<02:24,  5.03it/s] 81%|████████▏ | 3174/3900 [10:52<02:24,  5.03it/s] 81%|████████▏ | 3175/3900 [10:52<02:24,  5.03it/s] 81%|████████▏ | 3176/3900 [10:53<02:24,  5.03it/s] 81%|████████▏ | 3177/3900 [10:53<02:23,  5.03it/s] 81%|████████▏ | 3178/3900 [10:53<02:23,  5.03it/s] 82%|████████▏ | 3179/3900 [10:53<02:24,  4.99it/s] 82%|████████▏ | 3180/3900 [10:53<02:24,  5.00it/s] 82%|████████▏ | 3181/3900 [10:54<02:23,  5.01it/s] 82%|████████▏ | 3182/3900 [10:54<02:23,  5.01it/s] 82%|████████▏ | 3183/3900 [10:54<02:22,  5.02it/s] 82%|████████▏ | 3184/3900 [10:54<02:22,  5.02it/s] 82%|████████▏ | 3185/3900 [10:54<02:22,  5.02it/s] 82%|████████▏ | 3186/3900 [10:55<02:32,  4.70it/s] 82%|████████▏ | 3187/3900 [10:55<02:28,  4.79it/s] 82%|████████▏ | 3188/3900 [10:55<02:26,  4.86it/s] 82%|████████▏ | 3189/3900 [10:55<02:24,  4.91it/s] 82%|████████▏ | 3190/3900 [10:55<02:23,  4.94it/s] 82%|████████▏ | 3191/3900 [10:56<02:22,  4.97it/s] 82%|████████▏ | 3192/3900 [10:56<02:21,  4.99it/s] 82%|████████▏ | 3193/3900 [10:56<02:21,  5.00it/s] 82%|████████▏ | 3194/3900 [10:56<02:21,  5.01it/s] 82%|████████▏ | 3195/3900 [10:56<02:20,  5.01it/s] 82%|████████▏ | 3196/3900 [10:57<02:20,  5.02it/s] 82%|████████▏ | 3197/3900 [10:57<02:20,  5.02it/s] 82%|████████▏ | 3198/3900 [10:57<02:18,  5.08it/s] 82%|████████▏ | 3199/3900 [10:57<02:18,  5.06it/s] 82%|████████▏ | 3200/3900 [10:57<02:18,  5.05it/s] 82%|████████▏ | 3201/3900 [10:58<02:18,  5.04it/s] 82%|████████▏ | 3202/3900 [10:58<02:18,  5.04it/s] 82%|████████▏ | 3203/3900 [10:58<02:18,  5.03it/s] 82%|████████▏ | 3204/3900 [10:58<02:18,  5.03it/s] 82%|████████▏ | 3205/3900 [10:58<02:18,  5.03it/s] 82%|████████▏ | 3206/3900 [10:59<02:17,  5.03it/s] 82%|████████▏ | 3207/3900 [10:59<02:17,  5.03it/s] 82%|████████▏ | 3208/3900 [10:59<02:17,  5.03it/s] 82%|████████▏ | 3209/3900 [10:59<02:17,  5.03it/s] 82%|████████▏ | 3210/3900 [10:59<02:17,  5.03it/s] 82%|████████▏ | 3211/3900 [11:00<02:17,  5.03it/s] 82%|████████▏ | 3212/3900 [11:00<02:16,  5.02it/s] 82%|████████▏ | 3213/3900 [11:00<02:16,  5.02it/s] 82%|████████▏ | 3214/3900 [11:00<02:16,  5.03it/s] 82%|████████▏ | 3215/3900 [11:00<02:16,  5.03it/s] 82%|████████▏ | 3216/3900 [11:01<02:16,  5.02it/s] 82%|████████▏ | 3217/3900 [11:01<02:15,  5.03it/s] 83%|████████▎ | 3218/3900 [11:01<02:15,  5.02it/s] 83%|████████▎ | 3219/3900 [11:01<02:15,  5.03it/s] 83%|████████▎ | 3220/3900 [11:01<02:15,  5.03it/s] 83%|████████▎ | 3221/3900 [11:02<02:15,  5.03it/s] 83%|████████▎ | 3222/3900 [11:02<02:14,  5.03it/s] 83%|████████▎ | 3223/3900 [11:02<02:14,  5.03it/s] 83%|████████▎ | 3224/3900 [11:02<02:14,  5.03it/s] 83%|████████▎ | 3225/3900 [11:02<02:14,  5.02it/s] 83%|████████▎ | 3226/3900 [11:03<02:14,  5.03it/s] 83%|████████▎ | 3227/3900 [11:03<02:13,  5.03it/s] 83%|████████▎ | 3228/3900 [11:03<02:13,  5.03it/s] 83%|████████▎ | 3229/3900 [11:03<02:13,  5.03it/s] 83%|████████▎ | 3230/3900 [11:03<02:13,  5.03it/s] 83%|████████▎ | 3231/3900 [11:04<02:12,  5.03it/s] 83%|████████▎ | 3232/3900 [11:04<02:12,  5.03it/s] 83%|████████▎ | 3233/3900 [11:04<02:12,  5.03it/s] 83%|████████▎ | 3234/3900 [11:04<02:12,  5.03it/s] 83%|████████▎ | 3235/3900 [11:04<02:12,  5.03it/s] 83%|████████▎ | 3236/3900 [11:05<02:11,  5.03it/s] 83%|████████▎ | 3237/3900 [11:05<02:10,  5.09it/s] 83%|████████▎ | 3238/3900 [11:05<02:10,  5.07it/s] 83%|████████▎ | 3239/3900 [11:05<02:10,  5.05it/s] 83%|████████▎ | 3240/3900 [11:05<02:10,  5.04it/s] 83%|████████▎ | 3241/3900 [11:06<02:10,  5.04it/s] 83%|████████▎ | 3242/3900 [11:06<02:10,  5.04it/s] 83%|████████▎ | 3243/3900 [11:06<02:10,  5.03it/s] 83%|████████▎ | 3244/3900 [11:06<02:10,  5.03it/s] 83%|████████▎ | 3245/3900 [11:06<02:10,  5.03it/s] 83%|████████▎ | 3246/3900 [11:06<02:10,  5.03it/s] 83%|████████▎ | 3247/3900 [11:07<02:09,  5.03it/s] 83%|████████▎ | 3248/3900 [11:07<02:09,  5.03it/s] 83%|████████▎ | 3249/3900 [11:07<02:09,  5.03it/s] 83%|████████▎ | 3250/3900 [11:07<02:09,  5.03it/s] 83%|████████▎ | 3251/3900 [11:07<02:08,  5.04it/s] 83%|████████▎ | 3252/3900 [11:08<02:08,  5.03it/s] 83%|████████▎ | 3253/3900 [11:08<02:08,  5.03it/s] 83%|████████▎ | 3254/3900 [11:08<02:08,  5.03it/s] 83%|████████▎ | 3255/3900 [11:08<02:08,  5.03it/s] 83%|████████▎ | 3256/3900 [11:08<02:07,  5.04it/s] 84%|████████▎ | 3257/3900 [11:09<02:07,  5.04it/s] 84%|████████▎ | 3258/3900 [11:09<02:07,  5.03it/s] 84%|████████▎ | 3259/3900 [11:09<02:07,  5.03it/s] 84%|████████▎ | 3260/3900 [11:09<02:07,  5.04it/s] 84%|████████▎ | 3261/3900 [11:09<02:06,  5.04it/s] 84%|████████▎ | 3262/3900 [11:10<02:06,  5.04it/s] 84%|████████▎ | 3263/3900 [11:10<02:06,  5.04it/s] 84%|████████▎ | 3264/3900 [11:10<02:06,  5.04it/s] 84%|████████▎ | 3265/3900 [11:10<02:06,  5.04it/s] 84%|████████▎ | 3266/3900 [11:10<02:05,  5.03it/s] 84%|████████▍ | 3267/3900 [11:11<02:05,  5.04it/s] 84%|████████▍ | 3268/3900 [11:11<02:05,  5.04it/s] 84%|████████▍ | 3269/3900 [11:11<02:05,  5.04it/s] 84%|████████▍ | 3270/3900 [11:11<02:05,  5.04it/s] 84%|████████▍ | 3271/3900 [11:11<02:04,  5.04it/s] 84%|████████▍ | 3272/3900 [11:12<02:04,  5.03it/s] 84%|████████▍ | 3273/3900 [11:12<02:04,  5.03it/s] 84%|████████▍ | 3274/3900 [11:12<02:04,  5.03it/s] 84%|████████▍ | 3275/3900 [11:12<02:04,  5.02it/s] 84%|████████▍ | 3276/3900 [11:12<02:02,  5.08it/s] 84%|████████▍ | 3277/3900 [11:13<02:03,  5.06it/s] 84%|████████▍ | 3278/3900 [11:13<02:03,  5.05it/s] 84%|████████▍ | 3279/3900 [11:13<02:03,  5.04it/s] 84%|████████▍ | 3280/3900 [11:13<02:03,  5.03it/s] 84%|████████▍ | 3281/3900 [11:13<02:03,  5.03it/s] 84%|████████▍ | 3282/3900 [11:14<02:02,  5.03it/s] 84%|████████▍ | 3283/3900 [11:14<02:11,  4.70it/s] 84%|████████▍ | 3284/3900 [11:14<02:08,  4.79it/s] 84%|████████▍ | 3285/3900 [11:14<02:06,  4.86it/s] 84%|████████▍ | 3286/3900 [11:14<02:05,  4.90it/s] 84%|████████▍ | 3287/3900 [11:15<02:04,  4.94it/s] 84%|████████▍ | 3288/3900 [11:15<02:03,  4.96it/s] 84%|████████▍ | 3289/3900 [11:15<02:02,  4.98it/s] 84%|████████▍ | 3290/3900 [11:15<02:02,  4.99it/s] 84%|████████▍ | 3291/3900 [11:15<02:01,  5.00it/s] 84%|████████▍ | 3292/3900 [11:16<02:01,  5.00it/s] 84%|████████▍ | 3293/3900 [11:16<02:01,  5.01it/s] 84%|████████▍ | 3294/3900 [11:16<02:01,  5.01it/s] 84%|████████▍ | 3295/3900 [11:16<02:00,  5.01it/s] 85%|████████▍ | 3296/3900 [11:16<02:00,  5.00it/s] 85%|████████▍ | 3297/3900 [11:17<02:00,  5.01it/s] 85%|████████▍ | 3298/3900 [11:17<02:00,  5.01it/s] 85%|████████▍ | 3299/3900 [11:17<01:59,  5.01it/s] 85%|████████▍ | 3300/3900 [11:17<01:59,  5.01it/s] 85%|████████▍ | 3301/3900 [11:17<01:59,  5.02it/s] 85%|████████▍ | 3302/3900 [11:18<01:59,  5.02it/s] 85%|████████▍ | 3303/3900 [11:18<01:58,  5.02it/s] 85%|████████▍ | 3304/3900 [11:18<01:58,  5.02it/s] 85%|████████▍ | 3305/3900 [11:18<01:58,  5.02it/s] 85%|████████▍ | 3306/3900 [11:18<01:58,  5.02it/s] 85%|████████▍ | 3307/3900 [11:19<01:58,  5.02it/s] 85%|████████▍ | 3308/3900 [11:19<01:57,  5.02it/s] 85%|████████▍ | 3309/3900 [11:19<01:57,  5.02it/s] 85%|████████▍ | 3310/3900 [11:19<01:57,  5.02it/s] 85%|████████▍ | 3311/3900 [11:19<01:57,  5.03it/s] 85%|████████▍ | 3312/3900 [11:20<01:56,  5.03it/s] 85%|████████▍ | 3313/3900 [11:20<01:56,  5.03it/s] 85%|████████▍ | 3314/3900 [11:20<01:56,  5.03it/s] 85%|████████▌ | 3315/3900 [11:20<01:54,  5.09it/s] 85%|████████▌ | 3316/3900 [11:20<01:55,  5.06it/s] 85%|████████▌ | 3317/3900 [11:21<01:55,  5.05it/s] 85%|████████▌ | 3318/3900 [11:21<01:55,  5.05it/s] 85%|████████▌ | 3319/3900 [11:21<01:55,  5.04it/s] 85%|████████▌ | 3320/3900 [11:21<01:55,  5.03it/s] 85%|████████▌ | 3321/3900 [11:21<01:55,  5.03it/s] 85%|████████▌ | 3322/3900 [11:22<01:54,  5.03it/s] 85%|████████▌ | 3323/3900 [11:22<01:54,  5.03it/s] 85%|████████▌ | 3324/3900 [11:22<01:54,  5.03it/s] 85%|████████▌ | 3325/3900 [11:22<01:54,  5.03it/s] 85%|████████▌ | 3326/3900 [11:22<01:54,  5.03it/s] 85%|████████▌ | 3327/3900 [11:23<01:53,  5.03it/s] 85%|████████▌ | 3328/3900 [11:23<01:53,  5.03it/s] 85%|████████▌ | 3329/3900 [11:23<01:53,  5.03it/s] 85%|████████▌ | 3330/3900 [11:23<01:53,  5.03it/s] 85%|████████▌ | 3331/3900 [11:23<01:53,  5.03it/s] 85%|████████▌ | 3332/3900 [11:24<01:52,  5.03it/s] 85%|████████▌ | 3333/3900 [11:24<01:52,  5.03it/s] 85%|████████▌ | 3334/3900 [11:24<01:52,  5.03it/s] 86%|████████▌ | 3335/3900 [11:24<01:52,  5.03it/s] 86%|████████▌ | 3336/3900 [11:24<02:00,  4.67it/s] 86%|████████▌ | 3337/3900 [11:25<01:58,  4.77it/s] 86%|████████▌ | 3338/3900 [11:25<01:56,  4.84it/s] 86%|████████▌ | 3339/3900 [11:25<01:54,  4.89it/s] 86%|████████▌ | 3340/3900 [11:25<01:53,  4.93it/s] 86%|████████▌ | 3341/3900 [11:25<01:52,  4.96it/s] 86%|████████▌ | 3342/3900 [11:26<01:52,  4.98it/s] 86%|████████▌ | 3343/3900 [11:26<01:51,  4.99it/s] 86%|████████▌ | 3344/3900 [11:26<01:51,  5.00it/s] 86%|████████▌ | 3345/3900 [11:26<01:50,  5.01it/s] 86%|████████▌ | 3346/3900 [11:26<01:50,  5.01it/s] 86%|████████▌ | 3347/3900 [11:27<01:50,  5.01it/s] 86%|████████▌ | 3348/3900 [11:27<01:50,  5.02it/s] 86%|████████▌ | 3349/3900 [11:27<01:49,  5.02it/s] 86%|████████▌ | 3350/3900 [11:27<01:49,  5.03it/s] 86%|████████▌ | 3351/3900 [11:27<01:49,  5.02it/s] 86%|████████▌ | 3352/3900 [11:28<01:49,  5.02it/s] 86%|████████▌ | 3353/3900 [11:28<01:48,  5.02it/s] 86%|████████▌ | 3354/3900 [11:28<01:47,  5.08it/s] 86%|████████▌ | 3355/3900 [11:28<01:47,  5.06it/s] 86%|████████▌ | 3356/3900 [11:28<01:47,  5.05it/s] 86%|████████▌ | 3357/3900 [11:29<01:47,  5.04it/s] 86%|████████▌ | 3358/3900 [11:29<01:47,  5.04it/s] 86%|████████▌ | 3359/3900 [11:29<01:47,  5.04it/s] 86%|████████▌ | 3360/3900 [11:29<01:47,  5.04it/s] 86%|████████▌ | 3361/3900 [11:29<01:47,  5.04it/s] 86%|████████▌ | 3362/3900 [11:30<01:46,  5.03it/s] 86%|████████▌ | 3363/3900 [11:30<01:46,  5.03it/s] 86%|████████▋ | 3364/3900 [11:30<01:46,  5.03it/s] 86%|████████▋ | 3365/3900 [11:30<01:46,  5.03it/s] 86%|████████▋ | 3366/3900 [11:30<01:46,  5.03it/s] 86%|████████▋ | 3367/3900 [11:31<01:46,  5.03it/s] 86%|████████▋ | 3368/3900 [11:31<01:45,  5.03it/s] 86%|████████▋ | 3369/3900 [11:31<01:45,  5.03it/s] 86%|████████▋ | 3370/3900 [11:31<01:45,  5.03it/s] 86%|████████▋ | 3371/3900 [11:31<01:45,  5.03it/s] 86%|████████▋ | 3372/3900 [11:32<01:44,  5.03it/s] 86%|████████▋ | 3373/3900 [11:32<01:44,  5.03it/s] 87%|████████▋ | 3374/3900 [11:32<01:44,  5.03it/s] 87%|████████▋ | 3375/3900 [11:32<01:44,  5.03it/s] 87%|████████▋ | 3376/3900 [11:32<01:44,  5.03it/s] 87%|████████▋ | 3377/3900 [11:33<01:44,  5.03it/s] 87%|████████▋ | 3378/3900 [11:33<01:43,  5.03it/s] 87%|████████▋ | 3379/3900 [11:33<01:43,  5.03it/s] 87%|████████▋ | 3380/3900 [11:33<01:43,  5.03it/s] 87%|████████▋ | 3381/3900 [11:33<01:43,  5.03it/s] 87%|████████▋ | 3382/3900 [11:34<01:42,  5.03it/s] 87%|████████▋ | 3383/3900 [11:34<01:42,  5.03it/s] 87%|████████▋ | 3384/3900 [11:34<01:42,  5.03it/s] 87%|████████▋ | 3385/3900 [11:34<01:42,  5.03it/s] 87%|████████▋ | 3386/3900 [11:34<01:42,  5.04it/s] 87%|████████▋ | 3387/3900 [11:35<01:41,  5.04it/s] 87%|████████▋ | 3388/3900 [11:35<01:41,  5.04it/s] 87%|████████▋ | 3389/3900 [11:35<01:41,  5.04it/s] 87%|████████▋ | 3390/3900 [11:35<01:41,  5.04it/s] 87%|████████▋ | 3391/3900 [11:35<01:41,  5.03it/s] 87%|████████▋ | 3392/3900 [11:36<01:40,  5.04it/s] 87%|████████▋ | 3393/3900 [11:36<01:39,  5.09it/s] 87%|████████▋ | 3394/3900 [11:36<01:39,  5.07it/s] 87%|████████▋ | 3395/3900 [11:36<01:39,  5.06it/s] 87%|████████▋ | 3396/3900 [11:36<01:39,  5.05it/s] 87%|████████▋ | 3397/3900 [11:37<01:39,  5.05it/s] 87%|████████▋ | 3398/3900 [11:37<01:39,  5.04it/s] 87%|████████▋ | 3399/3900 [11:37<01:39,  5.04it/s] 87%|████████▋ | 3400/3900 [11:37<01:39,  5.04it/s] 87%|████████▋ | 3401/3900 [11:37<01:38,  5.04it/s] 87%|████████▋ | 3402/3900 [11:38<01:38,  5.04it/s] 87%|████████▋ | 3403/3900 [11:38<01:38,  5.04it/s] 87%|████████▋ | 3404/3900 [11:38<01:38,  5.05it/s] 87%|████████▋ | 3405/3900 [11:38<01:38,  5.05it/s] 87%|████████▋ | 3406/3900 [11:38<01:37,  5.04it/s] 87%|████████▋ | 3407/3900 [11:39<01:37,  5.04it/s] 87%|████████▋ | 3408/3900 [11:39<01:37,  5.04it/s] 87%|████████▋ | 3409/3900 [11:39<01:37,  5.04it/s] 87%|████████▋ | 3410/3900 [11:39<01:37,  5.04it/s] 87%|████████▋ | 3411/3900 [11:39<01:37,  5.03it/s] 87%|████████▋ | 3412/3900 [11:40<01:36,  5.04it/s] 88%|████████▊ | 3413/3900 [11:40<01:36,  5.04it/s] 88%|████████▊ | 3414/3900 [11:40<01:36,  5.04it/s] 88%|████████▊ | 3415/3900 [11:40<01:36,  5.04it/s] 88%|████████▊ | 3416/3900 [11:40<01:36,  5.04it/s] 88%|████████▊ | 3417/3900 [11:41<01:35,  5.04it/s] 88%|████████▊ | 3418/3900 [11:41<01:35,  5.04it/s] 88%|████████▊ | 3419/3900 [11:41<01:35,  5.04it/s] 88%|████████▊ | 3420/3900 [11:41<01:35,  5.04it/s] 88%|████████▊ | 3421/3900 [11:41<01:35,  5.04it/s] 88%|████████▊ | 3422/3900 [11:42<01:34,  5.04it/s] 88%|████████▊ | 3423/3900 [11:42<01:34,  5.04it/s] 88%|████████▊ | 3424/3900 [11:42<01:34,  5.04it/s] 88%|████████▊ | 3425/3900 [11:42<01:34,  5.04it/s] 88%|████████▊ | 3426/3900 [11:42<01:34,  5.04it/s] 88%|████████▊ | 3427/3900 [11:43<01:33,  5.03it/s] 88%|████████▊ | 3428/3900 [11:43<01:33,  5.03it/s] 88%|████████▊ | 3429/3900 [11:43<01:33,  5.03it/s] 88%|████████▊ | 3430/3900 [11:43<01:33,  5.02it/s] 88%|████████▊ | 3431/3900 [11:43<01:33,  5.03it/s] 88%|████████▊ | 3432/3900 [11:44<01:31,  5.09it/s] 88%|████████▊ | 3433/3900 [11:44<01:38,  4.74it/s] 88%|████████▊ | 3434/3900 [11:44<01:36,  4.82it/s] 88%|████████▊ | 3435/3900 [11:44<01:35,  4.88it/s] 88%|████████▊ | 3436/3900 [11:44<01:34,  4.92it/s] 88%|████████▊ | 3437/3900 [11:45<01:33,  4.95it/s] 88%|████████▊ | 3438/3900 [11:45<01:32,  4.98it/s] 88%|████████▊ | 3439/3900 [11:45<01:32,  4.99it/s] 88%|████████▊ | 3440/3900 [11:45<01:31,  5.01it/s] 88%|████████▊ | 3441/3900 [11:45<01:31,  5.01it/s] 88%|████████▊ | 3442/3900 [11:46<01:31,  5.02it/s] 88%|████████▊ | 3443/3900 [11:46<01:31,  5.02it/s] 88%|████████▊ | 3444/3900 [11:46<01:30,  5.02it/s] 88%|████████▊ | 3445/3900 [11:46<01:30,  5.03it/s] 88%|████████▊ | 3446/3900 [11:46<01:30,  5.03it/s] 88%|████████▊ | 3447/3900 [11:47<01:30,  5.03it/s] 88%|████████▊ | 3448/3900 [11:47<01:29,  5.03it/s] 88%|████████▊ | 3449/3900 [11:47<01:29,  5.04it/s] 88%|████████▊ | 3450/3900 [11:47<01:29,  5.04it/s] 88%|████████▊ | 3451/3900 [11:47<01:29,  5.04it/s] 89%|████████▊ | 3452/3900 [11:48<01:28,  5.04it/s] 89%|████████▊ | 3453/3900 [11:48<01:28,  5.04it/s] 89%|████████▊ | 3454/3900 [11:48<01:28,  5.04it/s] 89%|████████▊ | 3455/3900 [11:48<01:28,  5.04it/s] 89%|████████▊ | 3456/3900 [11:48<01:28,  5.04it/s] 89%|████████▊ | 3457/3900 [11:49<01:27,  5.04it/s] 89%|████████▊ | 3458/3900 [11:49<01:27,  5.04it/s] 89%|████████▊ | 3459/3900 [11:49<01:27,  5.04it/s] 89%|████████▊ | 3460/3900 [11:49<01:27,  5.04it/s] 89%|████████▊ | 3461/3900 [11:49<01:27,  5.04it/s] 89%|████████▉ | 3462/3900 [11:50<01:26,  5.04it/s] 89%|████████▉ | 3463/3900 [11:50<01:26,  5.04it/s] 89%|████████▉ | 3464/3900 [11:50<01:26,  5.04it/s] 89%|████████▉ | 3465/3900 [11:50<01:26,  5.04it/s] 89%|████████▉ | 3466/3900 [11:50<01:26,  5.04it/s] 89%|████████▉ | 3467/3900 [11:51<01:26,  5.03it/s] 89%|████████▉ | 3468/3900 [11:51<01:25,  5.03it/s] 89%|████████▉ | 3469/3900 [11:51<01:25,  5.03it/s] 89%|████████▉ | 3470/3900 [11:51<01:25,  5.03it/s] 89%|████████▉ | 3471/3900 [11:51<01:24,  5.09it/s] 89%|████████▉ | 3472/3900 [11:52<01:24,  5.07it/s] 89%|████████▉ | 3473/3900 [11:52<01:24,  5.06it/s] 89%|████████▉ | 3474/3900 [11:52<01:24,  5.05it/s] 89%|████████▉ | 3475/3900 [11:52<01:24,  5.05it/s] 89%|████████▉ | 3476/3900 [11:52<01:23,  5.05it/s] 89%|████████▉ | 3477/3900 [11:53<01:23,  5.05it/s] 89%|████████▉ | 3478/3900 [11:53<01:23,  5.05it/s] 89%|████████▉ | 3479/3900 [11:53<01:23,  5.04it/s] 89%|████████▉ | 3480/3900 [11:53<01:23,  5.04it/s] 89%|████████▉ | 3481/3900 [11:53<01:23,  5.04it/s] 89%|████████▉ | 3482/3900 [11:54<01:22,  5.04it/s] 89%|████████▉ | 3483/3900 [11:54<01:22,  5.04it/s] 89%|████████▉ | 3484/3900 [11:54<01:22,  5.04it/s] 89%|████████▉ | 3485/3900 [11:54<01:22,  5.04it/s] 89%|████████▉ | 3486/3900 [11:54<01:28,  4.69it/s] 89%|████████▉ | 3487/3900 [11:55<01:26,  4.79it/s] 89%|████████▉ | 3488/3900 [11:55<01:24,  4.86it/s] 89%|████████▉ | 3489/3900 [11:55<01:23,  4.91it/s] 89%|████████▉ | 3490/3900 [11:55<01:22,  4.95it/s] 90%|████████▉ | 3491/3900 [11:55<01:22,  4.97it/s] 90%|████████▉ | 3492/3900 [11:56<01:21,  4.99it/s] 90%|████████▉ | 3493/3900 [11:56<01:21,  5.00it/s] 90%|████████▉ | 3494/3900 [11:56<01:21,  5.01it/s] 90%|████████▉ | 3495/3900 [11:56<01:20,  5.01it/s] 90%|████████▉ | 3496/3900 [11:56<01:20,  5.02it/s] 90%|████████▉ | 3497/3900 [11:57<01:20,  5.02it/s] 90%|████████▉ | 3498/3900 [11:57<01:20,  5.02it/s] 90%|████████▉ | 3499/3900 [11:57<01:19,  5.03it/s] 90%|████████▉ | 3500/3900 [11:57<01:19,  5.03it/s]                                                    90%|████████▉ | 3500/3900 [11:57<01:19,  5.03it/s][INFO|trainer.py:2409] 2022-08-27 22:19:42,383 >> Saving model checkpoint to tmp/baseline/RTE/bert-base-uncased/HPO_S0/1e-06/64/100/checkpoint-3500
[INFO|configuration_utils.py:446] 2022-08-27 22:19:42,384 >> Configuration saved in tmp/baseline/RTE/bert-base-uncased/HPO_S0/1e-06/64/100/checkpoint-3500/config.json
[INFO|modeling_utils.py:1602] 2022-08-27 22:19:43,234 >> Model weights saved in tmp/baseline/RTE/bert-base-uncased/HPO_S0/1e-06/64/100/checkpoint-3500/pytorch_model.bin
[INFO|tokenization_utils_base.py:2108] 2022-08-27 22:19:43,235 >> tokenizer config file saved in tmp/baseline/RTE/bert-base-uncased/HPO_S0/1e-06/64/100/checkpoint-3500/tokenizer_config.json
[INFO|tokenization_utils_base.py:2114] 2022-08-27 22:19:43,235 >> Special tokens file saved in tmp/baseline/RTE/bert-base-uncased/HPO_S0/1e-06/64/100/checkpoint-3500/special_tokens_map.json
/home/ubuntu/anaconda3/envs/transformers/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 90%|████████▉ | 3501/3900 [12:00<06:32,  1.02it/s] 90%|████████▉ | 3502/3900 [12:00<04:57,  1.34it/s] 90%|████████▉ | 3503/3900 [12:00<03:51,  1.71it/s] 90%|████████▉ | 3504/3900 [12:01<03:05,  2.14it/s] 90%|████████▉ | 3505/3900 [12:01<02:32,  2.58it/s] 90%|████████▉ | 3506/3900 [12:01<02:10,  3.03it/s] 90%|████████▉ | 3507/3900 [12:01<01:54,  3.44it/s] 90%|████████▉ | 3508/3900 [12:01<01:43,  3.80it/s] 90%|████████▉ | 3509/3900 [12:02<01:35,  4.10it/s] 90%|█████████ | 3510/3900 [12:02<01:28,  4.39it/s] 90%|█████████ | 3511/3900 [12:02<01:25,  4.56it/s] 90%|█████████ | 3512/3900 [12:02<01:22,  4.70it/s] 90%|█████████ | 3513/3900 [12:02<01:20,  4.79it/s] 90%|█████████ | 3514/3900 [12:03<01:19,  4.86it/s] 90%|█████████ | 3515/3900 [12:03<01:18,  4.92it/s] 90%|█████████ | 3516/3900 [12:03<01:17,  4.95it/s] 90%|█████████ | 3517/3900 [12:03<01:16,  4.98it/s] 90%|█████████ | 3518/3900 [12:03<01:16,  5.00it/s] 90%|█████████ | 3519/3900 [12:04<01:16,  5.01it/s] 90%|█████████ | 3520/3900 [12:04<01:15,  5.02it/s] 90%|█████████ | 3521/3900 [12:04<01:15,  5.02it/s] 90%|█████████ | 3522/3900 [12:04<01:15,  5.02it/s] 90%|█████████ | 3523/3900 [12:04<01:15,  5.02it/s] 90%|█████████ | 3524/3900 [12:05<01:14,  5.02it/s] 90%|█████████ | 3525/3900 [12:05<01:14,  5.01it/s] 90%|█████████ | 3526/3900 [12:05<01:14,  5.02it/s] 90%|█████████ | 3527/3900 [12:05<01:14,  5.02it/s] 90%|█████████ | 3528/3900 [12:05<01:14,  5.03it/s] 90%|█████████ | 3529/3900 [12:06<01:14,  4.99it/s] 91%|█████████ | 3530/3900 [12:06<01:13,  5.00it/s] 91%|█████████ | 3531/3900 [12:06<01:13,  5.01it/s] 91%|█████████ | 3532/3900 [12:06<01:13,  5.02it/s] 91%|█████████ | 3533/3900 [12:06<01:13,  5.02it/s] 91%|█████████ | 3534/3900 [12:07<01:12,  5.02it/s] 91%|█████████ | 3535/3900 [12:07<01:12,  5.02it/s] 91%|█████████ | 3536/3900 [12:07<01:12,  5.02it/s] 91%|█████████ | 3537/3900 [12:07<01:12,  5.02it/s] 91%|█████████ | 3538/3900 [12:07<01:12,  5.02it/s] 91%|█████████ | 3539/3900 [12:08<01:11,  5.02it/s] 91%|█████████ | 3540/3900 [12:08<01:11,  5.03it/s] 91%|█████████ | 3541/3900 [12:08<01:11,  5.02it/s] 91%|█████████ | 3542/3900 [12:08<01:11,  5.03it/s] 91%|█████████ | 3543/3900 [12:08<01:11,  5.03it/s] 91%|█████████ | 3544/3900 [12:08<01:10,  5.03it/s] 91%|█████████ | 3545/3900 [12:09<01:10,  5.02it/s] 91%|█████████ | 3546/3900 [12:09<01:10,  5.03it/s] 91%|█████████ | 3547/3900 [12:09<01:10,  5.03it/s] 91%|█████████ | 3548/3900 [12:09<01:10,  5.03it/s] 91%|█████████ | 3549/3900 [12:09<01:09,  5.08it/s] 91%|█████████ | 3550/3900 [12:10<01:09,  5.06it/s] 91%|█████████ | 3551/3900 [12:10<01:09,  5.05it/s] 91%|█████████ | 3552/3900 [12:10<01:08,  5.05it/s] 91%|█████████ | 3553/3900 [12:10<01:08,  5.04it/s] 91%|█████████ | 3554/3900 [12:10<01:08,  5.04it/s] 91%|█████████ | 3555/3900 [12:11<01:08,  5.04it/s] 91%|█████████ | 3556/3900 [12:11<01:08,  5.03it/s] 91%|█████████ | 3557/3900 [12:11<01:08,  5.03it/s] 91%|█████████ | 3558/3900 [12:11<01:07,  5.03it/s] 91%|█████████▏| 3559/3900 [12:11<01:07,  5.03it/s] 91%|█████████▏| 3560/3900 [12:12<01:07,  5.03it/s] 91%|█████████▏| 3561/3900 [12:12<01:07,  5.03it/s] 91%|█████████▏| 3562/3900 [12:12<01:07,  5.03it/s] 91%|█████████▏| 3563/3900 [12:12<01:07,  5.03it/s] 91%|█████████▏| 3564/3900 [12:12<01:06,  5.03it/s] 91%|█████████▏| 3565/3900 [12:13<01:06,  5.02it/s] 91%|█████████▏| 3566/3900 [12:13<01:06,  5.03it/s] 91%|█████████▏| 3567/3900 [12:13<01:06,  5.03it/s] 91%|█████████▏| 3568/3900 [12:13<01:05,  5.03it/s] 92%|█████████▏| 3569/3900 [12:13<01:05,  5.04it/s] 92%|█████████▏| 3570/3900 [12:14<01:05,  5.04it/s] 92%|█████████▏| 3571/3900 [12:14<01:05,  5.04it/s] 92%|█████████▏| 3572/3900 [12:14<01:05,  5.04it/s] 92%|█████████▏| 3573/3900 [12:14<01:04,  5.04it/s] 92%|█████████▏| 3574/3900 [12:14<01:04,  5.04it/s] 92%|█████████▏| 3575/3900 [12:15<01:04,  5.04it/s] 92%|█████████▏| 3576/3900 [12:15<01:04,  5.04it/s] 92%|█████████▏| 3577/3900 [12:15<01:04,  5.04it/s] 92%|█████████▏| 3578/3900 [12:15<01:03,  5.03it/s] 92%|█████████▏| 3579/3900 [12:15<01:03,  5.04it/s] 92%|█████████▏| 3580/3900 [12:16<01:03,  5.04it/s] 92%|█████████▏| 3581/3900 [12:16<01:03,  5.04it/s] 92%|█████████▏| 3582/3900 [12:16<01:03,  5.04it/s] 92%|█████████▏| 3583/3900 [12:16<01:02,  5.04it/s] 92%|█████████▏| 3584/3900 [12:16<01:02,  5.03it/s] 92%|█████████▏| 3585/3900 [12:17<01:02,  5.02it/s] 92%|█████████▏| 3586/3900 [12:17<01:02,  5.02it/s] 92%|█████████▏| 3587/3900 [12:17<01:02,  5.01it/s] 92%|█████████▏| 3588/3900 [12:17<01:01,  5.08it/s] 92%|█████████▏| 3589/3900 [12:17<01:01,  5.06it/s] 92%|█████████▏| 3590/3900 [12:18<01:01,  5.05it/s] 92%|█████████▏| 3591/3900 [12:18<01:01,  5.04it/s] 92%|█████████▏| 3592/3900 [12:18<01:01,  5.04it/s] 92%|█████████▏| 3593/3900 [12:18<01:00,  5.04it/s] 92%|█████████▏| 3594/3900 [12:18<01:00,  5.04it/s] 92%|█████████▏| 3595/3900 [12:19<01:00,  5.04it/s] 92%|█████████▏| 3596/3900 [12:19<01:00,  5.04it/s] 92%|█████████▏| 3597/3900 [12:19<01:00,  5.04it/s] 92%|█████████▏| 3598/3900 [12:19<00:59,  5.04it/s] 92%|█████████▏| 3599/3900 [12:19<00:59,  5.04it/s] 92%|█████████▏| 3600/3900 [12:20<00:59,  5.04it/s] 92%|█████████▏| 3601/3900 [12:20<00:59,  5.04it/s] 92%|█████████▏| 3602/3900 [12:20<00:59,  5.04it/s] 92%|█████████▏| 3603/3900 [12:20<00:58,  5.04it/s] 92%|█████████▏| 3604/3900 [12:20<00:58,  5.04it/s] 92%|█████████▏| 3605/3900 [12:21<00:58,  5.04it/s] 92%|█████████▏| 3606/3900 [12:21<00:58,  5.03it/s] 92%|█████████▏| 3607/3900 [12:21<00:58,  5.04it/s] 93%|█████████▎| 3608/3900 [12:21<00:57,  5.04it/s] 93%|█████████▎| 3609/3900 [12:21<00:57,  5.04it/s] 93%|█████████▎| 3610/3900 [12:22<00:57,  5.04it/s] 93%|█████████▎| 3611/3900 [12:22<00:57,  5.04it/s] 93%|█████████▎| 3612/3900 [12:22<00:57,  5.04it/s] 93%|█████████▎| 3613/3900 [12:22<00:56,  5.04it/s] 93%|█████████▎| 3614/3900 [12:22<00:56,  5.04it/s] 93%|█████████▎| 3615/3900 [12:23<00:56,  5.04it/s] 93%|█████████▎| 3616/3900 [12:23<00:56,  5.04it/s] 93%|█████████▎| 3617/3900 [12:23<00:56,  5.04it/s] 93%|█████████▎| 3618/3900 [12:23<00:55,  5.04it/s] 93%|█████████▎| 3619/3900 [12:23<00:55,  5.04it/s] 93%|█████████▎| 3620/3900 [12:24<00:55,  5.04it/s] 93%|█████████▎| 3621/3900 [12:24<00:55,  5.04it/s] 93%|█████████▎| 3622/3900 [12:24<00:55,  5.04it/s] 93%|█████████▎| 3623/3900 [12:24<00:54,  5.04it/s] 93%|█████████▎| 3624/3900 [12:24<00:54,  5.04it/s] 93%|█████████▎| 3625/3900 [12:25<00:54,  5.04it/s] 93%|█████████▎| 3626/3900 [12:25<00:54,  5.04it/s] 93%|█████████▎| 3627/3900 [12:25<00:53,  5.10it/s] 93%|█████████▎| 3628/3900 [12:25<00:53,  5.07it/s] 93%|█████████▎| 3629/3900 [12:25<00:53,  5.06it/s] 93%|█████████▎| 3630/3900 [12:26<00:53,  5.06it/s] 93%|█████████▎| 3631/3900 [12:26<00:53,  5.05it/s] 93%|█████████▎| 3632/3900 [12:26<00:53,  5.05it/s] 93%|█████████▎| 3633/3900 [12:26<00:52,  5.05it/s] 93%|█████████▎| 3634/3900 [12:26<00:52,  5.05it/s] 93%|█████████▎| 3635/3900 [12:27<00:52,  5.05it/s] 93%|█████████▎| 3636/3900 [12:27<00:52,  5.04it/s] 93%|█████████▎| 3637/3900 [12:27<00:52,  5.04it/s] 93%|█████████▎| 3638/3900 [12:27<00:51,  5.04it/s] 93%|█████████▎| 3639/3900 [12:27<00:51,  5.04it/s] 93%|█████████▎| 3640/3900 [12:28<00:51,  5.04it/s] 93%|█████████▎| 3641/3900 [12:28<00:51,  5.04it/s] 93%|█████████▎| 3642/3900 [12:28<00:51,  5.04it/s] 93%|█████████▎| 3643/3900 [12:28<00:50,  5.04it/s] 93%|█████████▎| 3644/3900 [12:28<00:50,  5.04it/s] 93%|█████████▎| 3645/3900 [12:29<00:50,  5.04it/s] 93%|█████████▎| 3646/3900 [12:29<00:50,  5.04it/s] 94%|█████████▎| 3647/3900 [12:29<00:50,  5.03it/s] 94%|█████████▎| 3648/3900 [12:29<00:50,  5.04it/s] 94%|█████████▎| 3649/3900 [12:29<00:49,  5.04it/s] 94%|█████████▎| 3650/3900 [12:30<00:49,  5.04it/s] 94%|█████████▎| 3651/3900 [12:30<00:49,  5.04it/s] 94%|█████████▎| 3652/3900 [12:30<00:49,  5.04it/s] 94%|█████████▎| 3653/3900 [12:30<00:49,  5.04it/s] 94%|█████████▎| 3654/3900 [12:30<00:48,  5.04it/s] 94%|█████████▎| 3655/3900 [12:31<00:48,  5.04it/s] 94%|█████████▎| 3656/3900 [12:31<00:48,  5.04it/s] 94%|█████████▍| 3657/3900 [12:31<00:48,  5.04it/s] 94%|█████████▍| 3658/3900 [12:31<00:47,  5.04it/s] 94%|█████████▍| 3659/3900 [12:31<00:47,  5.04it/s] 94%|█████████▍| 3660/3900 [12:32<00:52,  4.55it/s] 94%|█████████▍| 3661/3900 [12:32<00:51,  4.68it/s] 94%|█████████▍| 3662/3900 [12:32<00:49,  4.78it/s] 94%|█████████▍| 3663/3900 [12:32<00:48,  4.86it/s] 94%|█████████▍| 3664/3900 [12:32<00:48,  4.91it/s] 94%|█████████▍| 3665/3900 [12:33<00:47,  4.95it/s] 94%|█████████▍| 3666/3900 [12:33<00:46,  5.03it/s] 94%|█████████▍| 3667/3900 [12:33<00:46,  5.03it/s] 94%|█████████▍| 3668/3900 [12:33<00:46,  5.03it/s] 94%|█████████▍| 3669/3900 [12:33<00:45,  5.04it/s] 94%|█████████▍| 3670/3900 [12:34<00:45,  5.04it/s] 94%|█████████▍| 3671/3900 [12:34<00:45,  5.04it/s] 94%|█████████▍| 3672/3900 [12:34<00:45,  5.04it/s] 94%|█████████▍| 3673/3900 [12:34<00:45,  5.04it/s] 94%|█████████▍| 3674/3900 [12:34<00:44,  5.04it/s] 94%|█████████▍| 3675/3900 [12:35<00:44,  5.04it/s] 94%|█████████▍| 3676/3900 [12:35<00:44,  5.04it/s] 94%|█████████▍| 3677/3900 [12:35<00:44,  5.04it/s] 94%|█████████▍| 3678/3900 [12:35<00:44,  5.04it/s] 94%|█████████▍| 3679/3900 [12:35<00:43,  5.04it/s] 94%|█████████▍| 3680/3900 [12:36<00:43,  5.04it/s] 94%|█████████▍| 3681/3900 [12:36<00:43,  5.04it/s] 94%|█████████▍| 3682/3900 [12:36<00:43,  5.03it/s] 94%|█████████▍| 3683/3900 [12:36<00:43,  5.04it/s] 94%|█████████▍| 3684/3900 [12:36<00:42,  5.04it/s] 94%|█████████▍| 3685/3900 [12:37<00:42,  5.04it/s] 95%|█████████▍| 3686/3900 [12:37<00:42,  5.04it/s] 95%|█████████▍| 3687/3900 [12:37<00:42,  5.04it/s] 95%|█████████▍| 3688/3900 [12:37<00:42,  5.04it/s] 95%|█████████▍| 3689/3900 [12:37<00:41,  5.04it/s] 95%|█████████▍| 3690/3900 [12:38<00:41,  5.04it/s] 95%|█████████▍| 3691/3900 [12:38<00:41,  5.04it/s] 95%|█████████▍| 3692/3900 [12:38<00:41,  5.04it/s] 95%|█████████▍| 3693/3900 [12:38<00:41,  5.04it/s] 95%|█████████▍| 3694/3900 [12:38<00:40,  5.04it/s] 95%|█████████▍| 3695/3900 [12:39<00:40,  5.04it/s] 95%|█████████▍| 3696/3900 [12:39<00:40,  5.04it/s] 95%|█████████▍| 3697/3900 [12:39<00:40,  5.04it/s] 95%|█████████▍| 3698/3900 [12:39<00:40,  5.04it/s] 95%|█████████▍| 3699/3900 [12:39<00:39,  5.04it/s] 95%|█████████▍| 3700/3900 [12:40<00:39,  5.04it/s] 95%|█████████▍| 3701/3900 [12:40<00:39,  5.04it/s] 95%|█████████▍| 3702/3900 [12:40<00:39,  5.04it/s] 95%|█████████▍| 3703/3900 [12:40<00:39,  5.04it/s] 95%|█████████▍| 3704/3900 [12:40<00:38,  5.04it/s] 95%|█████████▌| 3705/3900 [12:40<00:38,  5.10it/s] 95%|█████████▌| 3706/3900 [12:41<00:38,  5.08it/s] 95%|█████████▌| 3707/3900 [12:41<00:38,  5.07it/s] 95%|█████████▌| 3708/3900 [12:41<00:37,  5.06it/s] 95%|█████████▌| 3709/3900 [12:41<00:37,  5.05it/s] 95%|█████████▌| 3710/3900 [12:41<00:37,  5.05it/s] 95%|█████████▌| 3711/3900 [12:42<00:37,  5.05it/s] 95%|█████████▌| 3712/3900 [12:42<00:37,  5.05it/s] 95%|█████████▌| 3713/3900 [12:42<00:37,  5.05it/s] 95%|█████████▌| 3714/3900 [12:42<00:36,  5.04it/s] 95%|█████████▌| 3715/3900 [12:42<00:36,  5.04it/s] 95%|█████████▌| 3716/3900 [12:43<00:36,  5.04it/s] 95%|█████████▌| 3717/3900 [12:43<00:36,  5.03it/s] 95%|█████████▌| 3718/3900 [12:43<00:36,  5.03it/s] 95%|█████████▌| 3719/3900 [12:43<00:35,  5.03it/s] 95%|█████████▌| 3720/3900 [12:43<00:35,  5.03it/s] 95%|█████████▌| 3721/3900 [12:44<00:35,  5.03it/s] 95%|█████████▌| 3722/3900 [12:44<00:35,  5.04it/s] 95%|█████████▌| 3723/3900 [12:44<00:35,  5.04it/s] 95%|█████████▌| 3724/3900 [12:44<00:37,  4.72it/s] 96%|█████████▌| 3725/3900 [12:45<00:36,  4.81it/s] 96%|█████████▌| 3726/3900 [12:45<00:35,  4.87it/s] 96%|█████████▌| 3727/3900 [12:45<00:35,  4.92it/s] 96%|█████████▌| 3728/3900 [12:45<00:34,  4.96it/s] 96%|█████████▌| 3729/3900 [12:45<00:34,  4.99it/s] 96%|█████████▌| 3730/3900 [12:46<00:33,  5.00it/s] 96%|█████████▌| 3731/3900 [12:46<00:33,  5.01it/s] 96%|█████████▌| 3732/3900 [12:46<00:33,  5.02it/s] 96%|█████████▌| 3733/3900 [12:46<00:33,  5.02it/s] 96%|█████████▌| 3734/3900 [12:46<00:33,  5.03it/s] 96%|█████████▌| 3735/3900 [12:46<00:32,  5.03it/s] 96%|█████████▌| 3736/3900 [12:47<00:32,  5.03it/s] 96%|█████████▌| 3737/3900 [12:47<00:32,  5.03it/s] 96%|█████████▌| 3738/3900 [12:47<00:32,  5.03it/s] 96%|█████████▌| 3739/3900 [12:47<00:31,  5.03it/s] 96%|█████████▌| 3740/3900 [12:47<00:31,  5.03it/s] 96%|█████████▌| 3741/3900 [12:48<00:31,  5.03it/s] 96%|█████████▌| 3742/3900 [12:48<00:31,  5.03it/s] 96%|█████████▌| 3743/3900 [12:48<00:31,  5.03it/s] 96%|█████████▌| 3744/3900 [12:48<00:30,  5.09it/s] 96%|█████████▌| 3745/3900 [12:48<00:30,  5.07it/s] 96%|█████████▌| 3746/3900 [12:49<00:30,  5.07it/s] 96%|█████████▌| 3747/3900 [12:49<00:30,  5.06it/s] 96%|█████████▌| 3748/3900 [12:49<00:30,  5.05it/s] 96%|█████████▌| 3749/3900 [12:49<00:29,  5.05it/s] 96%|█████████▌| 3750/3900 [12:49<00:29,  5.05it/s] 96%|█████████▌| 3751/3900 [12:50<00:29,  5.05it/s] 96%|█████████▌| 3752/3900 [12:50<00:29,  5.04it/s] 96%|█████████▌| 3753/3900 [12:50<00:29,  5.04it/s] 96%|█████████▋| 3754/3900 [12:50<00:28,  5.04it/s] 96%|█████████▋| 3755/3900 [12:50<00:28,  5.04it/s] 96%|█████████▋| 3756/3900 [12:51<00:28,  5.04it/s] 96%|█████████▋| 3757/3900 [12:51<00:28,  5.04it/s] 96%|█████████▋| 3758/3900 [12:51<00:28,  5.04it/s] 96%|█████████▋| 3759/3900 [12:51<00:27,  5.04it/s] 96%|█████████▋| 3760/3900 [12:51<00:27,  5.04it/s] 96%|█████████▋| 3761/3900 [12:52<00:27,  5.04it/s] 96%|█████████▋| 3762/3900 [12:52<00:27,  5.04it/s] 96%|█████████▋| 3763/3900 [12:52<00:27,  5.04it/s] 97%|█████████▋| 3764/3900 [12:52<00:26,  5.04it/s] 97%|█████████▋| 3765/3900 [12:52<00:26,  5.04it/s] 97%|█████████▋| 3766/3900 [12:53<00:26,  5.04it/s] 97%|█████████▋| 3767/3900 [12:53<00:26,  5.04it/s] 97%|█████████▋| 3768/3900 [12:53<00:26,  5.04it/s] 97%|█████████▋| 3769/3900 [12:53<00:25,  5.04it/s] 97%|█████████▋| 3770/3900 [12:53<00:25,  5.04it/s] 97%|█████████▋| 3771/3900 [12:54<00:25,  5.04it/s] 97%|█████████▋| 3772/3900 [12:54<00:25,  5.04it/s] 97%|█████████▋| 3773/3900 [12:54<00:25,  5.04it/s] 97%|█████████▋| 3774/3900 [12:54<00:25,  5.04it/s] 97%|█████████▋| 3775/3900 [12:54<00:24,  5.03it/s] 97%|█████████▋| 3776/3900 [12:55<00:24,  5.04it/s] 97%|█████████▋| 3777/3900 [12:55<00:24,  5.04it/s] 97%|█████████▋| 3778/3900 [12:55<00:24,  5.04it/s] 97%|█████████▋| 3779/3900 [12:55<00:24,  5.03it/s] 97%|█████████▋| 3780/3900 [12:55<00:23,  5.03it/s] 97%|█████████▋| 3781/3900 [12:56<00:23,  5.03it/s] 97%|█████████▋| 3782/3900 [12:56<00:23,  5.03it/s] 97%|█████████▋| 3783/3900 [12:56<00:23,  5.09it/s] 97%|█████████▋| 3784/3900 [12:56<00:22,  5.06it/s] 97%|█████████▋| 3785/3900 [12:56<00:22,  5.06it/s] 97%|█████████▋| 3786/3900 [12:57<00:22,  5.05it/s] 97%|█████████▋| 3787/3900 [12:57<00:22,  5.04it/s] 97%|█████████▋| 3788/3900 [12:57<00:22,  5.04it/s] 97%|█████████▋| 3789/3900 [12:57<00:22,  5.04it/s] 97%|█████████▋| 3790/3900 [12:57<00:21,  5.04it/s] 97%|█████████▋| 3791/3900 [12:58<00:21,  5.04it/s] 97%|█████████▋| 3792/3900 [12:58<00:21,  5.04it/s] 97%|█████████▋| 3793/3900 [12:58<00:21,  5.04it/s] 97%|█████████▋| 3794/3900 [12:58<00:21,  5.04it/s] 97%|█████████▋| 3795/3900 [12:58<00:20,  5.04it/s] 97%|█████████▋| 3796/3900 [12:59<00:20,  5.04it/s] 97%|█████████▋| 3797/3900 [12:59<00:20,  5.04it/s] 97%|█████████▋| 3798/3900 [12:59<00:20,  5.04it/s] 97%|█████████▋| 3799/3900 [12:59<00:20,  5.04it/s] 97%|█████████▋| 3800/3900 [12:59<00:19,  5.04it/s] 97%|█████████▋| 3801/3900 [13:00<00:19,  5.04it/s] 97%|█████████▋| 3802/3900 [13:00<00:19,  5.04it/s] 98%|█████████▊| 3803/3900 [13:00<00:19,  5.04it/s] 98%|█████████▊| 3804/3900 [13:00<00:19,  5.04it/s] 98%|█████████▊| 3805/3900 [13:00<00:18,  5.04it/s] 98%|█████████▊| 3806/3900 [13:01<00:18,  5.04it/s] 98%|█████████▊| 3807/3900 [13:01<00:18,  5.04it/s] 98%|█████████▊| 3808/3900 [13:01<00:18,  5.04it/s] 98%|█████████▊| 3809/3900 [13:01<00:18,  5.04it/s] 98%|█████████▊| 3810/3900 [13:01<00:17,  5.04it/s] 98%|█████████▊| 3811/3900 [13:02<00:17,  5.04it/s] 98%|█████████▊| 3812/3900 [13:02<00:17,  5.04it/s] 98%|█████████▊| 3813/3900 [13:02<00:17,  5.04it/s] 98%|█████████▊| 3814/3900 [13:02<00:17,  5.04it/s] 98%|█████████▊| 3815/3900 [13:02<00:16,  5.04it/s] 98%|█████████▊| 3816/3900 [13:03<00:16,  5.04it/s] 98%|█████████▊| 3817/3900 [13:03<00:16,  5.04it/s] 98%|█████████▊| 3818/3900 [13:03<00:16,  5.04it/s] 98%|█████████▊| 3819/3900 [13:03<00:16,  5.04it/s] 98%|█████████▊| 3820/3900 [13:03<00:15,  5.04it/s] 98%|█████████▊| 3821/3900 [13:04<00:15,  5.04it/s] 98%|█████████▊| 3822/3900 [13:04<00:15,  5.09it/s] 98%|█████████▊| 3823/3900 [13:04<00:15,  5.07it/s] 98%|█████████▊| 3824/3900 [13:04<00:15,  5.06it/s] 98%|█████████▊| 3825/3900 [13:04<00:14,  5.06it/s] 98%|█████████▊| 3826/3900 [13:05<00:14,  5.05it/s] 98%|█████████▊| 3827/3900 [13:05<00:14,  5.05it/s] 98%|█████████▊| 3828/3900 [13:05<00:14,  5.04it/s] 98%|█████████▊| 3829/3900 [13:05<00:14,  5.04it/s] 98%|█████████▊| 3830/3900 [13:05<00:13,  5.04it/s] 98%|█████████▊| 3831/3900 [13:06<00:13,  5.04it/s] 98%|█████████▊| 3832/3900 [13:06<00:13,  5.03it/s] 98%|█████████▊| 3833/3900 [13:06<00:13,  5.03it/s] 98%|█████████▊| 3834/3900 [13:06<00:13,  5.03it/s] 98%|█████████▊| 3835/3900 [13:06<00:12,  5.03it/s] 98%|█████████▊| 3836/3900 [13:07<00:12,  5.03it/s] 98%|█████████▊| 3837/3900 [13:07<00:12,  5.04it/s] 98%|█████████▊| 3838/3900 [13:07<00:12,  5.04it/s] 98%|█████████▊| 3839/3900 [13:07<00:12,  5.03it/s] 98%|█████████▊| 3840/3900 [13:07<00:11,  5.04it/s] 98%|█████████▊| 3841/3900 [13:08<00:11,  5.04it/s] 99%|█████████▊| 3842/3900 [13:08<00:11,  5.03it/s] 99%|█████████▊| 3843/3900 [13:08<00:11,  5.04it/s] 99%|█████████▊| 3844/3900 [13:08<00:11,  5.04it/s] 99%|█████████▊| 3845/3900 [13:08<00:10,  5.04it/s] 99%|█████████▊| 3846/3900 [13:09<00:10,  5.04it/s] 99%|█████████▊| 3847/3900 [13:09<00:10,  5.04it/s] 99%|█████████▊| 3848/3900 [13:09<00:10,  5.04it/s] 99%|█████████▊| 3849/3900 [13:09<00:10,  5.04it/s] 99%|█████████▊| 3850/3900 [13:09<00:09,  5.04it/s] 99%|█████████▊| 3851/3900 [13:10<00:09,  5.04it/s] 99%|█████████▉| 3852/3900 [13:10<00:09,  5.04it/s] 99%|█████████▉| 3853/3900 [13:10<00:09,  5.04it/s] 99%|█████████▉| 3854/3900 [13:10<00:09,  5.04it/s] 99%|█████████▉| 3855/3900 [13:10<00:08,  5.04it/s] 99%|█████████▉| 3856/3900 [13:10<00:08,  5.03it/s] 99%|█████████▉| 3857/3900 [13:11<00:08,  5.04it/s] 99%|█████████▉| 3858/3900 [13:11<00:08,  5.04it/s] 99%|█████████▉| 3859/3900 [13:11<00:08,  5.04it/s] 99%|█████████▉| 3860/3900 [13:11<00:07,  5.04it/s] 99%|█████████▉| 3861/3900 [13:11<00:07,  5.09it/s] 99%|█████████▉| 3862/3900 [13:12<00:07,  5.07it/s] 99%|█████████▉| 3863/3900 [13:12<00:07,  5.07it/s] 99%|█████████▉| 3864/3900 [13:12<00:07,  5.06it/s] 99%|█████████▉| 3865/3900 [13:12<00:06,  5.05it/s] 99%|█████████▉| 3866/3900 [13:12<00:06,  5.05it/s] 99%|█████████▉| 3867/3900 [13:13<00:06,  5.05it/s] 99%|█████████▉| 3868/3900 [13:13<00:06,  5.04it/s] 99%|█████████▉| 3869/3900 [13:13<00:06,  5.04it/s] 99%|█████████▉| 3870/3900 [13:13<00:05,  5.03it/s] 99%|█████████▉| 3871/3900 [13:13<00:05,  5.03it/s] 99%|█████████▉| 3872/3900 [13:14<00:05,  5.03it/s] 99%|█████████▉| 3873/3900 [13:14<00:05,  5.03it/s] 99%|█████████▉| 3874/3900 [13:14<00:05,  5.03it/s] 99%|█████████▉| 3875/3900 [13:14<00:04,  5.02it/s] 99%|█████████▉| 3876/3900 [13:14<00:04,  5.02it/s] 99%|█████████▉| 3877/3900 [13:15<00:04,  5.01it/s] 99%|█████████▉| 3878/3900 [13:15<00:04,  5.01it/s] 99%|█████████▉| 3879/3900 [13:15<00:04,  5.02it/s] 99%|█████████▉| 3880/3900 [13:15<00:03,  5.01it/s]100%|█████████▉| 3881/3900 [13:15<00:03,  5.00it/s]100%|█████████▉| 3882/3900 [13:16<00:03,  4.99it/s]100%|█████████▉| 3883/3900 [13:16<00:03,  4.99it/s]100%|█████████▉| 3884/3900 [13:16<00:03,  4.98it/s]100%|█████████▉| 3885/3900 [13:16<00:03,  4.98it/s]100%|█████████▉| 3886/3900 [13:16<00:02,  4.98it/s]100%|█████████▉| 3887/3900 [13:17<00:02,  4.98it/s]100%|█████████▉| 3888/3900 [13:17<00:02,  4.98it/s]100%|█████████▉| 3889/3900 [13:17<00:02,  4.97it/s]100%|█████████▉| 3890/3900 [13:17<00:02,  4.98it/s]100%|█████████▉| 3891/3900 [13:17<00:01,  4.98it/s]100%|█████████▉| 3892/3900 [13:18<00:01,  4.98it/s]100%|█████████▉| 3893/3900 [13:18<00:01,  4.98it/s]100%|█████████▉| 3894/3900 [13:18<00:01,  4.98it/s]100%|█████████▉| 3895/3900 [13:18<00:01,  4.98it/s]100%|█████████▉| 3896/3900 [13:18<00:00,  4.98it/s]100%|█████████▉| 3897/3900 [13:19<00:00,  4.98it/s]100%|█████████▉| 3898/3900 [13:19<00:00,  4.98it/s]100%|█████████▉| 3899/3900 [13:19<00:00,  4.98it/s]100%|██████████| 3900/3900 [13:19<00:00,  5.04it/s][INFO|trainer.py:1679] 2022-08-27 22:21:04,520 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


                                                   100%|██████████| 3900/3900 [13:19<00:00,  5.04it/s]100%|██████████| 3900/3900 [13:19<00:00,  4.88it/s]
[INFO|trainer.py:2409] 2022-08-27 22:21:04,521 >> Saving model checkpoint to tmp/baseline/RTE/bert-base-uncased/HPO_S0/1e-06/64/100
[INFO|configuration_utils.py:446] 2022-08-27 22:21:04,522 >> Configuration saved in tmp/baseline/RTE/bert-base-uncased/HPO_S0/1e-06/64/100/config.json
[INFO|modeling_utils.py:1602] 2022-08-27 22:21:05,373 >> Model weights saved in tmp/baseline/RTE/bert-base-uncased/HPO_S0/1e-06/64/100/pytorch_model.bin
[INFO|tokenization_utils_base.py:2108] 2022-08-27 22:21:05,373 >> tokenizer config file saved in tmp/baseline/RTE/bert-base-uncased/HPO_S0/1e-06/64/100/tokenizer_config.json
[INFO|tokenization_utils_base.py:2114] 2022-08-27 22:21:05,373 >> Special tokens file saved in tmp/baseline/RTE/bert-base-uncased/HPO_S0/1e-06/64/100/special_tokens_map.json
{'loss': 0.7015, 'learning_rate': 6.410256410256411e-07, 'epoch': 12.82}
{'loss': 0.6964, 'learning_rate': 9.294871794871795e-07, 'epoch': 25.64}
{'loss': 0.6972, 'learning_rate': 7.692307692307693e-07, 'epoch': 38.46}
{'loss': 0.6957, 'learning_rate': 6.089743589743589e-07, 'epoch': 51.28}
{'loss': 0.6949, 'learning_rate': 4.487179487179487e-07, 'epoch': 64.1}
{'loss': 0.6948, 'learning_rate': 2.884615384615384e-07, 'epoch': 76.92}
{'loss': 0.6946, 'learning_rate': 1.2820512820512818e-07, 'epoch': 89.74}
{'train_runtime': 799.7769, 'train_samples_per_second': 311.337, 'train_steps_per_second': 4.876, 'train_loss': 0.6962478559445112, 'epoch': 100.0}
***** train metrics *****
  epoch                    =      100.0
  train_loss               =     0.6962
  train_runtime            = 0:13:19.77
  train_samples            =       2490
  train_samples_per_second =    311.337
  train_steps_per_second   =      4.876
08/27/2022 22:21:05 - INFO - __main__ - *** Evaluate ***
[INFO|trainer.py:628] 2022-08-27 22:21:05,422 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1, idx, sentence2. If sentence1, idx, sentence2 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:2659] 2022-08-27 22:21:05,425 >> ***** Running Evaluation *****
[INFO|trainer.py:2661] 2022-08-27 22:21:05,425 >>   Num examples = 277
[INFO|trainer.py:2664] 2022-08-27 22:21:05,425 >>   Batch size = 16
  0%|          | 0/18 [00:00<?, ?it/s] 11%|█         | 2/18 [00:00<00:00, 17.82it/s] 22%|██▏       | 4/18 [00:00<00:00, 18.70it/s] 33%|███▎      | 6/18 [00:00<00:00, 19.06it/s] 44%|████▍     | 8/18 [00:00<00:00, 19.04it/s] 56%|█████▌    | 10/18 [00:00<00:00, 19.14it/s] 67%|██████▋   | 12/18 [00:00<00:00, 19.24it/s] 78%|███████▊  | 14/18 [00:00<00:00, 19.24it/s] 89%|████████▉ | 16/18 [00:00<00:00, 19.42it/s]100%|██████████| 18/18 [00:00<00:00, 19.55it/s]08/27/2022 22:21:06 - INFO - datasets.metric - Removing /home/ubuntu/.cache/huggingface/metrics/glue/rte/default_experiment-1-0.arrow
100%|██████████| 18/18 [00:00<00:00, 19.10it/s]
***** eval metrics *****
  epoch                   =      100.0
  eval_accuracy           =     0.4729
  eval_loss               =     0.6934
  eval_runtime            = 0:00:00.99
  eval_samples            =        277
  eval_samples_per_second =    277.667
  eval_steps_per_second   =     18.043
08/27/2022 22:21:10 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 2distributed training: False, 16-bits training: True
08/27/2022 22:21:10 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=2,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=IntervalStrategy.NO,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_min_num_params=0,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-06,
length_column_name=length,
load_best_model_at_end=False,
local_rank=-1,
log_level=-1,
log_level_replica=-1,
log_on_each_node=True,
logging_dir=tmp/baseline/RTE/bert-base-uncased/HPO_S0/5e-06/256/10/runs/Aug27_22-21-10_ip-10-21-39-60,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
no_cuda=False,
num_train_epochs=10.0,
optim=OptimizerNames.ADAMW_HF,
output_dir=tmp/baseline/RTE/bert-base-uncased/HPO_S0/5e-06/256/10,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=128,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=tmp/baseline/RTE/bert-base-uncased/HPO_S0/5e-06/256/10,
save_on_each_node=False,
save_steps=500,
save_strategy=IntervalStrategy.STEPS,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tf32=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
xpu_backend=None,
)
08/27/2022 22:21:10 - INFO - datasets.info - Loading Dataset Infos from /home/ubuntu/.cache/huggingface/modules/datasets_modules/datasets/glue/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad
08/27/2022 22:21:10 - INFO - datasets.builder - Overwrite dataset info from restored data version.
08/27/2022 22:21:10 - INFO - datasets.info - Loading Dataset info from /home/ubuntu/.cache/huggingface/datasets/glue/rte/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad
08/27/2022 22:21:10 - WARNING - datasets.builder - Reusing dataset glue (/home/ubuntu/.cache/huggingface/datasets/glue/rte/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)
08/27/2022 22:21:10 - INFO - datasets.info - Loading Dataset info from /home/ubuntu/.cache/huggingface/datasets/glue/rte/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 774.95it/s]
[INFO|configuration_utils.py:657] 2022-08-27 22:21:10,741 >> loading configuration file /home/ubuntu/checkpoints/exp/RTE/config.json
[INFO|configuration_utils.py:708] 2022-08-27 22:21:10,742 >> Model config BertConfig {
  "_name_or_path": "/home/ubuntu/checkpoints/exp/RTE",
  "architectures": [
    "BertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "crypten": false,
  "finetuning_task": "rte",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "entailment",
    "1": "not_entailment"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "entailment": 0,
    "not_entailment": 1
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "softmax_act": "softmax",
  "torch_dtype": "float32",
  "transformers_version": "4.20.0.dev0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|tokenization_utils_base.py:1701] 2022-08-27 22:21:10,742 >> Didn't find file /home/ubuntu/checkpoints/exp/RTE/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1779] 2022-08-27 22:21:10,742 >> loading file /home/ubuntu/checkpoints/exp/RTE/vocab.txt
[INFO|tokenization_utils_base.py:1779] 2022-08-27 22:21:10,742 >> loading file /home/ubuntu/checkpoints/exp/RTE/tokenizer.json
[INFO|tokenization_utils_base.py:1779] 2022-08-27 22:21:10,742 >> loading file None
[INFO|tokenization_utils_base.py:1779] 2022-08-27 22:21:10,742 >> loading file /home/ubuntu/checkpoints/exp/RTE/special_tokens_map.json
[INFO|tokenization_utils_base.py:1779] 2022-08-27 22:21:10,743 >> loading file /home/ubuntu/checkpoints/exp/RTE/tokenizer_config.json
[INFO|modeling_utils.py:2047] 2022-08-27 22:21:10,782 >> loading weights file /home/ubuntu/checkpoints/exp/RTE/pytorch_model.bin
[INFO|modeling_bert.py:194] 2022-08-27 22:21:11,128 >> initializing embedding using nn.Embedding
[INFO|modeling_utils.py:2417] 2022-08-27 22:21:12,280 >> All model checkpoint weights were used when initializing BertForSequenceClassification.

[INFO|modeling_utils.py:2426] 2022-08-27 22:21:12,280 >> All the weights of BertForSequenceClassification were initialized from the model checkpoint at /home/ubuntu/checkpoints/exp/RTE.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.
using model config: BertConfig {
  "_name_or_path": "/home/ubuntu/checkpoints/exp/RTE",
  "architectures": [
    "BertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "crypten": false,
  "finetuning_task": "rte",
  "gradient_checkpointing": false,
  "hidden_act": "quad",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "entailment",
    "1": "not_entailment"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "entailment": 0,
    "not_entailment": 1
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "softmax_act": "2quad",
  "torch_dtype": "float32",
  "transformers_version": "4.20.0.dev0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

BertConfig {
  "_name_or_path": "/home/ubuntu/checkpoints/exp/RTE",
  "architectures": [
    "BertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "crypten": false,
  "finetuning_task": "rte",
  "gradient_checkpointing": false,
  "hidden_act": "quad",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "entailment",
    "1": "not_entailment"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "entailment": 0,
    "not_entailment": 1
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "softmax_act": "2quad",
  "torch_dtype": "float32",
  "transformers_version": "4.20.0.dev0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

model architecture: BertForSequenceClassification(
  (bert): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): QuadActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): QuadActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): QuadActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): QuadActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): QuadActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): QuadActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): QuadActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): QuadActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): QuadActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): QuadActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): QuadActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): QuadActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (dropout): Dropout(p=0.1, inplace=False)
  (classifier): Linear(in_features=768, out_features=2, bias=True)
)
08/27/2022 22:21:12 - WARNING - datasets.fingerprint - Parameter 'function'=<function main.<locals>.preprocess_function at 0x7f2b4a059488> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
Running tokenizer on dataset:   0%|          | 0/3 [00:00<?, ?ba/s]08/27/2022 22:21:12 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/ubuntu/.cache/huggingface/datasets/glue/rte/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-1c80317fa3b1799d.arrow
Running tokenizer on dataset:  33%|███▎      | 1/3 [00:00<00:00,  7.82ba/s]Running tokenizer on dataset:  67%|██████▋   | 2/3 [00:00<00:00,  6.79ba/s]Running tokenizer on dataset: 100%|██████████| 3/3 [00:00<00:00,  8.99ba/s]08/27/2022 22:21:12 - INFO - datasets.fingerprint - Parameter 'function'=<function main.<locals>.preprocess_function at 0x7f2b4a055950> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead.

Running tokenizer on dataset:   0%|          | 0/1 [00:00<?, ?ba/s]08/27/2022 22:21:12 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/ubuntu/.cache/huggingface/datasets/glue/rte/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-bdd640fb06671ad1.arrow
Running tokenizer on dataset: 100%|██████████| 1/1 [00:00<00:00, 36.56ba/s]08/27/2022 22:21:12 - INFO - datasets.fingerprint - Parameter 'function'=<function main.<locals>.preprocess_function at 0x7f2b4a059840> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead.

Running tokenizer on dataset:   0%|          | 0/3 [00:00<?, ?ba/s]08/27/2022 22:21:12 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/ubuntu/.cache/huggingface/datasets/glue/rte/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-3eb13b9046685257.arrow
Running tokenizer on dataset:  33%|███▎      | 1/3 [00:00<00:00,  8.35ba/s]Running tokenizer on dataset: 100%|██████████| 3/3 [00:00<00:00,  9.63ba/s]Running tokenizer on dataset: 100%|██████████| 3/3 [00:00<00:00,  9.47ba/s]128
08/27/2022 22:21:18 - INFO - __main__ - Sample 914 of the training set: {'sentence1': "Because of Reagan's economic strategy, the federal budget deficit ballooned.", 'sentence2': "Reagan's economic strategy led to huge federal budget deficits.", 'label': 0, 'idx': 914, 'input_ids': [101, 2138, 1997, 11531, 1005, 1055, 3171, 5656, 1010, 1996, 2976, 5166, 15074, 13212, 2098, 1012, 102, 11531, 1005, 1055, 3171, 5656, 2419, 2000, 4121, 2976, 5166, 15074, 2015, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.
