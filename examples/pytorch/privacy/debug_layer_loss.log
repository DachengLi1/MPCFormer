2022-08-29 15:56:18,965 The args: Namespace(act='gelu', aug_train=False, cache_dir='', data_dir='glue_data/MRPC', data_url='', do_eval=False, do_lower_case=True, eval_batch_size=32, eval_step=200, gradient_accumulation_steps=1, learning_rate=5e-05, log_path='tmp/distill/MRPC/quad_2quad/bert-base-uncased/log.txt', max_seq_length=128, no_cuda=False, num_train_epochs=3.0, output_dir='tmp/distill/MRPC/quad_2quad/bert-base-uncased/5e-05_1e-05_32', pred_distill=False, seed=42, softmax_act='softmax', student_model='/home/ubuntu/checkpoints/exp/MRPC', task_name='MRPC', teacher_model='/home/ubuntu/checkpoints/exp/MRPC', temperature=1.0, train_batch_size=32, warmup_proportion=0.1, weight_decay=0.0001)
2022-08-29 15:56:18,966 device: cuda n_gpu: 2
2022-08-29 15:56:19,087 Writing example 0 of 3668
2022-08-29 15:56:19,088 *** Example ***
2022-08-29 15:56:19,088 guid: train-1
2022-08-29 15:56:19,088 tokens: [CLS] am ##ro ##zi accused his brother , whom he called " the witness " , of deliberately di ##stor ##ting his evidence . [SEP] referring to him as only " the witness " , am ##ro ##zi accused his brother of deliberately di ##stor ##ting his evidence . [SEP]
2022-08-29 15:56:19,088 input_ids: 101 2572 3217 5831 5496 2010 2567 1010 3183 2002 2170 1000 1996 7409 1000 1010 1997 9969 4487 23809 3436 2010 3350 1012 102 7727 2000 2032 2004 2069 1000 1996 7409 1000 1010 2572 3217 5831 5496 2010 2567 1997 9969 4487 23809 3436 2010 3350 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-08-29 15:56:19,088 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-08-29 15:56:19,088 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-08-29 15:56:19,088 label: 1
2022-08-29 15:56:19,088 label_id: 1
2022-08-29 15:56:22,004 Writing example 0 of 408
2022-08-29 15:56:22,005 *** Example ***
2022-08-29 15:56:22,005 guid: dev-1
2022-08-29 15:56:22,005 tokens: [CLS] he said the foods ##er ##vic ##e pie business doesn ' t fit the company ' s long - term growth strategy . [SEP] " the foods ##er ##vic ##e pie business does not fit our long - term growth strategy . [SEP]
2022-08-29 15:56:22,005 input_ids: 101 2002 2056 1996 9440 2121 7903 2063 11345 2449 2987 1005 1056 4906 1996 2194 1005 1055 2146 1011 2744 3930 5656 1012 102 1000 1996 9440 2121 7903 2063 11345 2449 2515 2025 4906 2256 2146 1011 2744 3930 5656 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-08-29 15:56:22,005 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-08-29 15:56:22,006 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-08-29 15:56:22,006 label: 1
2022-08-29 15:56:22,006 label_id: 1
2022-08-29 15:56:22,335 Model config {
  "_name_or_path": "bert-base-uncased",
  "architectures": [
    "BertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "crypten": false,
  "finetuning_task": "mrpc",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "not_equivalent",
    "1": "equivalent"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "equivalent": 1,
    "not_equivalent": 0
  },
  "layer_norm_eps": 1e-12,
  "log_path": "tmp/distill/MRPC/quad_2quad/bert-base-uncased/log.txt",
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "pre_trained": "",
  "problem_type": "single_label_classification",
  "softmax_act": "softmax",
  "torch_dtype": "float32",
  "training": "",
  "transformers_version": "4.20.0.dev0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

2022-08-29 15:56:25,040 Loading model /home/ubuntu/checkpoints/exp/MRPC/pytorch_model.bin
2022-08-29 15:56:25,362 loading model...
2022-08-29 15:56:25,409 done!
2022-08-29 15:56:25,409 Weights from pretrained model not used in TinyBertForSequenceClassification: ['bert.embeddings.position_ids']
2022-08-29 15:56:29,972 ***** Teacher evaluation *****
2022-08-29 15:56:29,972 {'acc': 0.8602941176470589, 'f1': 0.9025641025641027, 'acc_and_f1': 0.8814291101055808, 'eval_loss': 0.3993110977686368}
2022-08-29 15:56:29,973 Model config {
  "_name_or_path": "bert-base-uncased",
  "architectures": [
    "BertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "crypten": false,
  "finetuning_task": "mrpc",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "not_equivalent",
    "1": "equivalent"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "equivalent": 1,
    "not_equivalent": 0
  },
  "layer_norm_eps": 1e-12,
  "log_path": "tmp/distill/MRPC/quad_2quad/bert-base-uncased/log.txt",
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "pre_trained": "",
  "problem_type": "single_label_classification",
  "softmax_act": "softmax",
  "torch_dtype": "float32",
  "training": "",
  "transformers_version": "4.20.0.dev0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

2022-08-29 15:56:32,667 Loading model /home/ubuntu/checkpoints/exp/MRPC/pytorch_model.bin
2022-08-29 15:56:32,982 loading model...
2022-08-29 15:56:33,026 done!
2022-08-29 15:56:33,026 Weights from pretrained model not used in TinyBertForSequenceClassification: ['bert.embeddings.position_ids']
2022-08-29 15:58:11,459 The args: Namespace(act='gelu', aug_train=False, cache_dir='', data_dir='glue_data/MRPC', data_url='', do_eval=False, do_lower_case=True, eval_batch_size=32, eval_step=200, gradient_accumulation_steps=1, learning_rate=5e-05, log_path='tmp/distill/MRPC/quad_2quad/bert-base-uncased/log.txt', max_seq_length=128, no_cuda=False, num_train_epochs=3.0, output_dir='tmp/distill/MRPC/quad_2quad/bert-base-uncased/5e-05_1e-05_32', pred_distill=False, seed=42, softmax_act='softmax', student_model='/home/ubuntu/checkpoints/exp/MRPC', task_name='MRPC', teacher_model='/home/ubuntu/checkpoints/exp/MRPC', temperature=1.0, train_batch_size=32, warmup_proportion=0.1, weight_decay=0.0001)
2022-08-29 15:58:11,460 device: cuda n_gpu: 2
2022-08-29 15:58:11,544 Writing example 0 of 3668
2022-08-29 15:58:11,545 *** Example ***
2022-08-29 15:58:11,545 guid: train-1
2022-08-29 15:58:11,545 tokens: [CLS] am ##ro ##zi accused his brother , whom he called " the witness " , of deliberately di ##stor ##ting his evidence . [SEP] referring to him as only " the witness " , am ##ro ##zi accused his brother of deliberately di ##stor ##ting his evidence . [SEP]
2022-08-29 15:58:11,545 input_ids: 101 2572 3217 5831 5496 2010 2567 1010 3183 2002 2170 1000 1996 7409 1000 1010 1997 9969 4487 23809 3436 2010 3350 1012 102 7727 2000 2032 2004 2069 1000 1996 7409 1000 1010 2572 3217 5831 5496 2010 2567 1997 9969 4487 23809 3436 2010 3350 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-08-29 15:58:11,546 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-08-29 15:58:11,546 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-08-29 15:58:11,546 label: 1
2022-08-29 15:58:11,546 label_id: 1
2022-08-29 15:58:14,457 Writing example 0 of 408
2022-08-29 15:58:14,458 *** Example ***
2022-08-29 15:58:14,458 guid: dev-1
2022-08-29 15:58:14,458 tokens: [CLS] he said the foods ##er ##vic ##e pie business doesn ' t fit the company ' s long - term growth strategy . [SEP] " the foods ##er ##vic ##e pie business does not fit our long - term growth strategy . [SEP]
2022-08-29 15:58:14,458 input_ids: 101 2002 2056 1996 9440 2121 7903 2063 11345 2449 2987 1005 1056 4906 1996 2194 1005 1055 2146 1011 2744 3930 5656 1012 102 1000 1996 9440 2121 7903 2063 11345 2449 2515 2025 4906 2256 2146 1011 2744 3930 5656 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-08-29 15:58:14,458 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-08-29 15:58:14,458 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-08-29 15:58:14,458 label: 1
2022-08-29 15:58:14,458 label_id: 1
2022-08-29 15:58:14,787 Model config {
  "_name_or_path": "bert-base-uncased",
  "architectures": [
    "BertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "crypten": false,
  "finetuning_task": "mrpc",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "not_equivalent",
    "1": "equivalent"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "equivalent": 1,
    "not_equivalent": 0
  },
  "layer_norm_eps": 1e-12,
  "log_path": "tmp/distill/MRPC/quad_2quad/bert-base-uncased/log.txt",
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "pre_trained": "",
  "problem_type": "single_label_classification",
  "softmax_act": "softmax",
  "torch_dtype": "float32",
  "training": "",
  "transformers_version": "4.20.0.dev0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

2022-08-29 15:58:17,504 Loading model /home/ubuntu/checkpoints/exp/MRPC/pytorch_model.bin
2022-08-29 15:58:17,811 loading model...
2022-08-29 15:58:17,861 done!
2022-08-29 15:58:17,861 Weights from pretrained model not used in TinyBertForSequenceClassification: ['bert.embeddings.position_ids']
2022-08-29 15:58:22,446 ***** Teacher evaluation *****
2022-08-29 15:58:22,446 {'acc': 0.8602941176470589, 'f1': 0.9025641025641027, 'acc_and_f1': 0.8814291101055808, 'eval_loss': 0.3993110977686368}
2022-08-29 15:58:22,447 Model config {
  "_name_or_path": "bert-base-uncased",
  "architectures": [
    "BertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "crypten": false,
  "finetuning_task": "mrpc",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "not_equivalent",
    "1": "equivalent"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "equivalent": 1,
    "not_equivalent": 0
  },
  "layer_norm_eps": 1e-12,
  "log_path": "tmp/distill/MRPC/quad_2quad/bert-base-uncased/log.txt",
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "pre_trained": "",
  "problem_type": "single_label_classification",
  "softmax_act": "softmax",
  "torch_dtype": "float32",
  "training": "",
  "transformers_version": "4.20.0.dev0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

2022-08-29 15:58:25,100 Loading model /home/ubuntu/checkpoints/exp/MRPC/pytorch_model.bin
2022-08-29 15:58:25,401 loading model...
2022-08-29 15:58:25,445 done!
2022-08-29 15:58:25,445 Weights from pretrained model not used in TinyBertForSequenceClassification: ['bert.embeddings.position_ids']
2022-08-29 16:01:56,388 The args: Namespace(act='gelu', aug_train=False, cache_dir='', data_dir='glue_data/MRPC', data_url='', do_eval=False, do_lower_case=True, eval_batch_size=32, eval_step=200, gradient_accumulation_steps=1, learning_rate=5e-05, log_path='tmp/distill/MRPC/quad_2quad/bert-base-uncased/log.txt', max_seq_length=128, no_cuda=False, num_train_epochs=3.0, output_dir='tmp/distill/MRPC/quad_2quad/bert-base-uncased/5e-05_1e-05_32', pred_distill=False, seed=42, softmax_act='softmax', student_model='/home/ubuntu/checkpoints/exp/MRPC', task_name='MRPC', teacher_model='/home/ubuntu/checkpoints/exp/MRPC', temperature=1.0, train_batch_size=32, warmup_proportion=0.1, weight_decay=0.0001)
2022-08-29 16:01:56,388 device: cuda n_gpu: 2
2022-08-29 16:01:56,474 Writing example 0 of 3668
2022-08-29 16:01:56,475 *** Example ***
2022-08-29 16:01:56,475 guid: train-1
2022-08-29 16:01:56,475 tokens: [CLS] am ##ro ##zi accused his brother , whom he called " the witness " , of deliberately di ##stor ##ting his evidence . [SEP] referring to him as only " the witness " , am ##ro ##zi accused his brother of deliberately di ##stor ##ting his evidence . [SEP]
2022-08-29 16:01:56,475 input_ids: 101 2572 3217 5831 5496 2010 2567 1010 3183 2002 2170 1000 1996 7409 1000 1010 1997 9969 4487 23809 3436 2010 3350 1012 102 7727 2000 2032 2004 2069 1000 1996 7409 1000 1010 2572 3217 5831 5496 2010 2567 1997 9969 4487 23809 3436 2010 3350 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-08-29 16:01:56,475 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-08-29 16:01:56,475 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-08-29 16:01:56,475 label: 1
2022-08-29 16:01:56,475 label_id: 1
2022-08-29 16:01:59,401 Writing example 0 of 408
2022-08-29 16:01:59,402 *** Example ***
2022-08-29 16:01:59,403 guid: dev-1
2022-08-29 16:01:59,403 tokens: [CLS] he said the foods ##er ##vic ##e pie business doesn ' t fit the company ' s long - term growth strategy . [SEP] " the foods ##er ##vic ##e pie business does not fit our long - term growth strategy . [SEP]
2022-08-29 16:01:59,403 input_ids: 101 2002 2056 1996 9440 2121 7903 2063 11345 2449 2987 1005 1056 4906 1996 2194 1005 1055 2146 1011 2744 3930 5656 1012 102 1000 1996 9440 2121 7903 2063 11345 2449 2515 2025 4906 2256 2146 1011 2744 3930 5656 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-08-29 16:01:59,403 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-08-29 16:01:59,403 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-08-29 16:01:59,403 label: 1
2022-08-29 16:01:59,403 label_id: 1
2022-08-29 16:01:59,733 Model config {
  "_name_or_path": "bert-base-uncased",
  "architectures": [
    "BertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "crypten": false,
  "finetuning_task": "mrpc",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "not_equivalent",
    "1": "equivalent"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "equivalent": 1,
    "not_equivalent": 0
  },
  "layer_norm_eps": 1e-12,
  "log_path": "tmp/distill/MRPC/quad_2quad/bert-base-uncased/log.txt",
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "pre_trained": "",
  "problem_type": "single_label_classification",
  "softmax_act": "softmax",
  "torch_dtype": "float32",
  "training": "",
  "transformers_version": "4.20.0.dev0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

2022-08-29 16:02:02,457 Loading model /home/ubuntu/checkpoints/exp/MRPC/pytorch_model.bin
2022-08-29 16:02:02,763 loading model...
2022-08-29 16:02:02,810 done!
2022-08-29 16:02:02,810 Weights from pretrained model not used in TinyBertForSequenceClassification: ['bert.embeddings.position_ids']
2022-08-29 16:02:07,392 ***** Teacher evaluation *****
2022-08-29 16:02:07,392 {'acc': 0.8602941176470589, 'f1': 0.9025641025641027, 'acc_and_f1': 0.8814291101055808, 'eval_loss': 0.3993110977686368}
2022-08-29 16:02:07,393 Model config {
  "_name_or_path": "bert-base-uncased",
  "architectures": [
    "BertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "crypten": false,
  "finetuning_task": "mrpc",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "not_equivalent",
    "1": "equivalent"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "equivalent": 1,
    "not_equivalent": 0
  },
  "layer_norm_eps": 1e-12,
  "log_path": "tmp/distill/MRPC/quad_2quad/bert-base-uncased/log.txt",
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "pre_trained": "",
  "problem_type": "single_label_classification",
  "softmax_act": "softmax",
  "torch_dtype": "float32",
  "training": "",
  "transformers_version": "4.20.0.dev0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

2022-08-29 16:02:10,048 Loading model /home/ubuntu/checkpoints/exp/MRPC/pytorch_model.bin
2022-08-29 16:02:10,355 loading model...
2022-08-29 16:02:10,400 done!
2022-08-29 16:02:10,400 Weights from pretrained model not used in TinyBertForSequenceClassification: ['bert.embeddings.position_ids']
2022-08-29 16:06:04,128 The args: Namespace(aug_train=False, cache_dir='', data_dir='glue_data/MRPC', data_url='', do_eval=False, do_lower_case=True, eval_batch_size=32, eval_step=200, gradient_accumulation_steps=1, hidden_act='quad', learning_rate=5e-05, log_path='tmp/distill/MRPC/quad_2quad/bert-base-uncased/log.txt', max_seq_length=128, no_cuda=False, num_train_epochs=3.0, output_dir='tmp/distill/MRPC/quad_2quad/bert-base-uncased/5e-05_1e-05_32', pred_distill=False, seed=42, softmax_act='2quad', student_model='/home/ubuntu/checkpoints/exp/MRPC', task_name='MRPC', teacher_model='/home/ubuntu/checkpoints/exp/MRPC', temperature=1.0, train_batch_size=32, warmup_proportion=0.1, weight_decay=0.0001)
2022-08-29 16:06:04,128 device: cuda n_gpu: 2
2022-08-29 16:06:04,212 Writing example 0 of 3668
2022-08-29 16:06:04,213 *** Example ***
2022-08-29 16:06:04,214 guid: train-1
2022-08-29 16:06:04,214 tokens: [CLS] am ##ro ##zi accused his brother , whom he called " the witness " , of deliberately di ##stor ##ting his evidence . [SEP] referring to him as only " the witness " , am ##ro ##zi accused his brother of deliberately di ##stor ##ting his evidence . [SEP]
2022-08-29 16:06:04,214 input_ids: 101 2572 3217 5831 5496 2010 2567 1010 3183 2002 2170 1000 1996 7409 1000 1010 1997 9969 4487 23809 3436 2010 3350 1012 102 7727 2000 2032 2004 2069 1000 1996 7409 1000 1010 2572 3217 5831 5496 2010 2567 1997 9969 4487 23809 3436 2010 3350 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-08-29 16:06:04,214 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-08-29 16:06:04,214 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-08-29 16:06:04,214 label: 1
2022-08-29 16:06:04,214 label_id: 1
2022-08-29 16:06:07,129 Writing example 0 of 408
2022-08-29 16:06:07,130 *** Example ***
2022-08-29 16:06:07,130 guid: dev-1
2022-08-29 16:06:07,131 tokens: [CLS] he said the foods ##er ##vic ##e pie business doesn ' t fit the company ' s long - term growth strategy . [SEP] " the foods ##er ##vic ##e pie business does not fit our long - term growth strategy . [SEP]
2022-08-29 16:06:07,131 input_ids: 101 2002 2056 1996 9440 2121 7903 2063 11345 2449 2987 1005 1056 4906 1996 2194 1005 1055 2146 1011 2744 3930 5656 1012 102 1000 1996 9440 2121 7903 2063 11345 2449 2515 2025 4906 2256 2146 1011 2744 3930 5656 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-08-29 16:06:07,131 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-08-29 16:06:07,131 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-08-29 16:06:07,131 label: 1
2022-08-29 16:06:07,131 label_id: 1
2022-08-29 16:06:07,460 Model config {
  "_name_or_path": "bert-base-uncased",
  "architectures": [
    "BertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "crypten": false,
  "finetuning_task": "mrpc",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "not_equivalent",
    "1": "equivalent"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "equivalent": 1,
    "not_equivalent": 0
  },
  "layer_norm_eps": 1e-12,
  "log_path": "tmp/distill/MRPC/quad_2quad/bert-base-uncased/log.txt",
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "pre_trained": "",
  "problem_type": "single_label_classification",
  "softmax_act": "softmax",
  "torch_dtype": "float32",
  "training": "",
  "transformers_version": "4.20.0.dev0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

2022-08-29 16:06:10,174 Loading model /home/ubuntu/checkpoints/exp/MRPC/pytorch_model.bin
2022-08-29 16:06:10,490 loading model...
2022-08-29 16:06:10,536 done!
2022-08-29 16:06:10,537 Weights from pretrained model not used in TinyBertForSequenceClassification: ['bert.embeddings.position_ids']
2022-08-29 16:06:15,106 ***** Teacher evaluation *****
2022-08-29 16:06:15,107 {'acc': 0.8602941176470589, 'f1': 0.9025641025641027, 'acc_and_f1': 0.8814291101055808, 'eval_loss': 0.3993110977686368}
2022-08-29 16:06:15,107 Model config {
  "_name_or_path": "bert-base-uncased",
  "architectures": [
    "BertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "crypten": false,
  "finetuning_task": "mrpc",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "not_equivalent",
    "1": "equivalent"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "equivalent": 1,
    "not_equivalent": 0
  },
  "layer_norm_eps": 1e-12,
  "log_path": "tmp/distill/MRPC/quad_2quad/bert-base-uncased/log.txt",
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "pre_trained": "",
  "problem_type": "single_label_classification",
  "softmax_act": "softmax",
  "torch_dtype": "float32",
  "training": "",
  "transformers_version": "4.20.0.dev0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

2022-08-29 16:06:17,757 Loading model /home/ubuntu/checkpoints/exp/MRPC/pytorch_model.bin
2022-08-29 16:06:18,055 loading model...
2022-08-29 16:06:18,099 done!
2022-08-29 16:06:18,099 Weights from pretrained model not used in TinyBertForSequenceClassification: ['bert.embeddings.position_ids']
2022-08-29 16:06:18,180 ***** Running training *****
2022-08-29 16:06:18,180   Num examples = 3668
2022-08-29 16:06:18,180   Batch size = 32
2022-08-29 16:06:18,181   Num steps = 2280
2022-08-29 16:06:18,182 n: module.bert.embeddings.word_embeddings.weight
2022-08-29 16:06:18,182 n: module.bert.embeddings.position_embeddings.weight
2022-08-29 16:06:18,182 n: module.bert.embeddings.token_type_embeddings.weight
2022-08-29 16:06:18,182 n: module.bert.embeddings.LayerNorm.weight
2022-08-29 16:06:18,182 n: module.bert.embeddings.LayerNorm.bias
2022-08-29 16:06:18,182 n: module.bert.encoder.layer.0.attention.self.query.weight
2022-08-29 16:06:18,182 n: module.bert.encoder.layer.0.attention.self.query.bias
2022-08-29 16:06:18,182 n: module.bert.encoder.layer.0.attention.self.key.weight
2022-08-29 16:06:18,182 n: module.bert.encoder.layer.0.attention.self.key.bias
2022-08-29 16:06:18,182 n: module.bert.encoder.layer.0.attention.self.value.weight
2022-08-29 16:06:18,182 n: module.bert.encoder.layer.0.attention.self.value.bias
2022-08-29 16:06:18,183 n: module.bert.encoder.layer.0.attention.output.dense.weight
2022-08-29 16:06:18,183 n: module.bert.encoder.layer.0.attention.output.dense.bias
2022-08-29 16:06:18,183 n: module.bert.encoder.layer.0.attention.output.LayerNorm.weight
2022-08-29 16:06:18,183 n: module.bert.encoder.layer.0.attention.output.LayerNorm.bias
2022-08-29 16:06:18,183 n: module.bert.encoder.layer.0.intermediate.dense.weight
2022-08-29 16:06:18,183 n: module.bert.encoder.layer.0.intermediate.dense.bias
2022-08-29 16:06:18,183 n: module.bert.encoder.layer.0.output.dense.weight
2022-08-29 16:06:18,183 n: module.bert.encoder.layer.0.output.dense.bias
2022-08-29 16:06:18,183 n: module.bert.encoder.layer.0.output.LayerNorm.weight
2022-08-29 16:06:18,183 n: module.bert.encoder.layer.0.output.LayerNorm.bias
2022-08-29 16:06:18,183 n: module.bert.encoder.layer.1.attention.self.query.weight
2022-08-29 16:06:18,183 n: module.bert.encoder.layer.1.attention.self.query.bias
2022-08-29 16:06:18,183 n: module.bert.encoder.layer.1.attention.self.key.weight
2022-08-29 16:06:18,183 n: module.bert.encoder.layer.1.attention.self.key.bias
2022-08-29 16:06:18,183 n: module.bert.encoder.layer.1.attention.self.value.weight
2022-08-29 16:06:18,183 n: module.bert.encoder.layer.1.attention.self.value.bias
2022-08-29 16:06:18,183 n: module.bert.encoder.layer.1.attention.output.dense.weight
2022-08-29 16:06:18,184 n: module.bert.encoder.layer.1.attention.output.dense.bias
2022-08-29 16:06:18,184 n: module.bert.encoder.layer.1.attention.output.LayerNorm.weight
2022-08-29 16:06:18,184 n: module.bert.encoder.layer.1.attention.output.LayerNorm.bias
2022-08-29 16:06:18,184 n: module.bert.encoder.layer.1.intermediate.dense.weight
2022-08-29 16:06:18,184 n: module.bert.encoder.layer.1.intermediate.dense.bias
2022-08-29 16:06:18,184 n: module.bert.encoder.layer.1.output.dense.weight
2022-08-29 16:06:18,184 n: module.bert.encoder.layer.1.output.dense.bias
2022-08-29 16:06:18,184 n: module.bert.encoder.layer.1.output.LayerNorm.weight
2022-08-29 16:06:18,184 n: module.bert.encoder.layer.1.output.LayerNorm.bias
2022-08-29 16:06:18,184 n: module.bert.encoder.layer.2.attention.self.query.weight
2022-08-29 16:06:18,184 n: module.bert.encoder.layer.2.attention.self.query.bias
2022-08-29 16:06:18,184 n: module.bert.encoder.layer.2.attention.self.key.weight
2022-08-29 16:06:18,184 n: module.bert.encoder.layer.2.attention.self.key.bias
2022-08-29 16:06:18,184 n: module.bert.encoder.layer.2.attention.self.value.weight
2022-08-29 16:06:18,184 n: module.bert.encoder.layer.2.attention.self.value.bias
2022-08-29 16:06:18,184 n: module.bert.encoder.layer.2.attention.output.dense.weight
2022-08-29 16:06:18,184 n: module.bert.encoder.layer.2.attention.output.dense.bias
2022-08-29 16:06:18,185 n: module.bert.encoder.layer.2.attention.output.LayerNorm.weight
2022-08-29 16:06:18,185 n: module.bert.encoder.layer.2.attention.output.LayerNorm.bias
2022-08-29 16:06:18,185 n: module.bert.encoder.layer.2.intermediate.dense.weight
2022-08-29 16:06:18,185 n: module.bert.encoder.layer.2.intermediate.dense.bias
2022-08-29 16:06:18,185 n: module.bert.encoder.layer.2.output.dense.weight
2022-08-29 16:06:18,185 n: module.bert.encoder.layer.2.output.dense.bias
2022-08-29 16:06:18,185 n: module.bert.encoder.layer.2.output.LayerNorm.weight
2022-08-29 16:06:18,185 n: module.bert.encoder.layer.2.output.LayerNorm.bias
2022-08-29 16:06:18,185 n: module.bert.encoder.layer.3.attention.self.query.weight
2022-08-29 16:06:18,185 n: module.bert.encoder.layer.3.attention.self.query.bias
2022-08-29 16:06:18,185 n: module.bert.encoder.layer.3.attention.self.key.weight
2022-08-29 16:06:18,185 n: module.bert.encoder.layer.3.attention.self.key.bias
2022-08-29 16:06:18,185 n: module.bert.encoder.layer.3.attention.self.value.weight
2022-08-29 16:06:18,185 n: module.bert.encoder.layer.3.attention.self.value.bias
2022-08-29 16:06:18,185 n: module.bert.encoder.layer.3.attention.output.dense.weight
2022-08-29 16:06:18,185 n: module.bert.encoder.layer.3.attention.output.dense.bias
2022-08-29 16:06:18,185 n: module.bert.encoder.layer.3.attention.output.LayerNorm.weight
2022-08-29 16:06:18,185 n: module.bert.encoder.layer.3.attention.output.LayerNorm.bias
2022-08-29 16:06:18,186 n: module.bert.encoder.layer.3.intermediate.dense.weight
2022-08-29 16:06:18,186 n: module.bert.encoder.layer.3.intermediate.dense.bias
2022-08-29 16:06:18,186 n: module.bert.encoder.layer.3.output.dense.weight
2022-08-29 16:06:18,186 n: module.bert.encoder.layer.3.output.dense.bias
2022-08-29 16:06:18,186 n: module.bert.encoder.layer.3.output.LayerNorm.weight
2022-08-29 16:06:18,186 n: module.bert.encoder.layer.3.output.LayerNorm.bias
2022-08-29 16:06:18,186 n: module.bert.encoder.layer.4.attention.self.query.weight
2022-08-29 16:06:18,186 n: module.bert.encoder.layer.4.attention.self.query.bias
2022-08-29 16:06:18,186 n: module.bert.encoder.layer.4.attention.self.key.weight
2022-08-29 16:06:18,186 n: module.bert.encoder.layer.4.attention.self.key.bias
2022-08-29 16:06:18,186 n: module.bert.encoder.layer.4.attention.self.value.weight
2022-08-29 16:06:18,186 n: module.bert.encoder.layer.4.attention.self.value.bias
2022-08-29 16:06:18,186 n: module.bert.encoder.layer.4.attention.output.dense.weight
2022-08-29 16:06:18,186 n: module.bert.encoder.layer.4.attention.output.dense.bias
2022-08-29 16:06:18,186 n: module.bert.encoder.layer.4.attention.output.LayerNorm.weight
2022-08-29 16:06:18,186 n: module.bert.encoder.layer.4.attention.output.LayerNorm.bias
2022-08-29 16:06:18,187 n: module.bert.encoder.layer.4.intermediate.dense.weight
2022-08-29 16:06:18,187 n: module.bert.encoder.layer.4.intermediate.dense.bias
2022-08-29 16:06:18,187 n: module.bert.encoder.layer.4.output.dense.weight
2022-08-29 16:06:18,187 n: module.bert.encoder.layer.4.output.dense.bias
2022-08-29 16:06:18,187 n: module.bert.encoder.layer.4.output.LayerNorm.weight
2022-08-29 16:06:18,187 n: module.bert.encoder.layer.4.output.LayerNorm.bias
2022-08-29 16:06:18,187 n: module.bert.encoder.layer.5.attention.self.query.weight
2022-08-29 16:06:18,187 n: module.bert.encoder.layer.5.attention.self.query.bias
2022-08-29 16:06:18,187 n: module.bert.encoder.layer.5.attention.self.key.weight
2022-08-29 16:06:18,187 n: module.bert.encoder.layer.5.attention.self.key.bias
2022-08-29 16:06:18,187 n: module.bert.encoder.layer.5.attention.self.value.weight
2022-08-29 16:06:18,187 n: module.bert.encoder.layer.5.attention.self.value.bias
2022-08-29 16:06:18,187 n: module.bert.encoder.layer.5.attention.output.dense.weight
2022-08-29 16:06:18,187 n: module.bert.encoder.layer.5.attention.output.dense.bias
2022-08-29 16:06:18,187 n: module.bert.encoder.layer.5.attention.output.LayerNorm.weight
2022-08-29 16:06:18,187 n: module.bert.encoder.layer.5.attention.output.LayerNorm.bias
2022-08-29 16:06:18,187 n: module.bert.encoder.layer.5.intermediate.dense.weight
2022-08-29 16:06:18,187 n: module.bert.encoder.layer.5.intermediate.dense.bias
2022-08-29 16:06:18,188 n: module.bert.encoder.layer.5.output.dense.weight
2022-08-29 16:06:18,188 n: module.bert.encoder.layer.5.output.dense.bias
2022-08-29 16:06:18,188 n: module.bert.encoder.layer.5.output.LayerNorm.weight
2022-08-29 16:06:18,188 n: module.bert.encoder.layer.5.output.LayerNorm.bias
2022-08-29 16:06:18,188 n: module.bert.encoder.layer.6.attention.self.query.weight
2022-08-29 16:06:18,188 n: module.bert.encoder.layer.6.attention.self.query.bias
2022-08-29 16:06:18,188 n: module.bert.encoder.layer.6.attention.self.key.weight
2022-08-29 16:06:18,188 n: module.bert.encoder.layer.6.attention.self.key.bias
2022-08-29 16:06:18,188 n: module.bert.encoder.layer.6.attention.self.value.weight
2022-08-29 16:06:18,188 n: module.bert.encoder.layer.6.attention.self.value.bias
2022-08-29 16:06:18,188 n: module.bert.encoder.layer.6.attention.output.dense.weight
2022-08-29 16:06:18,188 n: module.bert.encoder.layer.6.attention.output.dense.bias
2022-08-29 16:06:18,188 n: module.bert.encoder.layer.6.attention.output.LayerNorm.weight
2022-08-29 16:06:18,188 n: module.bert.encoder.layer.6.attention.output.LayerNorm.bias
2022-08-29 16:06:18,188 n: module.bert.encoder.layer.6.intermediate.dense.weight
2022-08-29 16:06:18,188 n: module.bert.encoder.layer.6.intermediate.dense.bias
2022-08-29 16:06:18,188 n: module.bert.encoder.layer.6.output.dense.weight
2022-08-29 16:06:18,189 n: module.bert.encoder.layer.6.output.dense.bias
2022-08-29 16:06:18,189 n: module.bert.encoder.layer.6.output.LayerNorm.weight
2022-08-29 16:06:18,189 n: module.bert.encoder.layer.6.output.LayerNorm.bias
2022-08-29 16:06:18,189 n: module.bert.encoder.layer.7.attention.self.query.weight
2022-08-29 16:06:18,189 n: module.bert.encoder.layer.7.attention.self.query.bias
2022-08-29 16:06:18,189 n: module.bert.encoder.layer.7.attention.self.key.weight
2022-08-29 16:06:18,189 n: module.bert.encoder.layer.7.attention.self.key.bias
2022-08-29 16:06:18,189 n: module.bert.encoder.layer.7.attention.self.value.weight
2022-08-29 16:06:18,189 n: module.bert.encoder.layer.7.attention.self.value.bias
2022-08-29 16:06:18,189 n: module.bert.encoder.layer.7.attention.output.dense.weight
2022-08-29 16:06:18,189 n: module.bert.encoder.layer.7.attention.output.dense.bias
2022-08-29 16:06:18,189 n: module.bert.encoder.layer.7.attention.output.LayerNorm.weight
2022-08-29 16:06:18,189 n: module.bert.encoder.layer.7.attention.output.LayerNorm.bias
2022-08-29 16:06:18,189 n: module.bert.encoder.layer.7.intermediate.dense.weight
2022-08-29 16:06:18,189 n: module.bert.encoder.layer.7.intermediate.dense.bias
2022-08-29 16:06:18,189 n: module.bert.encoder.layer.7.output.dense.weight
2022-08-29 16:06:18,189 n: module.bert.encoder.layer.7.output.dense.bias
2022-08-29 16:06:18,190 n: module.bert.encoder.layer.7.output.LayerNorm.weight
2022-08-29 16:06:18,190 n: module.bert.encoder.layer.7.output.LayerNorm.bias
2022-08-29 16:06:18,190 n: module.bert.encoder.layer.8.attention.self.query.weight
2022-08-29 16:06:18,190 n: module.bert.encoder.layer.8.attention.self.query.bias
2022-08-29 16:06:18,190 n: module.bert.encoder.layer.8.attention.self.key.weight
2022-08-29 16:06:18,190 n: module.bert.encoder.layer.8.attention.self.key.bias
2022-08-29 16:06:18,190 n: module.bert.encoder.layer.8.attention.self.value.weight
2022-08-29 16:06:18,190 n: module.bert.encoder.layer.8.attention.self.value.bias
2022-08-29 16:06:18,190 n: module.bert.encoder.layer.8.attention.output.dense.weight
2022-08-29 16:06:18,190 n: module.bert.encoder.layer.8.attention.output.dense.bias
2022-08-29 16:06:18,190 n: module.bert.encoder.layer.8.attention.output.LayerNorm.weight
2022-08-29 16:06:18,190 n: module.bert.encoder.layer.8.attention.output.LayerNorm.bias
2022-08-29 16:06:18,190 n: module.bert.encoder.layer.8.intermediate.dense.weight
2022-08-29 16:06:18,190 n: module.bert.encoder.layer.8.intermediate.dense.bias
2022-08-29 16:06:18,190 n: module.bert.encoder.layer.8.output.dense.weight
2022-08-29 16:06:18,190 n: module.bert.encoder.layer.8.output.dense.bias
2022-08-29 16:06:18,190 n: module.bert.encoder.layer.8.output.LayerNorm.weight
2022-08-29 16:06:18,191 n: module.bert.encoder.layer.8.output.LayerNorm.bias
2022-08-29 16:06:18,191 n: module.bert.encoder.layer.9.attention.self.query.weight
2022-08-29 16:06:18,191 n: module.bert.encoder.layer.9.attention.self.query.bias
2022-08-29 16:06:18,191 n: module.bert.encoder.layer.9.attention.self.key.weight
2022-08-29 16:06:18,191 n: module.bert.encoder.layer.9.attention.self.key.bias
2022-08-29 16:06:18,191 n: module.bert.encoder.layer.9.attention.self.value.weight
2022-08-29 16:06:18,191 n: module.bert.encoder.layer.9.attention.self.value.bias
2022-08-29 16:06:18,191 n: module.bert.encoder.layer.9.attention.output.dense.weight
2022-08-29 16:06:18,191 n: module.bert.encoder.layer.9.attention.output.dense.bias
2022-08-29 16:06:18,191 n: module.bert.encoder.layer.9.attention.output.LayerNorm.weight
2022-08-29 16:06:18,191 n: module.bert.encoder.layer.9.attention.output.LayerNorm.bias
2022-08-29 16:06:18,191 n: module.bert.encoder.layer.9.intermediate.dense.weight
2022-08-29 16:06:18,191 n: module.bert.encoder.layer.9.intermediate.dense.bias
2022-08-29 16:06:18,191 n: module.bert.encoder.layer.9.output.dense.weight
2022-08-29 16:06:18,191 n: module.bert.encoder.layer.9.output.dense.bias
2022-08-29 16:06:18,191 n: module.bert.encoder.layer.9.output.LayerNorm.weight
2022-08-29 16:06:18,191 n: module.bert.encoder.layer.9.output.LayerNorm.bias
2022-08-29 16:06:18,191 n: module.bert.encoder.layer.10.attention.self.query.weight
2022-08-29 16:06:18,192 n: module.bert.encoder.layer.10.attention.self.query.bias
2022-08-29 16:06:18,192 n: module.bert.encoder.layer.10.attention.self.key.weight
2022-08-29 16:06:18,192 n: module.bert.encoder.layer.10.attention.self.key.bias
2022-08-29 16:06:18,192 n: module.bert.encoder.layer.10.attention.self.value.weight
2022-08-29 16:06:18,192 n: module.bert.encoder.layer.10.attention.self.value.bias
2022-08-29 16:06:18,192 n: module.bert.encoder.layer.10.attention.output.dense.weight
2022-08-29 16:06:18,192 n: module.bert.encoder.layer.10.attention.output.dense.bias
2022-08-29 16:06:18,192 n: module.bert.encoder.layer.10.attention.output.LayerNorm.weight
2022-08-29 16:06:18,192 n: module.bert.encoder.layer.10.attention.output.LayerNorm.bias
2022-08-29 16:06:18,192 n: module.bert.encoder.layer.10.intermediate.dense.weight
2022-08-29 16:06:18,192 n: module.bert.encoder.layer.10.intermediate.dense.bias
2022-08-29 16:06:18,192 n: module.bert.encoder.layer.10.output.dense.weight
2022-08-29 16:06:18,192 n: module.bert.encoder.layer.10.output.dense.bias
2022-08-29 16:06:18,192 n: module.bert.encoder.layer.10.output.LayerNorm.weight
2022-08-29 16:06:18,192 n: module.bert.encoder.layer.10.output.LayerNorm.bias
2022-08-29 16:06:18,192 n: module.bert.encoder.layer.11.attention.self.query.weight
2022-08-29 16:06:18,192 n: module.bert.encoder.layer.11.attention.self.query.bias
2022-08-29 16:06:18,193 n: module.bert.encoder.layer.11.attention.self.key.weight
2022-08-29 16:06:18,193 n: module.bert.encoder.layer.11.attention.self.key.bias
2022-08-29 16:06:18,193 n: module.bert.encoder.layer.11.attention.self.value.weight
2022-08-29 16:06:18,193 n: module.bert.encoder.layer.11.attention.self.value.bias
2022-08-29 16:06:18,193 n: module.bert.encoder.layer.11.attention.output.dense.weight
2022-08-29 16:06:18,193 n: module.bert.encoder.layer.11.attention.output.dense.bias
2022-08-29 16:06:18,193 n: module.bert.encoder.layer.11.attention.output.LayerNorm.weight
2022-08-29 16:06:18,193 n: module.bert.encoder.layer.11.attention.output.LayerNorm.bias
2022-08-29 16:06:18,193 n: module.bert.encoder.layer.11.intermediate.dense.weight
2022-08-29 16:06:18,193 n: module.bert.encoder.layer.11.intermediate.dense.bias
2022-08-29 16:06:18,193 n: module.bert.encoder.layer.11.output.dense.weight
2022-08-29 16:06:18,193 n: module.bert.encoder.layer.11.output.dense.bias
2022-08-29 16:06:18,193 n: module.bert.encoder.layer.11.output.LayerNorm.weight
2022-08-29 16:06:18,193 n: module.bert.encoder.layer.11.output.LayerNorm.bias
2022-08-29 16:06:18,193 n: module.bert.pooler.dense.weight
2022-08-29 16:06:18,193 n: module.bert.pooler.dense.bias
2022-08-29 16:06:18,193 n: module.classifier.weight
2022-08-29 16:06:18,194 n: module.classifier.bias
2022-08-29 16:06:18,194 Total parameters: 109483778
2022-08-29 16:07:26,193 ***** Running evaluation *****
2022-08-29 16:07:26,193   Epoch = 1 iter 199 step
2022-08-29 16:07:26,194   Num examples = 408
2022-08-29 16:07:26,194   Batch size = 32
2022-08-29 16:07:26,195 ***** Eval results *****
2022-08-29 16:07:26,195   att_loss = 4.940654462926528
2022-08-29 16:07:26,195   cls_loss = 0.0
2022-08-29 16:07:26,195   global_step = 199
2022-08-29 16:07:26,195   loss = 6.517810406404383
2022-08-29 16:07:26,195   rep_loss = 1.5771559350630817
2022-08-29 16:07:26,195 ***** Save model *****
2022-08-29 16:24:22,148 The args: Namespace(aug_train=False, cache_dir='', data_dir='glue_data/MRPC', data_url='', do_eval=False, do_lower_case=True, eval_batch_size=32, eval_step=200, gradient_accumulation_steps=1, hidden_act='quad', learning_rate=5e-05, log_path='tmp/distill/MRPC/quad_2quad/bert-base-uncased/log.txt', max_seq_length=128, no_cuda=False, num_train_epochs=3.0, output_dir='tmp/distill/MRPC/quad_2quad/bert-base-uncased/5e-05_1e-05_32', pred_distill=False, seed=42, softmax_act='2quad', student_model='/home/ubuntu/checkpoints/exp/MRPC', task_name='MRPC', teacher_model='/home/ubuntu/checkpoints/exp/MRPC', temperature=1.0, train_batch_size=32, warmup_proportion=0.1, weight_decay=0.0001)
2022-08-29 16:24:22,149 device: cuda n_gpu: 2
2022-08-29 16:24:22,235 Writing example 0 of 3668
2022-08-29 16:24:22,236 *** Example ***
2022-08-29 16:24:22,236 guid: train-1
2022-08-29 16:24:22,236 tokens: [CLS] am ##ro ##zi accused his brother , whom he called " the witness " , of deliberately di ##stor ##ting his evidence . [SEP] referring to him as only " the witness " , am ##ro ##zi accused his brother of deliberately di ##stor ##ting his evidence . [SEP]
2022-08-29 16:24:22,236 input_ids: 101 2572 3217 5831 5496 2010 2567 1010 3183 2002 2170 1000 1996 7409 1000 1010 1997 9969 4487 23809 3436 2010 3350 1012 102 7727 2000 2032 2004 2069 1000 1996 7409 1000 1010 2572 3217 5831 5496 2010 2567 1997 9969 4487 23809 3436 2010 3350 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-08-29 16:24:22,236 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-08-29 16:24:22,236 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-08-29 16:24:22,236 label: 1
2022-08-29 16:24:22,236 label_id: 1
2022-08-29 16:24:25,207 Writing example 0 of 408
2022-08-29 16:24:25,208 *** Example ***
2022-08-29 16:24:25,208 guid: dev-1
2022-08-29 16:24:25,208 tokens: [CLS] he said the foods ##er ##vic ##e pie business doesn ' t fit the company ' s long - term growth strategy . [SEP] " the foods ##er ##vic ##e pie business does not fit our long - term growth strategy . [SEP]
2022-08-29 16:24:25,208 input_ids: 101 2002 2056 1996 9440 2121 7903 2063 11345 2449 2987 1005 1056 4906 1996 2194 1005 1055 2146 1011 2744 3930 5656 1012 102 1000 1996 9440 2121 7903 2063 11345 2449 2515 2025 4906 2256 2146 1011 2744 3930 5656 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-08-29 16:24:25,208 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-08-29 16:24:25,208 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-08-29 16:24:25,209 label: 1
2022-08-29 16:24:25,209 label_id: 1
2022-08-29 16:25:04,764 The args: Namespace(aug_train=False, cache_dir='', data_dir='glue_data/MRPC', data_url='', do_eval=False, do_lower_case=True, eval_batch_size=32, eval_step=200, gradient_accumulation_steps=1, hidden_act='quad', learning_rate=5e-05, log_path='tmp/distill/MRPC/quad_2quad/bert-base-uncased/log.txt', max_seq_length=128, no_cuda=False, num_train_epochs=3.0, output_dir='tmp/distill/MRPC/quad_2quad/bert-base-uncased/5e-05_1e-05_32', pred_distill=False, seed=42, softmax_act='2quad', student_model='/home/ubuntu/checkpoints/exp/MRPC', task_name='MRPC', teacher_model='/home/ubuntu/checkpoints/exp/MRPC', temperature=1.0, train_batch_size=32, warmup_proportion=0.1, weight_decay=0.0001)
2022-08-29 16:25:04,764 device: cuda n_gpu: 2
2022-08-29 16:25:04,850 Writing example 0 of 3668
2022-08-29 16:25:04,851 *** Example ***
2022-08-29 16:25:04,851 guid: train-1
2022-08-29 16:25:04,851 tokens: [CLS] am ##ro ##zi accused his brother , whom he called " the witness " , of deliberately di ##stor ##ting his evidence . [SEP] referring to him as only " the witness " , am ##ro ##zi accused his brother of deliberately di ##stor ##ting his evidence . [SEP]
2022-08-29 16:25:04,852 input_ids: 101 2572 3217 5831 5496 2010 2567 1010 3183 2002 2170 1000 1996 7409 1000 1010 1997 9969 4487 23809 3436 2010 3350 1012 102 7727 2000 2032 2004 2069 1000 1996 7409 1000 1010 2572 3217 5831 5496 2010 2567 1997 9969 4487 23809 3436 2010 3350 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-08-29 16:25:04,852 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-08-29 16:25:04,852 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-08-29 16:25:04,852 label: 1
2022-08-29 16:25:04,852 label_id: 1
2022-08-29 16:25:07,837 Writing example 0 of 408
2022-08-29 16:25:07,838 *** Example ***
2022-08-29 16:25:07,838 guid: dev-1
2022-08-29 16:25:07,838 tokens: [CLS] he said the foods ##er ##vic ##e pie business doesn ' t fit the company ' s long - term growth strategy . [SEP] " the foods ##er ##vic ##e pie business does not fit our long - term growth strategy . [SEP]
2022-08-29 16:25:07,839 input_ids: 101 2002 2056 1996 9440 2121 7903 2063 11345 2449 2987 1005 1056 4906 1996 2194 1005 1055 2146 1011 2744 3930 5656 1012 102 1000 1996 9440 2121 7903 2063 11345 2449 2515 2025 4906 2256 2146 1011 2744 3930 5656 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-08-29 16:25:07,839 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-08-29 16:25:07,839 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-08-29 16:25:07,839 label: 1
2022-08-29 16:25:07,839 label_id: 1
2022-08-29 16:25:08,172 Model config {
  "_name_or_path": "bert-base-uncased",
  "architectures": [
    "BertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "crypten": false,
  "finetuning_task": "mrpc",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "not_equivalent",
    "1": "equivalent"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "equivalent": 1,
    "not_equivalent": 0
  },
  "layer_norm_eps": 1e-12,
  "log_path": "tmp/distill/MRPC/quad_2quad/bert-base-uncased/log.txt",
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "pre_trained": "",
  "problem_type": "single_label_classification",
  "softmax_act": "softmax",
  "torch_dtype": "float32",
  "training": "",
  "transformers_version": "4.20.0.dev0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

2022-08-29 16:25:10,944 Loading model /home/ubuntu/checkpoints/exp/MRPC/pytorch_model.bin
2022-08-29 16:25:11,266 loading model...
2022-08-29 16:25:11,317 done!
2022-08-29 16:25:11,317 Weights from pretrained model not used in TinyBertForSequenceClassification: ['bert.embeddings.position_ids']
2022-08-29 16:25:16,062 ***** Teacher evaluation *****
2022-08-29 16:25:16,062 {'acc': 0.8602941176470589, 'f1': 0.9025641025641027, 'acc_and_f1': 0.8814291101055808, 'eval_loss': 0.3993110977686368}
2022-08-29 16:25:16,063 Model config {
  "_name_or_path": "bert-base-uncased",
  "architectures": [
    "BertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "crypten": false,
  "finetuning_task": "mrpc",
  "gradient_checkpointing": false,
  "hidden_act": "quad",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "not_equivalent",
    "1": "equivalent"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "equivalent": 1,
    "not_equivalent": 0
  },
  "layer_norm_eps": 1e-12,
  "log_path": "tmp/distill/MRPC/quad_2quad/bert-base-uncased/log.txt",
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "pre_trained": "",
  "problem_type": "single_label_classification",
  "softmax_act": "2quad",
  "torch_dtype": "float32",
  "training": "",
  "transformers_version": "4.20.0.dev0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

2022-08-29 16:25:18,791 Loading model /home/ubuntu/checkpoints/exp/MRPC/pytorch_model.bin
2022-08-29 16:25:19,107 loading model...
2022-08-29 16:25:19,151 done!
2022-08-29 16:25:19,151 Weights from pretrained model not used in TinyBertForSequenceClassification: ['bert.embeddings.position_ids']
2022-08-29 16:25:19,240 ***** Running training *****
2022-08-29 16:25:19,241   Num examples = 3668
2022-08-29 16:25:19,241   Batch size = 32
2022-08-29 16:25:19,241   Num steps = 2280
2022-08-29 16:25:19,242 n: module.bert.embeddings.word_embeddings.weight
2022-08-29 16:25:19,242 n: module.bert.embeddings.position_embeddings.weight
2022-08-29 16:25:19,243 n: module.bert.embeddings.token_type_embeddings.weight
2022-08-29 16:25:19,243 n: module.bert.embeddings.LayerNorm.weight
2022-08-29 16:25:19,243 n: module.bert.embeddings.LayerNorm.bias
2022-08-29 16:25:19,243 n: module.bert.encoder.layer.0.attention.self.query.weight
2022-08-29 16:25:19,243 n: module.bert.encoder.layer.0.attention.self.query.bias
2022-08-29 16:25:19,243 n: module.bert.encoder.layer.0.attention.self.key.weight
2022-08-29 16:25:19,243 n: module.bert.encoder.layer.0.attention.self.key.bias
2022-08-29 16:25:19,243 n: module.bert.encoder.layer.0.attention.self.value.weight
2022-08-29 16:25:19,243 n: module.bert.encoder.layer.0.attention.self.value.bias
2022-08-29 16:25:19,243 n: module.bert.encoder.layer.0.attention.output.dense.weight
2022-08-29 16:25:19,243 n: module.bert.encoder.layer.0.attention.output.dense.bias
2022-08-29 16:25:19,243 n: module.bert.encoder.layer.0.attention.output.LayerNorm.weight
2022-08-29 16:25:19,243 n: module.bert.encoder.layer.0.attention.output.LayerNorm.bias
2022-08-29 16:25:19,243 n: module.bert.encoder.layer.0.intermediate.dense.weight
2022-08-29 16:25:19,243 n: module.bert.encoder.layer.0.intermediate.dense.bias
2022-08-29 16:25:19,244 n: module.bert.encoder.layer.0.output.dense.weight
2022-08-29 16:25:19,244 n: module.bert.encoder.layer.0.output.dense.bias
2022-08-29 16:25:19,244 n: module.bert.encoder.layer.0.output.LayerNorm.weight
2022-08-29 16:25:19,244 n: module.bert.encoder.layer.0.output.LayerNorm.bias
2022-08-29 16:25:19,244 n: module.bert.encoder.layer.1.attention.self.query.weight
2022-08-29 16:25:19,244 n: module.bert.encoder.layer.1.attention.self.query.bias
2022-08-29 16:25:19,244 n: module.bert.encoder.layer.1.attention.self.key.weight
2022-08-29 16:25:19,244 n: module.bert.encoder.layer.1.attention.self.key.bias
2022-08-29 16:25:19,244 n: module.bert.encoder.layer.1.attention.self.value.weight
2022-08-29 16:25:19,244 n: module.bert.encoder.layer.1.attention.self.value.bias
2022-08-29 16:25:19,244 n: module.bert.encoder.layer.1.attention.output.dense.weight
2022-08-29 16:25:19,244 n: module.bert.encoder.layer.1.attention.output.dense.bias
2022-08-29 16:25:19,244 n: module.bert.encoder.layer.1.attention.output.LayerNorm.weight
2022-08-29 16:25:19,244 n: module.bert.encoder.layer.1.attention.output.LayerNorm.bias
2022-08-29 16:25:19,244 n: module.bert.encoder.layer.1.intermediate.dense.weight
2022-08-29 16:25:19,244 n: module.bert.encoder.layer.1.intermediate.dense.bias
2022-08-29 16:25:19,245 n: module.bert.encoder.layer.1.output.dense.weight
2022-08-29 16:25:19,245 n: module.bert.encoder.layer.1.output.dense.bias
2022-08-29 16:25:19,245 n: module.bert.encoder.layer.1.output.LayerNorm.weight
2022-08-29 16:25:19,245 n: module.bert.encoder.layer.1.output.LayerNorm.bias
2022-08-29 16:25:19,245 n: module.bert.encoder.layer.2.attention.self.query.weight
2022-08-29 16:25:19,245 n: module.bert.encoder.layer.2.attention.self.query.bias
2022-08-29 16:25:19,245 n: module.bert.encoder.layer.2.attention.self.key.weight
2022-08-29 16:25:19,245 n: module.bert.encoder.layer.2.attention.self.key.bias
2022-08-29 16:25:19,245 n: module.bert.encoder.layer.2.attention.self.value.weight
2022-08-29 16:25:19,245 n: module.bert.encoder.layer.2.attention.self.value.bias
2022-08-29 16:25:19,245 n: module.bert.encoder.layer.2.attention.output.dense.weight
2022-08-29 16:25:19,245 n: module.bert.encoder.layer.2.attention.output.dense.bias
2022-08-29 16:25:19,245 n: module.bert.encoder.layer.2.attention.output.LayerNorm.weight
2022-08-29 16:25:19,245 n: module.bert.encoder.layer.2.attention.output.LayerNorm.bias
2022-08-29 16:25:19,245 n: module.bert.encoder.layer.2.intermediate.dense.weight
2022-08-29 16:25:19,245 n: module.bert.encoder.layer.2.intermediate.dense.bias
2022-08-29 16:25:19,246 n: module.bert.encoder.layer.2.output.dense.weight
2022-08-29 16:25:19,246 n: module.bert.encoder.layer.2.output.dense.bias
2022-08-29 16:25:19,246 n: module.bert.encoder.layer.2.output.LayerNorm.weight
2022-08-29 16:25:19,246 n: module.bert.encoder.layer.2.output.LayerNorm.bias
2022-08-29 16:25:19,246 n: module.bert.encoder.layer.3.attention.self.query.weight
2022-08-29 16:25:19,246 n: module.bert.encoder.layer.3.attention.self.query.bias
2022-08-29 16:25:19,246 n: module.bert.encoder.layer.3.attention.self.key.weight
2022-08-29 16:25:19,246 n: module.bert.encoder.layer.3.attention.self.key.bias
2022-08-29 16:25:19,246 n: module.bert.encoder.layer.3.attention.self.value.weight
2022-08-29 16:25:19,246 n: module.bert.encoder.layer.3.attention.self.value.bias
2022-08-29 16:25:19,246 n: module.bert.encoder.layer.3.attention.output.dense.weight
2022-08-29 16:25:19,246 n: module.bert.encoder.layer.3.attention.output.dense.bias
2022-08-29 16:25:19,246 n: module.bert.encoder.layer.3.attention.output.LayerNorm.weight
2022-08-29 16:25:19,246 n: module.bert.encoder.layer.3.attention.output.LayerNorm.bias
2022-08-29 16:25:19,246 n: module.bert.encoder.layer.3.intermediate.dense.weight
2022-08-29 16:25:19,246 n: module.bert.encoder.layer.3.intermediate.dense.bias
2022-08-29 16:25:19,247 n: module.bert.encoder.layer.3.output.dense.weight
2022-08-29 16:25:19,247 n: module.bert.encoder.layer.3.output.dense.bias
2022-08-29 16:25:19,247 n: module.bert.encoder.layer.3.output.LayerNorm.weight
2022-08-29 16:25:19,247 n: module.bert.encoder.layer.3.output.LayerNorm.bias
2022-08-29 16:25:19,247 n: module.bert.encoder.layer.4.attention.self.query.weight
2022-08-29 16:25:19,247 n: module.bert.encoder.layer.4.attention.self.query.bias
2022-08-29 16:25:19,247 n: module.bert.encoder.layer.4.attention.self.key.weight
2022-08-29 16:25:19,247 n: module.bert.encoder.layer.4.attention.self.key.bias
2022-08-29 16:25:19,247 n: module.bert.encoder.layer.4.attention.self.value.weight
2022-08-29 16:25:19,247 n: module.bert.encoder.layer.4.attention.self.value.bias
2022-08-29 16:25:19,247 n: module.bert.encoder.layer.4.attention.output.dense.weight
2022-08-29 16:25:19,247 n: module.bert.encoder.layer.4.attention.output.dense.bias
2022-08-29 16:25:19,247 n: module.bert.encoder.layer.4.attention.output.LayerNorm.weight
2022-08-29 16:25:19,247 n: module.bert.encoder.layer.4.attention.output.LayerNorm.bias
2022-08-29 16:25:19,247 n: module.bert.encoder.layer.4.intermediate.dense.weight
2022-08-29 16:25:19,247 n: module.bert.encoder.layer.4.intermediate.dense.bias
2022-08-29 16:25:19,248 n: module.bert.encoder.layer.4.output.dense.weight
2022-08-29 16:25:19,248 n: module.bert.encoder.layer.4.output.dense.bias
2022-08-29 16:25:19,248 n: module.bert.encoder.layer.4.output.LayerNorm.weight
2022-08-29 16:25:19,248 n: module.bert.encoder.layer.4.output.LayerNorm.bias
2022-08-29 16:25:19,248 n: module.bert.encoder.layer.5.attention.self.query.weight
2022-08-29 16:25:19,248 n: module.bert.encoder.layer.5.attention.self.query.bias
2022-08-29 16:25:19,248 n: module.bert.encoder.layer.5.attention.self.key.weight
2022-08-29 16:25:19,248 n: module.bert.encoder.layer.5.attention.self.key.bias
2022-08-29 16:25:19,248 n: module.bert.encoder.layer.5.attention.self.value.weight
2022-08-29 16:25:19,248 n: module.bert.encoder.layer.5.attention.self.value.bias
2022-08-29 16:25:19,248 n: module.bert.encoder.layer.5.attention.output.dense.weight
2022-08-29 16:25:19,248 n: module.bert.encoder.layer.5.attention.output.dense.bias
2022-08-29 16:25:19,248 n: module.bert.encoder.layer.5.attention.output.LayerNorm.weight
2022-08-29 16:25:19,248 n: module.bert.encoder.layer.5.attention.output.LayerNorm.bias
2022-08-29 16:25:19,248 n: module.bert.encoder.layer.5.intermediate.dense.weight
2022-08-29 16:25:19,248 n: module.bert.encoder.layer.5.intermediate.dense.bias
2022-08-29 16:25:19,249 n: module.bert.encoder.layer.5.output.dense.weight
2022-08-29 16:25:19,249 n: module.bert.encoder.layer.5.output.dense.bias
2022-08-29 16:25:19,249 n: module.bert.encoder.layer.5.output.LayerNorm.weight
2022-08-29 16:25:19,249 n: module.bert.encoder.layer.5.output.LayerNorm.bias
2022-08-29 16:25:19,249 n: module.bert.encoder.layer.6.attention.self.query.weight
2022-08-29 16:25:19,249 n: module.bert.encoder.layer.6.attention.self.query.bias
2022-08-29 16:25:19,249 n: module.bert.encoder.layer.6.attention.self.key.weight
2022-08-29 16:25:19,249 n: module.bert.encoder.layer.6.attention.self.key.bias
2022-08-29 16:25:19,249 n: module.bert.encoder.layer.6.attention.self.value.weight
2022-08-29 16:25:19,249 n: module.bert.encoder.layer.6.attention.self.value.bias
2022-08-29 16:25:19,249 n: module.bert.encoder.layer.6.attention.output.dense.weight
2022-08-29 16:25:19,249 n: module.bert.encoder.layer.6.attention.output.dense.bias
2022-08-29 16:25:19,249 n: module.bert.encoder.layer.6.attention.output.LayerNorm.weight
2022-08-29 16:25:19,249 n: module.bert.encoder.layer.6.attention.output.LayerNorm.bias
2022-08-29 16:25:19,249 n: module.bert.encoder.layer.6.intermediate.dense.weight
2022-08-29 16:25:19,249 n: module.bert.encoder.layer.6.intermediate.dense.bias
2022-08-29 16:25:19,249 n: module.bert.encoder.layer.6.output.dense.weight
2022-08-29 16:25:19,250 n: module.bert.encoder.layer.6.output.dense.bias
2022-08-29 16:25:19,250 n: module.bert.encoder.layer.6.output.LayerNorm.weight
2022-08-29 16:25:19,250 n: module.bert.encoder.layer.6.output.LayerNorm.bias
2022-08-29 16:25:19,250 n: module.bert.encoder.layer.7.attention.self.query.weight
2022-08-29 16:25:19,250 n: module.bert.encoder.layer.7.attention.self.query.bias
2022-08-29 16:25:19,250 n: module.bert.encoder.layer.7.attention.self.key.weight
2022-08-29 16:25:19,250 n: module.bert.encoder.layer.7.attention.self.key.bias
2022-08-29 16:25:19,250 n: module.bert.encoder.layer.7.attention.self.value.weight
2022-08-29 16:25:19,250 n: module.bert.encoder.layer.7.attention.self.value.bias
2022-08-29 16:25:19,250 n: module.bert.encoder.layer.7.attention.output.dense.weight
2022-08-29 16:25:19,250 n: module.bert.encoder.layer.7.attention.output.dense.bias
2022-08-29 16:25:19,250 n: module.bert.encoder.layer.7.attention.output.LayerNorm.weight
2022-08-29 16:25:19,250 n: module.bert.encoder.layer.7.attention.output.LayerNorm.bias
2022-08-29 16:25:19,250 n: module.bert.encoder.layer.7.intermediate.dense.weight
2022-08-29 16:25:19,250 n: module.bert.encoder.layer.7.intermediate.dense.bias
2022-08-29 16:25:19,251 n: module.bert.encoder.layer.7.output.dense.weight
2022-08-29 16:25:19,251 n: module.bert.encoder.layer.7.output.dense.bias
2022-08-29 16:25:19,251 n: module.bert.encoder.layer.7.output.LayerNorm.weight
2022-08-29 16:25:19,251 n: module.bert.encoder.layer.7.output.LayerNorm.bias
2022-08-29 16:25:19,251 n: module.bert.encoder.layer.8.attention.self.query.weight
2022-08-29 16:25:19,251 n: module.bert.encoder.layer.8.attention.self.query.bias
2022-08-29 16:25:19,251 n: module.bert.encoder.layer.8.attention.self.key.weight
2022-08-29 16:25:19,251 n: module.bert.encoder.layer.8.attention.self.key.bias
2022-08-29 16:25:19,251 n: module.bert.encoder.layer.8.attention.self.value.weight
2022-08-29 16:25:19,251 n: module.bert.encoder.layer.8.attention.self.value.bias
2022-08-29 16:25:19,251 n: module.bert.encoder.layer.8.attention.output.dense.weight
2022-08-29 16:25:19,251 n: module.bert.encoder.layer.8.attention.output.dense.bias
2022-08-29 16:25:19,251 n: module.bert.encoder.layer.8.attention.output.LayerNorm.weight
2022-08-29 16:25:19,251 n: module.bert.encoder.layer.8.attention.output.LayerNorm.bias
2022-08-29 16:25:19,251 n: module.bert.encoder.layer.8.intermediate.dense.weight
2022-08-29 16:25:19,251 n: module.bert.encoder.layer.8.intermediate.dense.bias
2022-08-29 16:25:19,252 n: module.bert.encoder.layer.8.output.dense.weight
2022-08-29 16:25:19,252 n: module.bert.encoder.layer.8.output.dense.bias
2022-08-29 16:25:19,252 n: module.bert.encoder.layer.8.output.LayerNorm.weight
2022-08-29 16:25:19,252 n: module.bert.encoder.layer.8.output.LayerNorm.bias
2022-08-29 16:25:19,252 n: module.bert.encoder.layer.9.attention.self.query.weight
2022-08-29 16:25:19,252 n: module.bert.encoder.layer.9.attention.self.query.bias
2022-08-29 16:25:19,252 n: module.bert.encoder.layer.9.attention.self.key.weight
2022-08-29 16:25:19,252 n: module.bert.encoder.layer.9.attention.self.key.bias
2022-08-29 16:25:19,252 n: module.bert.encoder.layer.9.attention.self.value.weight
2022-08-29 16:25:19,252 n: module.bert.encoder.layer.9.attention.self.value.bias
2022-08-29 16:25:19,252 n: module.bert.encoder.layer.9.attention.output.dense.weight
2022-08-29 16:25:19,252 n: module.bert.encoder.layer.9.attention.output.dense.bias
2022-08-29 16:25:19,252 n: module.bert.encoder.layer.9.attention.output.LayerNorm.weight
2022-08-29 16:25:19,252 n: module.bert.encoder.layer.9.attention.output.LayerNorm.bias
2022-08-29 16:25:19,252 n: module.bert.encoder.layer.9.intermediate.dense.weight
2022-08-29 16:25:19,252 n: module.bert.encoder.layer.9.intermediate.dense.bias
2022-08-29 16:25:19,253 n: module.bert.encoder.layer.9.output.dense.weight
2022-08-29 16:25:19,253 n: module.bert.encoder.layer.9.output.dense.bias
2022-08-29 16:25:19,253 n: module.bert.encoder.layer.9.output.LayerNorm.weight
2022-08-29 16:25:19,253 n: module.bert.encoder.layer.9.output.LayerNorm.bias
2022-08-29 16:25:19,253 n: module.bert.encoder.layer.10.attention.self.query.weight
2022-08-29 16:25:19,253 n: module.bert.encoder.layer.10.attention.self.query.bias
2022-08-29 16:25:19,253 n: module.bert.encoder.layer.10.attention.self.key.weight
2022-08-29 16:25:19,253 n: module.bert.encoder.layer.10.attention.self.key.bias
2022-08-29 16:25:19,253 n: module.bert.encoder.layer.10.attention.self.value.weight
2022-08-29 16:25:19,253 n: module.bert.encoder.layer.10.attention.self.value.bias
2022-08-29 16:25:19,253 n: module.bert.encoder.layer.10.attention.output.dense.weight
2022-08-29 16:25:19,253 n: module.bert.encoder.layer.10.attention.output.dense.bias
2022-08-29 16:25:19,253 n: module.bert.encoder.layer.10.attention.output.LayerNorm.weight
2022-08-29 16:25:19,253 n: module.bert.encoder.layer.10.attention.output.LayerNorm.bias
2022-08-29 16:25:19,253 n: module.bert.encoder.layer.10.intermediate.dense.weight
2022-08-29 16:25:19,253 n: module.bert.encoder.layer.10.intermediate.dense.bias
2022-08-29 16:25:19,253 n: module.bert.encoder.layer.10.output.dense.weight
2022-08-29 16:25:19,254 n: module.bert.encoder.layer.10.output.dense.bias
2022-08-29 16:25:19,254 n: module.bert.encoder.layer.10.output.LayerNorm.weight
2022-08-29 16:25:19,254 n: module.bert.encoder.layer.10.output.LayerNorm.bias
2022-08-29 16:25:19,254 n: module.bert.encoder.layer.11.attention.self.query.weight
2022-08-29 16:25:19,254 n: module.bert.encoder.layer.11.attention.self.query.bias
2022-08-29 16:25:19,254 n: module.bert.encoder.layer.11.attention.self.key.weight
2022-08-29 16:25:19,254 n: module.bert.encoder.layer.11.attention.self.key.bias
2022-08-29 16:25:19,254 n: module.bert.encoder.layer.11.attention.self.value.weight
2022-08-29 16:25:19,254 n: module.bert.encoder.layer.11.attention.self.value.bias
2022-08-29 16:25:19,254 n: module.bert.encoder.layer.11.attention.output.dense.weight
2022-08-29 16:25:19,254 n: module.bert.encoder.layer.11.attention.output.dense.bias
2022-08-29 16:25:19,254 n: module.bert.encoder.layer.11.attention.output.LayerNorm.weight
2022-08-29 16:25:19,254 n: module.bert.encoder.layer.11.attention.output.LayerNorm.bias
2022-08-29 16:25:19,254 n: module.bert.encoder.layer.11.intermediate.dense.weight
2022-08-29 16:25:19,254 n: module.bert.encoder.layer.11.intermediate.dense.bias
2022-08-29 16:25:19,255 n: module.bert.encoder.layer.11.output.dense.weight
2022-08-29 16:25:19,255 n: module.bert.encoder.layer.11.output.dense.bias
2022-08-29 16:25:19,255 n: module.bert.encoder.layer.11.output.LayerNorm.weight
2022-08-29 16:25:19,255 n: module.bert.encoder.layer.11.output.LayerNorm.bias
2022-08-29 16:25:19,255 n: module.bert.pooler.dense.weight
2022-08-29 16:25:19,255 n: module.bert.pooler.dense.bias
2022-08-29 16:25:19,255 n: module.classifier.weight
2022-08-29 16:25:19,255 n: module.classifier.bias
2022-08-29 16:25:19,255 Total parameters: 109483778
2022-08-29 16:27:35,563 The args: Namespace(aug_train=False, cache_dir='', data_dir='glue_data/MNLI', data_url='', do_eval=False, do_lower_case=True, eval_batch_size=32, eval_step=200, gradient_accumulation_steps=1, hidden_act='quad', learning_rate=5e-05, log_path='tmp/distill/MNLI/quad_2quad/bert-base-uncased/log.txt', max_seq_length=128, no_cuda=False, num_train_epochs=3.0, output_dir='tmp/distill/MNLI/quad_2quad/bert-base-uncased/5e-05_1e-05_32', pred_distill=False, seed=42, softmax_act='2quad', student_model='/home/ubuntu/checkpoints/exp/MNLI', task_name='MNLI', teacher_model='/home/ubuntu/checkpoints/exp/MNLI', temperature=1.0, train_batch_size=32, warmup_proportion=0.1, weight_decay=0.0001)
2022-08-29 16:27:35,563 device: cuda n_gpu: 2
2022-08-29 16:27:35,568 The args: Namespace(aug_train=False, cache_dir='', data_dir='glue_data/SST2', data_url='', do_eval=False, do_lower_case=True, eval_batch_size=32, eval_step=200, gradient_accumulation_steps=1, hidden_act='quad', learning_rate=5e-05, log_path='tmp/distill/SST2/quad_2quad/bert-base-uncased/log.txt', max_seq_length=128, no_cuda=False, num_train_epochs=3.0, output_dir='tmp/distill/SST2/quad_2quad/bert-base-uncased/5e-05_1e-05_32', pred_distill=False, seed=42, softmax_act='2quad', student_model='/home/ubuntu/checkpoints/exp/SST2', task_name='SST2', teacher_model='/home/ubuntu/checkpoints/exp/SST2', temperature=1.0, train_batch_size=32, warmup_proportion=0.1, weight_decay=0.0001)
2022-08-29 16:27:35,569 device: cuda n_gpu: 2
2022-08-29 16:27:35,706 The args: Namespace(aug_train=False, cache_dir='', data_dir='glue_data/MRPC', data_url='', do_eval=False, do_lower_case=True, eval_batch_size=32, eval_step=200, gradient_accumulation_steps=1, hidden_act='quad', learning_rate=5e-05, log_path='tmp/distill/MRPC/quad_2quad/bert-base-uncased/log.txt', max_seq_length=128, no_cuda=False, num_train_epochs=3.0, output_dir='tmp/distill/MRPC/quad_2quad/bert-base-uncased/5e-05_1e-05_32', pred_distill=False, seed=42, softmax_act='2quad', student_model='/home/ubuntu/checkpoints/exp/MRPC', task_name='MRPC', teacher_model='/home/ubuntu/checkpoints/exp/MRPC', temperature=1.0, train_batch_size=32, warmup_proportion=0.1, weight_decay=0.0001)
2022-08-29 16:27:35,706 device: cuda n_gpu: 2
2022-08-29 16:27:35,802 Writing example 0 of 3668
2022-08-29 16:27:35,803 *** Example ***
2022-08-29 16:27:35,803 guid: train-1
2022-08-29 16:27:35,803 tokens: [CLS] am ##ro ##zi accused his brother , whom he called " the witness " , of deliberately di ##stor ##ting his evidence . [SEP] referring to him as only " the witness " , am ##ro ##zi accused his brother of deliberately di ##stor ##ting his evidence . [SEP]
2022-08-29 16:27:35,803 input_ids: 101 2572 3217 5831 5496 2010 2567 1010 3183 2002 2170 1000 1996 7409 1000 1010 1997 9969 4487 23809 3436 2010 3350 1012 102 7727 2000 2032 2004 2069 1000 1996 7409 1000 1010 2572 3217 5831 5496 2010 2567 1997 9969 4487 23809 3436 2010 3350 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-08-29 16:27:35,803 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-08-29 16:27:35,803 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-08-29 16:27:35,803 label: 1
2022-08-29 16:27:35,803 label_id: 1
2022-08-29 16:27:35,908 Writing example 0 of 67349
2022-08-29 16:27:35,909 *** Example ***
2022-08-29 16:27:35,909 guid: train-1
2022-08-29 16:27:35,909 tokens: [CLS] hide new secret ##ions from the parental units [SEP]
2022-08-29 16:27:35,909 input_ids: 101 5342 2047 3595 8496 2013 1996 18643 3197 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-08-29 16:27:35,909 input_mask: 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-08-29 16:27:35,909 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-08-29 16:27:35,909 label: 0
2022-08-29 16:27:35,909 label_id: 0
2022-08-29 16:27:37,832 Writing example 10000 of 67349
2022-08-29 16:27:38,822 Writing example 0 of 408
2022-08-29 16:27:38,823 *** Example ***
2022-08-29 16:27:38,823 guid: dev-1
2022-08-29 16:27:38,823 tokens: [CLS] he said the foods ##er ##vic ##e pie business doesn ' t fit the company ' s long - term growth strategy . [SEP] " the foods ##er ##vic ##e pie business does not fit our long - term growth strategy . [SEP]
2022-08-29 16:27:38,824 input_ids: 101 2002 2056 1996 9440 2121 7903 2063 11345 2449 2987 1005 1056 4906 1996 2194 1005 1055 2146 1011 2744 3930 5656 1012 102 1000 1996 9440 2121 7903 2063 11345 2449 2515 2025 4906 2256 2146 1011 2744 3930 5656 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-08-29 16:27:38,824 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-08-29 16:27:38,824 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-08-29 16:27:38,824 label: 1
2022-08-29 16:27:38,824 label_id: 1
2022-08-29 16:27:39,166 Model config {
  "_name_or_path": "bert-base-uncased",
  "architectures": [
    "BertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "crypten": false,
  "finetuning_task": "mrpc",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "not_equivalent",
    "1": "equivalent"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "equivalent": 1,
    "not_equivalent": 0
  },
  "layer_norm_eps": 1e-12,
  "log_path": "tmp/distill/MRPC/quad_2quad/bert-base-uncased/log.txt",
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "pre_trained": "",
  "problem_type": "single_label_classification",
  "softmax_act": "softmax",
  "torch_dtype": "float32",
  "training": "",
  "transformers_version": "4.20.0.dev0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

2022-08-29 16:27:39,887 Writing example 20000 of 67349
2022-08-29 16:27:41,798 Writing example 30000 of 67349
2022-08-29 16:27:41,897 Loading model /home/ubuntu/checkpoints/exp/MRPC/pytorch_model.bin
2022-08-29 16:27:42,207 loading model...
2022-08-29 16:27:42,255 done!
2022-08-29 16:27:42,255 Weights from pretrained model not used in TinyBertForSequenceClassification: ['bert.embeddings.position_ids']
2022-08-29 16:27:42,453 Writing example 0 of 392702
2022-08-29 16:27:42,455 *** Example ***
2022-08-29 16:27:42,455 guid: train-0
2022-08-29 16:27:42,455 tokens: [CLS] conceptual ##ly cream ski ##mming has two basic dimensions - product and geography . [SEP] product and geography are what make cream ski ##mming work . [SEP]
2022-08-29 16:27:42,455 input_ids: 101 17158 2135 6949 8301 25057 2038 2048 3937 9646 1011 4031 1998 10505 1012 102 4031 1998 10505 2024 2054 2191 6949 8301 25057 2147 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-08-29 16:27:42,455 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-08-29 16:27:42,455 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-08-29 16:27:42,455 label: neutral
2022-08-29 16:27:42,455 label_id: 1
2022-08-29 16:27:43,985 Writing example 40000 of 67349
2022-08-29 16:27:46,187 Writing example 50000 of 67349
2022-08-29 16:27:46,888 ***** Teacher evaluation *****
2022-08-29 16:27:46,889 {'acc': 0.8602941176470589, 'f1': 0.9025641025641027, 'acc_and_f1': 0.8814291101055808, 'eval_loss': 0.3993110977686368}
2022-08-29 16:27:46,890 Model config {
  "_name_or_path": "bert-base-uncased",
  "architectures": [
    "BertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "crypten": false,
  "finetuning_task": "mrpc",
  "gradient_checkpointing": false,
  "hidden_act": "quad",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "not_equivalent",
    "1": "equivalent"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "equivalent": 1,
    "not_equivalent": 0
  },
  "layer_norm_eps": 1e-12,
  "log_path": "tmp/distill/MRPC/quad_2quad/bert-base-uncased/log.txt",
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "pre_trained": "",
  "problem_type": "single_label_classification",
  "softmax_act": "2quad",
  "torch_dtype": "float32",
  "training": "",
  "transformers_version": "4.20.0.dev0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

2022-08-29 16:27:48,104 Writing example 60000 of 67349
2022-08-29 16:27:48,382 Writing example 10000 of 392702
2022-08-29 16:27:49,571 Loading model /home/ubuntu/checkpoints/exp/MRPC/pytorch_model.bin
2022-08-29 16:27:49,876 loading model...
2022-08-29 16:27:49,921 done!
2022-08-29 16:27:49,921 Weights from pretrained model not used in TinyBertForSequenceClassification: ['bert.embeddings.position_ids']
2022-08-29 16:27:50,007 ***** Running training *****
2022-08-29 16:27:50,007   Num examples = 3668
2022-08-29 16:27:50,007   Batch size = 32
2022-08-29 16:27:50,007   Num steps = 2280
2022-08-29 16:27:50,009 n: module.bert.embeddings.word_embeddings.weight
2022-08-29 16:27:50,009 n: module.bert.embeddings.position_embeddings.weight
2022-08-29 16:27:50,009 n: module.bert.embeddings.token_type_embeddings.weight
2022-08-29 16:27:50,009 n: module.bert.embeddings.LayerNorm.weight
2022-08-29 16:27:50,009 n: module.bert.embeddings.LayerNorm.bias
2022-08-29 16:27:50,009 n: module.bert.encoder.layer.0.attention.self.query.weight
2022-08-29 16:27:50,009 n: module.bert.encoder.layer.0.attention.self.query.bias
2022-08-29 16:27:50,009 n: module.bert.encoder.layer.0.attention.self.key.weight
2022-08-29 16:27:50,009 n: module.bert.encoder.layer.0.attention.self.key.bias
2022-08-29 16:27:50,009 n: module.bert.encoder.layer.0.attention.self.value.weight
2022-08-29 16:27:50,009 n: module.bert.encoder.layer.0.attention.self.value.bias
2022-08-29 16:27:50,010 n: module.bert.encoder.layer.0.attention.output.dense.weight
2022-08-29 16:27:50,010 n: module.bert.encoder.layer.0.attention.output.dense.bias
2022-08-29 16:27:50,010 n: module.bert.encoder.layer.0.attention.output.LayerNorm.weight
2022-08-29 16:27:50,010 n: module.bert.encoder.layer.0.attention.output.LayerNorm.bias
2022-08-29 16:27:50,010 n: module.bert.encoder.layer.0.intermediate.dense.weight
2022-08-29 16:27:50,010 n: module.bert.encoder.layer.0.intermediate.dense.bias
2022-08-29 16:27:50,010 n: module.bert.encoder.layer.0.output.dense.weight
2022-08-29 16:27:50,010 n: module.bert.encoder.layer.0.output.dense.bias
2022-08-29 16:27:50,010 n: module.bert.encoder.layer.0.output.LayerNorm.weight
2022-08-29 16:27:50,010 n: module.bert.encoder.layer.0.output.LayerNorm.bias
2022-08-29 16:27:50,010 n: module.bert.encoder.layer.1.attention.self.query.weight
2022-08-29 16:27:50,010 n: module.bert.encoder.layer.1.attention.self.query.bias
2022-08-29 16:27:50,010 n: module.bert.encoder.layer.1.attention.self.key.weight
2022-08-29 16:27:50,010 n: module.bert.encoder.layer.1.attention.self.key.bias
2022-08-29 16:27:50,010 n: module.bert.encoder.layer.1.attention.self.value.weight
2022-08-29 16:27:50,010 n: module.bert.encoder.layer.1.attention.self.value.bias
2022-08-29 16:27:50,010 n: module.bert.encoder.layer.1.attention.output.dense.weight
2022-08-29 16:27:50,011 n: module.bert.encoder.layer.1.attention.output.dense.bias
2022-08-29 16:27:50,011 n: module.bert.encoder.layer.1.attention.output.LayerNorm.weight
2022-08-29 16:27:50,011 n: module.bert.encoder.layer.1.attention.output.LayerNorm.bias
2022-08-29 16:27:50,011 n: module.bert.encoder.layer.1.intermediate.dense.weight
2022-08-29 16:27:50,011 n: module.bert.encoder.layer.1.intermediate.dense.bias
2022-08-29 16:27:50,011 n: module.bert.encoder.layer.1.output.dense.weight
2022-08-29 16:27:50,011 n: module.bert.encoder.layer.1.output.dense.bias
2022-08-29 16:27:50,011 n: module.bert.encoder.layer.1.output.LayerNorm.weight
2022-08-29 16:27:50,011 n: module.bert.encoder.layer.1.output.LayerNorm.bias
2022-08-29 16:27:50,011 n: module.bert.encoder.layer.2.attention.self.query.weight
2022-08-29 16:27:50,011 n: module.bert.encoder.layer.2.attention.self.query.bias
2022-08-29 16:27:50,011 n: module.bert.encoder.layer.2.attention.self.key.weight
2022-08-29 16:27:50,011 n: module.bert.encoder.layer.2.attention.self.key.bias
2022-08-29 16:27:50,011 n: module.bert.encoder.layer.2.attention.self.value.weight
2022-08-29 16:27:50,011 n: module.bert.encoder.layer.2.attention.self.value.bias
2022-08-29 16:27:50,011 n: module.bert.encoder.layer.2.attention.output.dense.weight
2022-08-29 16:27:50,011 n: module.bert.encoder.layer.2.attention.output.dense.bias
2022-08-29 16:27:50,012 n: module.bert.encoder.layer.2.attention.output.LayerNorm.weight
2022-08-29 16:27:50,012 n: module.bert.encoder.layer.2.attention.output.LayerNorm.bias
2022-08-29 16:27:50,012 n: module.bert.encoder.layer.2.intermediate.dense.weight
2022-08-29 16:27:50,012 n: module.bert.encoder.layer.2.intermediate.dense.bias
2022-08-29 16:27:50,012 n: module.bert.encoder.layer.2.output.dense.weight
2022-08-29 16:27:50,012 n: module.bert.encoder.layer.2.output.dense.bias
2022-08-29 16:27:50,012 n: module.bert.encoder.layer.2.output.LayerNorm.weight
2022-08-29 16:27:50,012 n: module.bert.encoder.layer.2.output.LayerNorm.bias
2022-08-29 16:27:50,012 n: module.bert.encoder.layer.3.attention.self.query.weight
2022-08-29 16:27:50,012 n: module.bert.encoder.layer.3.attention.self.query.bias
2022-08-29 16:27:50,012 n: module.bert.encoder.layer.3.attention.self.key.weight
2022-08-29 16:27:50,012 n: module.bert.encoder.layer.3.attention.self.key.bias
2022-08-29 16:27:50,012 n: module.bert.encoder.layer.3.attention.self.value.weight
2022-08-29 16:27:50,012 n: module.bert.encoder.layer.3.attention.self.value.bias
2022-08-29 16:27:50,012 n: module.bert.encoder.layer.3.attention.output.dense.weight
2022-08-29 16:27:50,012 n: module.bert.encoder.layer.3.attention.output.dense.bias
2022-08-29 16:27:50,012 n: module.bert.encoder.layer.3.attention.output.LayerNorm.weight
2022-08-29 16:27:50,013 n: module.bert.encoder.layer.3.attention.output.LayerNorm.bias
2022-08-29 16:27:50,013 n: module.bert.encoder.layer.3.intermediate.dense.weight
2022-08-29 16:27:50,013 n: module.bert.encoder.layer.3.intermediate.dense.bias
2022-08-29 16:27:50,013 n: module.bert.encoder.layer.3.output.dense.weight
2022-08-29 16:27:50,013 n: module.bert.encoder.layer.3.output.dense.bias
2022-08-29 16:27:50,013 n: module.bert.encoder.layer.3.output.LayerNorm.weight
2022-08-29 16:27:50,013 n: module.bert.encoder.layer.3.output.LayerNorm.bias
2022-08-29 16:27:50,013 n: module.bert.encoder.layer.4.attention.self.query.weight
2022-08-29 16:27:50,013 n: module.bert.encoder.layer.4.attention.self.query.bias
2022-08-29 16:27:50,013 n: module.bert.encoder.layer.4.attention.self.key.weight
2022-08-29 16:27:50,013 n: module.bert.encoder.layer.4.attention.self.key.bias
2022-08-29 16:27:50,013 n: module.bert.encoder.layer.4.attention.self.value.weight
2022-08-29 16:27:50,013 n: module.bert.encoder.layer.4.attention.self.value.bias
2022-08-29 16:27:50,013 n: module.bert.encoder.layer.4.attention.output.dense.weight
2022-08-29 16:27:50,013 n: module.bert.encoder.layer.4.attention.output.dense.bias
2022-08-29 16:27:50,013 n: module.bert.encoder.layer.4.attention.output.LayerNorm.weight
2022-08-29 16:27:50,013 n: module.bert.encoder.layer.4.attention.output.LayerNorm.bias
2022-08-29 16:27:50,014 n: module.bert.encoder.layer.4.intermediate.dense.weight
2022-08-29 16:27:50,014 n: module.bert.encoder.layer.4.intermediate.dense.bias
2022-08-29 16:27:50,014 n: module.bert.encoder.layer.4.output.dense.weight
2022-08-29 16:27:50,014 n: module.bert.encoder.layer.4.output.dense.bias
2022-08-29 16:27:50,014 n: module.bert.encoder.layer.4.output.LayerNorm.weight
2022-08-29 16:27:50,014 n: module.bert.encoder.layer.4.output.LayerNorm.bias
2022-08-29 16:27:50,014 n: module.bert.encoder.layer.5.attention.self.query.weight
2022-08-29 16:27:50,014 n: module.bert.encoder.layer.5.attention.self.query.bias
2022-08-29 16:27:50,014 n: module.bert.encoder.layer.5.attention.self.key.weight
2022-08-29 16:27:50,014 n: module.bert.encoder.layer.5.attention.self.key.bias
2022-08-29 16:27:50,014 n: module.bert.encoder.layer.5.attention.self.value.weight
2022-08-29 16:27:50,014 n: module.bert.encoder.layer.5.attention.self.value.bias
2022-08-29 16:27:50,014 n: module.bert.encoder.layer.5.attention.output.dense.weight
2022-08-29 16:27:50,014 n: module.bert.encoder.layer.5.attention.output.dense.bias
2022-08-29 16:27:50,014 n: module.bert.encoder.layer.5.attention.output.LayerNorm.weight
2022-08-29 16:27:50,014 n: module.bert.encoder.layer.5.attention.output.LayerNorm.bias
2022-08-29 16:27:50,014 n: module.bert.encoder.layer.5.intermediate.dense.weight
2022-08-29 16:27:50,015 n: module.bert.encoder.layer.5.intermediate.dense.bias
2022-08-29 16:27:50,015 n: module.bert.encoder.layer.5.output.dense.weight
2022-08-29 16:27:50,015 n: module.bert.encoder.layer.5.output.dense.bias
2022-08-29 16:27:50,015 n: module.bert.encoder.layer.5.output.LayerNorm.weight
2022-08-29 16:27:50,015 n: module.bert.encoder.layer.5.output.LayerNorm.bias
2022-08-29 16:27:50,015 n: module.bert.encoder.layer.6.attention.self.query.weight
2022-08-29 16:27:50,015 n: module.bert.encoder.layer.6.attention.self.query.bias
2022-08-29 16:27:50,015 n: module.bert.encoder.layer.6.attention.self.key.weight
2022-08-29 16:27:50,015 n: module.bert.encoder.layer.6.attention.self.key.bias
2022-08-29 16:27:50,015 n: module.bert.encoder.layer.6.attention.self.value.weight
2022-08-29 16:27:50,015 n: module.bert.encoder.layer.6.attention.self.value.bias
2022-08-29 16:27:50,015 n: module.bert.encoder.layer.6.attention.output.dense.weight
2022-08-29 16:27:50,015 n: module.bert.encoder.layer.6.attention.output.dense.bias
2022-08-29 16:27:50,015 n: module.bert.encoder.layer.6.attention.output.LayerNorm.weight
2022-08-29 16:27:50,015 n: module.bert.encoder.layer.6.attention.output.LayerNorm.bias
2022-08-29 16:27:50,015 n: module.bert.encoder.layer.6.intermediate.dense.weight
2022-08-29 16:27:50,015 n: module.bert.encoder.layer.6.intermediate.dense.bias
2022-08-29 16:27:50,015 n: module.bert.encoder.layer.6.output.dense.weight
2022-08-29 16:27:50,016 n: module.bert.encoder.layer.6.output.dense.bias
2022-08-29 16:27:50,016 n: module.bert.encoder.layer.6.output.LayerNorm.weight
2022-08-29 16:27:50,016 n: module.bert.encoder.layer.6.output.LayerNorm.bias
2022-08-29 16:27:50,016 n: module.bert.encoder.layer.7.attention.self.query.weight
2022-08-29 16:27:50,016 n: module.bert.encoder.layer.7.attention.self.query.bias
2022-08-29 16:27:50,016 n: module.bert.encoder.layer.7.attention.self.key.weight
2022-08-29 16:27:50,016 n: module.bert.encoder.layer.7.attention.self.key.bias
2022-08-29 16:27:50,016 n: module.bert.encoder.layer.7.attention.self.value.weight
2022-08-29 16:27:50,016 n: module.bert.encoder.layer.7.attention.self.value.bias
2022-08-29 16:27:50,016 n: module.bert.encoder.layer.7.attention.output.dense.weight
2022-08-29 16:27:50,016 n: module.bert.encoder.layer.7.attention.output.dense.bias
2022-08-29 16:27:50,016 n: module.bert.encoder.layer.7.attention.output.LayerNorm.weight
2022-08-29 16:27:50,016 n: module.bert.encoder.layer.7.attention.output.LayerNorm.bias
2022-08-29 16:27:50,016 n: module.bert.encoder.layer.7.intermediate.dense.weight
2022-08-29 16:27:50,016 n: module.bert.encoder.layer.7.intermediate.dense.bias
2022-08-29 16:27:50,016 n: module.bert.encoder.layer.7.output.dense.weight
2022-08-29 16:27:50,016 n: module.bert.encoder.layer.7.output.dense.bias
2022-08-29 16:27:50,017 n: module.bert.encoder.layer.7.output.LayerNorm.weight
2022-08-29 16:27:50,017 n: module.bert.encoder.layer.7.output.LayerNorm.bias
2022-08-29 16:27:50,017 n: module.bert.encoder.layer.8.attention.self.query.weight
2022-08-29 16:27:50,017 n: module.bert.encoder.layer.8.attention.self.query.bias
2022-08-29 16:27:50,017 n: module.bert.encoder.layer.8.attention.self.key.weight
2022-08-29 16:27:50,017 n: module.bert.encoder.layer.8.attention.self.key.bias
2022-08-29 16:27:50,017 n: module.bert.encoder.layer.8.attention.self.value.weight
2022-08-29 16:27:50,017 n: module.bert.encoder.layer.8.attention.self.value.bias
2022-08-29 16:27:50,017 n: module.bert.encoder.layer.8.attention.output.dense.weight
2022-08-29 16:27:50,017 n: module.bert.encoder.layer.8.attention.output.dense.bias
2022-08-29 16:27:50,017 n: module.bert.encoder.layer.8.attention.output.LayerNorm.weight
2022-08-29 16:27:50,017 n: module.bert.encoder.layer.8.attention.output.LayerNorm.bias
2022-08-29 16:27:50,017 n: module.bert.encoder.layer.8.intermediate.dense.weight
2022-08-29 16:27:50,017 n: module.bert.encoder.layer.8.intermediate.dense.bias
2022-08-29 16:27:50,017 n: module.bert.encoder.layer.8.output.dense.weight
2022-08-29 16:27:50,017 n: module.bert.encoder.layer.8.output.dense.bias
2022-08-29 16:27:50,017 n: module.bert.encoder.layer.8.output.LayerNorm.weight
2022-08-29 16:27:50,017 n: module.bert.encoder.layer.8.output.LayerNorm.bias
2022-08-29 16:27:50,018 n: module.bert.encoder.layer.9.attention.self.query.weight
2022-08-29 16:27:50,018 n: module.bert.encoder.layer.9.attention.self.query.bias
2022-08-29 16:27:50,018 n: module.bert.encoder.layer.9.attention.self.key.weight
2022-08-29 16:27:50,018 n: module.bert.encoder.layer.9.attention.self.key.bias
2022-08-29 16:27:50,018 n: module.bert.encoder.layer.9.attention.self.value.weight
2022-08-29 16:27:50,018 n: module.bert.encoder.layer.9.attention.self.value.bias
2022-08-29 16:27:50,018 n: module.bert.encoder.layer.9.attention.output.dense.weight
2022-08-29 16:27:50,018 n: module.bert.encoder.layer.9.attention.output.dense.bias
2022-08-29 16:27:50,018 n: module.bert.encoder.layer.9.attention.output.LayerNorm.weight
2022-08-29 16:27:50,018 n: module.bert.encoder.layer.9.attention.output.LayerNorm.bias
2022-08-29 16:27:50,018 n: module.bert.encoder.layer.9.intermediate.dense.weight
2022-08-29 16:27:50,018 n: module.bert.encoder.layer.9.intermediate.dense.bias
2022-08-29 16:27:50,018 n: module.bert.encoder.layer.9.output.dense.weight
2022-08-29 16:27:50,018 n: module.bert.encoder.layer.9.output.dense.bias
2022-08-29 16:27:50,018 n: module.bert.encoder.layer.9.output.LayerNorm.weight
2022-08-29 16:27:50,018 n: module.bert.encoder.layer.9.output.LayerNorm.bias
2022-08-29 16:27:50,019 n: module.bert.encoder.layer.10.attention.self.query.weight
2022-08-29 16:27:50,019 n: module.bert.encoder.layer.10.attention.self.query.bias
2022-08-29 16:27:50,019 n: module.bert.encoder.layer.10.attention.self.key.weight
2022-08-29 16:27:50,019 n: module.bert.encoder.layer.10.attention.self.key.bias
2022-08-29 16:27:50,019 n: module.bert.encoder.layer.10.attention.self.value.weight
2022-08-29 16:27:50,019 n: module.bert.encoder.layer.10.attention.self.value.bias
2022-08-29 16:27:50,019 n: module.bert.encoder.layer.10.attention.output.dense.weight
2022-08-29 16:27:50,019 n: module.bert.encoder.layer.10.attention.output.dense.bias
2022-08-29 16:27:50,019 n: module.bert.encoder.layer.10.attention.output.LayerNorm.weight
2022-08-29 16:27:50,019 n: module.bert.encoder.layer.10.attention.output.LayerNorm.bias
2022-08-29 16:27:50,019 n: module.bert.encoder.layer.10.intermediate.dense.weight
2022-08-29 16:27:50,019 n: module.bert.encoder.layer.10.intermediate.dense.bias
2022-08-29 16:27:50,019 n: module.bert.encoder.layer.10.output.dense.weight
2022-08-29 16:27:50,019 n: module.bert.encoder.layer.10.output.dense.bias
2022-08-29 16:27:50,019 n: module.bert.encoder.layer.10.output.LayerNorm.weight
2022-08-29 16:27:50,019 n: module.bert.encoder.layer.10.output.LayerNorm.bias
2022-08-29 16:27:50,019 n: module.bert.encoder.layer.11.attention.self.query.weight
2022-08-29 16:27:50,019 n: module.bert.encoder.layer.11.attention.self.query.bias
2022-08-29 16:27:50,020 n: module.bert.encoder.layer.11.attention.self.key.weight
2022-08-29 16:27:50,020 n: module.bert.encoder.layer.11.attention.self.key.bias
2022-08-29 16:27:50,020 n: module.bert.encoder.layer.11.attention.self.value.weight
2022-08-29 16:27:50,020 n: module.bert.encoder.layer.11.attention.self.value.bias
2022-08-29 16:27:50,020 n: module.bert.encoder.layer.11.attention.output.dense.weight
2022-08-29 16:27:50,020 n: module.bert.encoder.layer.11.attention.output.dense.bias
2022-08-29 16:27:50,020 n: module.bert.encoder.layer.11.attention.output.LayerNorm.weight
2022-08-29 16:27:50,020 n: module.bert.encoder.layer.11.attention.output.LayerNorm.bias
2022-08-29 16:27:50,020 n: module.bert.encoder.layer.11.intermediate.dense.weight
2022-08-29 16:27:50,020 n: module.bert.encoder.layer.11.intermediate.dense.bias
2022-08-29 16:27:50,020 n: module.bert.encoder.layer.11.output.dense.weight
2022-08-29 16:27:50,020 n: module.bert.encoder.layer.11.output.dense.bias
2022-08-29 16:27:50,020 n: module.bert.encoder.layer.11.output.LayerNorm.weight
2022-08-29 16:27:50,020 n: module.bert.encoder.layer.11.output.LayerNorm.bias
2022-08-29 16:27:50,020 n: module.bert.pooler.dense.weight
2022-08-29 16:27:50,020 n: module.bert.pooler.dense.bias
2022-08-29 16:27:50,020 n: module.classifier.weight
2022-08-29 16:27:50,020 n: module.classifier.bias
2022-08-29 16:27:50,021 Total parameters: 109483778
2022-08-29 16:27:50,168 Writing example 0 of 872
2022-08-29 16:27:50,168 *** Example ***
2022-08-29 16:27:50,168 guid: dev-1
2022-08-29 16:27:50,168 tokens: [CLS] it ' s a charming and often affecting journey . [SEP]
2022-08-29 16:27:50,168 input_ids: 101 2009 1005 1055 1037 11951 1998 2411 12473 4990 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-08-29 16:27:50,168 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-08-29 16:27:50,168 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-08-29 16:27:50,168 label: 1
2022-08-29 16:27:50,169 label_id: 1
2022-08-29 16:27:50,493 Model config {
  "_name_or_path": "bert-base-uncased",
  "architectures": [
    "BertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "crypten": false,
  "finetuning_task": "sst2",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "negative",
    "1": "positive"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "negative": 0,
    "positive": 1
  },
  "layer_norm_eps": 1e-12,
  "log_path": "tmp/distill/SST2/quad_2quad/bert-base-uncased/log.txt",
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "pre_trained": "",
  "problem_type": "single_label_classification",
  "softmax_act": "softmax",
  "torch_dtype": "float32",
  "training": "",
  "transformers_version": "4.20.0.dev0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

2022-08-29 16:27:53,154 Loading model /home/ubuntu/checkpoints/exp/SST2/pytorch_model.bin
2022-08-29 16:27:53,475 loading model...
2022-08-29 16:27:53,526 done!
2022-08-29 16:27:53,527 Weights from pretrained model not used in TinyBertForSequenceClassification: ['bert.embeddings.position_ids']
2022-08-29 16:27:54,176 Writing example 20000 of 392702
2022-08-29 16:27:59,109 ***** Teacher evaluation *****
2022-08-29 16:27:59,109 {'acc': 0.9311926605504587, 'eval_loss': 0.2749780447089246}
2022-08-29 16:27:59,110 Model config {
  "_name_or_path": "bert-base-uncased",
  "architectures": [
    "BertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "crypten": false,
  "finetuning_task": "sst2",
  "gradient_checkpointing": false,
  "hidden_act": "quad",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "negative",
    "1": "positive"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "negative": 0,
    "positive": 1
  },
  "layer_norm_eps": 1e-12,
  "log_path": "tmp/distill/SST2/quad_2quad/bert-base-uncased/log.txt",
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "pre_trained": "",
  "problem_type": "single_label_classification",
  "softmax_act": "2quad",
  "torch_dtype": "float32",
  "training": "",
  "transformers_version": "4.20.0.dev0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

2022-08-29 16:27:59,909 Writing example 30000 of 392702
2022-08-29 16:28:01,751 Loading model /home/ubuntu/checkpoints/exp/SST2/pytorch_model.bin
2022-08-29 16:28:02,052 loading model...
2022-08-29 16:28:02,099 done!
2022-08-29 16:28:02,099 Weights from pretrained model not used in TinyBertForSequenceClassification: ['bert.embeddings.position_ids']
2022-08-29 16:28:02,176 ***** Running training *****
2022-08-29 16:28:02,176   Num examples = 67349
2022-08-29 16:28:02,176   Batch size = 32
2022-08-29 16:28:02,176   Num steps = 21040
2022-08-29 16:28:02,178 n: module.bert.embeddings.word_embeddings.weight
2022-08-29 16:28:02,178 n: module.bert.embeddings.position_embeddings.weight
2022-08-29 16:28:02,178 n: module.bert.embeddings.token_type_embeddings.weight
2022-08-29 16:28:02,178 n: module.bert.embeddings.LayerNorm.weight
2022-08-29 16:28:02,178 n: module.bert.embeddings.LayerNorm.bias
2022-08-29 16:28:02,178 n: module.bert.encoder.layer.0.attention.self.query.weight
2022-08-29 16:28:02,178 n: module.bert.encoder.layer.0.attention.self.query.bias
2022-08-29 16:28:02,178 n: module.bert.encoder.layer.0.attention.self.key.weight
2022-08-29 16:28:02,178 n: module.bert.encoder.layer.0.attention.self.key.bias
2022-08-29 16:28:02,178 n: module.bert.encoder.layer.0.attention.self.value.weight
2022-08-29 16:28:02,178 n: module.bert.encoder.layer.0.attention.self.value.bias
2022-08-29 16:28:02,178 n: module.bert.encoder.layer.0.attention.output.dense.weight
2022-08-29 16:28:02,178 n: module.bert.encoder.layer.0.attention.output.dense.bias
2022-08-29 16:28:02,178 n: module.bert.encoder.layer.0.attention.output.LayerNorm.weight
2022-08-29 16:28:02,178 n: module.bert.encoder.layer.0.attention.output.LayerNorm.bias
2022-08-29 16:28:02,179 n: module.bert.encoder.layer.0.intermediate.dense.weight
2022-08-29 16:28:02,179 n: module.bert.encoder.layer.0.intermediate.dense.bias
2022-08-29 16:28:02,179 n: module.bert.encoder.layer.0.output.dense.weight
2022-08-29 16:28:02,179 n: module.bert.encoder.layer.0.output.dense.bias
2022-08-29 16:28:02,179 n: module.bert.encoder.layer.0.output.LayerNorm.weight
2022-08-29 16:28:02,179 n: module.bert.encoder.layer.0.output.LayerNorm.bias
2022-08-29 16:28:02,179 n: module.bert.encoder.layer.1.attention.self.query.weight
2022-08-29 16:28:02,179 n: module.bert.encoder.layer.1.attention.self.query.bias
2022-08-29 16:28:02,179 n: module.bert.encoder.layer.1.attention.self.key.weight
2022-08-29 16:28:02,179 n: module.bert.encoder.layer.1.attention.self.key.bias
2022-08-29 16:28:02,179 n: module.bert.encoder.layer.1.attention.self.value.weight
2022-08-29 16:28:02,179 n: module.bert.encoder.layer.1.attention.self.value.bias
2022-08-29 16:28:02,179 n: module.bert.encoder.layer.1.attention.output.dense.weight
2022-08-29 16:28:02,179 n: module.bert.encoder.layer.1.attention.output.dense.bias
2022-08-29 16:28:02,179 n: module.bert.encoder.layer.1.attention.output.LayerNorm.weight
2022-08-29 16:28:02,179 n: module.bert.encoder.layer.1.attention.output.LayerNorm.bias
2022-08-29 16:28:02,179 n: module.bert.encoder.layer.1.intermediate.dense.weight
2022-08-29 16:28:02,179 n: module.bert.encoder.layer.1.intermediate.dense.bias
2022-08-29 16:28:02,180 n: module.bert.encoder.layer.1.output.dense.weight
2022-08-29 16:28:02,180 n: module.bert.encoder.layer.1.output.dense.bias
2022-08-29 16:28:02,180 n: module.bert.encoder.layer.1.output.LayerNorm.weight
2022-08-29 16:28:02,180 n: module.bert.encoder.layer.1.output.LayerNorm.bias
2022-08-29 16:28:02,180 n: module.bert.encoder.layer.2.attention.self.query.weight
2022-08-29 16:28:02,180 n: module.bert.encoder.layer.2.attention.self.query.bias
2022-08-29 16:28:02,180 n: module.bert.encoder.layer.2.attention.self.key.weight
2022-08-29 16:28:02,180 n: module.bert.encoder.layer.2.attention.self.key.bias
2022-08-29 16:28:02,180 n: module.bert.encoder.layer.2.attention.self.value.weight
2022-08-29 16:28:02,180 n: module.bert.encoder.layer.2.attention.self.value.bias
2022-08-29 16:28:02,180 n: module.bert.encoder.layer.2.attention.output.dense.weight
2022-08-29 16:28:02,180 n: module.bert.encoder.layer.2.attention.output.dense.bias
2022-08-29 16:28:02,180 n: module.bert.encoder.layer.2.attention.output.LayerNorm.weight
2022-08-29 16:28:02,180 n: module.bert.encoder.layer.2.attention.output.LayerNorm.bias
2022-08-29 16:28:02,180 n: module.bert.encoder.layer.2.intermediate.dense.weight
2022-08-29 16:28:02,180 n: module.bert.encoder.layer.2.intermediate.dense.bias
2022-08-29 16:28:02,180 n: module.bert.encoder.layer.2.output.dense.weight
2022-08-29 16:28:02,180 n: module.bert.encoder.layer.2.output.dense.bias
2022-08-29 16:28:02,181 n: module.bert.encoder.layer.2.output.LayerNorm.weight
2022-08-29 16:28:02,181 n: module.bert.encoder.layer.2.output.LayerNorm.bias
2022-08-29 16:28:02,181 n: module.bert.encoder.layer.3.attention.self.query.weight
2022-08-29 16:28:02,181 n: module.bert.encoder.layer.3.attention.self.query.bias
2022-08-29 16:28:02,181 n: module.bert.encoder.layer.3.attention.self.key.weight
2022-08-29 16:28:02,181 n: module.bert.encoder.layer.3.attention.self.key.bias
2022-08-29 16:28:02,181 n: module.bert.encoder.layer.3.attention.self.value.weight
2022-08-29 16:28:02,181 n: module.bert.encoder.layer.3.attention.self.value.bias
2022-08-29 16:28:02,181 n: module.bert.encoder.layer.3.attention.output.dense.weight
2022-08-29 16:28:02,181 n: module.bert.encoder.layer.3.attention.output.dense.bias
2022-08-29 16:28:02,181 n: module.bert.encoder.layer.3.attention.output.LayerNorm.weight
2022-08-29 16:28:02,181 n: module.bert.encoder.layer.3.attention.output.LayerNorm.bias
2022-08-29 16:28:02,181 n: module.bert.encoder.layer.3.intermediate.dense.weight
2022-08-29 16:28:02,181 n: module.bert.encoder.layer.3.intermediate.dense.bias
2022-08-29 16:28:02,181 n: module.bert.encoder.layer.3.output.dense.weight
2022-08-29 16:28:02,181 n: module.bert.encoder.layer.3.output.dense.bias
2022-08-29 16:28:02,181 n: module.bert.encoder.layer.3.output.LayerNorm.weight
2022-08-29 16:28:02,181 n: module.bert.encoder.layer.3.output.LayerNorm.bias
2022-08-29 16:28:02,182 n: module.bert.encoder.layer.4.attention.self.query.weight
2022-08-29 16:28:02,182 n: module.bert.encoder.layer.4.attention.self.query.bias
2022-08-29 16:28:02,182 n: module.bert.encoder.layer.4.attention.self.key.weight
2022-08-29 16:28:02,182 n: module.bert.encoder.layer.4.attention.self.key.bias
2022-08-29 16:28:02,182 n: module.bert.encoder.layer.4.attention.self.value.weight
2022-08-29 16:28:02,182 n: module.bert.encoder.layer.4.attention.self.value.bias
2022-08-29 16:28:02,182 n: module.bert.encoder.layer.4.attention.output.dense.weight
2022-08-29 16:28:02,182 n: module.bert.encoder.layer.4.attention.output.dense.bias
2022-08-29 16:28:02,182 n: module.bert.encoder.layer.4.attention.output.LayerNorm.weight
2022-08-29 16:28:02,182 n: module.bert.encoder.layer.4.attention.output.LayerNorm.bias
2022-08-29 16:28:02,182 n: module.bert.encoder.layer.4.intermediate.dense.weight
2022-08-29 16:28:02,182 n: module.bert.encoder.layer.4.intermediate.dense.bias
2022-08-29 16:28:02,182 n: module.bert.encoder.layer.4.output.dense.weight
2022-08-29 16:28:02,182 n: module.bert.encoder.layer.4.output.dense.bias
2022-08-29 16:28:02,182 n: module.bert.encoder.layer.4.output.LayerNorm.weight
2022-08-29 16:28:02,182 n: module.bert.encoder.layer.4.output.LayerNorm.bias
2022-08-29 16:28:02,182 n: module.bert.encoder.layer.5.attention.self.query.weight
2022-08-29 16:28:02,183 n: module.bert.encoder.layer.5.attention.self.query.bias
2022-08-29 16:28:02,183 n: module.bert.encoder.layer.5.attention.self.key.weight
2022-08-29 16:28:02,183 n: module.bert.encoder.layer.5.attention.self.key.bias
2022-08-29 16:28:02,183 n: module.bert.encoder.layer.5.attention.self.value.weight
2022-08-29 16:28:02,183 n: module.bert.encoder.layer.5.attention.self.value.bias
2022-08-29 16:28:02,183 n: module.bert.encoder.layer.5.attention.output.dense.weight
2022-08-29 16:28:02,183 n: module.bert.encoder.layer.5.attention.output.dense.bias
2022-08-29 16:28:02,183 n: module.bert.encoder.layer.5.attention.output.LayerNorm.weight
2022-08-29 16:28:02,183 n: module.bert.encoder.layer.5.attention.output.LayerNorm.bias
2022-08-29 16:28:02,183 n: module.bert.encoder.layer.5.intermediate.dense.weight
2022-08-29 16:28:02,183 n: module.bert.encoder.layer.5.intermediate.dense.bias
2022-08-29 16:28:02,183 n: module.bert.encoder.layer.5.output.dense.weight
2022-08-29 16:28:02,183 n: module.bert.encoder.layer.5.output.dense.bias
2022-08-29 16:28:02,183 n: module.bert.encoder.layer.5.output.LayerNorm.weight
2022-08-29 16:28:02,183 n: module.bert.encoder.layer.5.output.LayerNorm.bias
2022-08-29 16:28:02,183 n: module.bert.encoder.layer.6.attention.self.query.weight
2022-08-29 16:28:02,183 n: module.bert.encoder.layer.6.attention.self.query.bias
2022-08-29 16:28:02,183 n: module.bert.encoder.layer.6.attention.self.key.weight
2022-08-29 16:28:02,184 n: module.bert.encoder.layer.6.attention.self.key.bias
2022-08-29 16:28:02,184 n: module.bert.encoder.layer.6.attention.self.value.weight
2022-08-29 16:28:02,184 n: module.bert.encoder.layer.6.attention.self.value.bias
2022-08-29 16:28:02,184 n: module.bert.encoder.layer.6.attention.output.dense.weight
2022-08-29 16:28:02,184 n: module.bert.encoder.layer.6.attention.output.dense.bias
2022-08-29 16:28:02,184 n: module.bert.encoder.layer.6.attention.output.LayerNorm.weight
2022-08-29 16:28:02,184 n: module.bert.encoder.layer.6.attention.output.LayerNorm.bias
2022-08-29 16:28:02,184 n: module.bert.encoder.layer.6.intermediate.dense.weight
2022-08-29 16:28:02,184 n: module.bert.encoder.layer.6.intermediate.dense.bias
2022-08-29 16:28:02,184 n: module.bert.encoder.layer.6.output.dense.weight
2022-08-29 16:28:02,184 n: module.bert.encoder.layer.6.output.dense.bias
2022-08-29 16:28:02,184 n: module.bert.encoder.layer.6.output.LayerNorm.weight
2022-08-29 16:28:02,184 n: module.bert.encoder.layer.6.output.LayerNorm.bias
2022-08-29 16:28:02,184 n: module.bert.encoder.layer.7.attention.self.query.weight
2022-08-29 16:28:02,184 n: module.bert.encoder.layer.7.attention.self.query.bias
2022-08-29 16:28:02,184 n: module.bert.encoder.layer.7.attention.self.key.weight
2022-08-29 16:28:02,184 n: module.bert.encoder.layer.7.attention.self.key.bias
2022-08-29 16:28:02,184 n: module.bert.encoder.layer.7.attention.self.value.weight
2022-08-29 16:28:02,185 n: module.bert.encoder.layer.7.attention.self.value.bias
2022-08-29 16:28:02,185 n: module.bert.encoder.layer.7.attention.output.dense.weight
2022-08-29 16:28:02,185 n: module.bert.encoder.layer.7.attention.output.dense.bias
2022-08-29 16:28:02,185 n: module.bert.encoder.layer.7.attention.output.LayerNorm.weight
2022-08-29 16:28:02,185 n: module.bert.encoder.layer.7.attention.output.LayerNorm.bias
2022-08-29 16:28:02,185 n: module.bert.encoder.layer.7.intermediate.dense.weight
2022-08-29 16:28:02,185 n: module.bert.encoder.layer.7.intermediate.dense.bias
2022-08-29 16:28:02,185 n: module.bert.encoder.layer.7.output.dense.weight
2022-08-29 16:28:02,185 n: module.bert.encoder.layer.7.output.dense.bias
2022-08-29 16:28:02,185 n: module.bert.encoder.layer.7.output.LayerNorm.weight
2022-08-29 16:28:02,185 n: module.bert.encoder.layer.7.output.LayerNorm.bias
2022-08-29 16:28:02,185 n: module.bert.encoder.layer.8.attention.self.query.weight
2022-08-29 16:28:02,185 n: module.bert.encoder.layer.8.attention.self.query.bias
2022-08-29 16:28:02,185 n: module.bert.encoder.layer.8.attention.self.key.weight
2022-08-29 16:28:02,185 n: module.bert.encoder.layer.8.attention.self.key.bias
2022-08-29 16:28:02,185 n: module.bert.encoder.layer.8.attention.self.value.weight
2022-08-29 16:28:02,185 n: module.bert.encoder.layer.8.attention.self.value.bias
2022-08-29 16:28:02,185 n: module.bert.encoder.layer.8.attention.output.dense.weight
2022-08-29 16:28:02,186 n: module.bert.encoder.layer.8.attention.output.dense.bias
2022-08-29 16:28:02,186 n: module.bert.encoder.layer.8.attention.output.LayerNorm.weight
2022-08-29 16:28:02,186 n: module.bert.encoder.layer.8.attention.output.LayerNorm.bias
2022-08-29 16:28:02,186 n: module.bert.encoder.layer.8.intermediate.dense.weight
2022-08-29 16:28:02,186 n: module.bert.encoder.layer.8.intermediate.dense.bias
2022-08-29 16:28:02,186 n: module.bert.encoder.layer.8.output.dense.weight
2022-08-29 16:28:02,186 n: module.bert.encoder.layer.8.output.dense.bias
2022-08-29 16:28:02,186 n: module.bert.encoder.layer.8.output.LayerNorm.weight
2022-08-29 16:28:02,186 n: module.bert.encoder.layer.8.output.LayerNorm.bias
2022-08-29 16:28:02,186 n: module.bert.encoder.layer.9.attention.self.query.weight
2022-08-29 16:28:02,186 n: module.bert.encoder.layer.9.attention.self.query.bias
2022-08-29 16:28:02,186 n: module.bert.encoder.layer.9.attention.self.key.weight
2022-08-29 16:28:02,186 n: module.bert.encoder.layer.9.attention.self.key.bias
2022-08-29 16:28:02,186 n: module.bert.encoder.layer.9.attention.self.value.weight
2022-08-29 16:28:02,186 n: module.bert.encoder.layer.9.attention.self.value.bias
2022-08-29 16:28:02,186 n: module.bert.encoder.layer.9.attention.output.dense.weight
2022-08-29 16:28:02,186 n: module.bert.encoder.layer.9.attention.output.dense.bias
2022-08-29 16:28:02,187 n: module.bert.encoder.layer.9.attention.output.LayerNorm.weight
2022-08-29 16:28:02,187 n: module.bert.encoder.layer.9.attention.output.LayerNorm.bias
2022-08-29 16:28:02,187 n: module.bert.encoder.layer.9.intermediate.dense.weight
2022-08-29 16:28:02,187 n: module.bert.encoder.layer.9.intermediate.dense.bias
2022-08-29 16:28:02,187 n: module.bert.encoder.layer.9.output.dense.weight
2022-08-29 16:28:02,187 n: module.bert.encoder.layer.9.output.dense.bias
2022-08-29 16:28:02,187 n: module.bert.encoder.layer.9.output.LayerNorm.weight
2022-08-29 16:28:02,187 n: module.bert.encoder.layer.9.output.LayerNorm.bias
2022-08-29 16:28:02,187 n: module.bert.encoder.layer.10.attention.self.query.weight
2022-08-29 16:28:02,187 n: module.bert.encoder.layer.10.attention.self.query.bias
2022-08-29 16:28:02,187 n: module.bert.encoder.layer.10.attention.self.key.weight
2022-08-29 16:28:02,187 n: module.bert.encoder.layer.10.attention.self.key.bias
2022-08-29 16:28:02,187 n: module.bert.encoder.layer.10.attention.self.value.weight
2022-08-29 16:28:02,187 n: module.bert.encoder.layer.10.attention.self.value.bias
2022-08-29 16:28:02,187 n: module.bert.encoder.layer.10.attention.output.dense.weight
2022-08-29 16:28:02,187 n: module.bert.encoder.layer.10.attention.output.dense.bias
2022-08-29 16:28:02,187 n: module.bert.encoder.layer.10.attention.output.LayerNorm.weight
2022-08-29 16:28:02,187 n: module.bert.encoder.layer.10.attention.output.LayerNorm.bias
2022-08-29 16:28:02,188 n: module.bert.encoder.layer.10.intermediate.dense.weight
2022-08-29 16:28:02,188 n: module.bert.encoder.layer.10.intermediate.dense.bias
2022-08-29 16:28:02,188 n: module.bert.encoder.layer.10.output.dense.weight
2022-08-29 16:28:02,188 n: module.bert.encoder.layer.10.output.dense.bias
2022-08-29 16:28:02,188 n: module.bert.encoder.layer.10.output.LayerNorm.weight
2022-08-29 16:28:02,188 n: module.bert.encoder.layer.10.output.LayerNorm.bias
2022-08-29 16:28:02,188 n: module.bert.encoder.layer.11.attention.self.query.weight
2022-08-29 16:28:02,188 n: module.bert.encoder.layer.11.attention.self.query.bias
2022-08-29 16:28:02,188 n: module.bert.encoder.layer.11.attention.self.key.weight
2022-08-29 16:28:02,188 n: module.bert.encoder.layer.11.attention.self.key.bias
2022-08-29 16:28:02,188 n: module.bert.encoder.layer.11.attention.self.value.weight
2022-08-29 16:28:02,188 n: module.bert.encoder.layer.11.attention.self.value.bias
2022-08-29 16:28:02,188 n: module.bert.encoder.layer.11.attention.output.dense.weight
2022-08-29 16:28:02,188 n: module.bert.encoder.layer.11.attention.output.dense.bias
2022-08-29 16:28:02,188 n: module.bert.encoder.layer.11.attention.output.LayerNorm.weight
2022-08-29 16:28:02,188 n: module.bert.encoder.layer.11.attention.output.LayerNorm.bias
2022-08-29 16:28:02,188 n: module.bert.encoder.layer.11.intermediate.dense.weight
2022-08-29 16:28:02,188 n: module.bert.encoder.layer.11.intermediate.dense.bias
2022-08-29 16:28:02,189 n: module.bert.encoder.layer.11.output.dense.weight
2022-08-29 16:28:02,189 n: module.bert.encoder.layer.11.output.dense.bias
2022-08-29 16:28:02,189 n: module.bert.encoder.layer.11.output.LayerNorm.weight
2022-08-29 16:28:02,189 n: module.bert.encoder.layer.11.output.LayerNorm.bias
2022-08-29 16:28:02,189 n: module.bert.pooler.dense.weight
2022-08-29 16:28:02,189 n: module.bert.pooler.dense.bias
2022-08-29 16:28:02,189 n: module.classifier.weight
2022-08-29 16:28:02,189 n: module.classifier.bias
2022-08-29 16:28:02,189 Total parameters: 109483778
2022-08-29 16:28:05,999 Writing example 40000 of 392702
2022-08-29 16:28:11,785 Writing example 50000 of 392702
2022-08-29 16:28:17,510 Writing example 60000 of 392702
2022-08-29 16:28:23,281 Writing example 70000 of 392702
2022-08-29 16:28:29,518 Writing example 80000 of 392702
2022-08-29 16:28:35,272 Writing example 90000 of 392702
2022-08-29 16:28:41,002 Writing example 100000 of 392702
2022-08-29 16:28:46,748 Writing example 110000 of 392702
2022-08-29 16:28:52,545 Writing example 120000 of 392702
2022-08-29 16:28:58,988 Writing example 130000 of 392702
2022-08-29 16:29:00,017 ***** Running evaluation *****
2022-08-29 16:29:00,018   Epoch = 1 iter 199 step
2022-08-29 16:29:00,018   Num examples = 408
2022-08-29 16:29:00,018   Batch size = 32
2022-08-29 16:29:00,019 ***** Eval results *****
2022-08-29 16:29:00,019   att_loss = 17.02212942908792
2022-08-29 16:29:00,019   cls_loss = 0.0
2022-08-29 16:29:00,019   global_step = 199
2022-08-29 16:29:00,019   loss = 21.655785796221565
2022-08-29 16:29:00,019   rep_loss = 4.63365644567153
2022-08-29 16:29:00,020 ***** Save model *****
2022-08-29 16:29:04,761 Writing example 140000 of 392702
2022-08-29 16:29:10,505 Writing example 150000 of 392702
2022-08-29 16:29:16,307 Writing example 160000 of 392702
2022-08-29 16:29:19,341 ***** Running evaluation *****
2022-08-29 16:29:19,341   Epoch = 0 iter 199 step
2022-08-29 16:29:19,341   Num examples = 872
2022-08-29 16:29:19,341   Batch size = 32
2022-08-29 16:29:19,342 ***** Eval results *****
2022-08-29 16:29:19,343   att_loss = 4.6193369806711395
2022-08-29 16:29:19,343   cls_loss = 0.0
2022-08-29 16:29:19,343   global_step = 199
2022-08-29 16:29:19,343   loss = 8.723699296539154
2022-08-29 16:29:19,343   rep_loss = 4.104362310476638
2022-08-29 16:29:19,343 ***** Save model *****
2022-08-29 16:29:22,078 Writing example 170000 of 392702
2022-08-29 16:29:27,861 Writing example 180000 of 392702
2022-08-29 16:29:34,546 Writing example 190000 of 392702
2022-08-29 16:29:40,322 Writing example 200000 of 392702
2022-08-29 16:29:46,085 Writing example 210000 of 392702
2022-08-29 16:29:51,848 Writing example 220000 of 392702
2022-08-29 16:29:57,639 Writing example 230000 of 392702
2022-08-29 16:30:03,345 Writing example 240000 of 392702
2022-08-29 16:30:08,232 ***** Running evaluation *****
2022-08-29 16:30:08,232   Epoch = 3 iter 399 step
2022-08-29 16:30:08,232   Num examples = 408
2022-08-29 16:30:08,232   Batch size = 32
2022-08-29 16:30:08,233 ***** Eval results *****
2022-08-29 16:30:08,233   att_loss = 12.491775880780136
2022-08-29 16:30:08,233   cls_loss = 0.0
2022-08-29 16:30:08,233   global_step = 399
2022-08-29 16:30:08,234   loss = 16.509412581460516
2022-08-29 16:30:08,234   rep_loss = 4.017636663035343
2022-08-29 16:30:08,234 ***** Save model *****
2022-08-29 16:30:09,102 Writing example 250000 of 392702
2022-08-29 16:30:16,222 Writing example 260000 of 392702
2022-08-29 16:30:22,017 Writing example 270000 of 392702
2022-08-29 16:30:27,704 Writing example 280000 of 392702
2022-08-29 16:30:33,439 Writing example 290000 of 392702
2022-08-29 16:30:35,386 ***** Running evaluation *****
2022-08-29 16:30:35,387   Epoch = 0 iter 399 step
2022-08-29 16:30:35,387   Num examples = 872
2022-08-29 16:30:35,387   Batch size = 32
2022-08-29 16:30:35,388 ***** Eval results *****
2022-08-29 16:30:35,388   att_loss = 3.489686653727577
2022-08-29 16:30:35,388   cls_loss = 0.0
2022-08-29 16:30:35,388   global_step = 399
2022-08-29 16:30:35,388   loss = 7.106935413857749
2022-08-29 16:30:35,388   rep_loss = 3.617248769392047
2022-08-29 16:30:35,388 ***** Save model *****
2022-08-29 16:30:39,177 Writing example 300000 of 392702
2022-08-29 16:30:44,985 Writing example 310000 of 392702
2022-08-29 16:30:50,764 Writing example 320000 of 392702
2022-08-29 16:30:56,526 Writing example 330000 of 392702
2022-08-29 16:31:02,360 Writing example 340000 of 392702
2022-08-29 16:31:09,811 Writing example 350000 of 392702
2022-08-29 16:31:15,593 Writing example 360000 of 392702
2022-08-29 16:31:18,869 ***** Running evaluation *****
2022-08-29 16:31:18,869   Epoch = 5 iter 599 step
2022-08-29 16:31:18,870   Num examples = 408
2022-08-29 16:31:18,870   Batch size = 32
2022-08-29 16:31:18,871 ***** Eval results *****
2022-08-29 16:31:18,871   att_loss = 10.903190744334253
2022-08-29 16:31:18,871   cls_loss = 0.0
2022-08-29 16:31:18,871   global_step = 599
2022-08-29 16:31:18,871   loss = 14.600825737262594
2022-08-29 16:31:18,871   rep_loss = 3.6976350422563224
2022-08-29 16:31:18,871 ***** Save model *****
2022-08-29 16:31:21,368 Writing example 370000 of 392702
2022-08-29 16:31:27,179 Writing example 380000 of 392702
2022-08-29 16:31:32,952 Writing example 390000 of 392702
2022-08-29 16:31:38,593 Writing example 0 of 9815
2022-08-29 16:31:38,594 *** Example ***
2022-08-29 16:31:38,594 guid: dev_matched-0
2022-08-29 16:31:38,594 tokens: [CLS] the new rights are nice enough [SEP] everyone really likes the newest benefits [SEP]
2022-08-29 16:31:38,594 input_ids: 101 1996 2047 2916 2024 3835 2438 102 3071 2428 7777 1996 14751 6666 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-08-29 16:31:38,594 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-08-29 16:31:38,594 segment_ids: 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-08-29 16:31:38,594 label: neutral
2022-08-29 16:31:38,594 label_id: 1
2022-08-29 16:31:44,233 Model config {
  "_name_or_path": "bert-base-uncased",
  "architectures": [
    "BertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "crypten": false,
  "finetuning_task": "mnli",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "entailment",
    "1": "neutral",
    "2": "contradiction"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "contradiction": 2,
    "entailment": 0,
    "neutral": 1
  },
  "layer_norm_eps": 1e-12,
  "log_path": "tmp/distill/MNLI/quad_2quad/bert-base-uncased/log.txt",
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "pre_trained": "",
  "problem_type": "single_label_classification",
  "softmax_act": "softmax",
  "torch_dtype": "float32",
  "training": "",
  "transformers_version": "4.20.0.dev0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

2022-08-29 16:31:46,872 Loading model /home/ubuntu/checkpoints/exp/MNLI/pytorch_model.bin
2022-08-29 16:31:50,211 loading model...
2022-08-29 16:31:50,263 done!
2022-08-29 16:31:50,263 Weights from pretrained model not used in TinyBertForSequenceClassification: ['bert.embeddings.position_ids']
2022-08-29 16:31:54,917 ***** Running evaluation *****
2022-08-29 16:31:54,917   Epoch = 0 iter 599 step
2022-08-29 16:31:54,917   Num examples = 872
2022-08-29 16:31:54,917   Batch size = 32
2022-08-29 16:31:54,918 ***** Eval results *****
2022-08-29 16:31:54,918   att_loss = 3.0325621312767117
2022-08-29 16:31:54,919   cls_loss = 0.0
2022-08-29 16:31:54,919   global_step = 599
2022-08-29 16:31:54,919   loss = 6.374905780082155
2022-08-29 16:31:54,919   rep_loss = 3.3423436531837476
2022-08-29 16:31:54,919 ***** Save model *****
2022-08-29 16:32:17,286 ***** Teacher evaluation *****
2022-08-29 16:32:17,287 {'acc': 0.8473764645950076, 'eval_loss': 0.5279645242209543}
2022-08-29 16:32:17,287 Model config {
  "_name_or_path": "bert-base-uncased",
  "architectures": [
    "BertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "crypten": false,
  "finetuning_task": "mnli",
  "gradient_checkpointing": false,
  "hidden_act": "quad",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "entailment",
    "1": "neutral",
    "2": "contradiction"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "contradiction": 2,
    "entailment": 0,
    "neutral": 1
  },
  "layer_norm_eps": 1e-12,
  "log_path": "tmp/distill/MNLI/quad_2quad/bert-base-uncased/log.txt",
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "pre_trained": "",
  "problem_type": "single_label_classification",
  "softmax_act": "2quad",
  "torch_dtype": "float32",
  "training": "",
  "transformers_version": "4.20.0.dev0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

2022-08-29 16:32:19,903 Loading model /home/ubuntu/checkpoints/exp/MNLI/pytorch_model.bin
2022-08-29 16:32:20,224 loading model...
2022-08-29 16:32:20,269 done!
2022-08-29 16:32:20,270 Weights from pretrained model not used in TinyBertForSequenceClassification: ['bert.embeddings.position_ids']
2022-08-29 16:32:20,350 ***** Running training *****
2022-08-29 16:32:20,350   Num examples = 392702
2022-08-29 16:32:20,350   Batch size = 32
2022-08-29 16:32:20,350   Num steps = 61355
2022-08-29 16:32:20,352 n: module.bert.embeddings.word_embeddings.weight
2022-08-29 16:32:20,352 n: module.bert.embeddings.position_embeddings.weight
2022-08-29 16:32:20,352 n: module.bert.embeddings.token_type_embeddings.weight
2022-08-29 16:32:20,352 n: module.bert.embeddings.LayerNorm.weight
2022-08-29 16:32:20,352 n: module.bert.embeddings.LayerNorm.bias
2022-08-29 16:32:20,352 n: module.bert.encoder.layer.0.attention.self.query.weight
2022-08-29 16:32:20,352 n: module.bert.encoder.layer.0.attention.self.query.bias
2022-08-29 16:32:20,352 n: module.bert.encoder.layer.0.attention.self.key.weight
2022-08-29 16:32:20,352 n: module.bert.encoder.layer.0.attention.self.key.bias
2022-08-29 16:32:20,352 n: module.bert.encoder.layer.0.attention.self.value.weight
2022-08-29 16:32:20,352 n: module.bert.encoder.layer.0.attention.self.value.bias
2022-08-29 16:32:20,352 n: module.bert.encoder.layer.0.attention.output.dense.weight
2022-08-29 16:32:20,352 n: module.bert.encoder.layer.0.attention.output.dense.bias
2022-08-29 16:32:20,353 n: module.bert.encoder.layer.0.attention.output.LayerNorm.weight
2022-08-29 16:32:20,353 n: module.bert.encoder.layer.0.attention.output.LayerNorm.bias
2022-08-29 16:32:20,353 n: module.bert.encoder.layer.0.intermediate.dense.weight
2022-08-29 16:32:20,353 n: module.bert.encoder.layer.0.intermediate.dense.bias
2022-08-29 16:32:20,353 n: module.bert.encoder.layer.0.output.dense.weight
2022-08-29 16:32:20,353 n: module.bert.encoder.layer.0.output.dense.bias
2022-08-29 16:32:20,353 n: module.bert.encoder.layer.0.output.LayerNorm.weight
2022-08-29 16:32:20,353 n: module.bert.encoder.layer.0.output.LayerNorm.bias
2022-08-29 16:32:20,353 n: module.bert.encoder.layer.1.attention.self.query.weight
2022-08-29 16:32:20,353 n: module.bert.encoder.layer.1.attention.self.query.bias
2022-08-29 16:32:20,353 n: module.bert.encoder.layer.1.attention.self.key.weight
2022-08-29 16:32:20,353 n: module.bert.encoder.layer.1.attention.self.key.bias
2022-08-29 16:32:20,353 n: module.bert.encoder.layer.1.attention.self.value.weight
2022-08-29 16:32:20,353 n: module.bert.encoder.layer.1.attention.self.value.bias
2022-08-29 16:32:20,353 n: module.bert.encoder.layer.1.attention.output.dense.weight
2022-08-29 16:32:20,353 n: module.bert.encoder.layer.1.attention.output.dense.bias
2022-08-29 16:32:20,353 n: module.bert.encoder.layer.1.attention.output.LayerNorm.weight
2022-08-29 16:32:20,353 n: module.bert.encoder.layer.1.attention.output.LayerNorm.bias
2022-08-29 16:32:20,354 n: module.bert.encoder.layer.1.intermediate.dense.weight
2022-08-29 16:32:20,354 n: module.bert.encoder.layer.1.intermediate.dense.bias
2022-08-29 16:32:20,354 n: module.bert.encoder.layer.1.output.dense.weight
2022-08-29 16:32:20,354 n: module.bert.encoder.layer.1.output.dense.bias
2022-08-29 16:32:20,354 n: module.bert.encoder.layer.1.output.LayerNorm.weight
2022-08-29 16:32:20,354 n: module.bert.encoder.layer.1.output.LayerNorm.bias
2022-08-29 16:32:20,354 n: module.bert.encoder.layer.2.attention.self.query.weight
2022-08-29 16:32:20,354 n: module.bert.encoder.layer.2.attention.self.query.bias
2022-08-29 16:32:20,354 n: module.bert.encoder.layer.2.attention.self.key.weight
2022-08-29 16:32:20,354 n: module.bert.encoder.layer.2.attention.self.key.bias
2022-08-29 16:32:20,354 n: module.bert.encoder.layer.2.attention.self.value.weight
2022-08-29 16:32:20,354 n: module.bert.encoder.layer.2.attention.self.value.bias
2022-08-29 16:32:20,354 n: module.bert.encoder.layer.2.attention.output.dense.weight
2022-08-29 16:32:20,354 n: module.bert.encoder.layer.2.attention.output.dense.bias
2022-08-29 16:32:20,354 n: module.bert.encoder.layer.2.attention.output.LayerNorm.weight
2022-08-29 16:32:20,354 n: module.bert.encoder.layer.2.attention.output.LayerNorm.bias
2022-08-29 16:32:20,354 n: module.bert.encoder.layer.2.intermediate.dense.weight
2022-08-29 16:32:20,354 n: module.bert.encoder.layer.2.intermediate.dense.bias
2022-08-29 16:32:20,355 n: module.bert.encoder.layer.2.output.dense.weight
2022-08-29 16:32:20,355 n: module.bert.encoder.layer.2.output.dense.bias
2022-08-29 16:32:20,355 n: module.bert.encoder.layer.2.output.LayerNorm.weight
2022-08-29 16:32:20,355 n: module.bert.encoder.layer.2.output.LayerNorm.bias
2022-08-29 16:32:20,355 n: module.bert.encoder.layer.3.attention.self.query.weight
2022-08-29 16:32:20,355 n: module.bert.encoder.layer.3.attention.self.query.bias
2022-08-29 16:32:20,355 n: module.bert.encoder.layer.3.attention.self.key.weight
2022-08-29 16:32:20,355 n: module.bert.encoder.layer.3.attention.self.key.bias
2022-08-29 16:32:20,355 n: module.bert.encoder.layer.3.attention.self.value.weight
2022-08-29 16:32:20,355 n: module.bert.encoder.layer.3.attention.self.value.bias
2022-08-29 16:32:20,355 n: module.bert.encoder.layer.3.attention.output.dense.weight
2022-08-29 16:32:20,355 n: module.bert.encoder.layer.3.attention.output.dense.bias
2022-08-29 16:32:20,355 n: module.bert.encoder.layer.3.attention.output.LayerNorm.weight
2022-08-29 16:32:20,355 n: module.bert.encoder.layer.3.attention.output.LayerNorm.bias
2022-08-29 16:32:20,355 n: module.bert.encoder.layer.3.intermediate.dense.weight
2022-08-29 16:32:20,355 n: module.bert.encoder.layer.3.intermediate.dense.bias
2022-08-29 16:32:20,355 n: module.bert.encoder.layer.3.output.dense.weight
2022-08-29 16:32:20,355 n: module.bert.encoder.layer.3.output.dense.bias
2022-08-29 16:32:20,356 n: module.bert.encoder.layer.3.output.LayerNorm.weight
2022-08-29 16:32:20,356 n: module.bert.encoder.layer.3.output.LayerNorm.bias
2022-08-29 16:32:20,356 n: module.bert.encoder.layer.4.attention.self.query.weight
2022-08-29 16:32:20,356 n: module.bert.encoder.layer.4.attention.self.query.bias
2022-08-29 16:32:20,356 n: module.bert.encoder.layer.4.attention.self.key.weight
2022-08-29 16:32:20,356 n: module.bert.encoder.layer.4.attention.self.key.bias
2022-08-29 16:32:20,356 n: module.bert.encoder.layer.4.attention.self.value.weight
2022-08-29 16:32:20,356 n: module.bert.encoder.layer.4.attention.self.value.bias
2022-08-29 16:32:20,356 n: module.bert.encoder.layer.4.attention.output.dense.weight
2022-08-29 16:32:20,356 n: module.bert.encoder.layer.4.attention.output.dense.bias
2022-08-29 16:32:20,356 n: module.bert.encoder.layer.4.attention.output.LayerNorm.weight
2022-08-29 16:32:20,356 n: module.bert.encoder.layer.4.attention.output.LayerNorm.bias
2022-08-29 16:32:20,356 n: module.bert.encoder.layer.4.intermediate.dense.weight
2022-08-29 16:32:20,356 n: module.bert.encoder.layer.4.intermediate.dense.bias
2022-08-29 16:32:20,356 n: module.bert.encoder.layer.4.output.dense.weight
2022-08-29 16:32:20,356 n: module.bert.encoder.layer.4.output.dense.bias
2022-08-29 16:32:20,356 n: module.bert.encoder.layer.4.output.LayerNorm.weight
2022-08-29 16:32:20,356 n: module.bert.encoder.layer.4.output.LayerNorm.bias
2022-08-29 16:32:20,357 n: module.bert.encoder.layer.5.attention.self.query.weight
2022-08-29 16:32:20,357 n: module.bert.encoder.layer.5.attention.self.query.bias
2022-08-29 16:32:20,357 n: module.bert.encoder.layer.5.attention.self.key.weight
2022-08-29 16:32:20,357 n: module.bert.encoder.layer.5.attention.self.key.bias
2022-08-29 16:32:20,357 n: module.bert.encoder.layer.5.attention.self.value.weight
2022-08-29 16:32:20,357 n: module.bert.encoder.layer.5.attention.self.value.bias
2022-08-29 16:32:20,357 n: module.bert.encoder.layer.5.attention.output.dense.weight
2022-08-29 16:32:20,357 n: module.bert.encoder.layer.5.attention.output.dense.bias
2022-08-29 16:32:20,357 n: module.bert.encoder.layer.5.attention.output.LayerNorm.weight
2022-08-29 16:32:20,357 n: module.bert.encoder.layer.5.attention.output.LayerNorm.bias
2022-08-29 16:32:20,357 n: module.bert.encoder.layer.5.intermediate.dense.weight
2022-08-29 16:32:20,357 n: module.bert.encoder.layer.5.intermediate.dense.bias
2022-08-29 16:32:20,357 n: module.bert.encoder.layer.5.output.dense.weight
2022-08-29 16:32:20,357 n: module.bert.encoder.layer.5.output.dense.bias
2022-08-29 16:32:20,357 n: module.bert.encoder.layer.5.output.LayerNorm.weight
2022-08-29 16:32:20,357 n: module.bert.encoder.layer.5.output.LayerNorm.bias
2022-08-29 16:32:20,357 n: module.bert.encoder.layer.6.attention.self.query.weight
2022-08-29 16:32:20,357 n: module.bert.encoder.layer.6.attention.self.query.bias
2022-08-29 16:32:20,358 n: module.bert.encoder.layer.6.attention.self.key.weight
2022-08-29 16:32:20,358 n: module.bert.encoder.layer.6.attention.self.key.bias
2022-08-29 16:32:20,358 n: module.bert.encoder.layer.6.attention.self.value.weight
2022-08-29 16:32:20,358 n: module.bert.encoder.layer.6.attention.self.value.bias
2022-08-29 16:32:20,358 n: module.bert.encoder.layer.6.attention.output.dense.weight
2022-08-29 16:32:20,358 n: module.bert.encoder.layer.6.attention.output.dense.bias
2022-08-29 16:32:20,358 n: module.bert.encoder.layer.6.attention.output.LayerNorm.weight
2022-08-29 16:32:20,358 n: module.bert.encoder.layer.6.attention.output.LayerNorm.bias
2022-08-29 16:32:20,358 n: module.bert.encoder.layer.6.intermediate.dense.weight
2022-08-29 16:32:20,358 n: module.bert.encoder.layer.6.intermediate.dense.bias
2022-08-29 16:32:20,358 n: module.bert.encoder.layer.6.output.dense.weight
2022-08-29 16:32:20,358 n: module.bert.encoder.layer.6.output.dense.bias
2022-08-29 16:32:20,358 n: module.bert.encoder.layer.6.output.LayerNorm.weight
2022-08-29 16:32:20,358 n: module.bert.encoder.layer.6.output.LayerNorm.bias
2022-08-29 16:32:20,358 n: module.bert.encoder.layer.7.attention.self.query.weight
2022-08-29 16:32:20,358 n: module.bert.encoder.layer.7.attention.self.query.bias
2022-08-29 16:32:20,358 n: module.bert.encoder.layer.7.attention.self.key.weight
2022-08-29 16:32:20,358 n: module.bert.encoder.layer.7.attention.self.key.bias
2022-08-29 16:32:20,359 n: module.bert.encoder.layer.7.attention.self.value.weight
2022-08-29 16:32:20,359 n: module.bert.encoder.layer.7.attention.self.value.bias
2022-08-29 16:32:20,359 n: module.bert.encoder.layer.7.attention.output.dense.weight
2022-08-29 16:32:20,359 n: module.bert.encoder.layer.7.attention.output.dense.bias
2022-08-29 16:32:20,359 n: module.bert.encoder.layer.7.attention.output.LayerNorm.weight
2022-08-29 16:32:20,359 n: module.bert.encoder.layer.7.attention.output.LayerNorm.bias
2022-08-29 16:32:20,359 n: module.bert.encoder.layer.7.intermediate.dense.weight
2022-08-29 16:32:20,359 n: module.bert.encoder.layer.7.intermediate.dense.bias
2022-08-29 16:32:20,359 n: module.bert.encoder.layer.7.output.dense.weight
2022-08-29 16:32:20,359 n: module.bert.encoder.layer.7.output.dense.bias
2022-08-29 16:32:20,359 n: module.bert.encoder.layer.7.output.LayerNorm.weight
2022-08-29 16:32:20,359 n: module.bert.encoder.layer.7.output.LayerNorm.bias
2022-08-29 16:32:20,359 n: module.bert.encoder.layer.8.attention.self.query.weight
2022-08-29 16:32:20,359 n: module.bert.encoder.layer.8.attention.self.query.bias
2022-08-29 16:32:20,359 n: module.bert.encoder.layer.8.attention.self.key.weight
2022-08-29 16:32:20,359 n: module.bert.encoder.layer.8.attention.self.key.bias
2022-08-29 16:32:20,359 n: module.bert.encoder.layer.8.attention.self.value.weight
2022-08-29 16:32:20,359 n: module.bert.encoder.layer.8.attention.self.value.bias
2022-08-29 16:32:20,360 n: module.bert.encoder.layer.8.attention.output.dense.weight
2022-08-29 16:32:20,360 n: module.bert.encoder.layer.8.attention.output.dense.bias
2022-08-29 16:32:20,360 n: module.bert.encoder.layer.8.attention.output.LayerNorm.weight
2022-08-29 16:32:20,360 n: module.bert.encoder.layer.8.attention.output.LayerNorm.bias
2022-08-29 16:32:20,360 n: module.bert.encoder.layer.8.intermediate.dense.weight
2022-08-29 16:32:20,360 n: module.bert.encoder.layer.8.intermediate.dense.bias
2022-08-29 16:32:20,360 n: module.bert.encoder.layer.8.output.dense.weight
2022-08-29 16:32:20,360 n: module.bert.encoder.layer.8.output.dense.bias
2022-08-29 16:32:20,360 n: module.bert.encoder.layer.8.output.LayerNorm.weight
2022-08-29 16:32:20,360 n: module.bert.encoder.layer.8.output.LayerNorm.bias
2022-08-29 16:32:20,360 n: module.bert.encoder.layer.9.attention.self.query.weight
2022-08-29 16:32:20,360 n: module.bert.encoder.layer.9.attention.self.query.bias
2022-08-29 16:32:20,360 n: module.bert.encoder.layer.9.attention.self.key.weight
2022-08-29 16:32:20,360 n: module.bert.encoder.layer.9.attention.self.key.bias
2022-08-29 16:32:20,360 n: module.bert.encoder.layer.9.attention.self.value.weight
2022-08-29 16:32:20,360 n: module.bert.encoder.layer.9.attention.self.value.bias
2022-08-29 16:32:20,360 n: module.bert.encoder.layer.9.attention.output.dense.weight
2022-08-29 16:32:20,360 n: module.bert.encoder.layer.9.attention.output.dense.bias
2022-08-29 16:32:20,361 n: module.bert.encoder.layer.9.attention.output.LayerNorm.weight
2022-08-29 16:32:20,361 n: module.bert.encoder.layer.9.attention.output.LayerNorm.bias
2022-08-29 16:32:20,361 n: module.bert.encoder.layer.9.intermediate.dense.weight
2022-08-29 16:32:20,361 n: module.bert.encoder.layer.9.intermediate.dense.bias
2022-08-29 16:32:20,361 n: module.bert.encoder.layer.9.output.dense.weight
2022-08-29 16:32:20,361 n: module.bert.encoder.layer.9.output.dense.bias
2022-08-29 16:32:20,361 n: module.bert.encoder.layer.9.output.LayerNorm.weight
2022-08-29 16:32:20,361 n: module.bert.encoder.layer.9.output.LayerNorm.bias
2022-08-29 16:32:20,361 n: module.bert.encoder.layer.10.attention.self.query.weight
2022-08-29 16:32:20,361 n: module.bert.encoder.layer.10.attention.self.query.bias
2022-08-29 16:32:20,361 n: module.bert.encoder.layer.10.attention.self.key.weight
2022-08-29 16:32:20,361 n: module.bert.encoder.layer.10.attention.self.key.bias
2022-08-29 16:32:20,361 n: module.bert.encoder.layer.10.attention.self.value.weight
2022-08-29 16:32:20,361 n: module.bert.encoder.layer.10.attention.self.value.bias
2022-08-29 16:32:20,361 n: module.bert.encoder.layer.10.attention.output.dense.weight
2022-08-29 16:32:20,361 n: module.bert.encoder.layer.10.attention.output.dense.bias
2022-08-29 16:32:20,361 n: module.bert.encoder.layer.10.attention.output.LayerNorm.weight
2022-08-29 16:32:20,361 n: module.bert.encoder.layer.10.attention.output.LayerNorm.bias
2022-08-29 16:32:20,362 n: module.bert.encoder.layer.10.intermediate.dense.weight
2022-08-29 16:32:20,362 n: module.bert.encoder.layer.10.intermediate.dense.bias
2022-08-29 16:32:20,362 n: module.bert.encoder.layer.10.output.dense.weight
2022-08-29 16:32:20,362 n: module.bert.encoder.layer.10.output.dense.bias
2022-08-29 16:32:20,362 n: module.bert.encoder.layer.10.output.LayerNorm.weight
2022-08-29 16:32:20,362 n: module.bert.encoder.layer.10.output.LayerNorm.bias
2022-08-29 16:32:20,362 n: module.bert.encoder.layer.11.attention.self.query.weight
2022-08-29 16:32:20,362 n: module.bert.encoder.layer.11.attention.self.query.bias
2022-08-29 16:32:20,362 n: module.bert.encoder.layer.11.attention.self.key.weight
2022-08-29 16:32:20,362 n: module.bert.encoder.layer.11.attention.self.key.bias
2022-08-29 16:32:20,362 n: module.bert.encoder.layer.11.attention.self.value.weight
2022-08-29 16:32:20,362 n: module.bert.encoder.layer.11.attention.self.value.bias
2022-08-29 16:32:20,362 n: module.bert.encoder.layer.11.attention.output.dense.weight
2022-08-29 16:32:20,362 n: module.bert.encoder.layer.11.attention.output.dense.bias
2022-08-29 16:32:20,362 n: module.bert.encoder.layer.11.attention.output.LayerNorm.weight
2022-08-29 16:32:20,362 n: module.bert.encoder.layer.11.attention.output.LayerNorm.bias
2022-08-29 16:32:20,362 n: module.bert.encoder.layer.11.intermediate.dense.weight
2022-08-29 16:32:20,362 n: module.bert.encoder.layer.11.intermediate.dense.bias
2022-08-29 16:32:20,363 n: module.bert.encoder.layer.11.output.dense.weight
2022-08-29 16:32:20,363 n: module.bert.encoder.layer.11.output.dense.bias
2022-08-29 16:32:20,363 n: module.bert.encoder.layer.11.output.LayerNorm.weight
2022-08-29 16:32:20,363 n: module.bert.encoder.layer.11.output.LayerNorm.bias
2022-08-29 16:32:20,363 n: module.bert.pooler.dense.weight
2022-08-29 16:32:20,363 n: module.bert.pooler.dense.bias
2022-08-29 16:32:20,363 n: module.classifier.weight
2022-08-29 16:32:20,363 n: module.classifier.bias
2022-08-29 16:32:20,363 Total parameters: 109484547
2022-08-29 16:32:31,967 ***** Running evaluation *****
2022-08-29 16:32:31,967   Epoch = 7 iter 799 step
2022-08-29 16:32:31,967   Num examples = 408
2022-08-29 16:32:31,967   Batch size = 32
2022-08-29 16:32:31,968 ***** Eval results *****
2022-08-29 16:32:31,968   att_loss = 8.83585262298584
2022-08-29 16:32:31,968   cls_loss = 0.0
2022-08-29 16:32:31,969   global_step = 799
2022-08-29 16:32:31,969   loss = 12.262778282165527
2022-08-29 16:32:31,969   rep_loss = 3.4269258975982666
2022-08-29 16:32:31,969 ***** Save model *****
2022-08-29 16:33:14,534 ***** Running evaluation *****
2022-08-29 16:33:14,535   Epoch = 0 iter 799 step
2022-08-29 16:33:14,535   Num examples = 872
2022-08-29 16:33:14,535   Batch size = 32
2022-08-29 16:33:14,536 ***** Eval results *****
2022-08-29 16:33:14,536   att_loss = 2.74689518614615
2022-08-29 16:33:14,536   cls_loss = 0.0
2022-08-29 16:33:14,537   global_step = 799
2022-08-29 16:33:14,537   loss = 5.891951343741674
2022-08-29 16:33:14,537   rep_loss = 3.145056162668259
2022-08-29 16:33:14,537 ***** Save model *****
2022-08-29 16:33:31,683 ***** Running evaluation *****
2022-08-29 16:33:31,683   Epoch = 0 iter 199 step
2022-08-29 16:33:31,683   Num examples = 9815
2022-08-29 16:33:31,683   Batch size = 32
2022-08-29 16:33:31,685 ***** Eval results *****
2022-08-29 16:33:31,685   att_loss = 19.181752684128345
2022-08-29 16:33:31,685   cls_loss = 0.0
2022-08-29 16:33:31,685   global_step = 199
2022-08-29 16:33:31,685   loss = 24.58004746844421
2022-08-29 16:33:31,685   rep_loss = 5.398294815466032
2022-08-29 16:33:31,685 ***** Save model *****
2022-08-29 16:33:45,210 ***** Running evaluation *****
2022-08-29 16:33:45,211   Epoch = 8 iter 999 step
2022-08-29 16:33:45,211   Num examples = 408
2022-08-29 16:33:45,211   Batch size = 32
2022-08-29 16:33:45,212 ***** Eval results *****
2022-08-29 16:33:45,212   att_loss = 9.123136043548584
2022-08-29 16:33:45,212   cls_loss = 0.0
2022-08-29 16:33:45,212   global_step = 999
2022-08-29 16:33:45,212   loss = 12.3838514130691
2022-08-29 16:33:45,212   rep_loss = 3.260715350337412
2022-08-29 16:33:45,212 ***** Save model *****
2022-08-29 16:34:34,122 ***** Running evaluation *****
2022-08-29 16:34:34,123   Epoch = 0 iter 999 step
2022-08-29 16:34:34,123   Num examples = 872
2022-08-29 16:34:34,123   Batch size = 32
2022-08-29 16:34:34,124 ***** Eval results *****
2022-08-29 16:34:34,124   att_loss = 2.549078388137741
2022-08-29 16:34:34,124   cls_loss = 0.0
2022-08-29 16:34:34,124   global_step = 999
2022-08-29 16:34:34,124   loss = 5.540783335854699
2022-08-29 16:34:34,124   rep_loss = 2.991704953444732
2022-08-29 16:34:34,124 ***** Save model *****
2022-08-29 16:34:39,327 ***** Running evaluation *****
2022-08-29 16:34:39,328   Epoch = 0 iter 399 step
2022-08-29 16:34:39,328   Num examples = 9815
2022-08-29 16:34:39,328   Batch size = 32
2022-08-29 16:34:39,329 ***** Eval results *****
2022-08-29 16:34:39,329   att_loss = 14.892868141183877
2022-08-29 16:34:39,329   cls_loss = 0.0
2022-08-29 16:34:39,329   global_step = 399
2022-08-29 16:34:39,329   loss = 19.661699428892973
2022-08-29 16:34:39,329   rep_loss = 4.76883129786728
2022-08-29 16:34:39,330 ***** Save model *****
2022-08-29 16:34:54,694 ***** Running evaluation *****
2022-08-29 16:34:54,694   Epoch = 10 iter 1199 step
2022-08-29 16:34:54,695   Num examples = 408
2022-08-29 16:34:54,695   Batch size = 32
2022-08-29 16:34:54,696 ***** Eval results *****
2022-08-29 16:34:54,696   att_loss = 8.606125977079747
2022-08-29 16:34:54,696   cls_loss = 0.0
2022-08-29 16:34:54,696   global_step = 1199
2022-08-29 16:34:54,696   loss = 11.649163246154785
2022-08-29 16:34:54,696   rep_loss = 3.0430372569520596
2022-08-29 16:34:54,696 ***** Save model *****
2022-08-29 16:35:54,251 ***** Running evaluation *****
2022-08-29 16:35:54,252   Epoch = 0 iter 1199 step
2022-08-29 16:35:54,252   Num examples = 872
2022-08-29 16:35:54,252   Batch size = 32
2022-08-29 16:35:54,253 ***** Eval results *****
2022-08-29 16:35:54,253   att_loss = 2.393550143826495
2022-08-29 16:35:54,253   cls_loss = 0.0
2022-08-29 16:35:54,253   global_step = 1199
2022-08-29 16:35:54,253   loss = 5.2588636135438565
2022-08-29 16:35:54,253   rep_loss = 2.865313476080493
2022-08-29 16:35:54,253 ***** Save model *****
2022-08-29 16:35:55,377 ***** Running evaluation *****
2022-08-29 16:35:55,377   Epoch = 0 iter 599 step
2022-08-29 16:35:55,377   Num examples = 9815
2022-08-29 16:35:55,377   Batch size = 32
2022-08-29 16:35:55,379 ***** Eval results *****
2022-08-29 16:35:55,379   att_loss = 12.98209514203971
2022-08-29 16:35:55,379   cls_loss = 0.0
2022-08-29 16:35:55,379   global_step = 599
2022-08-29 16:35:55,379   loss = 17.41227740318031
2022-08-29 16:35:55,379   rep_loss = 4.430182259946514
2022-08-29 16:35:55,379 ***** Save model *****
2022-08-29 16:36:04,080 ***** Running evaluation *****
2022-08-29 16:36:04,081   Epoch = 12 iter 1399 step
2022-08-29 16:36:04,081   Num examples = 408
2022-08-29 16:36:04,081   Batch size = 32
2022-08-29 16:36:04,082 ***** Eval results *****
2022-08-29 16:36:04,082   att_loss = 7.898946869757868
2022-08-29 16:36:04,082   cls_loss = 0.0
2022-08-29 16:36:04,082   global_step = 1399
2022-08-29 16:36:04,082   loss = 10.746117561094222
2022-08-29 16:36:04,082   rep_loss = 2.8471706605726674
2022-08-29 16:36:04,082 ***** Save model *****
2022-08-29 16:37:11,277 ***** Running evaluation *****
2022-08-29 16:37:11,278   Epoch = 0 iter 799 step
2022-08-29 16:37:11,278   Num examples = 9815
2022-08-29 16:37:11,278   Batch size = 32
2022-08-29 16:37:11,279 ***** Eval results *****
2022-08-29 16:37:11,279   att_loss = 11.798002643489719
2022-08-29 16:37:11,279   cls_loss = 0.0
2022-08-29 16:37:11,279   global_step = 799
2022-08-29 16:37:11,280   loss = 15.975769211264218
2022-08-29 16:37:11,280   rep_loss = 4.177766564492141
2022-08-29 16:37:11,280 ***** Save model *****
2022-08-29 16:37:16,761 ***** Running evaluation *****
2022-08-29 16:37:16,762   Epoch = 14 iter 1599 step
2022-08-29 16:37:16,762   Num examples = 408
2022-08-29 16:37:16,762   Batch size = 32
2022-08-29 16:37:16,763 ***** Eval results *****
2022-08-29 16:37:16,763   att_loss = 7.410017967224121
2022-08-29 16:37:16,763   cls_loss = 0.0
2022-08-29 16:37:16,763   global_step = 1599
2022-08-29 16:37:16,763   loss = 10.098730087280273
2022-08-29 16:37:16,763   rep_loss = 2.6887118816375732
2022-08-29 16:37:16,763 ***** Save model *****
2022-08-29 16:37:20,078 ***** Running evaluation *****
2022-08-29 16:37:20,079   Epoch = 0 iter 1399 step
2022-08-29 16:37:20,079   Num examples = 872
2022-08-29 16:37:20,079   Batch size = 32
2022-08-29 16:37:20,080 ***** Eval results *****
2022-08-29 16:37:20,080   att_loss = 2.2736964899782968
2022-08-29 16:37:20,080   cls_loss = 0.0
2022-08-29 16:37:20,080   global_step = 1399
2022-08-29 16:37:20,080   loss = 5.035829494645376
2022-08-29 16:37:20,080   rep_loss = 2.7621330102909627
2022-08-29 16:37:20,081 ***** Save model *****
2022-08-29 16:38:20,470 ***** Running evaluation *****
2022-08-29 16:38:20,471   Epoch = 0 iter 999 step
2022-08-29 16:38:20,471   Num examples = 9815
2022-08-29 16:38:20,471   Batch size = 32
2022-08-29 16:38:20,472 ***** Eval results *****
2022-08-29 16:38:20,472   att_loss = 10.981998392053553
2022-08-29 16:38:20,472   cls_loss = 0.0
2022-08-29 16:38:20,472   global_step = 999
2022-08-29 16:38:20,472   loss = 14.954748430528918
2022-08-29 16:38:20,472   rep_loss = 3.9727500411005945
2022-08-29 16:38:20,472 ***** Save model *****
2022-08-29 16:38:26,695 ***** Running evaluation *****
2022-08-29 16:38:26,696   Epoch = 15 iter 1799 step
2022-08-29 16:38:26,696   Num examples = 408
2022-08-29 16:38:26,696   Batch size = 32
2022-08-29 16:38:26,697 ***** Eval results *****
2022-08-29 16:38:26,698   att_loss = 7.395815865377362
2022-08-29 16:38:26,698   cls_loss = 0.0
2022-08-29 16:38:26,698   global_step = 1799
2022-08-29 16:38:26,698   loss = 10.031961676779758
2022-08-29 16:38:26,698   rep_loss = 2.63614577389835
2022-08-29 16:38:26,698 ***** Save model *****
2022-08-29 16:38:39,680 ***** Running evaluation *****
2022-08-29 16:38:39,680   Epoch = 0 iter 1599 step
2022-08-29 16:38:39,680   Num examples = 872
2022-08-29 16:38:39,680   Batch size = 32
2022-08-29 16:38:39,681 ***** Eval results *****
2022-08-29 16:38:39,682   att_loss = 2.1782992913471007
2022-08-29 16:38:39,682   cls_loss = 0.0
2022-08-29 16:38:39,682   global_step = 1599
2022-08-29 16:38:39,682   loss = 4.853883007304828
2022-08-29 16:38:39,682   rep_loss = 2.6755837208036337
2022-08-29 16:38:39,682 ***** Save model *****
2022-08-29 16:39:31,984 ***** Running evaluation *****
2022-08-29 16:39:31,984   Epoch = 0 iter 1199 step
2022-08-29 16:39:31,984   Num examples = 9815
2022-08-29 16:39:31,984   Batch size = 32
2022-08-29 16:39:31,985 ***** Eval results *****
2022-08-29 16:39:31,986   att_loss = 10.324325542036348
2022-08-29 16:39:31,986   cls_loss = 0.0
2022-08-29 16:39:31,986   global_step = 1199
2022-08-29 16:39:31,986   loss = 14.123701225229857
2022-08-29 16:39:31,986   rep_loss = 3.7993756857785312
2022-08-29 16:39:31,986 ***** Save model *****
2022-08-29 16:39:37,238 ***** Running evaluation *****
2022-08-29 16:39:37,238   Epoch = 17 iter 1999 step
2022-08-29 16:39:37,238   Num examples = 408
2022-08-29 16:39:37,238   Batch size = 32
2022-08-29 16:39:37,239 ***** Eval results *****
2022-08-29 16:39:37,239   att_loss = 7.1141595527774
2022-08-29 16:39:37,240   cls_loss = 0.0
2022-08-29 16:39:37,240   global_step = 1999
2022-08-29 16:39:37,240   loss = 9.662473772392898
2022-08-29 16:39:37,240   rep_loss = 2.548314239158005
2022-08-29 16:39:37,240 ***** Save model *****
2022-08-29 16:39:59,242 ***** Running evaluation *****
2022-08-29 16:39:59,242   Epoch = 0 iter 1799 step
2022-08-29 16:39:59,242   Num examples = 872
2022-08-29 16:39:59,242   Batch size = 32
2022-08-29 16:39:59,243 ***** Eval results *****
2022-08-29 16:39:59,244   att_loss = 2.0937791924598548
2022-08-29 16:39:59,244   cls_loss = 0.0
2022-08-29 16:39:59,244   global_step = 1799
2022-08-29 16:39:59,244   loss = 4.693390160418008
2022-08-29 16:39:59,244   rep_loss = 2.5996109726297782
2022-08-29 16:39:59,244 ***** Save model *****
2022-08-29 16:40:41,050 ***** Running evaluation *****
2022-08-29 16:40:41,051   Epoch = 0 iter 1399 step
2022-08-29 16:40:41,051   Num examples = 9815
2022-08-29 16:40:41,051   Batch size = 32
2022-08-29 16:40:41,052 ***** Eval results *****
2022-08-29 16:40:41,052   att_loss = 9.78968050924687
2022-08-29 16:40:41,052   cls_loss = 0.0
2022-08-29 16:40:41,052   global_step = 1399
2022-08-29 16:40:41,052   loss = 13.441800208497337
2022-08-29 16:40:41,052   rep_loss = 3.6521197076010825
2022-08-29 16:40:41,052 ***** Save model *****
2022-08-29 16:40:47,611 ***** Running evaluation *****
2022-08-29 16:40:47,611   Epoch = 19 iter 2199 step
2022-08-29 16:40:47,611   Num examples = 408
2022-08-29 16:40:47,611   Batch size = 32
2022-08-29 16:40:47,612 ***** Eval results *****
2022-08-29 16:40:47,613   att_loss = 6.8990326361222705
2022-08-29 16:40:47,613   cls_loss = 0.0
2022-08-29 16:40:47,613   global_step = 2199
2022-08-29 16:40:47,613   loss = 9.369659683921121
2022-08-29 16:40:47,613   rep_loss = 2.470627040574045
2022-08-29 16:40:47,613 ***** Save model *****
2022-08-29 16:41:17,692 ***** Running evaluation *****
2022-08-29 16:41:17,693   Epoch = 0 iter 1999 step
2022-08-29 16:41:17,693   Num examples = 872
2022-08-29 16:41:17,693   Batch size = 32
2022-08-29 16:41:17,694 ***** Eval results *****
2022-08-29 16:41:17,694   att_loss = 2.0252742572925637
2022-08-29 16:41:17,694   cls_loss = 0.0
2022-08-29 16:41:17,694   global_step = 1999
2022-08-29 16:41:17,694   loss = 4.560894103453361
2022-08-29 16:41:17,694   rep_loss = 2.535619850931554
2022-08-29 16:41:17,694 ***** Save model *****
2022-08-29 16:41:20,564 The args: Namespace(aug_train=False, cache_dir='', data_dir='glue_data/MRPC', data_url='', do_eval=False, do_lower_case=True, eval_batch_size=32, eval_step=100, gradient_accumulation_steps=1, hidden_act='quad', learning_rate=1e-05, log_path='tmp/distill/MRPC/quad_2quad/bert-base-uncased/log.txt', max_seq_length=128, no_cuda=False, num_train_epochs=5.0, output_dir='tmp/distill/MRPC/quad_2quad/bert-base-uncased/5e-05_1e-05_32_stage2', pred_distill=True, seed=42, softmax_act='2quad', student_model='tmp/distill/MRPC/quad_2quad/bert-base-uncased/5e-05_1e-05_32', task_name='MRPC', teacher_model='/home/ubuntu/checkpoints/exp/MRPC', temperature=1.0, train_batch_size=32, warmup_proportion=0.1, weight_decay=0.0001)
2022-08-29 16:41:20,564 device: cuda n_gpu: 2
2022-08-29 16:41:20,654 Writing example 0 of 3668
2022-08-29 16:41:20,655 *** Example ***
2022-08-29 16:41:20,655 guid: train-1
2022-08-29 16:41:20,655 tokens: [CLS] am ##ro ##zi accused his brother , whom he called " the witness " , of deliberately di ##stor ##ting his evidence . [SEP] referring to him as only " the witness " , am ##ro ##zi accused his brother of deliberately di ##stor ##ting his evidence . [SEP]
2022-08-29 16:41:20,655 input_ids: 101 2572 3217 5831 5496 2010 2567 1010 3183 2002 2170 1000 1996 7409 1000 1010 1997 9969 4487 23809 3436 2010 3350 1012 102 7727 2000 2032 2004 2069 1000 1996 7409 1000 1010 2572 3217 5831 5496 2010 2567 1997 9969 4487 23809 3436 2010 3350 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-08-29 16:41:20,655 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-08-29 16:41:20,655 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-08-29 16:41:20,655 label: 1
2022-08-29 16:41:20,655 label_id: 1
2022-08-29 16:41:23,567 Writing example 0 of 408
2022-08-29 16:41:23,568 *** Example ***
2022-08-29 16:41:23,568 guid: dev-1
2022-08-29 16:41:23,568 tokens: [CLS] he said the foods ##er ##vic ##e pie business doesn ' t fit the company ' s long - term growth strategy . [SEP] " the foods ##er ##vic ##e pie business does not fit our long - term growth strategy . [SEP]
2022-08-29 16:41:23,568 input_ids: 101 2002 2056 1996 9440 2121 7903 2063 11345 2449 2987 1005 1056 4906 1996 2194 1005 1055 2146 1011 2744 3930 5656 1012 102 1000 1996 9440 2121 7903 2063 11345 2449 2515 2025 4906 2256 2146 1011 2744 3930 5656 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-08-29 16:41:23,568 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-08-29 16:41:23,569 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-08-29 16:41:23,569 label: 1
2022-08-29 16:41:23,569 label_id: 1
2022-08-29 16:41:23,902 Model config {
  "_name_or_path": "bert-base-uncased",
  "architectures": [
    "BertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "crypten": false,
  "finetuning_task": "mrpc",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "not_equivalent",
    "1": "equivalent"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "equivalent": 1,
    "not_equivalent": 0
  },
  "layer_norm_eps": 1e-12,
  "log_path": "tmp/distill/MRPC/quad_2quad/bert-base-uncased/log.txt",
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "pre_trained": "",
  "problem_type": "single_label_classification",
  "softmax_act": "softmax",
  "torch_dtype": "float32",
  "training": "",
  "transformers_version": "4.20.0.dev0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

2022-08-29 16:41:26,583 Loading model /home/ubuntu/checkpoints/exp/MRPC/pytorch_model.bin
2022-08-29 16:41:26,895 loading model...
2022-08-29 16:41:26,950 done!
2022-08-29 16:41:26,950 Weights from pretrained model not used in TinyBertForSequenceClassification: ['bert.embeddings.position_ids']
2022-08-29 16:41:31,986 ***** Teacher evaluation *****
2022-08-29 16:41:31,986 {'acc': 0.8602941176470589, 'f1': 0.9025641025641027, 'acc_and_f1': 0.8814291101055808, 'eval_loss': 0.3993110977686368}
2022-08-29 16:41:31,987 Model config {
  "_name_or_path": "bert-base-uncased",
  "architectures": [
    "BertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "crypten": false,
  "finetuning_task": "mrpc",
  "gradient_checkpointing": false,
  "hidden_act": "quad",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "not_equivalent",
    "1": "equivalent"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "equivalent": 1,
    "not_equivalent": 0
  },
  "layer_norm_eps": 1e-12,
  "log_path": "tmp/distill/MRPC/quad_2quad/bert-base-uncased/log.txt",
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "pre_trained": "",
  "problem_type": "single_label_classification",
  "softmax_act": "2quad",
  "torch_dtype": "float32",
  "training": "",
  "transformers_version": "4.20.0.dev0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

2022-08-29 16:41:34,646 Loading model tmp/distill/MRPC/quad_2quad/bert-base-uncased/5e-05_1e-05_32/pytorch_model.bin
2022-08-29 16:41:34,923 loading model...
2022-08-29 16:41:34,972 done!
2022-08-29 16:41:35,052 ***** Running training *****
2022-08-29 16:41:35,052   Num examples = 3668
2022-08-29 16:41:35,052   Batch size = 32
2022-08-29 16:41:35,052   Num steps = 570
2022-08-29 16:41:35,053 n: module.bert.embeddings.word_embeddings.weight
2022-08-29 16:41:35,054 n: module.bert.embeddings.position_embeddings.weight
2022-08-29 16:41:35,054 n: module.bert.embeddings.token_type_embeddings.weight
2022-08-29 16:41:35,054 n: module.bert.embeddings.LayerNorm.weight
2022-08-29 16:41:35,054 n: module.bert.embeddings.LayerNorm.bias
2022-08-29 16:41:35,054 n: module.bert.encoder.layer.0.attention.self.query.weight
2022-08-29 16:41:35,054 n: module.bert.encoder.layer.0.attention.self.query.bias
2022-08-29 16:41:35,054 n: module.bert.encoder.layer.0.attention.self.key.weight
2022-08-29 16:41:35,054 n: module.bert.encoder.layer.0.attention.self.key.bias
2022-08-29 16:41:35,054 n: module.bert.encoder.layer.0.attention.self.value.weight
2022-08-29 16:41:35,054 n: module.bert.encoder.layer.0.attention.self.value.bias
2022-08-29 16:41:35,054 n: module.bert.encoder.layer.0.attention.output.dense.weight
2022-08-29 16:41:35,054 n: module.bert.encoder.layer.0.attention.output.dense.bias
2022-08-29 16:41:35,054 n: module.bert.encoder.layer.0.attention.output.LayerNorm.weight
2022-08-29 16:41:35,054 n: module.bert.encoder.layer.0.attention.output.LayerNorm.bias
2022-08-29 16:41:35,054 n: module.bert.encoder.layer.0.intermediate.dense.weight
2022-08-29 16:41:35,055 n: module.bert.encoder.layer.0.intermediate.dense.bias
2022-08-29 16:41:35,055 n: module.bert.encoder.layer.0.output.dense.weight
2022-08-29 16:41:35,055 n: module.bert.encoder.layer.0.output.dense.bias
2022-08-29 16:41:35,055 n: module.bert.encoder.layer.0.output.LayerNorm.weight
2022-08-29 16:41:35,055 n: module.bert.encoder.layer.0.output.LayerNorm.bias
2022-08-29 16:41:35,055 n: module.bert.encoder.layer.1.attention.self.query.weight
2022-08-29 16:41:35,055 n: module.bert.encoder.layer.1.attention.self.query.bias
2022-08-29 16:41:35,055 n: module.bert.encoder.layer.1.attention.self.key.weight
2022-08-29 16:41:35,055 n: module.bert.encoder.layer.1.attention.self.key.bias
2022-08-29 16:41:35,055 n: module.bert.encoder.layer.1.attention.self.value.weight
2022-08-29 16:41:35,055 n: module.bert.encoder.layer.1.attention.self.value.bias
2022-08-29 16:41:35,055 n: module.bert.encoder.layer.1.attention.output.dense.weight
2022-08-29 16:41:35,055 n: module.bert.encoder.layer.1.attention.output.dense.bias
2022-08-29 16:41:35,055 n: module.bert.encoder.layer.1.attention.output.LayerNorm.weight
2022-08-29 16:41:35,055 n: module.bert.encoder.layer.1.attention.output.LayerNorm.bias
2022-08-29 16:41:35,055 n: module.bert.encoder.layer.1.intermediate.dense.weight
2022-08-29 16:41:35,055 n: module.bert.encoder.layer.1.intermediate.dense.bias
2022-08-29 16:41:35,055 n: module.bert.encoder.layer.1.output.dense.weight
2022-08-29 16:41:35,056 n: module.bert.encoder.layer.1.output.dense.bias
2022-08-29 16:41:35,056 n: module.bert.encoder.layer.1.output.LayerNorm.weight
2022-08-29 16:41:35,056 n: module.bert.encoder.layer.1.output.LayerNorm.bias
2022-08-29 16:41:35,056 n: module.bert.encoder.layer.2.attention.self.query.weight
2022-08-29 16:41:35,056 n: module.bert.encoder.layer.2.attention.self.query.bias
2022-08-29 16:41:35,056 n: module.bert.encoder.layer.2.attention.self.key.weight
2022-08-29 16:41:35,056 n: module.bert.encoder.layer.2.attention.self.key.bias
2022-08-29 16:41:35,056 n: module.bert.encoder.layer.2.attention.self.value.weight
2022-08-29 16:41:35,056 n: module.bert.encoder.layer.2.attention.self.value.bias
2022-08-29 16:41:35,056 n: module.bert.encoder.layer.2.attention.output.dense.weight
2022-08-29 16:41:35,056 n: module.bert.encoder.layer.2.attention.output.dense.bias
2022-08-29 16:41:35,056 n: module.bert.encoder.layer.2.attention.output.LayerNorm.weight
2022-08-29 16:41:35,056 n: module.bert.encoder.layer.2.attention.output.LayerNorm.bias
2022-08-29 16:41:35,056 n: module.bert.encoder.layer.2.intermediate.dense.weight
2022-08-29 16:41:35,056 n: module.bert.encoder.layer.2.intermediate.dense.bias
2022-08-29 16:41:35,056 n: module.bert.encoder.layer.2.output.dense.weight
2022-08-29 16:41:35,056 n: module.bert.encoder.layer.2.output.dense.bias
2022-08-29 16:41:35,056 n: module.bert.encoder.layer.2.output.LayerNorm.weight
2022-08-29 16:41:35,056 n: module.bert.encoder.layer.2.output.LayerNorm.bias
2022-08-29 16:41:35,057 n: module.bert.encoder.layer.3.attention.self.query.weight
2022-08-29 16:41:35,057 n: module.bert.encoder.layer.3.attention.self.query.bias
2022-08-29 16:41:35,057 n: module.bert.encoder.layer.3.attention.self.key.weight
2022-08-29 16:41:35,057 n: module.bert.encoder.layer.3.attention.self.key.bias
2022-08-29 16:41:35,057 n: module.bert.encoder.layer.3.attention.self.value.weight
2022-08-29 16:41:35,057 n: module.bert.encoder.layer.3.attention.self.value.bias
2022-08-29 16:41:35,057 n: module.bert.encoder.layer.3.attention.output.dense.weight
2022-08-29 16:41:35,057 n: module.bert.encoder.layer.3.attention.output.dense.bias
2022-08-29 16:41:35,057 n: module.bert.encoder.layer.3.attention.output.LayerNorm.weight
2022-08-29 16:41:35,057 n: module.bert.encoder.layer.3.attention.output.LayerNorm.bias
2022-08-29 16:41:35,057 n: module.bert.encoder.layer.3.intermediate.dense.weight
2022-08-29 16:41:35,057 n: module.bert.encoder.layer.3.intermediate.dense.bias
2022-08-29 16:41:35,057 n: module.bert.encoder.layer.3.output.dense.weight
2022-08-29 16:41:35,057 n: module.bert.encoder.layer.3.output.dense.bias
2022-08-29 16:41:35,057 n: module.bert.encoder.layer.3.output.LayerNorm.weight
2022-08-29 16:41:35,057 n: module.bert.encoder.layer.3.output.LayerNorm.bias
2022-08-29 16:41:35,057 n: module.bert.encoder.layer.4.attention.self.query.weight
2022-08-29 16:41:35,057 n: module.bert.encoder.layer.4.attention.self.query.bias
2022-08-29 16:41:35,058 n: module.bert.encoder.layer.4.attention.self.key.weight
2022-08-29 16:41:35,058 n: module.bert.encoder.layer.4.attention.self.key.bias
2022-08-29 16:41:35,058 n: module.bert.encoder.layer.4.attention.self.value.weight
2022-08-29 16:41:35,058 n: module.bert.encoder.layer.4.attention.self.value.bias
2022-08-29 16:41:35,058 n: module.bert.encoder.layer.4.attention.output.dense.weight
2022-08-29 16:41:35,058 n: module.bert.encoder.layer.4.attention.output.dense.bias
2022-08-29 16:41:35,058 n: module.bert.encoder.layer.4.attention.output.LayerNorm.weight
2022-08-29 16:41:35,058 n: module.bert.encoder.layer.4.attention.output.LayerNorm.bias
2022-08-29 16:41:35,058 n: module.bert.encoder.layer.4.intermediate.dense.weight
2022-08-29 16:41:35,058 n: module.bert.encoder.layer.4.intermediate.dense.bias
2022-08-29 16:41:35,058 n: module.bert.encoder.layer.4.output.dense.weight
2022-08-29 16:41:35,058 n: module.bert.encoder.layer.4.output.dense.bias
2022-08-29 16:41:35,058 n: module.bert.encoder.layer.4.output.LayerNorm.weight
2022-08-29 16:41:35,058 n: module.bert.encoder.layer.4.output.LayerNorm.bias
2022-08-29 16:41:35,058 n: module.bert.encoder.layer.5.attention.self.query.weight
2022-08-29 16:41:35,058 n: module.bert.encoder.layer.5.attention.self.query.bias
2022-08-29 16:41:35,058 n: module.bert.encoder.layer.5.attention.self.key.weight
2022-08-29 16:41:35,058 n: module.bert.encoder.layer.5.attention.self.key.bias
2022-08-29 16:41:35,059 n: module.bert.encoder.layer.5.attention.self.value.weight
2022-08-29 16:41:35,059 n: module.bert.encoder.layer.5.attention.self.value.bias
2022-08-29 16:41:35,059 n: module.bert.encoder.layer.5.attention.output.dense.weight
2022-08-29 16:41:35,059 n: module.bert.encoder.layer.5.attention.output.dense.bias
2022-08-29 16:41:35,059 n: module.bert.encoder.layer.5.attention.output.LayerNorm.weight
2022-08-29 16:41:35,059 n: module.bert.encoder.layer.5.attention.output.LayerNorm.bias
2022-08-29 16:41:35,059 n: module.bert.encoder.layer.5.intermediate.dense.weight
2022-08-29 16:41:35,059 n: module.bert.encoder.layer.5.intermediate.dense.bias
2022-08-29 16:41:35,059 n: module.bert.encoder.layer.5.output.dense.weight
2022-08-29 16:41:35,059 n: module.bert.encoder.layer.5.output.dense.bias
2022-08-29 16:41:35,059 n: module.bert.encoder.layer.5.output.LayerNorm.weight
2022-08-29 16:41:35,059 n: module.bert.encoder.layer.5.output.LayerNorm.bias
2022-08-29 16:41:35,059 n: module.bert.encoder.layer.6.attention.self.query.weight
2022-08-29 16:41:35,059 n: module.bert.encoder.layer.6.attention.self.query.bias
2022-08-29 16:41:35,059 n: module.bert.encoder.layer.6.attention.self.key.weight
2022-08-29 16:41:35,059 n: module.bert.encoder.layer.6.attention.self.key.bias
2022-08-29 16:41:35,059 n: module.bert.encoder.layer.6.attention.self.value.weight
2022-08-29 16:41:35,059 n: module.bert.encoder.layer.6.attention.self.value.bias
2022-08-29 16:41:35,060 n: module.bert.encoder.layer.6.attention.output.dense.weight
2022-08-29 16:41:35,060 n: module.bert.encoder.layer.6.attention.output.dense.bias
2022-08-29 16:41:35,060 n: module.bert.encoder.layer.6.attention.output.LayerNorm.weight
2022-08-29 16:41:35,060 n: module.bert.encoder.layer.6.attention.output.LayerNorm.bias
2022-08-29 16:41:35,060 n: module.bert.encoder.layer.6.intermediate.dense.weight
2022-08-29 16:41:35,060 n: module.bert.encoder.layer.6.intermediate.dense.bias
2022-08-29 16:41:35,060 n: module.bert.encoder.layer.6.output.dense.weight
2022-08-29 16:41:35,060 n: module.bert.encoder.layer.6.output.dense.bias
2022-08-29 16:41:35,060 n: module.bert.encoder.layer.6.output.LayerNorm.weight
2022-08-29 16:41:35,060 n: module.bert.encoder.layer.6.output.LayerNorm.bias
2022-08-29 16:41:35,060 n: module.bert.encoder.layer.7.attention.self.query.weight
2022-08-29 16:41:35,060 n: module.bert.encoder.layer.7.attention.self.query.bias
2022-08-29 16:41:35,060 n: module.bert.encoder.layer.7.attention.self.key.weight
2022-08-29 16:41:35,060 n: module.bert.encoder.layer.7.attention.self.key.bias
2022-08-29 16:41:35,060 n: module.bert.encoder.layer.7.attention.self.value.weight
2022-08-29 16:41:35,060 n: module.bert.encoder.layer.7.attention.self.value.bias
2022-08-29 16:41:35,060 n: module.bert.encoder.layer.7.attention.output.dense.weight
2022-08-29 16:41:35,060 n: module.bert.encoder.layer.7.attention.output.dense.bias
2022-08-29 16:41:35,060 n: module.bert.encoder.layer.7.attention.output.LayerNorm.weight
2022-08-29 16:41:35,061 n: module.bert.encoder.layer.7.attention.output.LayerNorm.bias
2022-08-29 16:41:35,061 n: module.bert.encoder.layer.7.intermediate.dense.weight
2022-08-29 16:41:35,061 n: module.bert.encoder.layer.7.intermediate.dense.bias
2022-08-29 16:41:35,061 n: module.bert.encoder.layer.7.output.dense.weight
2022-08-29 16:41:35,061 n: module.bert.encoder.layer.7.output.dense.bias
2022-08-29 16:41:35,061 n: module.bert.encoder.layer.7.output.LayerNorm.weight
2022-08-29 16:41:35,061 n: module.bert.encoder.layer.7.output.LayerNorm.bias
2022-08-29 16:41:35,061 n: module.bert.encoder.layer.8.attention.self.query.weight
2022-08-29 16:41:35,061 n: module.bert.encoder.layer.8.attention.self.query.bias
2022-08-29 16:41:35,061 n: module.bert.encoder.layer.8.attention.self.key.weight
2022-08-29 16:41:35,061 n: module.bert.encoder.layer.8.attention.self.key.bias
2022-08-29 16:41:35,061 n: module.bert.encoder.layer.8.attention.self.value.weight
2022-08-29 16:41:35,061 n: module.bert.encoder.layer.8.attention.self.value.bias
2022-08-29 16:41:35,061 n: module.bert.encoder.layer.8.attention.output.dense.weight
2022-08-29 16:41:35,061 n: module.bert.encoder.layer.8.attention.output.dense.bias
2022-08-29 16:41:35,061 n: module.bert.encoder.layer.8.attention.output.LayerNorm.weight
2022-08-29 16:41:35,061 n: module.bert.encoder.layer.8.attention.output.LayerNorm.bias
2022-08-29 16:41:35,061 n: module.bert.encoder.layer.8.intermediate.dense.weight
2022-08-29 16:41:35,062 n: module.bert.encoder.layer.8.intermediate.dense.bias
2022-08-29 16:41:35,062 n: module.bert.encoder.layer.8.output.dense.weight
2022-08-29 16:41:35,062 n: module.bert.encoder.layer.8.output.dense.bias
2022-08-29 16:41:35,062 n: module.bert.encoder.layer.8.output.LayerNorm.weight
2022-08-29 16:41:35,062 n: module.bert.encoder.layer.8.output.LayerNorm.bias
2022-08-29 16:41:35,062 n: module.bert.encoder.layer.9.attention.self.query.weight
2022-08-29 16:41:35,062 n: module.bert.encoder.layer.9.attention.self.query.bias
2022-08-29 16:41:35,062 n: module.bert.encoder.layer.9.attention.self.key.weight
2022-08-29 16:41:35,062 n: module.bert.encoder.layer.9.attention.self.key.bias
2022-08-29 16:41:35,062 n: module.bert.encoder.layer.9.attention.self.value.weight
2022-08-29 16:41:35,062 n: module.bert.encoder.layer.9.attention.self.value.bias
2022-08-29 16:41:35,062 n: module.bert.encoder.layer.9.attention.output.dense.weight
2022-08-29 16:41:35,062 n: module.bert.encoder.layer.9.attention.output.dense.bias
2022-08-29 16:41:35,062 n: module.bert.encoder.layer.9.attention.output.LayerNorm.weight
2022-08-29 16:41:35,062 n: module.bert.encoder.layer.9.attention.output.LayerNorm.bias
2022-08-29 16:41:35,062 n: module.bert.encoder.layer.9.intermediate.dense.weight
2022-08-29 16:41:35,062 n: module.bert.encoder.layer.9.intermediate.dense.bias
2022-08-29 16:41:35,062 n: module.bert.encoder.layer.9.output.dense.weight
2022-08-29 16:41:35,063 n: module.bert.encoder.layer.9.output.dense.bias
2022-08-29 16:41:35,063 n: module.bert.encoder.layer.9.output.LayerNorm.weight
2022-08-29 16:41:35,063 n: module.bert.encoder.layer.9.output.LayerNorm.bias
2022-08-29 16:41:35,063 n: module.bert.encoder.layer.10.attention.self.query.weight
2022-08-29 16:41:35,063 n: module.bert.encoder.layer.10.attention.self.query.bias
2022-08-29 16:41:35,063 n: module.bert.encoder.layer.10.attention.self.key.weight
2022-08-29 16:41:35,063 n: module.bert.encoder.layer.10.attention.self.key.bias
2022-08-29 16:41:35,063 n: module.bert.encoder.layer.10.attention.self.value.weight
2022-08-29 16:41:35,063 n: module.bert.encoder.layer.10.attention.self.value.bias
2022-08-29 16:41:35,063 n: module.bert.encoder.layer.10.attention.output.dense.weight
2022-08-29 16:41:35,063 n: module.bert.encoder.layer.10.attention.output.dense.bias
2022-08-29 16:41:35,063 n: module.bert.encoder.layer.10.attention.output.LayerNorm.weight
2022-08-29 16:41:35,063 n: module.bert.encoder.layer.10.attention.output.LayerNorm.bias
2022-08-29 16:41:35,063 n: module.bert.encoder.layer.10.intermediate.dense.weight
2022-08-29 16:41:35,063 n: module.bert.encoder.layer.10.intermediate.dense.bias
2022-08-29 16:41:35,063 n: module.bert.encoder.layer.10.output.dense.weight
2022-08-29 16:41:35,063 n: module.bert.encoder.layer.10.output.dense.bias
2022-08-29 16:41:35,063 n: module.bert.encoder.layer.10.output.LayerNorm.weight
2022-08-29 16:41:35,063 n: module.bert.encoder.layer.10.output.LayerNorm.bias
2022-08-29 16:41:35,064 n: module.bert.encoder.layer.11.attention.self.query.weight
2022-08-29 16:41:35,064 n: module.bert.encoder.layer.11.attention.self.query.bias
2022-08-29 16:41:35,064 n: module.bert.encoder.layer.11.attention.self.key.weight
2022-08-29 16:41:35,064 n: module.bert.encoder.layer.11.attention.self.key.bias
2022-08-29 16:41:35,064 n: module.bert.encoder.layer.11.attention.self.value.weight
2022-08-29 16:41:35,064 n: module.bert.encoder.layer.11.attention.self.value.bias
2022-08-29 16:41:35,064 n: module.bert.encoder.layer.11.attention.output.dense.weight
2022-08-29 16:41:35,064 n: module.bert.encoder.layer.11.attention.output.dense.bias
2022-08-29 16:41:35,064 n: module.bert.encoder.layer.11.attention.output.LayerNorm.weight
2022-08-29 16:41:35,064 n: module.bert.encoder.layer.11.attention.output.LayerNorm.bias
2022-08-29 16:41:35,064 n: module.bert.encoder.layer.11.intermediate.dense.weight
2022-08-29 16:41:35,064 n: module.bert.encoder.layer.11.intermediate.dense.bias
2022-08-29 16:41:35,064 n: module.bert.encoder.layer.11.output.dense.weight
2022-08-29 16:41:35,064 n: module.bert.encoder.layer.11.output.dense.bias
2022-08-29 16:41:35,064 n: module.bert.encoder.layer.11.output.LayerNorm.weight
2022-08-29 16:41:35,064 n: module.bert.encoder.layer.11.output.LayerNorm.bias
2022-08-29 16:41:35,064 n: module.bert.pooler.dense.weight
2022-08-29 16:41:35,064 n: module.bert.pooler.dense.bias
2022-08-29 16:41:35,064 n: module.classifier.weight
2022-08-29 16:41:35,065 n: module.classifier.bias
2022-08-29 16:41:35,065 Total parameters: 109483778
2022-08-29 16:41:52,589 ***** Running evaluation *****
2022-08-29 16:41:52,590   Epoch = 0 iter 1599 step
2022-08-29 16:41:52,590   Num examples = 9815
2022-08-29 16:41:52,590   Batch size = 32
2022-08-29 16:41:52,591 ***** Eval results *****
2022-08-29 16:41:52,591   att_loss = 9.359334611982163
2022-08-29 16:41:52,591   cls_loss = 0.0
2022-08-29 16:41:52,591   global_step = 1599
2022-08-29 16:41:52,591   loss = 12.887752267850646
2022-08-29 16:41:52,591   rep_loss = 3.528417660788941
2022-08-29 16:41:52,592 ***** Save model *****
2022-08-29 16:42:08,780 ***** Running evaluation *****
2022-08-29 16:42:08,780   Epoch = 0 iter 99 step
2022-08-29 16:42:08,780   Num examples = 408
2022-08-29 16:42:08,780   Batch size = 32
2022-08-29 16:42:09,627 ***** Eval results *****
2022-08-29 16:42:09,627   acc = 0.7303921568627451
2022-08-29 16:42:09,627   acc_and_f1 = 0.7821145074343936
2022-08-29 16:42:09,627   att_loss = 0.0
2022-08-29 16:42:09,627   cls_loss = 0.20052136495859937
2022-08-29 16:42:09,627   eval_loss = 0.8625680815715057
2022-08-29 16:42:09,627   f1 = 0.8338368580060422
2022-08-29 16:42:09,628   global_step = 99
2022-08-29 16:42:09,628   loss = 0.20052136495859937
2022-08-29 16:42:09,628   rep_loss = 0.0
2022-08-29 16:42:09,628 ***** Save model *****
2022-08-29 16:42:39,154 ***** Running evaluation *****
2022-08-29 16:42:39,155   Epoch = 1 iter 2199 step
2022-08-29 16:42:39,155   Num examples = 872
2022-08-29 16:42:39,155   Batch size = 32
2022-08-29 16:42:39,156 ***** Eval results *****
2022-08-29 16:42:39,156   att_loss = 1.3403996586799622
2022-08-29 16:42:39,156   cls_loss = 0.0
2022-08-29 16:42:39,156   global_step = 2199
2022-08-29 16:42:39,156   loss = 3.2350498927266975
2022-08-29 16:42:39,156   rep_loss = 1.8946502447128295
2022-08-29 16:42:39,157 ***** Save model *****
2022-08-29 16:42:41,361 ***** Running evaluation *****
2022-08-29 16:42:41,362   Epoch = 1 iter 199 step
2022-08-29 16:42:41,362   Num examples = 408
2022-08-29 16:42:41,362   Batch size = 32
2022-08-29 16:42:42,264 ***** Eval results *****
2022-08-29 16:42:42,264   acc = 0.7916666666666666
2022-08-29 16:42:42,264   acc_and_f1 = 0.8239213197969544
2022-08-29 16:42:42,264   att_loss = 0.0
2022-08-29 16:42:42,264   cls_loss = 0.1661663566442097
2022-08-29 16:42:42,264   eval_loss = 0.6078017548872874
2022-08-29 16:42:42,264   f1 = 0.856175972927242
2022-08-29 16:42:42,264   global_step = 199
2022-08-29 16:42:42,265   loss = 0.1661663566442097
2022-08-29 16:42:42,265   rep_loss = 0.0
2022-08-29 16:42:42,265 ***** Save model *****
2022-08-29 16:43:05,181 ***** Running evaluation *****
2022-08-29 16:43:05,181   Epoch = 0 iter 1799 step
2022-08-29 16:43:05,182   Num examples = 9815
2022-08-29 16:43:05,182   Batch size = 32
2022-08-29 16:43:05,183 ***** Eval results *****
2022-08-29 16:43:05,183   att_loss = 9.002617962430621
2022-08-29 16:43:05,183   cls_loss = 0.0
2022-08-29 16:43:05,183   global_step = 1799
2022-08-29 16:43:05,183   loss = 12.426628730905394
2022-08-29 16:43:05,183   rep_loss = 3.424010774173493
2022-08-29 16:43:05,183 ***** Save model *****
2022-08-29 16:43:16,514 ***** Running evaluation *****
2022-08-29 16:43:16,514   Epoch = 2 iter 299 step
2022-08-29 16:43:16,514   Num examples = 408
2022-08-29 16:43:16,515   Batch size = 32
2022-08-29 16:43:17,435 ***** Eval results *****
2022-08-29 16:43:17,435   acc = 0.7549019607843137
2022-08-29 16:43:17,435   acc_and_f1 = 0.8002904865649965
2022-08-29 16:43:17,435   att_loss = 0.0
2022-08-29 16:43:17,435   cls_loss = 0.14583345030395078
2022-08-29 16:43:17,435   eval_loss = 0.7164405125838059
2022-08-29 16:43:17,435   f1 = 0.8456790123456792
2022-08-29 16:43:17,436   global_step = 299
2022-08-29 16:43:17,436   loss = 0.14583345030395078
2022-08-29 16:43:17,436   rep_loss = 0.0
2022-08-29 16:43:48,408 ***** Running evaluation *****
2022-08-29 16:43:48,409   Epoch = 3 iter 399 step
2022-08-29 16:43:48,409   Num examples = 408
2022-08-29 16:43:48,409   Batch size = 32
2022-08-29 16:43:49,257 ***** Eval results *****
2022-08-29 16:43:49,257   acc = 0.7598039215686274
2022-08-29 16:43:49,257   acc_and_f1 = 0.803577973245373
2022-08-29 16:43:49,257   att_loss = 0.0
2022-08-29 16:43:49,257   cls_loss = 0.14742795377969742
2022-08-29 16:43:49,257   eval_loss = 0.7277900748527967
2022-08-29 16:43:49,257   f1 = 0.8473520249221185
2022-08-29 16:43:49,257   global_step = 399
2022-08-29 16:43:49,257   loss = 0.14742795377969742
2022-08-29 16:43:49,257   rep_loss = 0.0
2022-08-29 16:43:57,391 ***** Running evaluation *****
2022-08-29 16:43:57,391   Epoch = 1 iter 2399 step
2022-08-29 16:43:57,391   Num examples = 872
2022-08-29 16:43:57,391   Batch size = 32
2022-08-29 16:43:57,392 ***** Eval results *****
2022-08-29 16:43:57,392   att_loss = 1.3453392160140862
2022-08-29 16:43:57,392   cls_loss = 0.0
2022-08-29 16:43:57,393   global_step = 2399
2022-08-29 16:43:57,393   loss = 3.2288446361735716
2022-08-29 16:43:57,393   rep_loss = 1.8835054187451379
2022-08-29 16:43:57,393 ***** Save model *****
2022-08-29 16:44:19,812 ***** Running evaluation *****
2022-08-29 16:44:19,812   Epoch = 0 iter 1999 step
2022-08-29 16:44:19,812   Num examples = 9815
2022-08-29 16:44:19,813   Batch size = 32
2022-08-29 16:44:19,814 ***** Eval results *****
2022-08-29 16:44:19,814   att_loss = 8.706130799202397
2022-08-29 16:44:19,814   cls_loss = 0.0
2022-08-29 16:44:19,814   global_step = 1999
2022-08-29 16:44:19,814   loss = 12.040539930676626
2022-08-29 16:44:19,814   rep_loss = 3.33440913481376
2022-08-29 16:44:19,814 ***** Save model *****
2022-08-29 16:44:21,146 ***** Running evaluation *****
2022-08-29 16:44:21,147   Epoch = 4 iter 499 step
2022-08-29 16:44:21,147   Num examples = 408
2022-08-29 16:44:21,147   Batch size = 32
2022-08-29 16:44:22,002 ***** Eval results *****
2022-08-29 16:44:22,002   acc = 0.7794117647058824
2022-08-29 16:44:22,002   acc_and_f1 = 0.8175904977375565
2022-08-29 16:44:22,002   att_loss = 0.0
2022-08-29 16:44:22,002   cls_loss = 0.1428380438754725
2022-08-29 16:44:22,002   eval_loss = 0.5974362939596176
2022-08-29 16:44:22,002   f1 = 0.8557692307692307
2022-08-29 16:44:22,002   global_step = 499
2022-08-29 16:44:22,002   loss = 0.1428380438754725
2022-08-29 16:44:22,002   rep_loss = 0.0
2022-08-29 16:45:15,442 ***** Running evaluation *****
2022-08-29 16:45:15,443   Epoch = 1 iter 2599 step
2022-08-29 16:45:15,443   Num examples = 872
2022-08-29 16:45:15,443   Batch size = 32
2022-08-29 16:45:15,444 ***** Eval results *****
2022-08-29 16:45:15,444   att_loss = 1.3338971443850585
2022-08-29 16:45:15,444   cls_loss = 0.0
2022-08-29 16:45:15,444   global_step = 2599
2022-08-29 16:45:15,444   loss = 3.20360662575924
2022-08-29 16:45:15,444   rep_loss = 1.869709481374182
2022-08-29 16:45:15,444 ***** Save model *****
2022-08-29 16:45:30,272 ***** Running evaluation *****
2022-08-29 16:45:30,272   Epoch = 0 iter 2199 step
2022-08-29 16:45:30,272   Num examples = 9815
2022-08-29 16:45:30,272   Batch size = 32
2022-08-29 16:45:30,274 ***** Eval results *****
2022-08-29 16:45:30,274   att_loss = 8.448323499186031
2022-08-29 16:45:30,274   cls_loss = 0.0
2022-08-29 16:45:30,274   global_step = 2199
2022-08-29 16:45:30,274   loss = 11.704994097358805
2022-08-29 16:45:30,274   rep_loss = 3.2566706048948975
2022-08-29 16:45:30,274 ***** Save model *****
2022-08-29 16:46:33,818 ***** Running evaluation *****
2022-08-29 16:46:33,818   Epoch = 1 iter 2799 step
2022-08-29 16:46:33,819   Num examples = 872
2022-08-29 16:46:33,819   Batch size = 32
2022-08-29 16:46:33,820 ***** Eval results *****
2022-08-29 16:46:33,820   att_loss = 1.326165063089604
2022-08-29 16:46:33,820   cls_loss = 0.0
2022-08-29 16:46:33,820   global_step = 2799
2022-08-29 16:46:33,820   loss = 3.184685496803668
2022-08-29 16:46:33,820   rep_loss = 1.8585204319988222
2022-08-29 16:46:33,820 ***** Save model *****
2022-08-29 16:46:42,948 ***** Running evaluation *****
2022-08-29 16:46:42,949   Epoch = 0 iter 2399 step
2022-08-29 16:46:42,949   Num examples = 9815
2022-08-29 16:46:42,949   Batch size = 32
2022-08-29 16:46:42,950 ***** Eval results *****
2022-08-29 16:46:42,950   att_loss = 8.218228080164348
2022-08-29 16:46:42,950   cls_loss = 0.0
2022-08-29 16:46:42,950   global_step = 2399
2022-08-29 16:46:42,950   loss = 11.40564104436387
2022-08-29 16:46:42,950   rep_loss = 3.187412968572352
2022-08-29 16:46:42,950 ***** Save model *****
2022-08-29 16:47:52,000 ***** Running evaluation *****
2022-08-29 16:47:52,000   Epoch = 1 iter 2999 step
2022-08-29 16:47:52,000   Num examples = 872
2022-08-29 16:47:52,000   Batch size = 32
2022-08-29 16:47:52,002 ***** Eval results *****
2022-08-29 16:47:52,002   att_loss = 1.3170119289579338
2022-08-29 16:47:52,002   cls_loss = 0.0
2022-08-29 16:47:52,002   global_step = 2999
2022-08-29 16:47:52,002   loss = 3.166530592614712
2022-08-29 16:47:52,002   rep_loss = 1.8495186608596887
2022-08-29 16:47:52,002 ***** Save model *****
2022-08-29 16:47:53,447 ***** Running evaluation *****
2022-08-29 16:47:53,448   Epoch = 0 iter 2599 step
2022-08-29 16:47:53,448   Num examples = 9815
2022-08-29 16:47:53,448   Batch size = 32
2022-08-29 16:47:53,449 ***** Eval results *****
2022-08-29 16:47:53,449   att_loss = 8.029318953532814
2022-08-29 16:47:53,449   cls_loss = 0.0
2022-08-29 16:47:53,450   global_step = 2599
2022-08-29 16:47:53,450   loss = 11.15666724242078
2022-08-29 16:47:53,450   rep_loss = 3.1273482934747032
2022-08-29 16:47:53,450 ***** Save model *****
2022-08-29 16:49:07,975 ***** Running evaluation *****
2022-08-29 16:49:07,976   Epoch = 0 iter 2799 step
2022-08-29 16:49:07,976   Num examples = 9815
2022-08-29 16:49:07,976   Batch size = 32
2022-08-29 16:49:07,977 ***** Eval results *****
2022-08-29 16:49:07,977   att_loss = 7.863469550080621
2022-08-29 16:49:07,977   cls_loss = 0.0
2022-08-29 16:49:07,977   global_step = 2799
2022-08-29 16:49:07,978   loss = 10.937611682281275
2022-08-29 16:49:07,978   rep_loss = 3.0741421364596504
2022-08-29 16:49:07,978 ***** Save model *****
2022-08-29 16:49:10,792 ***** Running evaluation *****
2022-08-29 16:49:10,793   Epoch = 1 iter 3199 step
2022-08-29 16:49:10,793   Num examples = 872
2022-08-29 16:49:10,793   Batch size = 32
2022-08-29 16:49:10,794 ***** Eval results *****
2022-08-29 16:49:10,795   att_loss = 1.3037795055946804
2022-08-29 16:49:10,795   cls_loss = 0.0
2022-08-29 16:49:10,795   global_step = 3199
2022-08-29 16:49:10,795   loss = 3.14146895669911
2022-08-29 16:49:10,795   rep_loss = 1.8376894478384218
2022-08-29 16:49:10,795 ***** Save model *****
2022-08-29 16:50:20,640 ***** Running evaluation *****
2022-08-29 16:50:20,641   Epoch = 0 iter 2999 step
2022-08-29 16:50:20,641   Num examples = 9815
2022-08-29 16:50:20,641   Batch size = 32
2022-08-29 16:50:20,642 ***** Eval results *****
2022-08-29 16:50:20,642   att_loss = 7.704717700900376
2022-08-29 16:50:20,642   cls_loss = 0.0
2022-08-29 16:50:20,642   global_step = 2999
2022-08-29 16:50:20,642   loss = 10.730377549924784
2022-08-29 16:50:20,642   rep_loss = 3.0256598504553955
2022-08-29 16:50:20,642 ***** Save model *****
2022-08-29 16:50:29,341 ***** Running evaluation *****
2022-08-29 16:50:29,341   Epoch = 1 iter 3399 step
2022-08-29 16:50:29,341   Num examples = 872
2022-08-29 16:50:29,342   Batch size = 32
2022-08-29 16:50:29,343 ***** Eval results *****
2022-08-29 16:50:29,343   att_loss = 1.2931505619328916
2022-08-29 16:50:29,343   cls_loss = 0.0
2022-08-29 16:50:29,343   global_step = 3399
2022-08-29 16:50:29,343   loss = 3.122713477823265
2022-08-29 16:50:29,343   rep_loss = 1.8295629132208218
2022-08-29 16:50:29,343 ***** Save model *****
2022-08-29 16:51:31,075 ***** Running evaluation *****
2022-08-29 16:51:31,076   Epoch = 0 iter 3199 step
2022-08-29 16:51:31,076   Num examples = 9815
2022-08-29 16:51:31,076   Batch size = 32
2022-08-29 16:51:31,077 ***** Eval results *****
2022-08-29 16:51:31,077   att_loss = 7.563381372224021
2022-08-29 16:51:31,077   cls_loss = 0.0
2022-08-29 16:51:31,077   global_step = 3199
2022-08-29 16:51:31,077   loss = 10.544875088167325
2022-08-29 16:51:31,077   rep_loss = 2.981493719595229
2022-08-29 16:51:31,077 ***** Save model *****
2022-08-29 16:51:47,820 ***** Running evaluation *****
2022-08-29 16:51:47,820   Epoch = 1 iter 3599 step
2022-08-29 16:51:47,820   Num examples = 872
2022-08-29 16:51:47,820   Batch size = 32
2022-08-29 16:51:47,821 ***** Eval results *****
2022-08-29 16:51:47,821   att_loss = 1.2853482274307455
2022-08-29 16:51:47,821   cls_loss = 0.0
2022-08-29 16:51:47,821   global_step = 3599
2022-08-29 16:51:47,822   loss = 3.1063944185059205
2022-08-29 16:51:47,822   rep_loss = 1.821046187885629
2022-08-29 16:51:47,822 ***** Save model *****
2022-08-29 16:52:40,150 ***** Running evaluation *****
2022-08-29 16:52:40,150   Epoch = 0 iter 3399 step
2022-08-29 16:52:40,150   Num examples = 9815
2022-08-29 16:52:40,150   Batch size = 32
2022-08-29 16:52:40,152 ***** Eval results *****
2022-08-29 16:52:40,152   att_loss = 7.4380974924609955
2022-08-29 16:52:40,152   cls_loss = 0.0
2022-08-29 16:52:40,152   global_step = 3399
2022-08-29 16:52:40,152   loss = 10.379805119748184
2022-08-29 16:52:40,152   rep_loss = 2.9417076300929383
2022-08-29 16:52:40,152 ***** Save model *****
2022-08-29 16:53:06,486 ***** Running evaluation *****
2022-08-29 16:53:06,487   Epoch = 1 iter 3799 step
2022-08-29 16:53:06,487   Num examples = 872
2022-08-29 16:53:06,487   Batch size = 32
2022-08-29 16:53:06,488 ***** Eval results *****
2022-08-29 16:53:06,488   att_loss = 1.2808718237201724
2022-08-29 16:53:06,488   cls_loss = 0.0
2022-08-29 16:53:06,488   global_step = 3799
2022-08-29 16:53:06,488   loss = 3.094147712291166
2022-08-29 16:53:06,488   rep_loss = 1.8132758867072496
2022-08-29 16:53:06,488 ***** Save model *****
2022-08-29 16:53:51,566 ***** Running evaluation *****
2022-08-29 16:53:51,566   Epoch = 0 iter 3599 step
2022-08-29 16:53:51,566   Num examples = 9815
2022-08-29 16:53:51,567   Batch size = 32
2022-08-29 16:53:51,568 ***** Eval results *****
2022-08-29 16:53:51,568   att_loss = 7.321345408713364
2022-08-29 16:53:51,568   cls_loss = 0.0
2022-08-29 16:53:51,568   global_step = 3599
2022-08-29 16:53:51,568   loss = 10.226501214832158
2022-08-29 16:53:51,568   rep_loss = 2.9051558087023803
2022-08-29 16:53:51,568 ***** Save model *****
2022-08-29 16:54:25,909 ***** Running evaluation *****
2022-08-29 16:54:25,909   Epoch = 1 iter 3999 step
2022-08-29 16:54:25,909   Num examples = 872
2022-08-29 16:54:25,909   Batch size = 32
2022-08-29 16:54:25,910 ***** Eval results *****
2022-08-29 16:54:25,911   att_loss = 1.2726335934409994
2022-08-29 16:54:25,911   cls_loss = 0.0
2022-08-29 16:54:25,911   global_step = 3999
2022-08-29 16:54:25,911   loss = 3.077769525321618
2022-08-29 16:54:25,911   rep_loss = 1.8051359301821224
2022-08-29 16:54:25,911 ***** Save model *****
2022-08-29 16:55:00,477 ***** Running evaluation *****
2022-08-29 16:55:00,477   Epoch = 0 iter 3799 step
2022-08-29 16:55:00,477   Num examples = 9815
2022-08-29 16:55:00,477   Batch size = 32
2022-08-29 16:55:00,478 ***** Eval results *****
2022-08-29 16:55:00,478   att_loss = 7.217375601364079
2022-08-29 16:55:00,479   cls_loss = 0.0
2022-08-29 16:55:00,479   global_step = 3799
2022-08-29 16:55:00,479   loss = 10.088918666709063
2022-08-29 16:55:00,479   rep_loss = 2.871543068106346
2022-08-29 16:55:00,479 ***** Save model *****
2022-08-29 16:55:44,462 ***** Running evaluation *****
2022-08-29 16:55:44,462   Epoch = 1 iter 4199 step
2022-08-29 16:55:44,462   Num examples = 872
2022-08-29 16:55:44,462   Batch size = 32
2022-08-29 16:55:44,463 ***** Eval results *****
2022-08-29 16:55:44,463   att_loss = 1.2659751612417454
2022-08-29 16:55:44,463   cls_loss = 0.0
2022-08-29 16:55:44,464   global_step = 4199
2022-08-29 16:55:44,464   loss = 3.0645145287661677
2022-08-29 16:55:44,464   rep_loss = 1.7985393656751136
2022-08-29 16:55:44,464 ***** Save model *****
2022-08-29 16:56:11,444 ***** Running evaluation *****
2022-08-29 16:56:11,445   Epoch = 0 iter 3999 step
2022-08-29 16:56:11,445   Num examples = 9815
2022-08-29 16:56:11,445   Batch size = 32
2022-08-29 16:56:11,446 ***** Eval results *****
2022-08-29 16:56:11,446   att_loss = 7.121313103379891
2022-08-29 16:56:11,446   cls_loss = 0.0
2022-08-29 16:56:11,446   global_step = 3999
2022-08-29 16:56:11,446   loss = 9.961761125238814
2022-08-29 16:56:11,446   rep_loss = 2.8404480256745743
2022-08-29 16:56:11,447 ***** Save model *****
2022-08-29 16:57:02,267 ***** Running evaluation *****
2022-08-29 16:57:02,268   Epoch = 2 iter 4399 step
2022-08-29 16:57:02,268   Num examples = 872
2022-08-29 16:57:02,268   Batch size = 32
2022-08-29 16:57:02,269 ***** Eval results *****
2022-08-29 16:57:02,269   att_loss = 1.176580568258675
2022-08-29 16:57:02,269   cls_loss = 0.0
2022-08-29 16:57:02,269   global_step = 4399
2022-08-29 16:57:02,269   loss = 2.8764553856475192
2022-08-29 16:57:02,270   rep_loss = 1.6998748186371089
2022-08-29 16:57:02,270 ***** Save model *****
2022-08-29 16:57:24,191 ***** Running evaluation *****
2022-08-29 16:57:24,191   Epoch = 0 iter 4199 step
2022-08-29 16:57:24,191   Num examples = 9815
2022-08-29 16:57:24,192   Batch size = 32
2022-08-29 16:57:24,193 ***** Eval results *****
2022-08-29 16:57:24,193   att_loss = 7.029564525377583
2022-08-29 16:57:24,193   cls_loss = 0.0
2022-08-29 16:57:24,193   global_step = 4199
2022-08-29 16:57:24,193   loss = 9.841011681026378
2022-08-29 16:57:24,193   rep_loss = 2.811447158885246
2022-08-29 16:57:24,193 ***** Save model *****
2022-08-29 16:58:20,301 ***** Running evaluation *****
2022-08-29 16:58:20,301   Epoch = 2 iter 4599 step
2022-08-29 16:58:20,301   Num examples = 872
2022-08-29 16:58:20,301   Batch size = 32
2022-08-29 16:58:20,302 ***** Eval results *****
2022-08-29 16:58:20,302   att_loss = 1.1775418414789087
2022-08-29 16:58:20,303   cls_loss = 0.0
2022-08-29 16:58:20,303   global_step = 4599
2022-08-29 16:58:20,303   loss = 2.876413167894954
2022-08-29 16:58:20,303   rep_loss = 1.698871321690357
2022-08-29 16:58:20,303 ***** Save model *****
2022-08-29 16:58:34,796 ***** Running evaluation *****
2022-08-29 16:58:34,796   Epoch = 0 iter 4399 step
2022-08-29 16:58:34,797   Num examples = 9815
2022-08-29 16:58:34,797   Batch size = 32
2022-08-29 16:58:34,798 ***** Eval results *****
2022-08-29 16:58:34,798   att_loss = 6.945791594357241
2022-08-29 16:58:34,798   cls_loss = 0.0
2022-08-29 16:58:34,798   global_step = 4399
2022-08-29 16:58:34,798   loss = 9.73022393677337
2022-08-29 16:58:34,798   rep_loss = 2.7844323469145946
2022-08-29 16:58:34,798 ***** Save model *****
2022-08-29 16:59:38,333 ***** Running evaluation *****
2022-08-29 16:59:38,333   Epoch = 2 iter 4799 step
2022-08-29 16:59:38,333   Num examples = 872
2022-08-29 16:59:38,333   Batch size = 32
2022-08-29 16:59:38,334 ***** Eval results *****
2022-08-29 16:59:38,334   att_loss = 1.1725355099300443
2022-08-29 16:59:38,335   cls_loss = 0.0
2022-08-29 16:59:38,335   global_step = 4799
2022-08-29 16:59:38,335   loss = 2.868855847118268
2022-08-29 16:59:38,335   rep_loss = 1.696320336885662
2022-08-29 16:59:38,335 ***** Save model *****
2022-08-29 16:59:45,232 ***** Running evaluation *****
2022-08-29 16:59:45,233   Epoch = 0 iter 4599 step
2022-08-29 16:59:45,233   Num examples = 9815
2022-08-29 16:59:45,233   Batch size = 32
2022-08-29 16:59:45,234 ***** Eval results *****
2022-08-29 16:59:45,234   att_loss = 6.869249446155144
2022-08-29 16:59:45,234   cls_loss = 0.0
2022-08-29 16:59:45,234   global_step = 4599
2022-08-29 16:59:45,234   loss = 9.628749414951185
2022-08-29 16:59:45,234   rep_loss = 2.7594999732025602
2022-08-29 16:59:45,235 ***** Save model *****
2022-08-29 17:00:56,393 ***** Running evaluation *****
2022-08-29 17:00:56,394   Epoch = 0 iter 4799 step
2022-08-29 17:00:56,394   Num examples = 9815
2022-08-29 17:00:56,394   Batch size = 32
2022-08-29 17:00:56,395 ***** Eval results *****
2022-08-29 17:00:56,395   att_loss = 6.794965784011669
2022-08-29 17:00:56,395   cls_loss = 0.0
2022-08-29 17:00:56,395   global_step = 4799
2022-08-29 17:00:56,395   loss = 9.530737957176404
2022-08-29 17:00:56,395   rep_loss = 2.735772177039844
2022-08-29 17:00:56,396 ***** Save model *****
2022-08-29 17:00:58,878 ***** Running evaluation *****
2022-08-29 17:00:58,879   Epoch = 2 iter 4999 step
2022-08-29 17:00:58,879   Num examples = 872
2022-08-29 17:00:58,879   Batch size = 32
2022-08-29 17:00:58,880 ***** Eval results *****
2022-08-29 17:00:58,880   att_loss = 1.17657936659233
2022-08-29 17:00:58,880   cls_loss = 0.0
2022-08-29 17:00:58,880   global_step = 4999
2022-08-29 17:00:58,880   loss = 2.871359981266798
2022-08-29 17:00:58,880   rep_loss = 1.6947806155787104
2022-08-29 17:00:58,880 ***** Save model *****
2022-08-29 17:02:05,305 ***** Running evaluation *****
2022-08-29 17:02:05,305   Epoch = 0 iter 4999 step
2022-08-29 17:02:05,305   Num examples = 9815
2022-08-29 17:02:05,305   Batch size = 32
2022-08-29 17:02:05,306 ***** Eval results *****
2022-08-29 17:02:05,306   att_loss = 6.726251321164196
2022-08-29 17:02:05,306   cls_loss = 0.0
2022-08-29 17:02:05,306   global_step = 4999
2022-08-29 17:02:05,307   loss = 9.439835568622819
2022-08-29 17:02:05,307   rep_loss = 2.713584252037175
2022-08-29 17:02:05,307 ***** Save model *****
2022-08-29 17:02:17,545 ***** Running evaluation *****
2022-08-29 17:02:17,546   Epoch = 2 iter 5199 step
2022-08-29 17:02:17,546   Num examples = 872
2022-08-29 17:02:17,546   Batch size = 32
2022-08-29 17:02:17,547 ***** Eval results *****
2022-08-29 17:02:17,547   att_loss = 1.1706050012474944
2022-08-29 17:02:17,547   cls_loss = 0.0
2022-08-29 17:02:17,547   global_step = 5199
2022-08-29 17:02:17,547   loss = 2.8614149314965056
2022-08-29 17:02:17,547   rep_loss = 1.6908099301287192
2022-08-29 17:02:17,547 ***** Save model *****
2022-08-29 17:03:16,435 ***** Running evaluation *****
2022-08-29 17:03:16,435   Epoch = 0 iter 5199 step
2022-08-29 17:03:16,435   Num examples = 9815
2022-08-29 17:03:16,435   Batch size = 32
2022-08-29 17:03:16,436 ***** Eval results *****
2022-08-29 17:03:16,437   att_loss = 6.661711646217777
2022-08-29 17:03:16,437   cls_loss = 0.0
2022-08-29 17:03:16,437   global_step = 5199
2022-08-29 17:03:16,437   loss = 9.354395178882726
2022-08-29 17:03:16,437   rep_loss = 2.692683537571814
2022-08-29 17:03:16,437 ***** Save model *****
2022-08-29 17:03:35,848 ***** Running evaluation *****
2022-08-29 17:03:35,849   Epoch = 2 iter 5399 step
2022-08-29 17:03:35,849   Num examples = 872
2022-08-29 17:03:35,849   Batch size = 32
2022-08-29 17:03:35,850 ***** Eval results *****
2022-08-29 17:03:35,850   att_loss = 1.171561520355875
2022-08-29 17:03:35,851   cls_loss = 0.0
2022-08-29 17:03:35,851   global_step = 5399
2022-08-29 17:03:35,851   loss = 2.8600580396019644
2022-08-29 17:03:35,851   rep_loss = 1.6884965207975118
2022-08-29 17:03:35,851 ***** Save model *****
2022-08-29 17:04:27,499 ***** Running evaluation *****
2022-08-29 17:04:27,500   Epoch = 0 iter 5399 step
2022-08-29 17:04:27,500   Num examples = 9815
2022-08-29 17:04:27,500   Batch size = 32
2022-08-29 17:04:27,501 ***** Eval results *****
2022-08-29 17:04:27,501   att_loss = 6.600498169699208
2022-08-29 17:04:27,501   cls_loss = 0.0
2022-08-29 17:04:27,501   global_step = 5399
2022-08-29 17:04:27,501   loss = 9.27347155442214
2022-08-29 17:04:27,501   rep_loss = 2.6729733898013053
2022-08-29 17:04:27,501 ***** Save model *****
2022-08-29 17:04:53,840 ***** Running evaluation *****
2022-08-29 17:04:53,841   Epoch = 2 iter 5599 step
2022-08-29 17:04:53,841   Num examples = 872
2022-08-29 17:04:53,841   Batch size = 32
2022-08-29 17:04:53,842 ***** Eval results *****
2022-08-29 17:04:53,842   att_loss = 1.1660352685226973
2022-08-29 17:04:53,842   cls_loss = 0.0
2022-08-29 17:04:53,842   global_step = 5599
2022-08-29 17:04:53,842   loss = 2.8512467170772924
2022-08-29 17:04:53,843   rep_loss = 1.6852114509113567
2022-08-29 17:04:53,843 ***** Save model *****
2022-08-29 17:05:36,735 ***** Running evaluation *****
2022-08-29 17:05:36,735   Epoch = 0 iter 5599 step
2022-08-29 17:05:36,735   Num examples = 9815
2022-08-29 17:05:36,735   Batch size = 32
2022-08-29 17:05:36,737 ***** Eval results *****
2022-08-29 17:05:36,737   att_loss = 6.54372964111433
2022-08-29 17:05:36,737   cls_loss = 0.0
2022-08-29 17:05:36,737   global_step = 5599
2022-08-29 17:05:36,737   loss = 9.19803811482264
2022-08-29 17:05:36,737   rep_loss = 2.654308478009128
2022-08-29 17:05:36,737 ***** Save model *****
2022-08-29 17:06:13,091 ***** Running evaluation *****
2022-08-29 17:06:13,091   Epoch = 2 iter 5799 step
2022-08-29 17:06:13,091   Num examples = 872
2022-08-29 17:06:13,091   Batch size = 32
2022-08-29 17:06:13,092 ***** Eval results *****
2022-08-29 17:06:13,092   att_loss = 1.1654922284846183
2022-08-29 17:06:13,093   cls_loss = 0.0
2022-08-29 17:06:13,093   global_step = 5799
2022-08-29 17:06:13,093   loss = 2.8475816990128857
2022-08-29 17:06:13,093   rep_loss = 1.682089472026813
2022-08-29 17:06:13,093 ***** Save model *****
2022-08-29 17:06:46,169 ***** Running evaluation *****
2022-08-29 17:06:46,170   Epoch = 0 iter 5799 step
2022-08-29 17:06:46,170   Num examples = 9815
2022-08-29 17:06:46,170   Batch size = 32
2022-08-29 17:06:46,171 ***** Eval results *****
2022-08-29 17:06:46,171   att_loss = 6.490047429836831
2022-08-29 17:06:46,171   cls_loss = 0.0
2022-08-29 17:06:46,171   global_step = 5799
2022-08-29 17:06:46,171   loss = 9.12671032303838
2022-08-29 17:06:46,171   rep_loss = 2.63666289733348
2022-08-29 17:06:46,171 ***** Save model *****
2022-08-29 17:07:35,116 ***** Running evaluation *****
2022-08-29 17:07:35,117   Epoch = 2 iter 5999 step
2022-08-29 17:07:35,117   Num examples = 872
2022-08-29 17:07:35,117   Batch size = 32
2022-08-29 17:07:35,118 ***** Eval results *****
2022-08-29 17:07:35,118   att_loss = 1.1630341098469639
2022-08-29 17:07:35,118   cls_loss = 0.0
2022-08-29 17:07:35,118   global_step = 5999
2022-08-29 17:07:35,118   loss = 2.8424771629452774
2022-08-29 17:07:35,118   rep_loss = 1.6794430547290378
2022-08-29 17:07:35,118 ***** Save model *****
2022-08-29 17:07:52,616 The args: Namespace(aug_train=False, cache_dir='', data_dir='glue_data/MRPC', data_url='', do_eval=False, do_lower_case=True, eval_batch_size=32, eval_step=200, gradient_accumulation_steps=1, hidden_act='quad', learning_rate=5e-05, log_path='tmp/distill/MRPC/quad_2quad/bert-base-uncased/log.txt', max_seq_length=128, no_cuda=False, num_train_epochs=3.0, output_dir='tmp/distill/MRPC/quad_2quad/bert-base-uncased/5e-05_1e-05_8', pred_distill=False, seed=42, softmax_act='2quad', student_model='/home/ubuntu/checkpoints/exp/MRPC', task_name='MRPC', teacher_model='/home/ubuntu/checkpoints/exp/MRPC', temperature=1.0, train_batch_size=8, warmup_proportion=0.1, weight_decay=0.0001)
2022-08-29 17:07:52,617 device: cuda n_gpu: 2
2022-08-29 17:07:52,700 Writing example 0 of 3668
2022-08-29 17:07:52,700 *** Example ***
2022-08-29 17:07:52,701 guid: train-1
2022-08-29 17:07:52,701 tokens: [CLS] am ##ro ##zi accused his brother , whom he called " the witness " , of deliberately di ##stor ##ting his evidence . [SEP] referring to him as only " the witness " , am ##ro ##zi accused his brother of deliberately di ##stor ##ting his evidence . [SEP]
2022-08-29 17:07:52,701 input_ids: 101 2572 3217 5831 5496 2010 2567 1010 3183 2002 2170 1000 1996 7409 1000 1010 1997 9969 4487 23809 3436 2010 3350 1012 102 7727 2000 2032 2004 2069 1000 1996 7409 1000 1010 2572 3217 5831 5496 2010 2567 1997 9969 4487 23809 3436 2010 3350 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-08-29 17:07:52,701 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-08-29 17:07:52,701 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-08-29 17:07:52,701 label: 1
2022-08-29 17:07:52,701 label_id: 1
2022-08-29 17:07:55,597 Writing example 0 of 408
2022-08-29 17:07:55,598 *** Example ***
2022-08-29 17:07:55,598 guid: dev-1
2022-08-29 17:07:55,598 tokens: [CLS] he said the foods ##er ##vic ##e pie business doesn ' t fit the company ' s long - term growth strategy . [SEP] " the foods ##er ##vic ##e pie business does not fit our long - term growth strategy . [SEP]
2022-08-29 17:07:55,598 input_ids: 101 2002 2056 1996 9440 2121 7903 2063 11345 2449 2987 1005 1056 4906 1996 2194 1005 1055 2146 1011 2744 3930 5656 1012 102 1000 1996 9440 2121 7903 2063 11345 2449 2515 2025 4906 2256 2146 1011 2744 3930 5656 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-08-29 17:07:55,598 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-08-29 17:07:55,599 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-08-29 17:07:55,599 label: 1
2022-08-29 17:07:55,599 label_id: 1
2022-08-29 17:07:55,925 Model config {
  "_name_or_path": "bert-base-uncased",
  "architectures": [
    "BertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "crypten": false,
  "finetuning_task": "mrpc",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "not_equivalent",
    "1": "equivalent"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "equivalent": 1,
    "not_equivalent": 0
  },
  "layer_norm_eps": 1e-12,
  "log_path": "tmp/distill/MRPC/quad_2quad/bert-base-uncased/log.txt",
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "pre_trained": "",
  "problem_type": "single_label_classification",
  "softmax_act": "softmax",
  "torch_dtype": "float32",
  "training": "",
  "transformers_version": "4.20.0.dev0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

2022-08-29 17:07:58,627 Loading model /home/ubuntu/checkpoints/exp/MRPC/pytorch_model.bin
2022-08-29 17:07:58,884 ***** Running evaluation *****
2022-08-29 17:07:58,884   Epoch = 0 iter 5999 step
2022-08-29 17:07:58,884   Num examples = 9815
2022-08-29 17:07:58,885   Batch size = 32
2022-08-29 17:07:58,886 ***** Eval results *****
2022-08-29 17:07:58,886   att_loss = 6.4376331296915374
2022-08-29 17:07:58,886   cls_loss = 0.0
2022-08-29 17:07:58,886   global_step = 5999
2022-08-29 17:07:58,886   loss = 9.057425652926993
2022-08-29 17:07:58,886   rep_loss = 2.6197925266732294
2022-08-29 17:07:58,886 ***** Save model *****
2022-08-29 17:07:58,926 loading model...
2022-08-29 17:07:58,980 done!
2022-08-29 17:07:58,980 Weights from pretrained model not used in TinyBertForSequenceClassification: ['bert.embeddings.position_ids']
2022-08-29 17:08:03,749 ***** Teacher evaluation *****
2022-08-29 17:08:03,749 {'acc': 0.8602941176470589, 'f1': 0.9025641025641027, 'acc_and_f1': 0.8814291101055808, 'eval_loss': 0.3993110977686368}
2022-08-29 17:08:03,750 Model config {
  "_name_or_path": "bert-base-uncased",
  "architectures": [
    "BertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "crypten": false,
  "finetuning_task": "mrpc",
  "gradient_checkpointing": false,
  "hidden_act": "quad",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "not_equivalent",
    "1": "equivalent"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "equivalent": 1,
    "not_equivalent": 0
  },
  "layer_norm_eps": 1e-12,
  "log_path": "tmp/distill/MRPC/quad_2quad/bert-base-uncased/log.txt",
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "pre_trained": "",
  "problem_type": "single_label_classification",
  "softmax_act": "2quad",
  "torch_dtype": "float32",
  "training": "",
  "transformers_version": "4.20.0.dev0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

2022-08-29 17:08:06,395 Loading model /home/ubuntu/checkpoints/exp/MRPC/pytorch_model.bin
2022-08-29 17:08:06,689 loading model...
2022-08-29 17:08:06,733 done!
2022-08-29 17:08:06,733 Weights from pretrained model not used in TinyBertForSequenceClassification: ['bert.embeddings.position_ids']
2022-08-29 17:08:06,814 ***** Running training *****
2022-08-29 17:08:06,814   Num examples = 3668
2022-08-29 17:08:06,814   Batch size = 8
2022-08-29 17:08:06,814   Num steps = 9160
2022-08-29 17:08:06,815 n: module.bert.embeddings.word_embeddings.weight
2022-08-29 17:08:06,815 n: module.bert.embeddings.position_embeddings.weight
2022-08-29 17:08:06,816 n: module.bert.embeddings.token_type_embeddings.weight
2022-08-29 17:08:06,816 n: module.bert.embeddings.LayerNorm.weight
2022-08-29 17:08:06,816 n: module.bert.embeddings.LayerNorm.bias
2022-08-29 17:08:06,816 n: module.bert.encoder.layer.0.attention.self.query.weight
2022-08-29 17:08:06,816 n: module.bert.encoder.layer.0.attention.self.query.bias
2022-08-29 17:08:06,816 n: module.bert.encoder.layer.0.attention.self.key.weight
2022-08-29 17:08:06,816 n: module.bert.encoder.layer.0.attention.self.key.bias
2022-08-29 17:08:06,816 n: module.bert.encoder.layer.0.attention.self.value.weight
2022-08-29 17:08:06,816 n: module.bert.encoder.layer.0.attention.self.value.bias
2022-08-29 17:08:06,816 n: module.bert.encoder.layer.0.attention.output.dense.weight
2022-08-29 17:08:06,816 n: module.bert.encoder.layer.0.attention.output.dense.bias
2022-08-29 17:08:06,816 n: module.bert.encoder.layer.0.attention.output.LayerNorm.weight
2022-08-29 17:08:06,816 n: module.bert.encoder.layer.0.attention.output.LayerNorm.bias
2022-08-29 17:08:06,816 n: module.bert.encoder.layer.0.intermediate.dense.weight
2022-08-29 17:08:06,816 n: module.bert.encoder.layer.0.intermediate.dense.bias
2022-08-29 17:08:06,816 n: module.bert.encoder.layer.0.output.dense.weight
2022-08-29 17:08:06,816 n: module.bert.encoder.layer.0.output.dense.bias
2022-08-29 17:08:06,817 n: module.bert.encoder.layer.0.output.LayerNorm.weight
2022-08-29 17:08:06,817 n: module.bert.encoder.layer.0.output.LayerNorm.bias
2022-08-29 17:08:06,817 n: module.bert.encoder.layer.1.attention.self.query.weight
2022-08-29 17:08:06,817 n: module.bert.encoder.layer.1.attention.self.query.bias
2022-08-29 17:08:06,817 n: module.bert.encoder.layer.1.attention.self.key.weight
2022-08-29 17:08:06,817 n: module.bert.encoder.layer.1.attention.self.key.bias
2022-08-29 17:08:06,817 n: module.bert.encoder.layer.1.attention.self.value.weight
2022-08-29 17:08:06,817 n: module.bert.encoder.layer.1.attention.self.value.bias
2022-08-29 17:08:06,817 n: module.bert.encoder.layer.1.attention.output.dense.weight
2022-08-29 17:08:06,817 n: module.bert.encoder.layer.1.attention.output.dense.bias
2022-08-29 17:08:06,817 n: module.bert.encoder.layer.1.attention.output.LayerNorm.weight
2022-08-29 17:08:06,817 n: module.bert.encoder.layer.1.attention.output.LayerNorm.bias
2022-08-29 17:08:06,817 n: module.bert.encoder.layer.1.intermediate.dense.weight
2022-08-29 17:08:06,817 n: module.bert.encoder.layer.1.intermediate.dense.bias
2022-08-29 17:08:06,817 n: module.bert.encoder.layer.1.output.dense.weight
2022-08-29 17:08:06,817 n: module.bert.encoder.layer.1.output.dense.bias
2022-08-29 17:08:06,817 n: module.bert.encoder.layer.1.output.LayerNorm.weight
2022-08-29 17:08:06,817 n: module.bert.encoder.layer.1.output.LayerNorm.bias
2022-08-29 17:08:06,818 n: module.bert.encoder.layer.2.attention.self.query.weight
2022-08-29 17:08:06,818 n: module.bert.encoder.layer.2.attention.self.query.bias
2022-08-29 17:08:06,818 n: module.bert.encoder.layer.2.attention.self.key.weight
2022-08-29 17:08:06,818 n: module.bert.encoder.layer.2.attention.self.key.bias
2022-08-29 17:08:06,818 n: module.bert.encoder.layer.2.attention.self.value.weight
2022-08-29 17:08:06,818 n: module.bert.encoder.layer.2.attention.self.value.bias
2022-08-29 17:08:06,818 n: module.bert.encoder.layer.2.attention.output.dense.weight
2022-08-29 17:08:06,818 n: module.bert.encoder.layer.2.attention.output.dense.bias
2022-08-29 17:08:06,818 n: module.bert.encoder.layer.2.attention.output.LayerNorm.weight
2022-08-29 17:08:06,818 n: module.bert.encoder.layer.2.attention.output.LayerNorm.bias
2022-08-29 17:08:06,818 n: module.bert.encoder.layer.2.intermediate.dense.weight
2022-08-29 17:08:06,818 n: module.bert.encoder.layer.2.intermediate.dense.bias
2022-08-29 17:08:06,818 n: module.bert.encoder.layer.2.output.dense.weight
2022-08-29 17:08:06,818 n: module.bert.encoder.layer.2.output.dense.bias
2022-08-29 17:08:06,818 n: module.bert.encoder.layer.2.output.LayerNorm.weight
2022-08-29 17:08:06,818 n: module.bert.encoder.layer.2.output.LayerNorm.bias
2022-08-29 17:08:06,818 n: module.bert.encoder.layer.3.attention.self.query.weight
2022-08-29 17:08:06,818 n: module.bert.encoder.layer.3.attention.self.query.bias
2022-08-29 17:08:06,819 n: module.bert.encoder.layer.3.attention.self.key.weight
2022-08-29 17:08:06,819 n: module.bert.encoder.layer.3.attention.self.key.bias
2022-08-29 17:08:06,819 n: module.bert.encoder.layer.3.attention.self.value.weight
2022-08-29 17:08:06,819 n: module.bert.encoder.layer.3.attention.self.value.bias
2022-08-29 17:08:06,819 n: module.bert.encoder.layer.3.attention.output.dense.weight
2022-08-29 17:08:06,819 n: module.bert.encoder.layer.3.attention.output.dense.bias
2022-08-29 17:08:06,819 n: module.bert.encoder.layer.3.attention.output.LayerNorm.weight
2022-08-29 17:08:06,819 n: module.bert.encoder.layer.3.attention.output.LayerNorm.bias
2022-08-29 17:08:06,819 n: module.bert.encoder.layer.3.intermediate.dense.weight
2022-08-29 17:08:06,819 n: module.bert.encoder.layer.3.intermediate.dense.bias
2022-08-29 17:08:06,819 n: module.bert.encoder.layer.3.output.dense.weight
2022-08-29 17:08:06,819 n: module.bert.encoder.layer.3.output.dense.bias
2022-08-29 17:08:06,819 n: module.bert.encoder.layer.3.output.LayerNorm.weight
2022-08-29 17:08:06,819 n: module.bert.encoder.layer.3.output.LayerNorm.bias
2022-08-29 17:08:06,819 n: module.bert.encoder.layer.4.attention.self.query.weight
2022-08-29 17:08:06,819 n: module.bert.encoder.layer.4.attention.self.query.bias
2022-08-29 17:08:06,819 n: module.bert.encoder.layer.4.attention.self.key.weight
2022-08-29 17:08:06,819 n: module.bert.encoder.layer.4.attention.self.key.bias
2022-08-29 17:08:06,819 n: module.bert.encoder.layer.4.attention.self.value.weight
2022-08-29 17:08:06,820 n: module.bert.encoder.layer.4.attention.self.value.bias
2022-08-29 17:08:06,820 n: module.bert.encoder.layer.4.attention.output.dense.weight
2022-08-29 17:08:06,820 n: module.bert.encoder.layer.4.attention.output.dense.bias
2022-08-29 17:08:06,820 n: module.bert.encoder.layer.4.attention.output.LayerNorm.weight
2022-08-29 17:08:06,820 n: module.bert.encoder.layer.4.attention.output.LayerNorm.bias
2022-08-29 17:08:06,820 n: module.bert.encoder.layer.4.intermediate.dense.weight
2022-08-29 17:08:06,820 n: module.bert.encoder.layer.4.intermediate.dense.bias
2022-08-29 17:08:06,820 n: module.bert.encoder.layer.4.output.dense.weight
2022-08-29 17:08:06,820 n: module.bert.encoder.layer.4.output.dense.bias
2022-08-29 17:08:06,820 n: module.bert.encoder.layer.4.output.LayerNorm.weight
2022-08-29 17:08:06,820 n: module.bert.encoder.layer.4.output.LayerNorm.bias
2022-08-29 17:08:06,820 n: module.bert.encoder.layer.5.attention.self.query.weight
2022-08-29 17:08:06,820 n: module.bert.encoder.layer.5.attention.self.query.bias
2022-08-29 17:08:06,820 n: module.bert.encoder.layer.5.attention.self.key.weight
2022-08-29 17:08:06,820 n: module.bert.encoder.layer.5.attention.self.key.bias
2022-08-29 17:08:06,820 n: module.bert.encoder.layer.5.attention.self.value.weight
2022-08-29 17:08:06,820 n: module.bert.encoder.layer.5.attention.self.value.bias
2022-08-29 17:08:06,820 n: module.bert.encoder.layer.5.attention.output.dense.weight
2022-08-29 17:08:06,821 n: module.bert.encoder.layer.5.attention.output.dense.bias
2022-08-29 17:08:06,821 n: module.bert.encoder.layer.5.attention.output.LayerNorm.weight
2022-08-29 17:08:06,821 n: module.bert.encoder.layer.5.attention.output.LayerNorm.bias
2022-08-29 17:08:06,821 n: module.bert.encoder.layer.5.intermediate.dense.weight
2022-08-29 17:08:06,821 n: module.bert.encoder.layer.5.intermediate.dense.bias
2022-08-29 17:08:06,821 n: module.bert.encoder.layer.5.output.dense.weight
2022-08-29 17:08:06,821 n: module.bert.encoder.layer.5.output.dense.bias
2022-08-29 17:08:06,821 n: module.bert.encoder.layer.5.output.LayerNorm.weight
2022-08-29 17:08:06,821 n: module.bert.encoder.layer.5.output.LayerNorm.bias
2022-08-29 17:08:06,821 n: module.bert.encoder.layer.6.attention.self.query.weight
2022-08-29 17:08:06,821 n: module.bert.encoder.layer.6.attention.self.query.bias
2022-08-29 17:08:06,821 n: module.bert.encoder.layer.6.attention.self.key.weight
2022-08-29 17:08:06,821 n: module.bert.encoder.layer.6.attention.self.key.bias
2022-08-29 17:08:06,821 n: module.bert.encoder.layer.6.attention.self.value.weight
2022-08-29 17:08:06,821 n: module.bert.encoder.layer.6.attention.self.value.bias
2022-08-29 17:08:06,821 n: module.bert.encoder.layer.6.attention.output.dense.weight
2022-08-29 17:08:06,821 n: module.bert.encoder.layer.6.attention.output.dense.bias
2022-08-29 17:08:06,821 n: module.bert.encoder.layer.6.attention.output.LayerNorm.weight
2022-08-29 17:08:06,821 n: module.bert.encoder.layer.6.attention.output.LayerNorm.bias
2022-08-29 17:08:06,822 n: module.bert.encoder.layer.6.intermediate.dense.weight
2022-08-29 17:08:06,822 n: module.bert.encoder.layer.6.intermediate.dense.bias
2022-08-29 17:08:06,822 n: module.bert.encoder.layer.6.output.dense.weight
2022-08-29 17:08:06,822 n: module.bert.encoder.layer.6.output.dense.bias
2022-08-29 17:08:06,822 n: module.bert.encoder.layer.6.output.LayerNorm.weight
2022-08-29 17:08:06,822 n: module.bert.encoder.layer.6.output.LayerNorm.bias
2022-08-29 17:08:06,822 n: module.bert.encoder.layer.7.attention.self.query.weight
2022-08-29 17:08:06,822 n: module.bert.encoder.layer.7.attention.self.query.bias
2022-08-29 17:08:06,822 n: module.bert.encoder.layer.7.attention.self.key.weight
2022-08-29 17:08:06,822 n: module.bert.encoder.layer.7.attention.self.key.bias
2022-08-29 17:08:06,822 n: module.bert.encoder.layer.7.attention.self.value.weight
2022-08-29 17:08:06,822 n: module.bert.encoder.layer.7.attention.self.value.bias
2022-08-29 17:08:06,822 n: module.bert.encoder.layer.7.attention.output.dense.weight
2022-08-29 17:08:06,822 n: module.bert.encoder.layer.7.attention.output.dense.bias
2022-08-29 17:08:06,822 n: module.bert.encoder.layer.7.attention.output.LayerNorm.weight
2022-08-29 17:08:06,822 n: module.bert.encoder.layer.7.attention.output.LayerNorm.bias
2022-08-29 17:08:06,822 n: module.bert.encoder.layer.7.intermediate.dense.weight
2022-08-29 17:08:06,822 n: module.bert.encoder.layer.7.intermediate.dense.bias
2022-08-29 17:08:06,823 n: module.bert.encoder.layer.7.output.dense.weight
2022-08-29 17:08:06,823 n: module.bert.encoder.layer.7.output.dense.bias
2022-08-29 17:08:06,823 n: module.bert.encoder.layer.7.output.LayerNorm.weight
2022-08-29 17:08:06,823 n: module.bert.encoder.layer.7.output.LayerNorm.bias
2022-08-29 17:08:06,823 n: module.bert.encoder.layer.8.attention.self.query.weight
2022-08-29 17:08:06,823 n: module.bert.encoder.layer.8.attention.self.query.bias
2022-08-29 17:08:06,823 n: module.bert.encoder.layer.8.attention.self.key.weight
2022-08-29 17:08:06,823 n: module.bert.encoder.layer.8.attention.self.key.bias
2022-08-29 17:08:06,823 n: module.bert.encoder.layer.8.attention.self.value.weight
2022-08-29 17:08:06,823 n: module.bert.encoder.layer.8.attention.self.value.bias
2022-08-29 17:08:06,823 n: module.bert.encoder.layer.8.attention.output.dense.weight
2022-08-29 17:08:06,823 n: module.bert.encoder.layer.8.attention.output.dense.bias
2022-08-29 17:08:06,823 n: module.bert.encoder.layer.8.attention.output.LayerNorm.weight
2022-08-29 17:08:06,823 n: module.bert.encoder.layer.8.attention.output.LayerNorm.bias
2022-08-29 17:08:06,823 n: module.bert.encoder.layer.8.intermediate.dense.weight
2022-08-29 17:08:06,823 n: module.bert.encoder.layer.8.intermediate.dense.bias
2022-08-29 17:08:06,823 n: module.bert.encoder.layer.8.output.dense.weight
2022-08-29 17:08:06,823 n: module.bert.encoder.layer.8.output.dense.bias
2022-08-29 17:08:06,824 n: module.bert.encoder.layer.8.output.LayerNorm.weight
2022-08-29 17:08:06,824 n: module.bert.encoder.layer.8.output.LayerNorm.bias
2022-08-29 17:08:06,824 n: module.bert.encoder.layer.9.attention.self.query.weight
2022-08-29 17:08:06,824 n: module.bert.encoder.layer.9.attention.self.query.bias
2022-08-29 17:08:06,824 n: module.bert.encoder.layer.9.attention.self.key.weight
2022-08-29 17:08:06,824 n: module.bert.encoder.layer.9.attention.self.key.bias
2022-08-29 17:08:06,824 n: module.bert.encoder.layer.9.attention.self.value.weight
2022-08-29 17:08:06,824 n: module.bert.encoder.layer.9.attention.self.value.bias
2022-08-29 17:08:06,824 n: module.bert.encoder.layer.9.attention.output.dense.weight
2022-08-29 17:08:06,824 n: module.bert.encoder.layer.9.attention.output.dense.bias
2022-08-29 17:08:06,824 n: module.bert.encoder.layer.9.attention.output.LayerNorm.weight
2022-08-29 17:08:06,824 n: module.bert.encoder.layer.9.attention.output.LayerNorm.bias
2022-08-29 17:08:06,824 n: module.bert.encoder.layer.9.intermediate.dense.weight
2022-08-29 17:08:06,824 n: module.bert.encoder.layer.9.intermediate.dense.bias
2022-08-29 17:08:06,824 n: module.bert.encoder.layer.9.output.dense.weight
2022-08-29 17:08:06,824 n: module.bert.encoder.layer.9.output.dense.bias
2022-08-29 17:08:06,824 n: module.bert.encoder.layer.9.output.LayerNorm.weight
2022-08-29 17:08:06,824 n: module.bert.encoder.layer.9.output.LayerNorm.bias
2022-08-29 17:08:06,824 n: module.bert.encoder.layer.10.attention.self.query.weight
2022-08-29 17:08:06,825 n: module.bert.encoder.layer.10.attention.self.query.bias
2022-08-29 17:08:06,825 n: module.bert.encoder.layer.10.attention.self.key.weight
2022-08-29 17:08:06,825 n: module.bert.encoder.layer.10.attention.self.key.bias
2022-08-29 17:08:06,825 n: module.bert.encoder.layer.10.attention.self.value.weight
2022-08-29 17:08:06,825 n: module.bert.encoder.layer.10.attention.self.value.bias
2022-08-29 17:08:06,825 n: module.bert.encoder.layer.10.attention.output.dense.weight
2022-08-29 17:08:06,825 n: module.bert.encoder.layer.10.attention.output.dense.bias
2022-08-29 17:08:06,825 n: module.bert.encoder.layer.10.attention.output.LayerNorm.weight
2022-08-29 17:08:06,825 n: module.bert.encoder.layer.10.attention.output.LayerNorm.bias
2022-08-29 17:08:06,825 n: module.bert.encoder.layer.10.intermediate.dense.weight
2022-08-29 17:08:06,825 n: module.bert.encoder.layer.10.intermediate.dense.bias
2022-08-29 17:08:06,825 n: module.bert.encoder.layer.10.output.dense.weight
2022-08-29 17:08:06,825 n: module.bert.encoder.layer.10.output.dense.bias
2022-08-29 17:08:06,825 n: module.bert.encoder.layer.10.output.LayerNorm.weight
2022-08-29 17:08:06,825 n: module.bert.encoder.layer.10.output.LayerNorm.bias
2022-08-29 17:08:06,825 n: module.bert.encoder.layer.11.attention.self.query.weight
2022-08-29 17:08:06,825 n: module.bert.encoder.layer.11.attention.self.query.bias
2022-08-29 17:08:06,825 n: module.bert.encoder.layer.11.attention.self.key.weight
2022-08-29 17:08:06,825 n: module.bert.encoder.layer.11.attention.self.key.bias
2022-08-29 17:08:06,826 n: module.bert.encoder.layer.11.attention.self.value.weight
2022-08-29 17:08:06,826 n: module.bert.encoder.layer.11.attention.self.value.bias
2022-08-29 17:08:06,826 n: module.bert.encoder.layer.11.attention.output.dense.weight
2022-08-29 17:08:06,826 n: module.bert.encoder.layer.11.attention.output.dense.bias
2022-08-29 17:08:06,826 n: module.bert.encoder.layer.11.attention.output.LayerNorm.weight
2022-08-29 17:08:06,826 n: module.bert.encoder.layer.11.attention.output.LayerNorm.bias
2022-08-29 17:08:06,826 n: module.bert.encoder.layer.11.intermediate.dense.weight
2022-08-29 17:08:06,826 n: module.bert.encoder.layer.11.intermediate.dense.bias
2022-08-29 17:08:06,826 n: module.bert.encoder.layer.11.output.dense.weight
2022-08-29 17:08:06,826 n: module.bert.encoder.layer.11.output.dense.bias
2022-08-29 17:08:06,826 n: module.bert.encoder.layer.11.output.LayerNorm.weight
2022-08-29 17:08:06,826 n: module.bert.encoder.layer.11.output.LayerNorm.bias
2022-08-29 17:08:06,826 n: module.bert.pooler.dense.weight
2022-08-29 17:08:06,826 n: module.bert.pooler.dense.bias
2022-08-29 17:08:06,826 n: module.classifier.weight
2022-08-29 17:08:06,826 n: module.classifier.bias
2022-08-29 17:08:06,826 Total parameters: 109483778
2022-08-29 17:08:57,744 ***** Running evaluation *****
2022-08-29 17:08:57,745   Epoch = 2 iter 6199 step
2022-08-29 17:08:57,745   Num examples = 872
2022-08-29 17:08:57,745   Batch size = 32
2022-08-29 17:08:57,748 ***** Eval results *****
2022-08-29 17:08:57,748   att_loss = 1.1599479626016602
2022-08-29 17:08:57,748   cls_loss = 0.0
2022-08-29 17:08:57,748   global_step = 6199
2022-08-29 17:08:57,748   loss = 2.8362988864040326
2022-08-29 17:08:57,749   rep_loss = 1.6763509262871468
2022-08-29 17:08:57,749 ***** Save model *****
2022-08-29 17:08:57,757 ***** Running evaluation *****
2022-08-29 17:08:57,757   Epoch = 0 iter 199 step
2022-08-29 17:08:57,757   Num examples = 408
2022-08-29 17:08:57,757   Batch size = 32
2022-08-29 17:08:57,758 ***** Eval results *****
2022-08-29 17:08:57,759   att_loss = 25.018154695405432
2022-08-29 17:08:57,759   cls_loss = 0.0
2022-08-29 17:08:57,759   global_step = 199
2022-08-29 17:08:57,759   loss = 30.667434682798145
2022-08-29 17:08:57,759   rep_loss = 5.6492800472968785
2022-08-29 17:08:57,759 ***** Save model *****
2022-08-29 17:09:08,889 ***** Running evaluation *****
2022-08-29 17:09:08,890   Epoch = 0 iter 6199 step
2022-08-29 17:09:08,890   Num examples = 9815
2022-08-29 17:09:08,890   Batch size = 32
2022-08-29 17:09:08,891 ***** Eval results *****
2022-08-29 17:09:08,891   att_loss = 6.38805850860976
2022-08-29 17:09:08,891   cls_loss = 0.0
2022-08-29 17:09:08,891   global_step = 6199
2022-08-29 17:09:08,891   loss = 8.991667448964728
2022-08-29 17:09:08,891   rep_loss = 2.6036089442587413
2022-08-29 17:09:08,891 ***** Save model *****
2022-08-29 17:09:47,029 ***** Running evaluation *****
2022-08-29 17:09:47,029   Epoch = 0 iter 399 step
2022-08-29 17:09:47,029   Num examples = 408
2022-08-29 17:09:47,029   Batch size = 32
2022-08-29 17:09:47,031 ***** Eval results *****
2022-08-29 17:09:47,031   att_loss = 19.83309096620794
2022-08-29 17:09:47,031   cls_loss = 0.0
2022-08-29 17:09:47,031   global_step = 399
2022-08-29 17:09:47,031   loss = 24.797544223622868
2022-08-29 17:09:47,031   rep_loss = 4.964453310596017
2022-08-29 17:09:47,031 ***** Save model *****
2022-08-29 17:10:16,659 ***** Running evaluation *****
2022-08-29 17:10:16,660   Epoch = 3 iter 6399 step
2022-08-29 17:10:16,660   Num examples = 872
2022-08-29 17:10:16,660   Batch size = 32
2022-08-29 17:10:16,661 ***** Eval results *****
2022-08-29 17:10:16,661   att_loss = 1.090553455654232
2022-08-29 17:10:16,661   cls_loss = 0.0
2022-08-29 17:10:16,661   global_step = 6399
2022-08-29 17:10:16,661   loss = 2.72053250225111
2022-08-29 17:10:16,661   rep_loss = 1.629979051392654
2022-08-29 17:10:16,661 ***** Save model *****
2022-08-29 17:10:20,211 ***** Running evaluation *****
2022-08-29 17:10:20,212   Epoch = 0 iter 6399 step
2022-08-29 17:10:20,212   Num examples = 9815
2022-08-29 17:10:20,212   Batch size = 32
2022-08-29 17:10:20,213 ***** Eval results *****
2022-08-29 17:10:20,213   att_loss = 6.34173503643387
2022-08-29 17:10:20,213   cls_loss = 0.0
2022-08-29 17:10:20,213   global_step = 6399
2022-08-29 17:10:20,213   loss = 8.930051948003683
2022-08-29 17:10:20,213   rep_loss = 2.588316915239798
2022-08-29 17:10:20,214 ***** Save model *****
2022-08-29 17:10:43,382 ***** Running evaluation *****
2022-08-29 17:10:43,382   Epoch = 1 iter 599 step
2022-08-29 17:10:43,382   Num examples = 408
2022-08-29 17:10:43,382   Batch size = 32
2022-08-29 17:10:43,383 ***** Eval results *****
2022-08-29 17:10:43,383   att_loss = 12.255622302386778
2022-08-29 17:10:43,383   cls_loss = 0.0
2022-08-29 17:10:43,383   global_step = 599
2022-08-29 17:10:43,383   loss = 16.181649958833734
2022-08-29 17:10:43,384   rep_loss = 3.926027678428812
2022-08-29 17:10:43,384 ***** Save model *****
2022-08-29 17:11:32,039 ***** Running evaluation *****
2022-08-29 17:11:32,040   Epoch = 0 iter 6599 step
2022-08-29 17:11:32,040   Num examples = 9815
2022-08-29 17:11:32,040   Batch size = 32
2022-08-29 17:11:32,041 ***** Eval results *****
2022-08-29 17:11:32,041   att_loss = 6.296869410549371
2022-08-29 17:11:32,041   cls_loss = 0.0
2022-08-29 17:11:32,041   global_step = 6599
2022-08-29 17:11:32,041   loss = 8.870719073848086
2022-08-29 17:11:32,041   rep_loss = 2.5738496665684343
2022-08-29 17:11:32,041 ***** Save model *****
2022-08-29 17:11:35,373 ***** Running evaluation *****
2022-08-29 17:11:35,373   Epoch = 3 iter 6599 step
2022-08-29 17:11:35,373   Num examples = 872
2022-08-29 17:11:35,373   Batch size = 32
2022-08-29 17:11:35,374 ***** Eval results *****
2022-08-29 17:11:35,374   att_loss = 1.1140122642085113
2022-08-29 17:11:35,374   cls_loss = 0.0
2022-08-29 17:11:35,375   global_step = 6599
2022-08-29 17:11:35,375   loss = 2.7435376162313
2022-08-29 17:11:35,375   rep_loss = 1.6295253516074257
2022-08-29 17:11:35,375 ***** Save model *****
2022-08-29 17:11:36,438 ***** Running evaluation *****
2022-08-29 17:11:36,439   Epoch = 1 iter 799 step
2022-08-29 17:11:36,439   Num examples = 408
2022-08-29 17:11:36,439   Batch size = 32
2022-08-29 17:11:36,440 ***** Eval results *****
2022-08-29 17:11:36,440   att_loss = 11.93005525471528
2022-08-29 17:11:36,440   cls_loss = 0.0
2022-08-29 17:11:36,440   global_step = 799
2022-08-29 17:11:36,440   loss = 15.756709028898445
2022-08-29 17:11:36,441   rep_loss = 3.826653786768312
2022-08-29 17:11:36,441 ***** Save model *****
2022-08-29 17:12:31,789 ***** Running evaluation *****
2022-08-29 17:12:31,789   Epoch = 2 iter 999 step
2022-08-29 17:12:31,789   Num examples = 408
2022-08-29 17:12:31,789   Batch size = 32
2022-08-29 17:12:31,791 ***** Eval results *****
2022-08-29 17:12:31,791   att_loss = 10.699675163590765
2022-08-29 17:12:31,791   cls_loss = 0.0
2022-08-29 17:12:31,791   global_step = 999
2022-08-29 17:12:31,791   loss = 14.226281924420093
2022-08-29 17:12:31,791   rep_loss = 3.526606711996607
2022-08-29 17:12:31,791 ***** Save model *****
2022-08-29 17:12:41,303 ***** Running evaluation *****
2022-08-29 17:12:41,304   Epoch = 0 iter 6799 step
2022-08-29 17:12:41,304   Num examples = 9815
2022-08-29 17:12:41,304   Batch size = 32
2022-08-29 17:12:41,305 ***** Eval results *****
2022-08-29 17:12:41,305   att_loss = 6.254956189946824
2022-08-29 17:12:41,305   cls_loss = 0.0
2022-08-29 17:12:41,305   global_step = 6799
2022-08-29 17:12:41,306   loss = 8.814849435871078
2022-08-29 17:12:41,306   rep_loss = 2.5598932496062585
2022-08-29 17:12:41,306 ***** Save model *****
2022-08-29 17:12:55,300 ***** Running evaluation *****
2022-08-29 17:12:55,300   Epoch = 3 iter 6799 step
2022-08-29 17:12:55,300   Num examples = 872
2022-08-29 17:12:55,300   Batch size = 32
2022-08-29 17:12:55,302 ***** Eval results *****
2022-08-29 17:12:55,302   att_loss = 1.1204988312672295
2022-08-29 17:12:55,302   cls_loss = 0.0
2022-08-29 17:12:55,302   global_step = 6799
2022-08-29 17:12:55,302   loss = 2.7513911474411983
2022-08-29 17:12:55,302   rep_loss = 1.6308923196009297
2022-08-29 17:12:55,302 ***** Save model *****
2022-08-29 17:13:25,213 ***** Running evaluation *****
2022-08-29 17:13:25,214   Epoch = 2 iter 1199 step
2022-08-29 17:13:25,214   Num examples = 408
2022-08-29 17:13:25,214   Batch size = 32
2022-08-29 17:13:25,215 ***** Eval results *****
2022-08-29 17:13:25,215   att_loss = 10.442348764978954
2022-08-29 17:13:25,215   cls_loss = 0.0
2022-08-29 17:13:25,215   global_step = 1199
2022-08-29 17:13:25,215   loss = 13.896329913459482
2022-08-29 17:13:25,215   rep_loss = 3.4539811585901483
2022-08-29 17:13:25,215 ***** Save model *****
2022-08-29 17:13:52,709 ***** Running evaluation *****
2022-08-29 17:13:52,709   Epoch = 0 iter 6999 step
2022-08-29 17:13:52,709   Num examples = 9815
2022-08-29 17:13:52,709   Batch size = 32
2022-08-29 17:13:52,711 ***** Eval results *****
2022-08-29 17:13:52,711   att_loss = 6.213172836531263
2022-08-29 17:13:52,711   cls_loss = 0.0
2022-08-29 17:13:52,711   global_step = 6999
2022-08-29 17:13:52,711   loss = 8.75965612182312
2022-08-29 17:13:52,711   rep_loss = 2.546483288783485
2022-08-29 17:13:52,711 ***** Save model *****
2022-08-29 17:14:14,483 ***** Running evaluation *****
2022-08-29 17:14:14,484   Epoch = 3 iter 6999 step
2022-08-29 17:14:14,484   Num examples = 872
2022-08-29 17:14:14,484   Batch size = 32
2022-08-29 17:14:14,485 ***** Eval results *****
2022-08-29 17:14:14,485   att_loss = 1.1178083729535733
2022-08-29 17:14:14,485   cls_loss = 0.0
2022-08-29 17:14:14,485   global_step = 6999
2022-08-29 17:14:14,485   loss = 2.7461588313312335
2022-08-29 17:14:14,486   rep_loss = 1.6283504614142872
2022-08-29 17:14:14,486 ***** Save model *****
2022-08-29 17:14:23,270 ***** Running evaluation *****
2022-08-29 17:14:23,270   Epoch = 3 iter 1399 step
2022-08-29 17:14:23,270   Num examples = 408
2022-08-29 17:14:23,270   Batch size = 32
2022-08-29 17:14:23,271 ***** Eval results *****
2022-08-29 17:14:23,272   att_loss = 9.285211009979248
2022-08-29 17:14:23,272   cls_loss = 0.0
2022-08-29 17:14:23,272   global_step = 1399
2022-08-29 17:14:23,272   loss = 12.5141215133667
2022-08-29 17:14:23,272   rep_loss = 3.2289105319976805
2022-08-29 17:14:23,272 ***** Save model *****
2022-08-29 17:15:01,959 ***** Running evaluation *****
2022-08-29 17:15:01,959   Epoch = 0 iter 7199 step
2022-08-29 17:15:01,959   Num examples = 9815
2022-08-29 17:15:01,960   Batch size = 32
2022-08-29 17:15:01,961 ***** Eval results *****
2022-08-29 17:15:01,961   att_loss = 6.174648934361802
2022-08-29 17:15:01,961   cls_loss = 0.0
2022-08-29 17:15:01,961   global_step = 7199
2022-08-29 17:15:01,961   loss = 8.708338857839664
2022-08-29 17:15:01,961   rep_loss = 2.533689926574422
2022-08-29 17:15:01,961 ***** Save model *****
2022-08-29 17:15:16,862 ***** Running evaluation *****
2022-08-29 17:15:16,862   Epoch = 3 iter 1599 step
2022-08-29 17:15:16,862   Num examples = 408
2022-08-29 17:15:16,862   Batch size = 32
2022-08-29 17:15:16,863 ***** Eval results *****
2022-08-29 17:15:16,863   att_loss = 9.356720015207927
2022-08-29 17:15:16,863   cls_loss = 0.0
2022-08-29 17:15:16,863   global_step = 1599
2022-08-29 17:15:16,863   loss = 12.536250987582736
2022-08-29 17:15:16,863   rep_loss = 3.1795309755537247
2022-08-29 17:15:16,864 ***** Save model *****
2022-08-29 17:15:33,736 ***** Running evaluation *****
2022-08-29 17:15:33,737   Epoch = 3 iter 7199 step
2022-08-29 17:15:33,737   Num examples = 872
2022-08-29 17:15:33,737   Batch size = 32
2022-08-29 17:15:33,738 ***** Eval results *****
2022-08-29 17:15:33,738   att_loss = 1.1101132336302566
2022-08-29 17:15:33,738   cls_loss = 0.0
2022-08-29 17:15:33,738   global_step = 7199
2022-08-29 17:15:33,738   loss = 2.734427818451014
2022-08-29 17:15:33,738   rep_loss = 1.6243145852911431
2022-08-29 17:15:33,739 ***** Save model *****
2022-08-29 17:16:09,950 ***** Running evaluation *****
2022-08-29 17:16:09,950   Epoch = 3 iter 1799 step
2022-08-29 17:16:09,950   Num examples = 408
2022-08-29 17:16:09,950   Batch size = 32
2022-08-29 17:16:09,951 ***** Eval results *****
2022-08-29 17:16:09,951   att_loss = 9.208279414457433
2022-08-29 17:16:09,951   cls_loss = 0.0
2022-08-29 17:16:09,951   global_step = 1799
2022-08-29 17:16:09,951   loss = 12.33798027487362
2022-08-29 17:16:09,951   rep_loss = 3.129700863782097
2022-08-29 17:16:09,952 ***** Save model *****
2022-08-29 17:16:13,988 ***** Running evaluation *****
2022-08-29 17:16:13,989   Epoch = 0 iter 7399 step
2022-08-29 17:16:13,989   Num examples = 9815
2022-08-29 17:16:13,989   Batch size = 32
2022-08-29 17:16:13,990 ***** Eval results *****
2022-08-29 17:16:13,990   att_loss = 6.136369070856615
2022-08-29 17:16:13,990   cls_loss = 0.0
2022-08-29 17:16:13,990   global_step = 7399
2022-08-29 17:16:13,990   loss = 8.657727555820049
2022-08-29 17:16:13,990   rep_loss = 2.521358488072961
2022-08-29 17:16:13,990 ***** Save model *****
2022-08-29 17:16:51,879 ***** Running evaluation *****
2022-08-29 17:16:51,879   Epoch = 3 iter 7399 step
2022-08-29 17:16:51,880   Num examples = 872
2022-08-29 17:16:51,880   Batch size = 32
2022-08-29 17:16:51,881 ***** Eval results *****
2022-08-29 17:16:51,881   att_loss = 1.1102506469561622
2022-08-29 17:16:51,881   cls_loss = 0.0
2022-08-29 17:16:51,881   global_step = 7399
2022-08-29 17:16:51,881   loss = 2.7331057680563413
2022-08-29 17:16:51,881   rep_loss = 1.6228551219775245
2022-08-29 17:16:51,881 ***** Save model *****
2022-08-29 17:17:03,445 ***** Running evaluation *****
2022-08-29 17:17:03,445   Epoch = 4 iter 1999 step
2022-08-29 17:17:03,445   Num examples = 408
2022-08-29 17:17:03,445   Batch size = 32
2022-08-29 17:17:03,447 ***** Eval results *****
2022-08-29 17:17:03,447   att_loss = 8.49919428511294
2022-08-29 17:17:03,447   cls_loss = 0.0
2022-08-29 17:17:03,447   global_step = 1999
2022-08-29 17:17:03,447   loss = 11.454418016764931
2022-08-29 17:17:03,447   rep_loss = 2.9552237302243354
2022-08-29 17:17:03,447 ***** Save model *****
2022-08-29 17:17:24,890 ***** Running evaluation *****
2022-08-29 17:17:24,891   Epoch = 0 iter 7599 step
2022-08-29 17:17:24,891   Num examples = 9815
2022-08-29 17:17:24,891   Batch size = 32
2022-08-29 17:17:24,892 ***** Eval results *****
2022-08-29 17:17:24,892   att_loss = 6.1034773076925015
2022-08-29 17:17:24,892   cls_loss = 0.0
2022-08-29 17:17:24,892   global_step = 7599
2022-08-29 17:17:24,892   loss = 8.613298522373425
2022-08-29 17:17:24,892   rep_loss = 2.509821217975298
2022-08-29 17:17:24,892 ***** Save model *****
2022-08-29 17:17:58,346 ***** Running evaluation *****
2022-08-29 17:17:58,346   Epoch = 4 iter 2199 step
2022-08-29 17:17:58,346   Num examples = 408
2022-08-29 17:17:58,347   Batch size = 32
2022-08-29 17:17:58,348 ***** Eval results *****
2022-08-29 17:17:58,348   att_loss = 8.398923725783012
2022-08-29 17:17:58,348   cls_loss = 0.0
2022-08-29 17:17:58,348   global_step = 2199
2022-08-29 17:17:58,348   loss = 11.312330053677675
2022-08-29 17:17:58,348   rep_loss = 2.91340632009896
2022-08-29 17:17:58,348 ***** Save model *****
2022-08-29 17:18:12,404 ***** Running evaluation *****
2022-08-29 17:18:12,405   Epoch = 3 iter 7599 step
2022-08-29 17:18:12,405   Num examples = 872
2022-08-29 17:18:12,405   Batch size = 32
2022-08-29 17:18:12,406 ***** Eval results *****
2022-08-29 17:18:12,406   att_loss = 1.1096832044567355
2022-08-29 17:18:12,406   cls_loss = 0.0
2022-08-29 17:18:12,406   global_step = 7599
2022-08-29 17:18:12,406   loss = 2.7305466365517805
2022-08-29 17:18:12,407   rep_loss = 1.620863434132811
2022-08-29 17:18:12,407 ***** Save model *****
2022-08-29 17:18:35,879 ***** Running evaluation *****
2022-08-29 17:18:35,880   Epoch = 0 iter 7799 step
2022-08-29 17:18:35,880   Num examples = 9815
2022-08-29 17:18:35,880   Batch size = 32
2022-08-29 17:18:35,881 ***** Eval results *****
2022-08-29 17:18:35,881   att_loss = 6.066581534397787
2022-08-29 17:18:35,881   cls_loss = 0.0
2022-08-29 17:18:35,881   global_step = 7799
2022-08-29 17:18:35,881   loss = 8.564898747024605
2022-08-29 17:18:35,881   rep_loss = 2.4983172154087243
2022-08-29 17:18:35,881 ***** Save model *****
2022-08-29 17:18:51,176 ***** Running evaluation *****
2022-08-29 17:18:51,176   Epoch = 5 iter 2399 step
2022-08-29 17:18:51,176   Num examples = 408
2022-08-29 17:18:51,176   Batch size = 32
2022-08-29 17:18:51,177 ***** Eval results *****
2022-08-29 17:18:51,177   att_loss = 7.997113459700838
2022-08-29 17:18:51,177   cls_loss = 0.0
2022-08-29 17:18:51,177   global_step = 2399
2022-08-29 17:18:51,177   loss = 10.782099085116606
2022-08-29 17:18:51,177   rep_loss = 2.784985651663684
2022-08-29 17:18:51,177 ***** Save model *****
2022-08-29 17:19:31,080 ***** Running evaluation *****
2022-08-29 17:19:31,081   Epoch = 3 iter 7799 step
2022-08-29 17:19:31,081   Num examples = 872
2022-08-29 17:19:31,081   Batch size = 32
2022-08-29 17:19:31,082 ***** Eval results *****
2022-08-29 17:19:31,082   att_loss = 1.1108441964924216
2022-08-29 17:19:31,082   cls_loss = 0.0
2022-08-29 17:19:31,082   global_step = 7799
2022-08-29 17:19:31,082   loss = 2.7299779481188757
2022-08-29 17:19:31,082   rep_loss = 1.6191337512656996
2022-08-29 17:19:31,082 ***** Save model *****
2022-08-29 17:19:44,228 ***** Running evaluation *****
2022-08-29 17:19:44,229   Epoch = 5 iter 2599 step
2022-08-29 17:19:44,229   Num examples = 408
2022-08-29 17:19:44,229   Batch size = 32
2022-08-29 17:19:44,230 ***** Eval results *****
2022-08-29 17:19:44,230   att_loss = 7.90020885436666
2022-08-29 17:19:44,230   cls_loss = 0.0
2022-08-29 17:19:44,230   global_step = 2599
2022-08-29 17:19:44,230   loss = 10.659094782708918
2022-08-29 17:19:44,230   rep_loss = 2.7588859291138386
2022-08-29 17:19:44,230 ***** Save model *****
2022-08-29 17:19:47,838 ***** Running evaluation *****
2022-08-29 17:19:47,839   Epoch = 0 iter 7999 step
2022-08-29 17:19:47,839   Num examples = 9815
2022-08-29 17:19:47,839   Batch size = 32
2022-08-29 17:19:47,840 ***** Eval results *****
2022-08-29 17:19:47,840   att_loss = 6.032824475521712
2022-08-29 17:19:47,840   cls_loss = 0.0
2022-08-29 17:19:47,840   global_step = 7999
2022-08-29 17:19:47,840   loss = 8.520149843590545
2022-08-29 17:19:47,840   rep_loss = 2.4873253708855034
2022-08-29 17:19:47,840 ***** Save model *****
2022-08-29 17:20:37,170 ***** Running evaluation *****
2022-08-29 17:20:37,170   Epoch = 6 iter 2799 step
2022-08-29 17:20:37,170   Num examples = 408
2022-08-29 17:20:37,171   Batch size = 32
2022-08-29 17:20:37,172 ***** Eval results *****
2022-08-29 17:20:37,172   att_loss = 7.39133191576191
2022-08-29 17:20:37,172   cls_loss = 0.0
2022-08-29 17:20:37,172   global_step = 2799
2022-08-29 17:20:37,172   loss = 10.045266029881496
2022-08-29 17:20:37,172   rep_loss = 2.6539341328190824
2022-08-29 17:20:37,172 ***** Save model *****
2022-08-29 17:20:49,263 ***** Running evaluation *****
2022-08-29 17:20:49,263   Epoch = 3 iter 7999 step
2022-08-29 17:20:49,263   Num examples = 872
2022-08-29 17:20:49,263   Batch size = 32
2022-08-29 17:20:49,264 ***** Eval results *****
2022-08-29 17:20:49,264   att_loss = 1.1082672876723787
2022-08-29 17:20:49,264   cls_loss = 0.0
2022-08-29 17:20:49,264   global_step = 7999
2022-08-29 17:20:49,265   loss = 2.725974055812001
2022-08-29 17:20:49,265   rep_loss = 1.6177067681749544
2022-08-29 17:20:49,265 ***** Save model *****
2022-08-29 17:20:56,887 ***** Running evaluation *****
2022-08-29 17:20:56,888   Epoch = 0 iter 8199 step
2022-08-29 17:20:56,888   Num examples = 9815
2022-08-29 17:20:56,888   Batch size = 32
2022-08-29 17:20:56,889 ***** Eval results *****
2022-08-29 17:20:56,889   att_loss = 6.002285058417834
2022-08-29 17:20:56,889   cls_loss = 0.0
2022-08-29 17:20:56,889   global_step = 8199
2022-08-29 17:20:56,889   loss = 8.479121497817586
2022-08-29 17:20:56,889   rep_loss = 2.476836442642058
2022-08-29 17:20:56,889 ***** Save model *****
2022-08-29 17:21:30,286 ***** Running evaluation *****
2022-08-29 17:21:30,286   Epoch = 6 iter 2999 step
2022-08-29 17:21:30,286   Num examples = 408
2022-08-29 17:21:30,286   Batch size = 32
2022-08-29 17:21:30,287 ***** Eval results *****
2022-08-29 17:21:30,287   att_loss = 7.586154327924508
2022-08-29 17:21:30,288   cls_loss = 0.0
2022-08-29 17:21:30,288   global_step = 2999
2022-08-29 17:21:30,288   loss = 10.228985144322612
2022-08-29 17:21:30,288   rep_loss = 2.6428308420447237
2022-08-29 17:21:30,288 ***** Save model *****
2022-08-29 17:22:08,150 ***** Running evaluation *****
2022-08-29 17:22:08,151   Epoch = 0 iter 8399 step
2022-08-29 17:22:08,151   Num examples = 9815
2022-08-29 17:22:08,151   Batch size = 32
2022-08-29 17:22:08,152 ***** Eval results *****
2022-08-29 17:22:08,152   att_loss = 5.969910690423661
2022-08-29 17:22:08,152   cls_loss = 0.0
2022-08-29 17:22:08,152   global_step = 8399
2022-08-29 17:22:08,152   loss = 8.436483473788558
2022-08-29 17:22:08,153   rep_loss = 2.4665727864590297
2022-08-29 17:22:08,153 ***** Save model *****
2022-08-29 17:22:09,296 ***** Running evaluation *****
2022-08-29 17:22:09,297   Epoch = 3 iter 8199 step
2022-08-29 17:22:09,297   Num examples = 872
2022-08-29 17:22:09,297   Batch size = 32
2022-08-29 17:22:09,298 ***** Eval results *****
2022-08-29 17:22:09,298   att_loss = 1.1052666613837052
2022-08-29 17:22:09,298   cls_loss = 0.0
2022-08-29 17:22:09,298   global_step = 8199
2022-08-29 17:22:09,298   loss = 2.720849121444492
2022-08-29 17:22:09,298   rep_loss = 1.615582459365873
2022-08-29 17:22:09,299 ***** Save model *****
2022-08-29 17:22:26,829 ***** Running evaluation *****
2022-08-29 17:22:26,829   Epoch = 6 iter 3199 step
2022-08-29 17:22:26,829   Num examples = 408
2022-08-29 17:22:26,829   Batch size = 32
2022-08-29 17:22:26,830 ***** Eval results *****
2022-08-29 17:22:26,830   att_loss = 7.485183100478348
2022-08-29 17:22:26,830   cls_loss = 0.0
2022-08-29 17:22:26,830   global_step = 3199
2022-08-29 17:22:26,830   loss = 10.103158883139194
2022-08-29 17:22:26,831   rep_loss = 2.617975802220685
2022-08-29 17:22:26,831 ***** Save model *****
2022-08-29 17:23:19,606 ***** Running evaluation *****
2022-08-29 17:23:19,606   Epoch = 7 iter 3399 step
2022-08-29 17:23:19,606   Num examples = 408
2022-08-29 17:23:19,606   Batch size = 32
2022-08-29 17:23:19,607 ***** Eval results *****
2022-08-29 17:23:19,607   att_loss = 7.231126498681894
2022-08-29 17:23:19,607   cls_loss = 0.0
2022-08-29 17:23:19,607   global_step = 3399
2022-08-29 17:23:19,607   loss = 9.776291093678054
2022-08-29 17:23:19,607   rep_loss = 2.545164665409938
2022-08-29 17:23:19,607 ***** Save model *****
2022-08-29 17:23:20,234 ***** Running evaluation *****
2022-08-29 17:23:20,235   Epoch = 0 iter 8599 step
2022-08-29 17:23:20,235   Num examples = 9815
2022-08-29 17:23:20,235   Batch size = 32
2022-08-29 17:23:20,236 ***** Eval results *****
2022-08-29 17:23:20,236   att_loss = 5.939896832373076
2022-08-29 17:23:20,236   cls_loss = 0.0
2022-08-29 17:23:20,236   global_step = 8599
2022-08-29 17:23:20,236   loss = 8.396659686768079
2022-08-29 17:23:20,236   rep_loss = 2.4567628570290023
2022-08-29 17:23:20,236 ***** Save model *****
2022-08-29 17:23:30,726 ***** Running evaluation *****
2022-08-29 17:23:30,726   Epoch = 3 iter 8399 step
2022-08-29 17:23:30,726   Num examples = 872
2022-08-29 17:23:30,726   Batch size = 32
2022-08-29 17:23:30,727 ***** Eval results *****
2022-08-29 17:23:30,728   att_loss = 1.1048399925403294
2022-08-29 17:23:30,728   cls_loss = 0.0
2022-08-29 17:23:30,728   global_step = 8399
2022-08-29 17:23:30,728   loss = 2.7185376291361845
2022-08-29 17:23:30,728   rep_loss = 1.6136976357676163
2022-08-29 17:23:30,728 ***** Save model *****
2022-08-29 17:24:13,288 ***** Running evaluation *****
2022-08-29 17:24:13,288   Epoch = 7 iter 3599 step
2022-08-29 17:24:13,288   Num examples = 408
2022-08-29 17:24:13,288   Batch size = 32
2022-08-29 17:24:13,289 ***** Eval results *****
2022-08-29 17:24:13,289   att_loss = 7.195141285127052
2022-08-29 17:24:13,290   cls_loss = 0.0
2022-08-29 17:24:13,290   global_step = 3599
2022-08-29 17:24:13,290   loss = 9.724540960394396
2022-08-29 17:24:13,290   rep_loss = 2.529399717127094
2022-08-29 17:24:13,290 ***** Save model *****
2022-08-29 17:24:32,746 ***** Running evaluation *****
2022-08-29 17:24:32,746   Epoch = 0 iter 8799 step
2022-08-29 17:24:32,746   Num examples = 9815
2022-08-29 17:24:32,746   Batch size = 32
2022-08-29 17:24:32,748 ***** Eval results *****
2022-08-29 17:24:32,748   att_loss = 5.912116348316675
2022-08-29 17:24:32,748   cls_loss = 0.0
2022-08-29 17:24:32,748   global_step = 8799
2022-08-29 17:24:32,748   loss = 8.359439590272883
2022-08-29 17:24:32,748   rep_loss = 2.4473232446251734
2022-08-29 17:24:32,748 ***** Save model *****
2022-08-29 17:24:55,703 ***** Running evaluation *****
2022-08-29 17:24:55,703   Epoch = 4 iter 8599 step
2022-08-29 17:24:55,703   Num examples = 872
2022-08-29 17:24:55,703   Batch size = 32
2022-08-29 17:24:55,704 ***** Eval results *****
2022-08-29 17:24:55,704   att_loss = 1.0646224275964205
2022-08-29 17:24:55,704   cls_loss = 0.0
2022-08-29 17:24:55,704   global_step = 8599
2022-08-29 17:24:55,704   loss = 2.651254433751758
2022-08-29 17:24:55,705   rep_loss = 1.5866320107152554
2022-08-29 17:24:55,705 ***** Save model *****
2022-08-29 17:25:05,865 ***** Running evaluation *****
2022-08-29 17:25:05,865   Epoch = 8 iter 3799 step
2022-08-29 17:25:05,865   Num examples = 408
2022-08-29 17:25:05,865   Batch size = 32
2022-08-29 17:25:05,866 ***** Eval results *****
2022-08-29 17:25:05,866   att_loss = 6.946975209977892
2022-08-29 17:25:05,866   cls_loss = 0.0
2022-08-29 17:25:05,867   global_step = 3799
2022-08-29 17:25:05,867   loss = 9.407752287829364
2022-08-29 17:25:05,867   rep_loss = 2.4607770725532814
2022-08-29 17:25:05,867 ***** Save model *****
2022-08-29 17:25:42,805 ***** Running evaluation *****
2022-08-29 17:25:42,806   Epoch = 0 iter 8999 step
2022-08-29 17:25:42,806   Num examples = 9815
2022-08-29 17:25:42,806   Batch size = 32
2022-08-29 17:25:42,807 ***** Eval results *****
2022-08-29 17:25:42,807   att_loss = 5.885276344779174
2022-08-29 17:25:42,807   cls_loss = 0.0
2022-08-29 17:25:42,807   global_step = 8999
2022-08-29 17:25:42,808   loss = 8.323494961426276
2022-08-29 17:25:42,808   rep_loss = 2.438218618859343
2022-08-29 17:25:42,808 ***** Save model *****
2022-08-29 17:25:57,476 ***** Running evaluation *****
2022-08-29 17:25:57,476   Epoch = 8 iter 3999 step
2022-08-29 17:25:57,476   Num examples = 408
2022-08-29 17:25:57,476   Batch size = 32
2022-08-29 17:25:57,477 ***** Eval results *****
2022-08-29 17:25:57,477   att_loss = 6.981765173442328
2022-08-29 17:25:57,477   cls_loss = 0.0
2022-08-29 17:25:57,477   global_step = 3999
2022-08-29 17:25:57,477   loss = 9.439539539280222
2022-08-29 17:25:57,477   rep_loss = 2.457774350892252
2022-08-29 17:25:57,478 ***** Save model *****
2022-08-29 17:26:15,663 ***** Running evaluation *****
2022-08-29 17:26:15,663   Epoch = 4 iter 8799 step
2022-08-29 17:26:15,663   Num examples = 872
2022-08-29 17:26:15,663   Batch size = 32
2022-08-29 17:26:15,664 ***** Eval results *****
2022-08-29 17:26:15,664   att_loss = 1.0871112545850072
2022-08-29 17:26:15,664   cls_loss = 0.0
2022-08-29 17:26:15,664   global_step = 8799
2022-08-29 17:26:15,664   loss = 2.674499293842764
2022-08-29 17:26:15,665   rep_loss = 1.5873880383240024
2022-08-29 17:26:15,665 ***** Save model *****
2022-08-29 17:26:50,243 ***** Running evaluation *****
2022-08-29 17:26:50,243   Epoch = 9 iter 4199 step
2022-08-29 17:26:50,243   Num examples = 408
2022-08-29 17:26:50,244   Batch size = 32
2022-08-29 17:26:50,245 ***** Eval results *****
2022-08-29 17:26:50,245   att_loss = 6.821321889951632
2022-08-29 17:26:50,245   cls_loss = 0.0
2022-08-29 17:26:50,245   global_step = 4199
2022-08-29 17:26:50,245   loss = 9.229954564726198
2022-08-29 17:26:50,245   rep_loss = 2.4086326345220788
2022-08-29 17:26:50,245 ***** Save model *****
2022-08-29 17:26:54,571 ***** Running evaluation *****
2022-08-29 17:26:54,572   Epoch = 0 iter 9199 step
2022-08-29 17:26:54,572   Num examples = 9815
2022-08-29 17:26:54,572   Batch size = 32
2022-08-29 17:26:54,573 ***** Eval results *****
2022-08-29 17:26:54,573   att_loss = 5.858888153200993
2022-08-29 17:26:54,573   cls_loss = 0.0
2022-08-29 17:26:54,573   global_step = 9199
2022-08-29 17:26:54,573   loss = 8.288262692569766
2022-08-29 17:26:54,573   rep_loss = 2.4293745415717938
2022-08-29 17:26:54,573 ***** Save model *****
2022-08-29 17:27:35,266 ***** Running evaluation *****
2022-08-29 17:27:35,267   Epoch = 4 iter 8999 step
2022-08-29 17:27:35,267   Num examples = 872
2022-08-29 17:27:35,267   Batch size = 32
2022-08-29 17:27:35,268 ***** Eval results *****
2022-08-29 17:27:35,268   att_loss = 1.0781220871424797
2022-08-29 17:27:35,268   cls_loss = 0.0
2022-08-29 17:27:35,268   global_step = 8999
2022-08-29 17:27:35,268   loss = 2.6640510174667855
2022-08-29 17:27:35,268   rep_loss = 1.5859289271549335
2022-08-29 17:27:35,268 ***** Save model *****
2022-08-29 17:27:48,409 ***** Running evaluation *****
2022-08-29 17:27:48,409   Epoch = 9 iter 4399 step
2022-08-29 17:27:48,409   Num examples = 408
2022-08-29 17:27:48,409   Batch size = 32
2022-08-29 17:27:48,410 ***** Eval results *****
2022-08-29 17:27:48,411   att_loss = 6.8169697596278
2022-08-29 17:27:48,411   cls_loss = 0.0
2022-08-29 17:27:48,411   global_step = 4399
2022-08-29 17:27:48,411   loss = 9.221075175040896
2022-08-29 17:27:48,411   rep_loss = 2.4041054179952464
2022-08-29 17:27:48,411 ***** Save model *****
2022-08-29 17:28:09,246 ***** Running evaluation *****
2022-08-29 17:28:09,246   Epoch = 0 iter 9399 step
2022-08-29 17:28:09,246   Num examples = 9815
2022-08-29 17:28:09,246   Batch size = 32
2022-08-29 17:28:09,248 ***** Eval results *****
2022-08-29 17:28:09,248   att_loss = 5.832593033067444
2022-08-29 17:28:09,248   cls_loss = 0.0
2022-08-29 17:28:09,248   global_step = 9399
2022-08-29 17:28:09,248   loss = 8.253379817566525
2022-08-29 17:28:09,248   rep_loss = 2.420786786287411
2022-08-29 17:28:09,248 ***** Save model *****
2022-08-29 17:28:40,168 ***** Running evaluation *****
2022-08-29 17:28:40,169   Epoch = 10 iter 4599 step
2022-08-29 17:28:40,169   Num examples = 408
2022-08-29 17:28:40,169   Batch size = 32
2022-08-29 17:28:40,170 ***** Eval results *****
2022-08-29 17:28:40,170   att_loss = 6.702932081724468
2022-08-29 17:28:40,170   cls_loss = 0.0
2022-08-29 17:28:40,170   global_step = 4599
2022-08-29 17:28:40,170   loss = 9.076079820331774
2022-08-29 17:28:40,170   rep_loss = 2.3731477009622672
2022-08-29 17:28:40,170 ***** Save model *****
2022-08-29 17:28:54,636 ***** Running evaluation *****
2022-08-29 17:28:54,636   Epoch = 4 iter 9199 step
2022-08-29 17:28:54,636   Num examples = 872
2022-08-29 17:28:54,636   Batch size = 32
2022-08-29 17:28:54,637 ***** Eval results *****
2022-08-29 17:28:54,637   att_loss = 1.076165551807384
2022-08-29 17:28:54,637   cls_loss = 0.0
2022-08-29 17:28:54,637   global_step = 9199
2022-08-29 17:28:54,637   loss = 2.659971816938651
2022-08-29 17:28:54,637   rep_loss = 1.5838062649028968
2022-08-29 17:28:54,638 ***** Save model *****
2022-08-29 17:29:20,430 ***** Running evaluation *****
2022-08-29 17:29:20,431   Epoch = 0 iter 9599 step
2022-08-29 17:29:20,431   Num examples = 9815
2022-08-29 17:29:20,431   Batch size = 32
2022-08-29 17:29:20,432 ***** Eval results *****
2022-08-29 17:29:20,432   att_loss = 5.807804490392538
2022-08-29 17:29:20,432   cls_loss = 0.0
2022-08-29 17:29:20,432   global_step = 9599
2022-08-29 17:29:20,432   loss = 8.220302497388074
2022-08-29 17:29:20,432   rep_loss = 2.4124980088211196
2022-08-29 17:29:20,432 ***** Save model *****
2022-08-29 17:29:33,250 ***** Running evaluation *****
2022-08-29 17:29:33,251   Epoch = 10 iter 4799 step
2022-08-29 17:29:33,251   Num examples = 408
2022-08-29 17:29:33,251   Batch size = 32
2022-08-29 17:29:33,252 ***** Eval results *****
2022-08-29 17:29:33,252   att_loss = 6.722407465111719
2022-08-29 17:29:33,252   cls_loss = 0.0
2022-08-29 17:29:33,252   global_step = 4799
2022-08-29 17:29:33,252   loss = 9.076044043449507
2022-08-29 17:29:33,252   rep_loss = 2.353636565273755
2022-08-29 17:29:33,252 ***** Save model *****
2022-08-29 17:30:12,768 ***** Running evaluation *****
2022-08-29 17:30:12,769   Epoch = 4 iter 9399 step
2022-08-29 17:30:12,769   Num examples = 872
2022-08-29 17:30:12,769   Batch size = 32
2022-08-29 17:30:12,770 ***** Eval results *****
2022-08-29 17:30:12,770   att_loss = 1.0730490993143704
2022-08-29 17:30:12,770   cls_loss = 0.0
2022-08-29 17:30:12,770   global_step = 9399
2022-08-29 17:30:12,770   loss = 2.6553579950866233
2022-08-29 17:30:12,770   rep_loss = 1.5823088955903466
2022-08-29 17:30:12,770 ***** Save model *****
2022-08-29 17:30:26,449 ***** Running evaluation *****
2022-08-29 17:30:26,449   Epoch = 10 iter 4999 step
2022-08-29 17:30:26,449   Num examples = 408
2022-08-29 17:30:26,449   Batch size = 32
2022-08-29 17:30:26,450 ***** Eval results *****
2022-08-29 17:30:26,450   att_loss = 6.6584959087053175
2022-08-29 17:30:26,450   cls_loss = 0.0
2022-08-29 17:30:26,450   global_step = 4999
2022-08-29 17:30:26,450   loss = 9.00315315342177
2022-08-29 17:30:26,450   rep_loss = 2.3446572350431456
2022-08-29 17:30:26,451 ***** Save model *****
2022-08-29 17:30:31,634 ***** Running evaluation *****
2022-08-29 17:30:31,635   Epoch = 0 iter 9799 step
2022-08-29 17:30:31,635   Num examples = 9815
2022-08-29 17:30:31,635   Batch size = 32
2022-08-29 17:30:31,636 ***** Eval results *****
2022-08-29 17:30:31,636   att_loss = 5.784963864308861
2022-08-29 17:30:31,636   cls_loss = 0.0
2022-08-29 17:30:31,636   global_step = 9799
2022-08-29 17:30:31,636   loss = 8.189455625876832
2022-08-29 17:30:31,636   rep_loss = 2.4044917631616447
2022-08-29 17:30:31,636 ***** Save model *****
2022-08-29 17:31:20,324 ***** Running evaluation *****
2022-08-29 17:31:20,324   Epoch = 11 iter 5199 step
2022-08-29 17:31:20,325   Num examples = 408
2022-08-29 17:31:20,325   Batch size = 32
2022-08-29 17:31:20,326 ***** Eval results *****
2022-08-29 17:31:20,326   att_loss = 6.4775257229064565
2022-08-29 17:31:20,326   cls_loss = 0.0
2022-08-29 17:31:20,326   global_step = 5199
2022-08-29 17:31:20,326   loss = 8.786092713752888
2022-08-29 17:31:20,326   rep_loss = 2.308567007135901
2022-08-29 17:31:20,326 ***** Save model *****
2022-08-29 17:31:31,158 ***** Running evaluation *****
2022-08-29 17:31:31,158   Epoch = 4 iter 9599 step
2022-08-29 17:31:31,158   Num examples = 872
2022-08-29 17:31:31,158   Batch size = 32
2022-08-29 17:31:31,159 ***** Eval results *****
2022-08-29 17:31:31,159   att_loss = 1.0705205507935454
2022-08-29 17:31:31,159   cls_loss = 0.0
2022-08-29 17:31:31,160   global_step = 9599
2022-08-29 17:31:31,160   loss = 2.6506236228314073
2022-08-29 17:31:31,160   rep_loss = 1.5801030728943954
2022-08-29 17:31:31,160 ***** Save model *****
2022-08-29 17:31:40,765 ***** Running evaluation *****
2022-08-29 17:31:40,765   Epoch = 0 iter 9999 step
2022-08-29 17:31:40,765   Num examples = 9815
2022-08-29 17:31:40,765   Batch size = 32
2022-08-29 17:31:40,767 ***** Eval results *****
2022-08-29 17:31:40,767   att_loss = 5.764266776375704
2022-08-29 17:31:40,767   cls_loss = 0.0
2022-08-29 17:31:40,767   global_step = 9999
2022-08-29 17:31:40,767   loss = 8.161122735398616
2022-08-29 17:31:40,767   rep_loss = 2.3968559611450493
2022-08-29 17:31:40,767 ***** Save model *****
2022-08-29 17:32:13,957 ***** Running evaluation *****
2022-08-29 17:32:13,958   Epoch = 11 iter 5399 step
2022-08-29 17:32:13,958   Num examples = 408
2022-08-29 17:32:13,958   Batch size = 32
2022-08-29 17:32:13,959 ***** Eval results *****
2022-08-29 17:32:13,959   att_loss = 6.504673860079694
2022-08-29 17:32:13,959   cls_loss = 0.0
2022-08-29 17:32:13,959   global_step = 5399
2022-08-29 17:32:13,959   loss = 8.80757926375582
2022-08-29 17:32:13,959   rep_loss = 2.3029054267915003
2022-08-29 17:32:13,959 ***** Save model *****
2022-08-29 17:32:49,912 ***** Running evaluation *****
2022-08-29 17:32:49,912   Epoch = 0 iter 10199 step
2022-08-29 17:32:49,912   Num examples = 9815
2022-08-29 17:32:49,912   Batch size = 32
2022-08-29 17:32:49,914 ***** Eval results *****
2022-08-29 17:32:49,914   att_loss = 5.7412895980051
2022-08-29 17:32:49,914   cls_loss = 0.0
2022-08-29 17:32:49,914   global_step = 10199
2022-08-29 17:32:49,914   loss = 8.130527242841364
2022-08-29 17:32:49,914   rep_loss = 2.389237646975229
2022-08-29 17:32:49,914 ***** Save model *****
2022-08-29 17:32:50,259 ***** Running evaluation *****
2022-08-29 17:32:50,260   Epoch = 4 iter 9799 step
2022-08-29 17:32:50,260   Num examples = 872
2022-08-29 17:32:50,260   Batch size = 32
2022-08-29 17:32:50,261 ***** Eval results *****
2022-08-29 17:32:50,261   att_loss = 1.0741091636497735
2022-08-29 17:32:50,261   cls_loss = 0.0
2022-08-29 17:32:50,261   global_step = 9799
2022-08-29 17:32:50,261   loss = 2.6534572083796615
2022-08-29 17:32:50,261   rep_loss = 1.5793480451608686
2022-08-29 17:32:50,261 ***** Save model *****
2022-08-29 17:33:05,598 ***** Running evaluation *****
2022-08-29 17:33:05,598   Epoch = 12 iter 5599 step
2022-08-29 17:33:05,599   Num examples = 408
2022-08-29 17:33:05,599   Batch size = 32
2022-08-29 17:33:05,600 ***** Eval results *****
2022-08-29 17:33:05,600   att_loss = 6.439209225108322
2022-08-29 17:33:05,600   cls_loss = 0.0
2022-08-29 17:33:05,600   global_step = 5599
2022-08-29 17:33:05,600   loss = 8.715599393381655
2022-08-29 17:33:05,600   rep_loss = 2.276390196050255
2022-08-29 17:33:05,600 ***** Save model *****
2022-08-29 17:33:57,240 ***** Running evaluation *****
2022-08-29 17:33:57,240   Epoch = 12 iter 5799 step
2022-08-29 17:33:57,241   Num examples = 408
2022-08-29 17:33:57,241   Batch size = 32
2022-08-29 17:33:57,242 ***** Eval results *****
2022-08-29 17:33:57,242   att_loss = 6.396397984067206
2022-08-29 17:33:57,242   cls_loss = 0.0
2022-08-29 17:33:57,242   global_step = 5799
2022-08-29 17:33:57,242   loss = 8.662044270203845
2022-08-29 17:33:57,242   rep_loss = 2.2656462845629197
2022-08-29 17:33:57,242 ***** Save model *****
2022-08-29 17:34:02,802 ***** Running evaluation *****
2022-08-29 17:34:02,803   Epoch = 0 iter 10399 step
2022-08-29 17:34:02,803   Num examples = 9815
2022-08-29 17:34:02,803   Batch size = 32
2022-08-29 17:34:02,804 ***** Eval results *****
2022-08-29 17:34:02,804   att_loss = 5.7214438040814315
2022-08-29 17:34:02,804   cls_loss = 0.0
2022-08-29 17:34:02,805   global_step = 10399
2022-08-29 17:34:02,805   loss = 8.103418827698842
2022-08-29 17:34:02,805   rep_loss = 2.3819750260132895
2022-08-29 17:34:02,805 ***** Save model *****
2022-08-29 17:34:12,119 ***** Running evaluation *****
2022-08-29 17:34:12,120   Epoch = 4 iter 9999 step
2022-08-29 17:34:12,120   Num examples = 872
2022-08-29 17:34:12,120   Batch size = 32
2022-08-29 17:34:12,121 ***** Eval results *****
2022-08-29 17:34:12,121   att_loss = 1.0728670243816292
2022-08-29 17:34:12,121   cls_loss = 0.0
2022-08-29 17:34:12,121   global_step = 9999
2022-08-29 17:34:12,121   loss = 2.6506804618666764
2022-08-29 17:34:12,121   rep_loss = 1.5778134376733122
2022-08-29 17:34:12,121 ***** Save model *****
2022-08-29 17:34:48,912 ***** Running evaluation *****
2022-08-29 17:34:48,912   Epoch = 13 iter 5999 step
2022-08-29 17:34:48,912   Num examples = 408
2022-08-29 17:34:48,913   Batch size = 32
2022-08-29 17:34:48,914 ***** Eval results *****
2022-08-29 17:34:48,914   att_loss = 6.39400872124566
2022-08-29 17:34:48,914   cls_loss = 0.0
2022-08-29 17:34:48,914   global_step = 5999
2022-08-29 17:34:48,914   loss = 8.634350978003608
2022-08-29 17:34:48,914   rep_loss = 2.2403422408633764
2022-08-29 17:34:48,914 ***** Save model *****
2022-08-29 17:35:11,857 ***** Running evaluation *****
2022-08-29 17:35:11,858   Epoch = 0 iter 10599 step
2022-08-29 17:35:11,858   Num examples = 9815
2022-08-29 17:35:11,858   Batch size = 32
2022-08-29 17:35:11,859 ***** Eval results *****
2022-08-29 17:35:11,859   att_loss = 5.700219266852609
2022-08-29 17:35:11,859   cls_loss = 0.0
2022-08-29 17:35:11,859   global_step = 10599
2022-08-29 17:35:11,859   loss = 8.074996119367029
2022-08-29 17:35:11,859   rep_loss = 2.374776855011303
2022-08-29 17:35:11,859 ***** Save model *****
2022-08-29 17:35:31,106 ***** Running evaluation *****
2022-08-29 17:35:31,106   Epoch = 4 iter 10199 step
2022-08-29 17:35:31,106   Num examples = 872
2022-08-29 17:35:31,106   Batch size = 32
2022-08-29 17:35:31,107 ***** Eval results *****
2022-08-29 17:35:31,107   att_loss = 1.070792634267855
2022-08-29 17:35:31,107   cls_loss = 0.0
2022-08-29 17:35:31,107   global_step = 10199
2022-08-29 17:35:31,108   loss = 2.6466171700033123
2022-08-29 17:35:31,108   rep_loss = 1.5758245360363219
2022-08-29 17:35:31,108 ***** Save model *****
2022-08-29 17:35:40,807 ***** Running evaluation *****
2022-08-29 17:35:40,807   Epoch = 13 iter 6199 step
2022-08-29 17:35:40,807   Num examples = 408
2022-08-29 17:35:40,807   Batch size = 32
2022-08-29 17:35:40,809 ***** Eval results *****
2022-08-29 17:35:40,809   att_loss = 6.333039924076625
2022-08-29 17:35:40,809   cls_loss = 0.0
2022-08-29 17:35:40,809   global_step = 6199
2022-08-29 17:35:40,809   loss = 8.566875494742881
2022-08-29 17:35:40,809   rep_loss = 2.2338355706662547
2022-08-29 17:35:40,809 ***** Save model *****
2022-08-29 17:36:23,036 ***** Running evaluation *****
2022-08-29 17:36:23,037   Epoch = 0 iter 10799 step
2022-08-29 17:36:23,037   Num examples = 9815
2022-08-29 17:36:23,037   Batch size = 32
2022-08-29 17:36:23,038 ***** Eval results *****
2022-08-29 17:36:23,038   att_loss = 5.679991127661129
2022-08-29 17:36:23,038   cls_loss = 0.0
2022-08-29 17:36:23,038   global_step = 10799
2022-08-29 17:36:23,038   loss = 8.047764352962192
2022-08-29 17:36:23,039   rep_loss = 2.3677732281159862
2022-08-29 17:36:23,039 ***** Save model *****
2022-08-29 17:36:32,654 ***** Running evaluation *****
2022-08-29 17:36:32,654   Epoch = 13 iter 6399 step
2022-08-29 17:36:32,654   Num examples = 408
2022-08-29 17:36:32,654   Batch size = 32
2022-08-29 17:36:32,655 ***** Eval results *****
2022-08-29 17:36:32,655   att_loss = 6.289114981019095
2022-08-29 17:36:32,655   cls_loss = 0.0
2022-08-29 17:36:32,655   global_step = 6399
2022-08-29 17:36:32,655   loss = 8.519046294823122
2022-08-29 17:36:32,655   rep_loss = 2.229931323447924
2022-08-29 17:36:32,656 ***** Save model *****
2022-08-29 17:36:49,809 ***** Running evaluation *****
2022-08-29 17:36:49,810   Epoch = 4 iter 10399 step
2022-08-29 17:36:49,810   Num examples = 872
2022-08-29 17:36:49,810   Batch size = 32
2022-08-29 17:36:49,811 ***** Eval results *****
2022-08-29 17:36:49,811   att_loss = 1.0720304232682474
2022-08-29 17:36:49,811   cls_loss = 0.0
2022-08-29 17:36:49,811   global_step = 10399
2022-08-29 17:36:49,811   loss = 2.6466466525679455
2022-08-29 17:36:49,811   rep_loss = 1.5746162300812014
2022-08-29 17:36:49,811 ***** Save model *****
2022-08-29 17:37:25,910 ***** Running evaluation *****
2022-08-29 17:37:25,910   Epoch = 14 iter 6599 step
2022-08-29 17:37:25,910   Num examples = 408
2022-08-29 17:37:25,910   Batch size = 32
2022-08-29 17:37:25,912 ***** Eval results *****
2022-08-29 17:37:25,912   att_loss = 6.150957778175885
2022-08-29 17:37:25,912   cls_loss = 0.0
2022-08-29 17:37:25,912   global_step = 6599
2022-08-29 17:37:25,912   loss = 8.353942858344093
2022-08-29 17:37:25,912   rep_loss = 2.2029850916429
2022-08-29 17:37:25,912 ***** Save model *****
2022-08-29 17:37:34,133 ***** Running evaluation *****
2022-08-29 17:37:34,133   Epoch = 0 iter 10999 step
2022-08-29 17:37:34,133   Num examples = 9815
2022-08-29 17:37:34,133   Batch size = 32
2022-08-29 17:37:34,134 ***** Eval results *****
2022-08-29 17:37:34,134   att_loss = 5.660130233762048
2022-08-29 17:37:34,134   cls_loss = 0.0
2022-08-29 17:37:34,135   global_step = 10999
2022-08-29 17:37:34,135   loss = 8.021142671472193
2022-08-29 17:37:34,135   rep_loss = 2.3610124404738846
2022-08-29 17:37:34,135 ***** Save model *****
2022-08-29 17:38:08,133 ***** Running evaluation *****
2022-08-29 17:38:08,133   Epoch = 5 iter 10599 step
2022-08-29 17:38:08,133   Num examples = 872
2022-08-29 17:38:08,133   Batch size = 32
2022-08-29 17:38:08,134 ***** Eval results *****
2022-08-29 17:38:08,135   att_loss = 1.0550118156626254
2022-08-29 17:38:08,135   cls_loss = 0.0
2022-08-29 17:38:08,135   global_step = 10599
2022-08-29 17:38:08,135   loss = 2.6098849471611314
2022-08-29 17:38:08,135   rep_loss = 1.554873145079311
2022-08-29 17:38:08,135 ***** Save model *****
2022-08-29 17:38:19,120 ***** Running evaluation *****
2022-08-29 17:38:19,121   Epoch = 14 iter 6799 step
2022-08-29 17:38:19,121   Num examples = 408
2022-08-29 17:38:19,121   Batch size = 32
2022-08-29 17:38:19,122 ***** Eval results *****
2022-08-29 17:38:19,122   att_loss = 6.207617020422174
2022-08-29 17:38:19,122   cls_loss = 0.0
2022-08-29 17:38:19,122   global_step = 6799
2022-08-29 17:38:19,122   loss = 8.412253606535051
2022-08-29 17:38:19,122   rep_loss = 2.2046365953539078
2022-08-29 17:38:19,122 ***** Save model *****
2022-08-29 17:38:43,372 ***** Running evaluation *****
2022-08-29 17:38:43,372   Epoch = 0 iter 11199 step
2022-08-29 17:38:43,372   Num examples = 9815
2022-08-29 17:38:43,372   Batch size = 32
2022-08-29 17:38:43,373 ***** Eval results *****
2022-08-29 17:38:43,373   att_loss = 5.641285526740763
2022-08-29 17:38:43,373   cls_loss = 0.0
2022-08-29 17:38:43,374   global_step = 11199
2022-08-29 17:38:43,374   loss = 7.99571596634091
2022-08-29 17:38:43,374   rep_loss = 2.3544304421761493
2022-08-29 17:38:43,374 ***** Save model *****
2022-08-29 17:39:11,392 ***** Running evaluation *****
2022-08-29 17:39:11,392   Epoch = 15 iter 6999 step
2022-08-29 17:39:11,392   Num examples = 408
2022-08-29 17:39:11,392   Batch size = 32
2022-08-29 17:39:11,393 ***** Eval results *****
2022-08-29 17:39:11,394   att_loss = 6.172223475552345
2022-08-29 17:39:11,394   cls_loss = 0.0
2022-08-29 17:39:11,394   global_step = 6999
2022-08-29 17:39:11,394   loss = 8.357453287109848
2022-08-29 17:39:11,394   rep_loss = 2.185229781986207
2022-08-29 17:39:11,394 ***** Save model *****
2022-08-29 17:39:26,231 ***** Running evaluation *****
2022-08-29 17:39:26,232   Epoch = 5 iter 10799 step
2022-08-29 17:39:26,232   Num examples = 872
2022-08-29 17:39:26,232   Batch size = 32
2022-08-29 17:39:26,233 ***** Eval results *****
2022-08-29 17:39:26,233   att_loss = 1.0393531262660967
2022-08-29 17:39:26,233   cls_loss = 0.0
2022-08-29 17:39:26,233   global_step = 10799
2022-08-29 17:39:26,233   loss = 2.5891416722301086
2022-08-29 17:39:26,233   rep_loss = 1.549788545109465
2022-08-29 17:39:26,233 ***** Save model *****
2022-08-29 17:39:54,361 ***** Running evaluation *****
2022-08-29 17:39:54,362   Epoch = 0 iter 11399 step
2022-08-29 17:39:54,362   Num examples = 9815
2022-08-29 17:39:54,362   Batch size = 32
2022-08-29 17:39:54,363 ***** Eval results *****
2022-08-29 17:39:54,363   att_loss = 5.623286061291528
2022-08-29 17:39:54,363   cls_loss = 0.0
2022-08-29 17:39:54,363   global_step = 11399
2022-08-29 17:39:54,364   loss = 7.9713644433391755
2022-08-29 17:39:54,364   rep_loss = 2.348078384798068
2022-08-29 17:39:54,364 ***** Save model *****
2022-08-29 17:40:04,566 ***** Running evaluation *****
2022-08-29 17:40:04,566   Epoch = 15 iter 7199 step
2022-08-29 17:40:04,567   Num examples = 408
2022-08-29 17:40:04,567   Batch size = 32
2022-08-29 17:40:04,568 ***** Eval results *****
2022-08-29 17:40:04,568   att_loss = 6.153804477587297
2022-08-29 17:40:04,568   cls_loss = 0.0
2022-08-29 17:40:04,568   global_step = 7199
2022-08-29 17:40:04,568   loss = 8.335133806188056
2022-08-29 17:40:04,568   rep_loss = 2.1813293242526997
2022-08-29 17:40:04,568 ***** Save model *****
2022-08-29 17:40:44,278 ***** Running evaluation *****
2022-08-29 17:40:44,278   Epoch = 5 iter 10999 step
2022-08-29 17:40:44,278   Num examples = 872
2022-08-29 17:40:44,278   Batch size = 32
2022-08-29 17:40:44,279 ***** Eval results *****
2022-08-29 17:40:44,280   att_loss = 1.0469057221502251
2022-08-29 17:40:44,280   cls_loss = 0.0
2022-08-29 17:40:44,280   global_step = 10999
2022-08-29 17:40:44,280   loss = 2.5983695754925242
2022-08-29 17:40:44,280   rep_loss = 1.5514638553332685
2022-08-29 17:40:44,280 ***** Save model *****
2022-08-29 17:40:56,156 ***** Running evaluation *****
2022-08-29 17:40:56,156   Epoch = 16 iter 7399 step
2022-08-29 17:40:56,156   Num examples = 408
2022-08-29 17:40:56,156   Batch size = 32
2022-08-29 17:40:56,157 ***** Eval results *****
2022-08-29 17:40:56,157   att_loss = 5.879551551711391
2022-08-29 17:40:56,157   cls_loss = 0.0
2022-08-29 17:40:56,157   global_step = 7399
2022-08-29 17:40:56,157   loss = 8.02645947899617
2022-08-29 17:40:56,158   rep_loss = 2.146907942395815
2022-08-29 17:40:56,158 ***** Save model *****
2022-08-29 17:41:05,498 ***** Running evaluation *****
2022-08-29 17:41:05,498   Epoch = 0 iter 11599 step
2022-08-29 17:41:05,498   Num examples = 9815
2022-08-29 17:41:05,498   Batch size = 32
2022-08-29 17:41:05,499 ***** Eval results *****
2022-08-29 17:41:05,499   att_loss = 5.605466817204156
2022-08-29 17:41:05,499   cls_loss = 0.0
2022-08-29 17:41:05,499   global_step = 11599
2022-08-29 17:41:05,499   loss = 7.947316029758554
2022-08-29 17:41:05,499   rep_loss = 2.341849215432111
2022-08-29 17:41:05,500 ***** Save model *****
2022-08-29 17:41:48,444 ***** Running evaluation *****
2022-08-29 17:41:48,444   Epoch = 16 iter 7599 step
2022-08-29 17:41:48,444   Num examples = 408
2022-08-29 17:41:48,444   Batch size = 32
2022-08-29 17:41:48,445 ***** Eval results *****
2022-08-29 17:41:48,445   att_loss = 6.051024174778224
2022-08-29 17:41:48,445   cls_loss = 0.0
2022-08-29 17:41:48,446   global_step = 7599
2022-08-29 17:41:48,446   loss = 8.205384975869718
2022-08-29 17:41:48,446   rep_loss = 2.1543608076897933
2022-08-29 17:41:48,446 ***** Save model *****
2022-08-29 17:42:02,397 ***** Running evaluation *****
2022-08-29 17:42:02,398   Epoch = 5 iter 11199 step
2022-08-29 17:42:02,398   Num examples = 872
2022-08-29 17:42:02,398   Batch size = 32
2022-08-29 17:42:02,399 ***** Eval results *****
2022-08-29 17:42:02,400   att_loss = 1.0465618455357333
2022-08-29 17:42:02,400   cls_loss = 0.0
2022-08-29 17:42:02,400   global_step = 11199
2022-08-29 17:42:02,400   loss = 2.596130083165569
2022-08-29 17:42:02,400   rep_loss = 1.54956824070224
2022-08-29 17:42:02,400 ***** Save model *****
2022-08-29 17:42:16,021 ***** Running evaluation *****
2022-08-29 17:42:16,022   Epoch = 0 iter 11799 step
2022-08-29 17:42:16,022   Num examples = 9815
2022-08-29 17:42:16,022   Batch size = 32
2022-08-29 17:42:16,024 ***** Eval results *****
2022-08-29 17:42:16,024   att_loss = 5.587594288467845
2022-08-29 17:42:16,024   cls_loss = 0.0
2022-08-29 17:42:16,024   global_step = 11799
2022-08-29 17:42:16,024   loss = 7.923340052848691
2022-08-29 17:42:16,024   rep_loss = 2.3357457670683335
2022-08-29 17:42:16,024 ***** Save model *****
2022-08-29 17:42:44,555 ***** Running evaluation *****
2022-08-29 17:42:44,556   Epoch = 17 iter 7799 step
2022-08-29 17:42:44,556   Num examples = 408
2022-08-29 17:42:44,556   Batch size = 32
2022-08-29 17:42:44,557 ***** Eval results *****
2022-08-29 17:42:44,557   att_loss = 6.223331708174485
2022-08-29 17:42:44,557   cls_loss = 0.0
2022-08-29 17:42:44,557   global_step = 7799
2022-08-29 17:42:44,557   loss = 8.407481120182918
2022-08-29 17:42:44,557   rep_loss = 2.184149448688214
2022-08-29 17:42:44,557 ***** Save model *****
2022-08-29 17:43:20,956 ***** Running evaluation *****
2022-08-29 17:43:20,957   Epoch = 5 iter 11399 step
2022-08-29 17:43:20,957   Num examples = 872
2022-08-29 17:43:20,957   Batch size = 32
2022-08-29 17:43:20,958 ***** Eval results *****
2022-08-29 17:43:20,958   att_loss = 1.04257964158899
2022-08-29 17:43:20,958   cls_loss = 0.0
2022-08-29 17:43:20,958   global_step = 11399
2022-08-29 17:43:20,958   loss = 2.5907849183261598
2022-08-29 17:43:20,958   rep_loss = 1.548205277211837
2022-08-29 17:43:20,958 ***** Save model *****
2022-08-29 17:43:27,410 ***** Running evaluation *****
2022-08-29 17:43:27,410   Epoch = 0 iter 11999 step
2022-08-29 17:43:27,411   Num examples = 9815
2022-08-29 17:43:27,411   Batch size = 32
2022-08-29 17:43:27,412 ***** Eval results *****
2022-08-29 17:43:27,412   att_loss = 5.568908950886892
2022-08-29 17:43:27,412   cls_loss = 0.0
2022-08-29 17:43:27,412   global_step = 11999
2022-08-29 17:43:27,412   loss = 7.89864236281826
2022-08-29 17:43:27,412   rep_loss = 2.3297334145939304
2022-08-29 17:43:27,412 ***** Save model *****
2022-08-29 17:43:37,263 ***** Running evaluation *****
2022-08-29 17:43:37,263   Epoch = 17 iter 7999 step
2022-08-29 17:43:37,263   Num examples = 408
2022-08-29 17:43:37,263   Batch size = 32
2022-08-29 17:43:37,264 ***** Eval results *****
2022-08-29 17:43:37,264   att_loss = 5.9846641603210164
2022-08-29 17:43:37,264   cls_loss = 0.0
2022-08-29 17:43:37,264   global_step = 7999
2022-08-29 17:43:37,265   loss = 8.124377277535451
2022-08-29 17:43:37,265   rep_loss = 2.139713129527132
2022-08-29 17:43:37,265 ***** Save model *****
2022-08-29 17:44:29,959 ***** Running evaluation *****
2022-08-29 17:44:29,960   Epoch = 17 iter 8199 step
2022-08-29 17:44:29,960   Num examples = 408
2022-08-29 17:44:29,960   Batch size = 32
2022-08-29 17:44:29,961 ***** Eval results *****
2022-08-29 17:44:29,961   att_loss = 5.98249164100998
2022-08-29 17:44:29,961   cls_loss = 0.0
2022-08-29 17:44:29,961   global_step = 8199
2022-08-29 17:44:29,961   loss = 8.117125340293164
2022-08-29 17:44:29,961   rep_loss = 2.1346337122720898
2022-08-29 17:44:29,961 ***** Save model *****
2022-08-29 17:44:39,169 ***** Running evaluation *****
2022-08-29 17:44:39,169   Epoch = 0 iter 12199 step
2022-08-29 17:44:39,169   Num examples = 9815
2022-08-29 17:44:39,169   Batch size = 32
2022-08-29 17:44:39,170 ***** Eval results *****
2022-08-29 17:44:39,170   att_loss = 5.5519332682092575
2022-08-29 17:44:39,170   cls_loss = 0.0
2022-08-29 17:44:39,170   global_step = 12199
2022-08-29 17:44:39,171   loss = 7.875865801229508
2022-08-29 17:44:39,171   rep_loss = 2.3239325355902998
2022-08-29 17:44:39,171 ***** Save model *****
2022-08-29 17:44:39,219 ***** Running evaluation *****
2022-08-29 17:44:39,219   Epoch = 5 iter 11599 step
2022-08-29 17:44:39,219   Num examples = 872
2022-08-29 17:44:39,219   Batch size = 32
2022-08-29 17:44:39,220 ***** Eval results *****
2022-08-29 17:44:39,220   att_loss = 1.0417046862912465
2022-08-29 17:44:39,221   cls_loss = 0.0
2022-08-29 17:44:39,221   global_step = 11599
2022-08-29 17:44:39,221   loss = 2.5885040207633936
2022-08-29 17:44:39,221   rep_loss = 1.5467993325939655
2022-08-29 17:44:39,221 ***** Save model *****
2022-08-29 17:45:23,174 ***** Running evaluation *****
2022-08-29 17:45:23,175   Epoch = 18 iter 8399 step
2022-08-29 17:45:23,175   Num examples = 408
2022-08-29 17:45:23,175   Batch size = 32
2022-08-29 17:45:23,176 ***** Eval results *****
2022-08-29 17:45:23,176   att_loss = 6.007840737988872
2022-08-29 17:45:23,176   cls_loss = 0.0
2022-08-29 17:45:23,176   global_step = 8399
2022-08-29 17:45:23,176   loss = 8.128809587417111
2022-08-29 17:45:23,176   rep_loss = 2.1209688455827775
2022-08-29 17:45:23,177 ***** Save model *****
2022-08-29 17:45:51,664 ***** Running evaluation *****
2022-08-29 17:45:51,664   Epoch = 1 iter 12399 step
2022-08-29 17:45:51,664   Num examples = 9815
2022-08-29 17:45:51,664   Batch size = 32
2022-08-29 17:45:51,665 ***** Eval results *****
2022-08-29 17:45:51,665   att_loss = 4.560026925057173
2022-08-29 17:45:51,666   cls_loss = 0.0
2022-08-29 17:45:51,666   global_step = 12399
2022-08-29 17:45:51,666   loss = 6.5293033719062805
2022-08-29 17:45:51,666   rep_loss = 1.9692764393985271
2022-08-29 17:45:51,666 ***** Save model *****
2022-08-29 17:46:00,809 ***** Running evaluation *****
2022-08-29 17:46:00,810   Epoch = 5 iter 11799 step
2022-08-29 17:46:00,810   Num examples = 872
2022-08-29 17:46:00,810   Batch size = 32
2022-08-29 17:46:00,811 ***** Eval results *****
2022-08-29 17:46:00,811   att_loss = 1.0433236993878403
2022-08-29 17:46:00,811   cls_loss = 0.0
2022-08-29 17:46:00,811   global_step = 11799
2022-08-29 17:46:00,811   loss = 2.5894107859613746
2022-08-29 17:46:00,812   rep_loss = 1.5460870856880862
2022-08-29 17:46:00,812 ***** Save model *****
2022-08-29 17:46:20,250 ***** Running evaluation *****
2022-08-29 17:46:20,250   Epoch = 18 iter 8599 step
2022-08-29 17:46:20,250   Num examples = 408
2022-08-29 17:46:20,250   Batch size = 32
2022-08-29 17:46:20,252 ***** Eval results *****
2022-08-29 17:46:20,252   att_loss = 5.93573485092378
2022-08-29 17:46:20,252   cls_loss = 0.0
2022-08-29 17:46:20,252   global_step = 8599
2022-08-29 17:46:20,252   loss = 8.05119241526429
2022-08-29 17:46:20,252   rep_loss = 2.1154575539306855
2022-08-29 17:46:20,252 ***** Save model *****
2022-08-29 17:47:01,730 ***** Running evaluation *****
2022-08-29 17:47:01,730   Epoch = 1 iter 12599 step
2022-08-29 17:47:01,730   Num examples = 9815
2022-08-29 17:47:01,730   Batch size = 32
2022-08-29 17:47:01,732 ***** Eval results *****
2022-08-29 17:47:01,732   att_loss = 4.5064407107306685
2022-08-29 17:47:01,732   cls_loss = 0.0
2022-08-29 17:47:01,732   global_step = 12599
2022-08-29 17:47:01,732   loss = 6.4716035592846755
2022-08-29 17:47:01,732   rep_loss = 1.9651628507346641
2022-08-29 17:47:01,732 ***** Save model *****
2022-08-29 17:47:13,917 ***** Running evaluation *****
2022-08-29 17:47:13,917   Epoch = 19 iter 8799 step
2022-08-29 17:47:13,917   Num examples = 408
2022-08-29 17:47:13,918   Batch size = 32
2022-08-29 17:47:13,920 ***** Eval results *****
2022-08-29 17:47:13,920   att_loss = 5.894838691986713
2022-08-29 17:47:13,920   cls_loss = 0.0
2022-08-29 17:47:13,920   global_step = 8799
2022-08-29 17:47:13,920   loss = 8.006418154411708
2022-08-29 17:47:13,920   rep_loss = 2.1115794697987664
2022-08-29 17:47:13,921 ***** Save model *****
2022-08-29 17:47:19,985 ***** Running evaluation *****
2022-08-29 17:47:19,986   Epoch = 5 iter 11999 step
2022-08-29 17:47:19,986   Num examples = 872
2022-08-29 17:47:19,986   Batch size = 32
2022-08-29 17:47:19,987 ***** Eval results *****
2022-08-29 17:47:19,987   att_loss = 1.0429859724779884
2022-08-29 17:47:19,987   cls_loss = 0.0
2022-08-29 17:47:19,987   global_step = 11999
2022-08-29 17:47:19,987   loss = 2.588507562009284
2022-08-29 17:47:19,987   rep_loss = 1.5455215880804727
2022-08-29 17:47:19,987 ***** Save model *****
2022-08-29 17:48:05,443 ***** Running evaluation *****
2022-08-29 17:48:05,443   Epoch = 19 iter 8999 step
2022-08-29 17:48:05,443   Num examples = 408
2022-08-29 17:48:05,443   Batch size = 32
2022-08-29 17:48:05,444 ***** Eval results *****
2022-08-29 17:48:05,444   att_loss = 5.877749573100697
2022-08-29 17:48:05,445   cls_loss = 0.0
2022-08-29 17:48:05,445   global_step = 8999
2022-08-29 17:48:05,445   loss = 7.97808776238952
2022-08-29 17:48:05,445   rep_loss = 2.100338196915007
2022-08-29 17:48:05,445 ***** Save model *****
2022-08-29 17:48:14,411 ***** Running evaluation *****
2022-08-29 17:48:14,411   Epoch = 1 iter 12799 step
2022-08-29 17:48:14,411   Num examples = 9815
2022-08-29 17:48:14,412   Batch size = 32
2022-08-29 17:48:14,413 ***** Eval results *****
2022-08-29 17:48:14,413   att_loss = 4.481733376781146
2022-08-29 17:48:14,413   cls_loss = 0.0
2022-08-29 17:48:14,413   global_step = 12799
2022-08-29 17:48:14,413   loss = 6.443852953838579
2022-08-29 17:48:14,413   rep_loss = 1.9621195768316586
2022-08-29 17:48:14,413 ***** Save model *****
2022-08-29 17:48:38,306 ***** Running evaluation *****
2022-08-29 17:48:38,306   Epoch = 5 iter 12199 step
2022-08-29 17:48:38,307   Num examples = 872
2022-08-29 17:48:38,307   Batch size = 32
2022-08-29 17:48:38,308 ***** Eval results *****
2022-08-29 17:48:38,308   att_loss = 1.040815560678558
2022-08-29 17:48:38,308   cls_loss = 0.0
2022-08-29 17:48:38,308   global_step = 12199
2022-08-29 17:48:38,308   loss = 2.5852367346878915
2022-08-29 17:48:38,308   rep_loss = 1.5444211723408292
2022-08-29 17:48:38,308 ***** Save model *****
2022-08-29 17:48:50,545 The args: Namespace(aug_train=False, cache_dir='', data_dir='glue_data/MRPC', data_url='', do_eval=False, do_lower_case=True, eval_batch_size=32, eval_step=100, gradient_accumulation_steps=1, hidden_act='quad', learning_rate=1e-05, log_path='tmp/distill/MRPC/quad_2quad/bert-base-uncased/log.txt', max_seq_length=128, no_cuda=False, num_train_epochs=5.0, output_dir='tmp/distill/MRPC/quad_2quad/bert-base-uncased/5e-05_1e-05_8_stage2', pred_distill=True, seed=42, softmax_act='2quad', student_model='tmp/distill/MRPC/quad_2quad/bert-base-uncased/5e-05_1e-05_8', task_name='MRPC', teacher_model='/home/ubuntu/checkpoints/exp/MRPC', temperature=1.0, train_batch_size=8, warmup_proportion=0.1, weight_decay=0.0001)
2022-08-29 17:48:50,546 device: cuda n_gpu: 2
2022-08-29 17:48:50,631 Writing example 0 of 3668
2022-08-29 17:48:50,632 *** Example ***
2022-08-29 17:48:50,632 guid: train-1
2022-08-29 17:48:50,632 tokens: [CLS] am ##ro ##zi accused his brother , whom he called " the witness " , of deliberately di ##stor ##ting his evidence . [SEP] referring to him as only " the witness " , am ##ro ##zi accused his brother of deliberately di ##stor ##ting his evidence . [SEP]
2022-08-29 17:48:50,632 input_ids: 101 2572 3217 5831 5496 2010 2567 1010 3183 2002 2170 1000 1996 7409 1000 1010 1997 9969 4487 23809 3436 2010 3350 1012 102 7727 2000 2032 2004 2069 1000 1996 7409 1000 1010 2572 3217 5831 5496 2010 2567 1997 9969 4487 23809 3436 2010 3350 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-08-29 17:48:50,632 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-08-29 17:48:50,632 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-08-29 17:48:50,632 label: 1
2022-08-29 17:48:50,632 label_id: 1
2022-08-29 17:48:53,534 Writing example 0 of 408
2022-08-29 17:48:53,535 *** Example ***
2022-08-29 17:48:53,535 guid: dev-1
2022-08-29 17:48:53,535 tokens: [CLS] he said the foods ##er ##vic ##e pie business doesn ' t fit the company ' s long - term growth strategy . [SEP] " the foods ##er ##vic ##e pie business does not fit our long - term growth strategy . [SEP]
2022-08-29 17:48:53,535 input_ids: 101 2002 2056 1996 9440 2121 7903 2063 11345 2449 2987 1005 1056 4906 1996 2194 1005 1055 2146 1011 2744 3930 5656 1012 102 1000 1996 9440 2121 7903 2063 11345 2449 2515 2025 4906 2256 2146 1011 2744 3930 5656 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-08-29 17:48:53,535 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-08-29 17:48:53,535 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-08-29 17:48:53,535 label: 1
2022-08-29 17:48:53,535 label_id: 1
2022-08-29 17:48:53,862 Model config {
  "_name_or_path": "bert-base-uncased",
  "architectures": [
    "BertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "crypten": false,
  "finetuning_task": "mrpc",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "not_equivalent",
    "1": "equivalent"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "equivalent": 1,
    "not_equivalent": 0
  },
  "layer_norm_eps": 1e-12,
  "log_path": "tmp/distill/MRPC/quad_2quad/bert-base-uncased/log.txt",
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "pre_trained": "",
  "problem_type": "single_label_classification",
  "softmax_act": "softmax",
  "torch_dtype": "float32",
  "training": "",
  "transformers_version": "4.20.0.dev0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

2022-08-29 17:48:56,590 Loading model /home/ubuntu/checkpoints/exp/MRPC/pytorch_model.bin
2022-08-29 17:48:56,895 loading model...
2022-08-29 17:48:56,943 done!
2022-08-29 17:48:56,944 Weights from pretrained model not used in TinyBertForSequenceClassification: ['bert.embeddings.position_ids']
2022-08-29 17:49:01,981 ***** Teacher evaluation *****
2022-08-29 17:49:01,981 {'acc': 0.8602941176470589, 'f1': 0.9025641025641027, 'acc_and_f1': 0.8814291101055808, 'eval_loss': 0.3993110977686368}
2022-08-29 17:49:01,982 Model config {
  "_name_or_path": "bert-base-uncased",
  "architectures": [
    "BertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "crypten": false,
  "finetuning_task": "mrpc",
  "gradient_checkpointing": false,
  "hidden_act": "quad",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "not_equivalent",
    "1": "equivalent"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "equivalent": 1,
    "not_equivalent": 0
  },
  "layer_norm_eps": 1e-12,
  "log_path": "tmp/distill/MRPC/quad_2quad/bert-base-uncased/log.txt",
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "pre_trained": "",
  "problem_type": "single_label_classification",
  "softmax_act": "2quad",
  "torch_dtype": "float32",
  "training": "",
  "transformers_version": "4.20.0.dev0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

2022-08-29 17:49:04,891 Loading model tmp/distill/MRPC/quad_2quad/bert-base-uncased/5e-05_1e-05_8/pytorch_model.bin
2022-08-29 17:49:05,199 loading model...
2022-08-29 17:49:05,245 done!
2022-08-29 17:49:05,327 ***** Running training *****
2022-08-29 17:49:05,327   Num examples = 3668
2022-08-29 17:49:05,328   Batch size = 8
2022-08-29 17:49:05,328   Num steps = 2290
2022-08-29 17:49:05,329 n: module.bert.embeddings.word_embeddings.weight
2022-08-29 17:49:05,329 n: module.bert.embeddings.position_embeddings.weight
2022-08-29 17:49:05,329 n: module.bert.embeddings.token_type_embeddings.weight
2022-08-29 17:49:05,329 n: module.bert.embeddings.LayerNorm.weight
2022-08-29 17:49:05,329 n: module.bert.embeddings.LayerNorm.bias
2022-08-29 17:49:05,329 n: module.bert.encoder.layer.0.attention.self.query.weight
2022-08-29 17:49:05,330 n: module.bert.encoder.layer.0.attention.self.query.bias
2022-08-29 17:49:05,330 n: module.bert.encoder.layer.0.attention.self.key.weight
2022-08-29 17:49:05,330 n: module.bert.encoder.layer.0.attention.self.key.bias
2022-08-29 17:49:05,330 n: module.bert.encoder.layer.0.attention.self.value.weight
2022-08-29 17:49:05,330 n: module.bert.encoder.layer.0.attention.self.value.bias
2022-08-29 17:49:05,330 n: module.bert.encoder.layer.0.attention.output.dense.weight
2022-08-29 17:49:05,330 n: module.bert.encoder.layer.0.attention.output.dense.bias
2022-08-29 17:49:05,330 n: module.bert.encoder.layer.0.attention.output.LayerNorm.weight
2022-08-29 17:49:05,330 n: module.bert.encoder.layer.0.attention.output.LayerNorm.bias
2022-08-29 17:49:05,330 n: module.bert.encoder.layer.0.intermediate.dense.weight
2022-08-29 17:49:05,330 n: module.bert.encoder.layer.0.intermediate.dense.bias
2022-08-29 17:49:05,330 n: module.bert.encoder.layer.0.output.dense.weight
2022-08-29 17:49:05,330 n: module.bert.encoder.layer.0.output.dense.bias
2022-08-29 17:49:05,330 n: module.bert.encoder.layer.0.output.LayerNorm.weight
2022-08-29 17:49:05,330 n: module.bert.encoder.layer.0.output.LayerNorm.bias
2022-08-29 17:49:05,330 n: module.bert.encoder.layer.1.attention.self.query.weight
2022-08-29 17:49:05,330 n: module.bert.encoder.layer.1.attention.self.query.bias
2022-08-29 17:49:05,331 n: module.bert.encoder.layer.1.attention.self.key.weight
2022-08-29 17:49:05,331 n: module.bert.encoder.layer.1.attention.self.key.bias
2022-08-29 17:49:05,331 n: module.bert.encoder.layer.1.attention.self.value.weight
2022-08-29 17:49:05,331 n: module.bert.encoder.layer.1.attention.self.value.bias
2022-08-29 17:49:05,331 n: module.bert.encoder.layer.1.attention.output.dense.weight
2022-08-29 17:49:05,331 n: module.bert.encoder.layer.1.attention.output.dense.bias
2022-08-29 17:49:05,331 n: module.bert.encoder.layer.1.attention.output.LayerNorm.weight
2022-08-29 17:49:05,331 n: module.bert.encoder.layer.1.attention.output.LayerNorm.bias
2022-08-29 17:49:05,331 n: module.bert.encoder.layer.1.intermediate.dense.weight
2022-08-29 17:49:05,331 n: module.bert.encoder.layer.1.intermediate.dense.bias
2022-08-29 17:49:05,331 n: module.bert.encoder.layer.1.output.dense.weight
2022-08-29 17:49:05,331 n: module.bert.encoder.layer.1.output.dense.bias
2022-08-29 17:49:05,331 n: module.bert.encoder.layer.1.output.LayerNorm.weight
2022-08-29 17:49:05,331 n: module.bert.encoder.layer.1.output.LayerNorm.bias
2022-08-29 17:49:05,331 n: module.bert.encoder.layer.2.attention.self.query.weight
2022-08-29 17:49:05,331 n: module.bert.encoder.layer.2.attention.self.query.bias
2022-08-29 17:49:05,331 n: module.bert.encoder.layer.2.attention.self.key.weight
2022-08-29 17:49:05,331 n: module.bert.encoder.layer.2.attention.self.key.bias
2022-08-29 17:49:05,332 n: module.bert.encoder.layer.2.attention.self.value.weight
2022-08-29 17:49:05,332 n: module.bert.encoder.layer.2.attention.self.value.bias
2022-08-29 17:49:05,332 n: module.bert.encoder.layer.2.attention.output.dense.weight
2022-08-29 17:49:05,332 n: module.bert.encoder.layer.2.attention.output.dense.bias
2022-08-29 17:49:05,332 n: module.bert.encoder.layer.2.attention.output.LayerNorm.weight
2022-08-29 17:49:05,332 n: module.bert.encoder.layer.2.attention.output.LayerNorm.bias
2022-08-29 17:49:05,332 n: module.bert.encoder.layer.2.intermediate.dense.weight
2022-08-29 17:49:05,332 n: module.bert.encoder.layer.2.intermediate.dense.bias
2022-08-29 17:49:05,332 n: module.bert.encoder.layer.2.output.dense.weight
2022-08-29 17:49:05,332 n: module.bert.encoder.layer.2.output.dense.bias
2022-08-29 17:49:05,332 n: module.bert.encoder.layer.2.output.LayerNorm.weight
2022-08-29 17:49:05,332 n: module.bert.encoder.layer.2.output.LayerNorm.bias
2022-08-29 17:49:05,332 n: module.bert.encoder.layer.3.attention.self.query.weight
2022-08-29 17:49:05,332 n: module.bert.encoder.layer.3.attention.self.query.bias
2022-08-29 17:49:05,332 n: module.bert.encoder.layer.3.attention.self.key.weight
2022-08-29 17:49:05,332 n: module.bert.encoder.layer.3.attention.self.key.bias
2022-08-29 17:49:05,332 n: module.bert.encoder.layer.3.attention.self.value.weight
2022-08-29 17:49:05,333 n: module.bert.encoder.layer.3.attention.self.value.bias
2022-08-29 17:49:05,333 n: module.bert.encoder.layer.3.attention.output.dense.weight
2022-08-29 17:49:05,333 n: module.bert.encoder.layer.3.attention.output.dense.bias
2022-08-29 17:49:05,333 n: module.bert.encoder.layer.3.attention.output.LayerNorm.weight
2022-08-29 17:49:05,333 n: module.bert.encoder.layer.3.attention.output.LayerNorm.bias
2022-08-29 17:49:05,333 n: module.bert.encoder.layer.3.intermediate.dense.weight
2022-08-29 17:49:05,333 n: module.bert.encoder.layer.3.intermediate.dense.bias
2022-08-29 17:49:05,333 n: module.bert.encoder.layer.3.output.dense.weight
2022-08-29 17:49:05,333 n: module.bert.encoder.layer.3.output.dense.bias
2022-08-29 17:49:05,333 n: module.bert.encoder.layer.3.output.LayerNorm.weight
2022-08-29 17:49:05,333 n: module.bert.encoder.layer.3.output.LayerNorm.bias
2022-08-29 17:49:05,333 n: module.bert.encoder.layer.4.attention.self.query.weight
2022-08-29 17:49:05,333 n: module.bert.encoder.layer.4.attention.self.query.bias
2022-08-29 17:49:05,333 n: module.bert.encoder.layer.4.attention.self.key.weight
2022-08-29 17:49:05,333 n: module.bert.encoder.layer.4.attention.self.key.bias
2022-08-29 17:49:05,333 n: module.bert.encoder.layer.4.attention.self.value.weight
2022-08-29 17:49:05,333 n: module.bert.encoder.layer.4.attention.self.value.bias
2022-08-29 17:49:05,333 n: module.bert.encoder.layer.4.attention.output.dense.weight
2022-08-29 17:49:05,334 n: module.bert.encoder.layer.4.attention.output.dense.bias
2022-08-29 17:49:05,334 n: module.bert.encoder.layer.4.attention.output.LayerNorm.weight
2022-08-29 17:49:05,334 n: module.bert.encoder.layer.4.attention.output.LayerNorm.bias
2022-08-29 17:49:05,334 n: module.bert.encoder.layer.4.intermediate.dense.weight
2022-08-29 17:49:05,334 n: module.bert.encoder.layer.4.intermediate.dense.bias
2022-08-29 17:49:05,334 n: module.bert.encoder.layer.4.output.dense.weight
2022-08-29 17:49:05,334 n: module.bert.encoder.layer.4.output.dense.bias
2022-08-29 17:49:05,334 n: module.bert.encoder.layer.4.output.LayerNorm.weight
2022-08-29 17:49:05,334 n: module.bert.encoder.layer.4.output.LayerNorm.bias
2022-08-29 17:49:05,334 n: module.bert.encoder.layer.5.attention.self.query.weight
2022-08-29 17:49:05,334 n: module.bert.encoder.layer.5.attention.self.query.bias
2022-08-29 17:49:05,334 n: module.bert.encoder.layer.5.attention.self.key.weight
2022-08-29 17:49:05,334 n: module.bert.encoder.layer.5.attention.self.key.bias
2022-08-29 17:49:05,334 n: module.bert.encoder.layer.5.attention.self.value.weight
2022-08-29 17:49:05,334 n: module.bert.encoder.layer.5.attention.self.value.bias
2022-08-29 17:49:05,334 n: module.bert.encoder.layer.5.attention.output.dense.weight
2022-08-29 17:49:05,334 n: module.bert.encoder.layer.5.attention.output.dense.bias
2022-08-29 17:49:05,335 n: module.bert.encoder.layer.5.attention.output.LayerNorm.weight
2022-08-29 17:49:05,335 n: module.bert.encoder.layer.5.attention.output.LayerNorm.bias
2022-08-29 17:49:05,335 n: module.bert.encoder.layer.5.intermediate.dense.weight
2022-08-29 17:49:05,335 n: module.bert.encoder.layer.5.intermediate.dense.bias
2022-08-29 17:49:05,335 n: module.bert.encoder.layer.5.output.dense.weight
2022-08-29 17:49:05,335 n: module.bert.encoder.layer.5.output.dense.bias
2022-08-29 17:49:05,335 n: module.bert.encoder.layer.5.output.LayerNorm.weight
2022-08-29 17:49:05,335 n: module.bert.encoder.layer.5.output.LayerNorm.bias
2022-08-29 17:49:05,335 n: module.bert.encoder.layer.6.attention.self.query.weight
2022-08-29 17:49:05,335 n: module.bert.encoder.layer.6.attention.self.query.bias
2022-08-29 17:49:05,335 n: module.bert.encoder.layer.6.attention.self.key.weight
2022-08-29 17:49:05,335 n: module.bert.encoder.layer.6.attention.self.key.bias
2022-08-29 17:49:05,335 n: module.bert.encoder.layer.6.attention.self.value.weight
2022-08-29 17:49:05,335 n: module.bert.encoder.layer.6.attention.self.value.bias
2022-08-29 17:49:05,335 n: module.bert.encoder.layer.6.attention.output.dense.weight
2022-08-29 17:49:05,335 n: module.bert.encoder.layer.6.attention.output.dense.bias
2022-08-29 17:49:05,335 n: module.bert.encoder.layer.6.attention.output.LayerNorm.weight
2022-08-29 17:49:05,335 n: module.bert.encoder.layer.6.attention.output.LayerNorm.bias
2022-08-29 17:49:05,336 n: module.bert.encoder.layer.6.intermediate.dense.weight
2022-08-29 17:49:05,336 n: module.bert.encoder.layer.6.intermediate.dense.bias
2022-08-29 17:49:05,336 n: module.bert.encoder.layer.6.output.dense.weight
2022-08-29 17:49:05,336 n: module.bert.encoder.layer.6.output.dense.bias
2022-08-29 17:49:05,336 n: module.bert.encoder.layer.6.output.LayerNorm.weight
2022-08-29 17:49:05,336 n: module.bert.encoder.layer.6.output.LayerNorm.bias
2022-08-29 17:49:05,336 n: module.bert.encoder.layer.7.attention.self.query.weight
2022-08-29 17:49:05,336 n: module.bert.encoder.layer.7.attention.self.query.bias
2022-08-29 17:49:05,336 n: module.bert.encoder.layer.7.attention.self.key.weight
2022-08-29 17:49:05,336 n: module.bert.encoder.layer.7.attention.self.key.bias
2022-08-29 17:49:05,336 n: module.bert.encoder.layer.7.attention.self.value.weight
2022-08-29 17:49:05,336 n: module.bert.encoder.layer.7.attention.self.value.bias
2022-08-29 17:49:05,336 n: module.bert.encoder.layer.7.attention.output.dense.weight
2022-08-29 17:49:05,336 n: module.bert.encoder.layer.7.attention.output.dense.bias
2022-08-29 17:49:05,336 n: module.bert.encoder.layer.7.attention.output.LayerNorm.weight
2022-08-29 17:49:05,336 n: module.bert.encoder.layer.7.attention.output.LayerNorm.bias
2022-08-29 17:49:05,336 n: module.bert.encoder.layer.7.intermediate.dense.weight
2022-08-29 17:49:05,337 n: module.bert.encoder.layer.7.intermediate.dense.bias
2022-08-29 17:49:05,337 n: module.bert.encoder.layer.7.output.dense.weight
2022-08-29 17:49:05,337 n: module.bert.encoder.layer.7.output.dense.bias
2022-08-29 17:49:05,337 n: module.bert.encoder.layer.7.output.LayerNorm.weight
2022-08-29 17:49:05,337 n: module.bert.encoder.layer.7.output.LayerNorm.bias
2022-08-29 17:49:05,337 n: module.bert.encoder.layer.8.attention.self.query.weight
2022-08-29 17:49:05,337 n: module.bert.encoder.layer.8.attention.self.query.bias
2022-08-29 17:49:05,337 n: module.bert.encoder.layer.8.attention.self.key.weight
2022-08-29 17:49:05,337 n: module.bert.encoder.layer.8.attention.self.key.bias
2022-08-29 17:49:05,337 n: module.bert.encoder.layer.8.attention.self.value.weight
2022-08-29 17:49:05,337 n: module.bert.encoder.layer.8.attention.self.value.bias
2022-08-29 17:49:05,337 n: module.bert.encoder.layer.8.attention.output.dense.weight
2022-08-29 17:49:05,337 n: module.bert.encoder.layer.8.attention.output.dense.bias
2022-08-29 17:49:05,337 n: module.bert.encoder.layer.8.attention.output.LayerNorm.weight
2022-08-29 17:49:05,337 n: module.bert.encoder.layer.8.attention.output.LayerNorm.bias
2022-08-29 17:49:05,337 n: module.bert.encoder.layer.8.intermediate.dense.weight
2022-08-29 17:49:05,337 n: module.bert.encoder.layer.8.intermediate.dense.bias
2022-08-29 17:49:05,337 n: module.bert.encoder.layer.8.output.dense.weight
2022-08-29 17:49:05,338 n: module.bert.encoder.layer.8.output.dense.bias
2022-08-29 17:49:05,338 n: module.bert.encoder.layer.8.output.LayerNorm.weight
2022-08-29 17:49:05,338 n: module.bert.encoder.layer.8.output.LayerNorm.bias
2022-08-29 17:49:05,338 n: module.bert.encoder.layer.9.attention.self.query.weight
2022-08-29 17:49:05,338 n: module.bert.encoder.layer.9.attention.self.query.bias
2022-08-29 17:49:05,338 n: module.bert.encoder.layer.9.attention.self.key.weight
2022-08-29 17:49:05,338 n: module.bert.encoder.layer.9.attention.self.key.bias
2022-08-29 17:49:05,338 n: module.bert.encoder.layer.9.attention.self.value.weight
2022-08-29 17:49:05,338 n: module.bert.encoder.layer.9.attention.self.value.bias
2022-08-29 17:49:05,338 n: module.bert.encoder.layer.9.attention.output.dense.weight
2022-08-29 17:49:05,338 n: module.bert.encoder.layer.9.attention.output.dense.bias
2022-08-29 17:49:05,338 n: module.bert.encoder.layer.9.attention.output.LayerNorm.weight
2022-08-29 17:49:05,338 n: module.bert.encoder.layer.9.attention.output.LayerNorm.bias
2022-08-29 17:49:05,338 n: module.bert.encoder.layer.9.intermediate.dense.weight
2022-08-29 17:49:05,338 n: module.bert.encoder.layer.9.intermediate.dense.bias
2022-08-29 17:49:05,338 n: module.bert.encoder.layer.9.output.dense.weight
2022-08-29 17:49:05,338 n: module.bert.encoder.layer.9.output.dense.bias
2022-08-29 17:49:05,339 n: module.bert.encoder.layer.9.output.LayerNorm.weight
2022-08-29 17:49:05,339 n: module.bert.encoder.layer.9.output.LayerNorm.bias
2022-08-29 17:49:05,339 n: module.bert.encoder.layer.10.attention.self.query.weight
2022-08-29 17:49:05,339 n: module.bert.encoder.layer.10.attention.self.query.bias
2022-08-29 17:49:05,339 n: module.bert.encoder.layer.10.attention.self.key.weight
2022-08-29 17:49:05,339 n: module.bert.encoder.layer.10.attention.self.key.bias
2022-08-29 17:49:05,339 n: module.bert.encoder.layer.10.attention.self.value.weight
2022-08-29 17:49:05,339 n: module.bert.encoder.layer.10.attention.self.value.bias
2022-08-29 17:49:05,339 n: module.bert.encoder.layer.10.attention.output.dense.weight
2022-08-29 17:49:05,339 n: module.bert.encoder.layer.10.attention.output.dense.bias
2022-08-29 17:49:05,339 n: module.bert.encoder.layer.10.attention.output.LayerNorm.weight
2022-08-29 17:49:05,339 n: module.bert.encoder.layer.10.attention.output.LayerNorm.bias
2022-08-29 17:49:05,339 n: module.bert.encoder.layer.10.intermediate.dense.weight
2022-08-29 17:49:05,339 n: module.bert.encoder.layer.10.intermediate.dense.bias
2022-08-29 17:49:05,339 n: module.bert.encoder.layer.10.output.dense.weight
2022-08-29 17:49:05,339 n: module.bert.encoder.layer.10.output.dense.bias
2022-08-29 17:49:05,339 n: module.bert.encoder.layer.10.output.LayerNorm.weight
2022-08-29 17:49:05,339 n: module.bert.encoder.layer.10.output.LayerNorm.bias
2022-08-29 17:49:05,340 n: module.bert.encoder.layer.11.attention.self.query.weight
2022-08-29 17:49:05,340 n: module.bert.encoder.layer.11.attention.self.query.bias
2022-08-29 17:49:05,340 n: module.bert.encoder.layer.11.attention.self.key.weight
2022-08-29 17:49:05,340 n: module.bert.encoder.layer.11.attention.self.key.bias
2022-08-29 17:49:05,340 n: module.bert.encoder.layer.11.attention.self.value.weight
2022-08-29 17:49:05,340 n: module.bert.encoder.layer.11.attention.self.value.bias
2022-08-29 17:49:05,340 n: module.bert.encoder.layer.11.attention.output.dense.weight
2022-08-29 17:49:05,340 n: module.bert.encoder.layer.11.attention.output.dense.bias
2022-08-29 17:49:05,340 n: module.bert.encoder.layer.11.attention.output.LayerNorm.weight
2022-08-29 17:49:05,340 n: module.bert.encoder.layer.11.attention.output.LayerNorm.bias
2022-08-29 17:49:05,340 n: module.bert.encoder.layer.11.intermediate.dense.weight
2022-08-29 17:49:05,340 n: module.bert.encoder.layer.11.intermediate.dense.bias
2022-08-29 17:49:05,340 n: module.bert.encoder.layer.11.output.dense.weight
2022-08-29 17:49:05,340 n: module.bert.encoder.layer.11.output.dense.bias
2022-08-29 17:49:05,340 n: module.bert.encoder.layer.11.output.LayerNorm.weight
2022-08-29 17:49:05,340 n: module.bert.encoder.layer.11.output.LayerNorm.bias
2022-08-29 17:49:05,340 n: module.bert.pooler.dense.weight
2022-08-29 17:49:05,340 n: module.bert.pooler.dense.bias
2022-08-29 17:49:05,341 n: module.classifier.weight
2022-08-29 17:49:05,341 n: module.classifier.bias
2022-08-29 17:49:05,341 Total parameters: 109483778
2022-08-29 17:49:24,686 ***** Running evaluation *****
2022-08-29 17:49:24,687   Epoch = 1 iter 12999 step
2022-08-29 17:49:24,687   Num examples = 9815
2022-08-29 17:49:24,687   Batch size = 32
2022-08-29 17:49:24,688 ***** Eval results *****
2022-08-29 17:49:24,688   att_loss = 4.469360349270014
2022-08-29 17:49:24,688   cls_loss = 0.0
2022-08-29 17:49:24,688   global_step = 12999
2022-08-29 17:49:24,688   loss = 6.429925101143973
2022-08-29 17:49:24,688   rep_loss = 1.9605647513827125
2022-08-29 17:49:24,688 ***** Save model *****
2022-08-29 17:49:32,555 ***** Running evaluation *****
2022-08-29 17:49:32,556   Epoch = 0 iter 99 step
2022-08-29 17:49:32,556   Num examples = 408
2022-08-29 17:49:32,556   Batch size = 32
2022-08-29 17:49:33,400 ***** Eval results *****
2022-08-29 17:49:33,401   acc = 0.7647058823529411
2022-08-29 17:49:33,401   acc_and_f1 = 0.80873330927463
2022-08-29 17:49:33,401   att_loss = 0.0
2022-08-29 17:49:33,401   cls_loss = 0.16155083097442233
2022-08-29 17:49:33,401   eval_loss = 0.638922855257988
2022-08-29 17:49:33,401   f1 = 0.8527607361963191
2022-08-29 17:49:33,401   global_step = 99
2022-08-29 17:49:33,401   loss = 0.16155083097442233
2022-08-29 17:49:33,401   rep_loss = 0.0
2022-08-29 17:49:33,401 ***** Save model *****
2022-08-29 17:49:57,119 ***** Running evaluation *****
2022-08-29 17:49:57,120   Epoch = 5 iter 12399 step
2022-08-29 17:49:57,120   Num examples = 872
2022-08-29 17:49:57,120   Batch size = 32
2022-08-29 17:49:57,121 ***** Eval results *****
2022-08-29 17:49:57,121   att_loss = 1.0425391912840984
2022-08-29 17:49:57,121   cls_loss = 0.0
2022-08-29 17:49:57,121   global_step = 12399
2022-08-29 17:49:57,121   loss = 2.586545698182135
2022-08-29 17:49:57,121   rep_loss = 1.5440065054388488
2022-08-29 17:49:57,121 ***** Save model *****
2022-08-29 17:49:58,254 ***** Running evaluation *****
2022-08-29 17:49:58,254   Epoch = 0 iter 199 step
2022-08-29 17:49:58,254   Num examples = 408
2022-08-29 17:49:58,255   Batch size = 32
2022-08-29 17:49:59,102 ***** Eval results *****
2022-08-29 17:49:59,102   acc = 0.8333333333333334
2022-08-29 17:49:59,102   acc_and_f1 = 0.858646188850967
2022-08-29 17:49:59,102   att_loss = 0.0
2022-08-29 17:49:59,102   cls_loss = 0.1625529303959566
2022-08-29 17:49:59,102   eval_loss = 0.4312553485998741
2022-08-29 17:49:59,102   f1 = 0.8839590443686006
2022-08-29 17:49:59,102   global_step = 199
2022-08-29 17:49:59,102   loss = 0.1625529303959566
2022-08-29 17:49:59,102   rep_loss = 0.0
2022-08-29 17:49:59,103 ***** Save model *****
2022-08-29 17:50:27,921 ***** Running evaluation *****
2022-08-29 17:50:27,922   Epoch = 0 iter 299 step
2022-08-29 17:50:27,922   Num examples = 408
2022-08-29 17:50:27,922   Batch size = 32
2022-08-29 17:50:28,768 ***** Eval results *****
2022-08-29 17:50:28,768   acc = 0.7941176470588235
2022-08-29 17:50:28,768   acc_and_f1 = 0.829097658480868
2022-08-29 17:50:28,768   att_loss = 0.0
2022-08-29 17:50:28,768   cls_loss = 0.16656250938972503
2022-08-29 17:50:28,768   eval_loss = 0.5377917347046045
2022-08-29 17:50:28,768   f1 = 0.8640776699029126
2022-08-29 17:50:28,769   global_step = 299
2022-08-29 17:50:28,769   loss = 0.16656250938972503
2022-08-29 17:50:28,769   rep_loss = 0.0
2022-08-29 17:50:36,044 ***** Running evaluation *****
2022-08-29 17:50:36,045   Epoch = 1 iter 13199 step
2022-08-29 17:50:36,045   Num examples = 9815
2022-08-29 17:50:36,045   Batch size = 32
2022-08-29 17:50:36,046 ***** Eval results *****
2022-08-29 17:50:36,046   att_loss = 4.469944113801265
2022-08-29 17:50:36,046   cls_loss = 0.0
2022-08-29 17:50:36,046   global_step = 13199
2022-08-29 17:50:36,046   loss = 6.428848596482442
2022-08-29 17:50:36,047   rep_loss = 1.9589044761298031
2022-08-29 17:50:36,047 ***** Save model *****
2022-08-29 17:50:52,934 ***** Running evaluation *****
2022-08-29 17:50:52,934   Epoch = 0 iter 399 step
2022-08-29 17:50:52,934   Num examples = 408
2022-08-29 17:50:52,934   Batch size = 32
2022-08-29 17:50:53,851 ***** Eval results *****
2022-08-29 17:50:53,851   acc = 0.8137254901960784
2022-08-29 17:50:53,851   acc_and_f1 = 0.845572422517394
2022-08-29 17:50:53,851   att_loss = 0.0
2022-08-29 17:50:53,851   cls_loss = 0.16693570241705516
2022-08-29 17:50:53,851   eval_loss = 0.5262282376105969
2022-08-29 17:50:53,851   f1 = 0.8774193548387096
2022-08-29 17:50:53,851   global_step = 399
2022-08-29 17:50:53,851   loss = 0.16693570241705516
2022-08-29 17:50:53,851   rep_loss = 0.0
2022-08-29 17:51:16,445 ***** Running evaluation *****
2022-08-29 17:51:16,445   Epoch = 5 iter 12599 step
2022-08-29 17:51:16,445   Num examples = 872
2022-08-29 17:51:16,445   Batch size = 32
2022-08-29 17:51:16,447 ***** Eval results *****
2022-08-29 17:51:16,447   att_loss = 1.0446849055104441
2022-08-29 17:51:16,447   cls_loss = 0.0
2022-08-29 17:51:16,447   global_step = 12599
2022-08-29 17:51:16,447   loss = 2.588528673656862
2022-08-29 17:51:16,447   rep_loss = 1.543843766196867
2022-08-29 17:51:16,447 ***** Save model *****
2022-08-29 17:51:18,079 ***** Running evaluation *****
2022-08-29 17:51:18,080   Epoch = 1 iter 499 step
2022-08-29 17:51:18,080   Num examples = 408
2022-08-29 17:51:18,080   Batch size = 32
2022-08-29 17:51:18,980 ***** Eval results *****
2022-08-29 17:51:18,980   acc = 0.8186274509803921
2022-08-29 17:51:18,980   acc_and_f1 = 0.848257619879635
2022-08-29 17:51:18,980   att_loss = 0.0
2022-08-29 17:51:18,980   cls_loss = 0.17502364625291125
2022-08-29 17:51:18,980   eval_loss = 0.42470402213243336
2022-08-29 17:51:18,980   f1 = 0.8778877887788779
2022-08-29 17:51:18,980   global_step = 499
2022-08-29 17:51:18,980   loss = 0.17502364625291125
2022-08-29 17:51:18,980   rep_loss = 0.0
2022-08-29 17:51:43,122 ***** Running evaluation *****
2022-08-29 17:51:43,122   Epoch = 1 iter 599 step
2022-08-29 17:51:43,122   Num examples = 408
2022-08-29 17:51:43,122   Batch size = 32
2022-08-29 17:51:43,979 ***** Eval results *****
2022-08-29 17:51:43,979   acc = 0.8186274509803921
2022-08-29 17:51:43,979   acc_and_f1 = 0.8480554473445008
2022-08-29 17:51:43,979   att_loss = 0.0
2022-08-29 17:51:43,979   cls_loss = 0.1590847723872949
2022-08-29 17:51:43,979   eval_loss = 0.4656674277323943
2022-08-29 17:51:43,979   f1 = 0.8774834437086093
2022-08-29 17:51:43,979   global_step = 599
2022-08-29 17:51:43,980   loss = 0.1590847723872949
2022-08-29 17:51:43,980   rep_loss = 0.0
2022-08-29 17:51:50,708 ***** Running evaluation *****
2022-08-29 17:51:50,708   Epoch = 1 iter 13399 step
2022-08-29 17:51:50,708   Num examples = 9815
2022-08-29 17:51:50,708   Batch size = 32
2022-08-29 17:51:50,709 ***** Eval results *****
2022-08-29 17:51:50,709   att_loss = 4.46654804188309
2022-08-29 17:51:50,709   cls_loss = 0.0
2022-08-29 17:51:50,709   global_step = 13399
2022-08-29 17:51:50,710   loss = 6.4248321682003375
2022-08-29 17:51:50,710   rep_loss = 1.9582841213501938
2022-08-29 17:51:50,710 ***** Save model *****
2022-08-29 17:52:08,120 ***** Running evaluation *****
2022-08-29 17:52:08,120   Epoch = 1 iter 699 step
2022-08-29 17:52:08,121   Num examples = 408
2022-08-29 17:52:08,121   Batch size = 32
2022-08-29 17:52:08,968 ***** Eval results *****
2022-08-29 17:52:08,968   acc = 0.8235294117647058
2022-08-29 17:52:08,968   acc_and_f1 = 0.8538868923775298
2022-08-29 17:52:08,968   att_loss = 0.0
2022-08-29 17:52:08,968   cls_loss = 0.15366727934524232
2022-08-29 17:52:08,968   eval_loss = 0.5197314757567185
2022-08-29 17:52:08,968   f1 = 0.8842443729903537
2022-08-29 17:52:08,968   global_step = 699
2022-08-29 17:52:08,969   loss = 0.15366727934524232
2022-08-29 17:52:08,969   rep_loss = 0.0
2022-08-29 17:52:33,270 ***** Running evaluation *****
2022-08-29 17:52:33,270   Epoch = 1 iter 799 step
2022-08-29 17:52:33,271   Num examples = 408
2022-08-29 17:52:33,271   Batch size = 32
2022-08-29 17:52:34,115 ***** Eval results *****
2022-08-29 17:52:34,115   acc = 0.8504901960784313
2022-08-29 17:52:34,115   acc_and_f1 = 0.873984593837535
2022-08-29 17:52:34,115   att_loss = 0.0
2022-08-29 17:52:34,115   cls_loss = 0.1529276195006811
2022-08-29 17:52:34,115   eval_loss = 0.4095815294064008
2022-08-29 17:52:34,115   f1 = 0.8974789915966387
2022-08-29 17:52:34,115   global_step = 799
2022-08-29 17:52:34,115   loss = 0.1529276195006811
2022-08-29 17:52:34,116   rep_loss = 0.0
2022-08-29 17:52:34,116 ***** Save model *****
2022-08-29 17:52:35,042 ***** Running evaluation *****
2022-08-29 17:52:35,043   Epoch = 6 iter 12799 step
2022-08-29 17:52:35,043   Num examples = 872
2022-08-29 17:52:35,043   Batch size = 32
2022-08-29 17:52:35,044 ***** Eval results *****
2022-08-29 17:52:35,044   att_loss = 1.027677434512547
2022-08-29 17:52:35,044   cls_loss = 0.0
2022-08-29 17:52:35,044   global_step = 12799
2022-08-29 17:52:35,044   loss = 2.5511796774183
2022-08-29 17:52:35,044   rep_loss = 1.5235022360937935
2022-08-29 17:52:35,044 ***** Save model *****
2022-08-29 17:52:59,791 ***** Running evaluation *****
2022-08-29 17:52:59,792   Epoch = 1 iter 13599 step
2022-08-29 17:52:59,792   Num examples = 9815
2022-08-29 17:52:59,792   Batch size = 32
2022-08-29 17:52:59,793 ***** Eval results *****
2022-08-29 17:52:59,793   att_loss = 4.468874610691185
2022-08-29 17:52:59,793   cls_loss = 0.0
2022-08-29 17:52:59,793   global_step = 13599
2022-08-29 17:52:59,794   loss = 6.426336748772357
2022-08-29 17:52:59,794   rep_loss = 1.9574621329645077
2022-08-29 17:52:59,794 ***** Save model *****
2022-08-29 17:53:02,422 ***** Running evaluation *****
2022-08-29 17:53:02,422   Epoch = 1 iter 899 step
2022-08-29 17:53:02,422   Num examples = 408
2022-08-29 17:53:02,422   Batch size = 32
2022-08-29 17:53:03,269 ***** Eval results *****
2022-08-29 17:53:03,269   acc = 0.8406862745098039
2022-08-29 17:53:03,269   acc_and_f1 = 0.8673251927198286
2022-08-29 17:53:03,269   att_loss = 0.0
2022-08-29 17:53:03,269   cls_loss = 0.154589927436408
2022-08-29 17:53:03,269   eval_loss = 0.4527971457976561
2022-08-29 17:53:03,269   f1 = 0.8939641109298532
2022-08-29 17:53:03,269   global_step = 899
2022-08-29 17:53:03,270   loss = 0.154589927436408
2022-08-29 17:53:03,270   rep_loss = 0.0
2022-08-29 17:53:27,240 ***** Running evaluation *****
2022-08-29 17:53:27,240   Epoch = 2 iter 999 step
2022-08-29 17:53:27,240   Num examples = 408
2022-08-29 17:53:27,240   Batch size = 32
2022-08-29 17:53:28,137 ***** Eval results *****
2022-08-29 17:53:28,137   acc = 0.8137254901960784
2022-08-29 17:53:28,138   acc_and_f1 = 0.8463531909579118
2022-08-29 17:53:28,138   att_loss = 0.0
2022-08-29 17:53:28,138   cls_loss = 0.14192640036344528
2022-08-29 17:53:28,138   eval_loss = 0.6277520255400584
2022-08-29 17:53:28,138   f1 = 0.8789808917197452
2022-08-29 17:53:28,138   global_step = 999
2022-08-29 17:53:28,138   loss = 0.14192640036344528
2022-08-29 17:53:28,138   rep_loss = 0.0
2022-08-29 17:53:52,109 ***** Running evaluation *****
2022-08-29 17:53:52,110   Epoch = 2 iter 1099 step
2022-08-29 17:53:52,110   Num examples = 408
2022-08-29 17:53:52,110   Batch size = 32
2022-08-29 17:53:52,955 ***** Eval results *****
2022-08-29 17:53:52,956   acc = 0.8112745098039216
2022-08-29 17:53:52,956   acc_and_f1 = 0.8446229918274759
2022-08-29 17:53:52,956   att_loss = 0.0
2022-08-29 17:53:52,956   cls_loss = 0.1440467395987667
2022-08-29 17:53:52,956   eval_loss = 0.6148819373204157
2022-08-29 17:53:52,956   f1 = 0.8779714738510301
2022-08-29 17:53:52,956   global_step = 1099
2022-08-29 17:53:52,956   loss = 0.1440467395987667
2022-08-29 17:53:52,956   rep_loss = 0.0
2022-08-29 17:53:55,852 ***** Running evaluation *****
2022-08-29 17:53:55,853   Epoch = 6 iter 12999 step
2022-08-29 17:53:55,853   Num examples = 872
2022-08-29 17:53:55,853   Batch size = 32
2022-08-29 17:53:55,854 ***** Eval results *****
2022-08-29 17:53:55,854   att_loss = 1.0342848065694172
2022-08-29 17:53:55,854   cls_loss = 0.0
2022-08-29 17:53:55,854   global_step = 12999
2022-08-29 17:53:55,855   loss = 2.5583616797129314
2022-08-29 17:53:55,855   rep_loss = 1.524076865196228
2022-08-29 17:53:55,855 ***** Save model *****
2022-08-29 17:54:10,905 ***** Running evaluation *****
2022-08-29 17:54:10,905   Epoch = 1 iter 13799 step
2022-08-29 17:54:10,905   Num examples = 9815
2022-08-29 17:54:10,905   Batch size = 32
2022-08-29 17:54:10,906 ***** Eval results *****
2022-08-29 17:54:10,906   att_loss = 4.463823036373598
2022-08-29 17:54:10,907   cls_loss = 0.0
2022-08-29 17:54:10,907   global_step = 13799
2022-08-29 17:54:10,907   loss = 6.4201656692315145
2022-08-29 17:54:10,907   rep_loss = 1.9563426290351058
2022-08-29 17:54:10,907 ***** Save model *****
2022-08-29 17:54:17,063 ***** Running evaluation *****
2022-08-29 17:54:17,064   Epoch = 2 iter 1199 step
2022-08-29 17:54:17,064   Num examples = 408
2022-08-29 17:54:17,064   Batch size = 32
2022-08-29 17:54:17,911 ***** Eval results *****
2022-08-29 17:54:17,911   acc = 0.8308823529411765
2022-08-29 17:54:17,911   acc_and_f1 = 0.8587909301651695
2022-08-29 17:54:17,911   att_loss = 0.0
2022-08-29 17:54:17,912   cls_loss = 0.1441605703595881
2022-08-29 17:54:17,912   eval_loss = 0.4795585125684738
2022-08-29 17:54:17,912   f1 = 0.8866995073891626
2022-08-29 17:54:17,912   global_step = 1199
2022-08-29 17:54:17,912   loss = 0.1441605703595881
2022-08-29 17:54:17,912   rep_loss = 0.0
2022-08-29 17:54:44,213 ***** Running evaluation *****
2022-08-29 17:54:44,214   Epoch = 2 iter 1299 step
2022-08-29 17:54:44,214   Num examples = 408
2022-08-29 17:54:44,214   Batch size = 32
2022-08-29 17:54:45,063 ***** Eval results *****
2022-08-29 17:54:45,063   acc = 0.8161764705882353
2022-08-29 17:54:45,063   acc_and_f1 = 0.8473102774335017
2022-08-29 17:54:45,063   att_loss = 0.0
2022-08-29 17:54:45,063   cls_loss = 0.14507102415782663
2022-08-29 17:54:45,064   eval_loss = 0.49506058945105624
2022-08-29 17:54:45,064   f1 = 0.8784440842787682
2022-08-29 17:54:45,064   global_step = 1299
2022-08-29 17:54:45,064   loss = 0.14507102415782663
2022-08-29 17:54:45,064   rep_loss = 0.0
2022-08-29 17:55:10,850 ***** Running evaluation *****
2022-08-29 17:55:10,851   Epoch = 3 iter 1399 step
2022-08-29 17:55:10,851   Num examples = 408
2022-08-29 17:55:10,851   Batch size = 32
2022-08-29 17:55:11,701 ***** Eval results *****
2022-08-29 17:55:11,701   acc = 0.8137254901960784
2022-08-29 17:55:11,701   acc_and_f1 = 0.8457694975096148
2022-08-29 17:55:11,702   att_loss = 0.0
2022-08-29 17:55:11,702   cls_loss = 0.1457024559378624
2022-08-29 17:55:11,702   eval_loss = 0.5266658503275651
2022-08-29 17:55:11,702   f1 = 0.8778135048231511
2022-08-29 17:55:11,702   global_step = 1399
2022-08-29 17:55:11,702   loss = 0.1457024559378624
2022-08-29 17:55:11,702   rep_loss = 0.0
2022-08-29 17:55:14,234 ***** Running evaluation *****
2022-08-29 17:55:14,235   Epoch = 6 iter 13199 step
2022-08-29 17:55:14,235   Num examples = 872
2022-08-29 17:55:14,235   Batch size = 32
2022-08-29 17:55:14,236 ***** Eval results *****
2022-08-29 17:55:14,236   att_loss = 1.0272226415509762
2022-08-29 17:55:14,236   cls_loss = 0.0
2022-08-29 17:55:14,236   global_step = 13199
2022-08-29 17:55:14,237   loss = 2.549431375835253
2022-08-29 17:55:14,237   rep_loss = 1.5222087265097577
2022-08-29 17:55:14,237 ***** Save model *****
2022-08-29 17:55:22,456 ***** Running evaluation *****
2022-08-29 17:55:22,457   Epoch = 1 iter 13999 step
2022-08-29 17:55:22,457   Num examples = 9815
2022-08-29 17:55:22,457   Batch size = 32
2022-08-29 17:55:22,458 ***** Eval results *****
2022-08-29 17:55:22,458   att_loss = 4.463433004087872
2022-08-29 17:55:22,458   cls_loss = 0.0
2022-08-29 17:55:22,458   global_step = 13999
2022-08-29 17:55:22,458   loss = 6.418766194195659
2022-08-29 17:55:22,458   rep_loss = 1.955333187210339
2022-08-29 17:55:22,458 ***** Save model *****
2022-08-29 17:55:36,920 ***** Running evaluation *****
2022-08-29 17:55:36,921   Epoch = 3 iter 1499 step
2022-08-29 17:55:36,921   Num examples = 408
2022-08-29 17:55:36,921   Batch size = 32
2022-08-29 17:55:37,826 ***** Eval results *****
2022-08-29 17:55:37,826   acc = 0.8235294117647058
2022-08-29 17:55:37,826   acc_and_f1 = 0.8537001897533207
2022-08-29 17:55:37,826   att_loss = 0.0
2022-08-29 17:55:37,826   cls_loss = 0.136211355894804
2022-08-29 17:55:37,826   eval_loss = 0.5204071436937039
2022-08-29 17:55:37,826   f1 = 0.8838709677419355
2022-08-29 17:55:37,826   global_step = 1499
2022-08-29 17:55:37,826   loss = 0.136211355894804
2022-08-29 17:55:37,827   rep_loss = 0.0
2022-08-29 17:56:03,076 ***** Running evaluation *****
2022-08-29 17:56:03,077   Epoch = 3 iter 1599 step
2022-08-29 17:56:03,077   Num examples = 408
2022-08-29 17:56:03,077   Batch size = 32
2022-08-29 17:56:03,925 ***** Eval results *****
2022-08-29 17:56:03,925   acc = 0.8284313725490197
2022-08-29 17:56:03,925   acc_and_f1 = 0.8566498968008256
2022-08-29 17:56:03,925   att_loss = 0.0
2022-08-29 17:56:03,925   cls_loss = 0.13522791650560168
2022-08-29 17:56:03,925   eval_loss = 0.4732352541043208
2022-08-29 17:56:03,925   f1 = 0.8848684210526315
2022-08-29 17:56:03,925   global_step = 1599
2022-08-29 17:56:03,925   loss = 0.13522791650560168
2022-08-29 17:56:03,926   rep_loss = 0.0
2022-08-29 17:56:29,315 ***** Running evaluation *****
2022-08-29 17:56:29,315   Epoch = 3 iter 1699 step
2022-08-29 17:56:29,315   Num examples = 408
2022-08-29 17:56:29,315   Batch size = 32
2022-08-29 17:56:30,164 ***** Eval results *****
2022-08-29 17:56:30,164   acc = 0.8088235294117647
2022-08-29 17:56:30,164   acc_and_f1 = 0.8423098538778568
2022-08-29 17:56:30,164   att_loss = 0.0
2022-08-29 17:56:30,164   cls_loss = 0.13707345359600506
2022-08-29 17:56:30,164   eval_loss = 0.5311544732405589
2022-08-29 17:56:30,164   f1 = 0.8757961783439491
2022-08-29 17:56:30,164   global_step = 1699
2022-08-29 17:56:30,165   loss = 0.13707345359600506
2022-08-29 17:56:30,165   rep_loss = 0.0
2022-08-29 17:56:32,420 ***** Running evaluation *****
2022-08-29 17:56:32,420   Epoch = 6 iter 13399 step
2022-08-29 17:56:32,420   Num examples = 872
2022-08-29 17:56:32,420   Batch size = 32
2022-08-29 17:56:32,421 ***** Eval results *****
2022-08-29 17:56:32,421   att_loss = 1.0321469500757032
2022-08-29 17:56:32,421   cls_loss = 0.0
2022-08-29 17:56:32,422   global_step = 13399
2022-08-29 17:56:32,422   loss = 2.5566566454979682
2022-08-29 17:56:32,422   rep_loss = 1.5245096895771642
2022-08-29 17:56:32,422 ***** Save model *****
2022-08-29 17:56:35,005 ***** Running evaluation *****
2022-08-29 17:56:35,006   Epoch = 1 iter 14199 step
2022-08-29 17:56:35,006   Num examples = 9815
2022-08-29 17:56:35,006   Batch size = 32
2022-08-29 17:56:35,007 ***** Eval results *****
2022-08-29 17:56:35,007   att_loss = 4.4611314310077805
2022-08-29 17:56:35,007   cls_loss = 0.0
2022-08-29 17:56:35,007   global_step = 14199
2022-08-29 17:56:35,007   loss = 6.415044322795393
2022-08-29 17:56:35,008   rep_loss = 1.9539128895617124
2022-08-29 17:56:35,008 ***** Save model *****
2022-08-29 17:56:55,583 ***** Running evaluation *****
2022-08-29 17:56:55,584   Epoch = 3 iter 1799 step
2022-08-29 17:56:55,584   Num examples = 408
2022-08-29 17:56:55,584   Batch size = 32
2022-08-29 17:56:56,431 ***** Eval results *****
2022-08-29 17:56:56,431   acc = 0.821078431372549
2022-08-29 17:56:56,431   acc_and_f1 = 0.8511896221903394
2022-08-29 17:56:56,431   att_loss = 0.0
2022-08-29 17:56:56,431   cls_loss = 0.13757245131275234
2022-08-29 17:56:56,431   eval_loss = 0.5127696016660104
2022-08-29 17:56:56,431   f1 = 0.88130081300813
2022-08-29 17:56:56,432   global_step = 1799
2022-08-29 17:56:56,432   loss = 0.13757245131275234
2022-08-29 17:56:56,432   rep_loss = 0.0
2022-08-29 17:57:21,805 ***** Running evaluation *****
2022-08-29 17:57:21,805   Epoch = 4 iter 1899 step
2022-08-29 17:57:21,805   Num examples = 408
2022-08-29 17:57:21,805   Batch size = 32
2022-08-29 17:57:22,658 ***** Eval results *****
2022-08-29 17:57:22,658   acc = 0.8137254901960784
2022-08-29 17:57:22,658   acc_and_f1 = 0.8459653092006034
2022-08-29 17:57:22,658   att_loss = 0.0
2022-08-29 17:57:22,658   cls_loss = 0.13322419628723345
2022-08-29 17:57:22,658   eval_loss = 0.5440254429211984
2022-08-29 17:57:22,658   f1 = 0.8782051282051283
2022-08-29 17:57:22,658   global_step = 1899
2022-08-29 17:57:22,658   loss = 0.13322419628723345
2022-08-29 17:57:22,659   rep_loss = 0.0
2022-08-29 17:57:44,817 ***** Running evaluation *****
2022-08-29 17:57:44,817   Epoch = 1 iter 14399 step
2022-08-29 17:57:44,818   Num examples = 9815
2022-08-29 17:57:44,818   Batch size = 32
2022-08-29 17:57:44,819 ***** Eval results *****
2022-08-29 17:57:44,819   att_loss = 4.456231680243535
2022-08-29 17:57:44,819   cls_loss = 0.0
2022-08-29 17:57:44,819   global_step = 14399
2022-08-29 17:57:44,819   loss = 6.408800291163581
2022-08-29 17:57:44,819   rep_loss = 1.9525686082871336
2022-08-29 17:57:44,819 ***** Save model *****
2022-08-29 17:57:47,762 ***** Running evaluation *****
2022-08-29 17:57:47,762   Epoch = 4 iter 1999 step
2022-08-29 17:57:47,762   Num examples = 408
2022-08-29 17:57:47,762   Batch size = 32
2022-08-29 17:57:48,612 ***** Eval results *****
2022-08-29 17:57:48,612   acc = 0.821078431372549
2022-08-29 17:57:48,612   acc_and_f1 = 0.8515731413728658
2022-08-29 17:57:48,612   att_loss = 0.0
2022-08-29 17:57:48,612   cls_loss = 0.13778255911108978
2022-08-29 17:57:48,612   eval_loss = 0.5247147106207334
2022-08-29 17:57:48,612   f1 = 0.8820678513731827
2022-08-29 17:57:48,612   global_step = 1999
2022-08-29 17:57:48,612   loss = 0.13778255911108978
2022-08-29 17:57:48,612   rep_loss = 0.0
2022-08-29 17:57:50,347 ***** Running evaluation *****
2022-08-29 17:57:50,348   Epoch = 6 iter 13599 step
2022-08-29 17:57:50,348   Num examples = 872
2022-08-29 17:57:50,348   Batch size = 32
2022-08-29 17:57:50,349 ***** Eval results *****
2022-08-29 17:57:50,349   att_loss = 1.0298860477178524
2022-08-29 17:57:50,349   cls_loss = 0.0
2022-08-29 17:57:50,349   global_step = 13599
2022-08-29 17:57:50,349   loss = 2.553817828740829
2022-08-29 17:57:50,349   rep_loss = 1.5239317759489401
2022-08-29 17:57:50,349 ***** Save model *****
2022-08-29 17:58:13,773 ***** Running evaluation *****
2022-08-29 17:58:13,774   Epoch = 4 iter 2099 step
2022-08-29 17:58:13,774   Num examples = 408
2022-08-29 17:58:13,774   Batch size = 32
2022-08-29 17:58:14,677 ***** Eval results *****
2022-08-29 17:58:14,677   acc = 0.8259803921568627
2022-08-29 17:58:14,677   acc_and_f1 = 0.855453729303715
2022-08-29 17:58:14,678   att_loss = 0.0
2022-08-29 17:58:14,678   cls_loss = 0.13708038354634347
2022-08-29 17:58:14,678   eval_loss = 0.48422513329065764
2022-08-29 17:58:14,678   f1 = 0.8849270664505673
2022-08-29 17:58:14,678   global_step = 2099
2022-08-29 17:58:14,678   loss = 0.13708038354634347
2022-08-29 17:58:14,678   rep_loss = 0.0
2022-08-29 17:58:39,132 ***** Running evaluation *****
2022-08-29 17:58:39,132   Epoch = 4 iter 2199 step
2022-08-29 17:58:39,132   Num examples = 408
2022-08-29 17:58:39,132   Batch size = 32
2022-08-29 17:58:39,976 ***** Eval results *****
2022-08-29 17:58:39,976   acc = 0.8284313725490197
2022-08-29 17:58:39,976   acc_and_f1 = 0.8575813820673901
2022-08-29 17:58:39,976   att_loss = 0.0
2022-08-29 17:58:39,976   cls_loss = 0.1346098369327488
2022-08-29 17:58:39,977   eval_loss = 0.5111178835997214
2022-08-29 17:58:39,977   f1 = 0.8867313915857604
2022-08-29 17:58:39,977   global_step = 2199
2022-08-29 17:58:39,977   loss = 0.1346098369327488
2022-08-29 17:58:39,977   rep_loss = 0.0
2022-08-29 17:58:54,062 ***** Running evaluation *****
2022-08-29 17:58:54,062   Epoch = 1 iter 14599 step
2022-08-29 17:58:54,062   Num examples = 9815
2022-08-29 17:58:54,062   Batch size = 32
2022-08-29 17:58:54,064 ***** Eval results *****
2022-08-29 17:58:54,064   att_loss = 4.45609358966965
2022-08-29 17:58:54,064   cls_loss = 0.0
2022-08-29 17:58:54,064   global_step = 14599
2022-08-29 17:58:54,064   loss = 6.407422164051803
2022-08-29 17:58:54,064   rep_loss = 1.951328571565782
2022-08-29 17:58:54,064 ***** Save model *****
2022-08-29 17:59:08,597 ***** Running evaluation *****
2022-08-29 17:59:08,597   Epoch = 6 iter 13799 step
2022-08-29 17:59:08,597   Num examples = 872
2022-08-29 17:59:08,597   Batch size = 32
2022-08-29 17:59:08,598 ***** Eval results *****
2022-08-29 17:59:08,598   att_loss = 1.027130635291972
2022-08-29 17:59:08,598   cls_loss = 0.0
2022-08-29 17:59:08,598   global_step = 13799
2022-08-29 17:59:08,598   loss = 2.549876289367676
2022-08-29 17:59:08,599   rep_loss = 1.5227456498653331
2022-08-29 17:59:08,599 ***** Save model *****
2022-08-29 18:00:05,282 ***** Running evaluation *****
2022-08-29 18:00:05,283   Epoch = 1 iter 14799 step
2022-08-29 18:00:05,283   Num examples = 9815
2022-08-29 18:00:05,283   Batch size = 32
2022-08-29 18:00:05,284 ***** Eval results *****
2022-08-29 18:00:05,284   att_loss = 4.454158909807477
2022-08-29 18:00:05,284   cls_loss = 0.0
2022-08-29 18:00:05,284   global_step = 14799
2022-08-29 18:00:05,284   loss = 6.404388096513627
2022-08-29 18:00:05,284   rep_loss = 1.9502291838296606
2022-08-29 18:00:05,284 ***** Save model *****
2022-08-29 18:00:26,603 ***** Running evaluation *****
2022-08-29 18:00:26,603   Epoch = 6 iter 13999 step
2022-08-29 18:00:26,603   Num examples = 872
2022-08-29 18:00:26,603   Batch size = 32
2022-08-29 18:00:26,604 ***** Eval results *****
2022-08-29 18:00:26,604   att_loss = 1.0266210658333519
2022-08-29 18:00:26,604   cls_loss = 0.0
2022-08-29 18:00:26,605   global_step = 13999
2022-08-29 18:00:26,605   loss = 2.5483713508952746
2022-08-29 18:00:26,605   rep_loss = 1.5217502821142024
2022-08-29 18:00:26,605 ***** Save model *****
2022-08-29 18:01:14,387 ***** Running evaluation *****
2022-08-29 18:01:14,387   Epoch = 1 iter 14999 step
2022-08-29 18:01:14,387   Num examples = 9815
2022-08-29 18:01:14,387   Batch size = 32
2022-08-29 18:01:14,388 ***** Eval results *****
2022-08-29 18:01:14,388   att_loss = 4.449712505700651
2022-08-29 18:01:14,388   cls_loss = 0.0
2022-08-29 18:01:14,389   global_step = 14999
2022-08-29 18:01:14,389   loss = 6.398628635315601
2022-08-29 18:01:14,389   rep_loss = 1.948916127517426
2022-08-29 18:01:14,389 ***** Save model *****
2022-08-29 18:01:46,025 ***** Running evaluation *****
2022-08-29 18:01:46,025   Epoch = 6 iter 14199 step
2022-08-29 18:01:46,025   Num examples = 872
2022-08-29 18:01:46,025   Batch size = 32
2022-08-29 18:01:46,027 ***** Eval results *****
2022-08-29 18:01:46,027   att_loss = 1.0256419315792265
2022-08-29 18:01:46,027   cls_loss = 0.0
2022-08-29 18:01:46,027   global_step = 14199
2022-08-29 18:01:46,027   loss = 2.5471622527591764
2022-08-29 18:01:46,027   rep_loss = 1.5215203188336084
2022-08-29 18:01:46,027 ***** Save model *****
2022-08-29 18:02:25,267 ***** Running evaluation *****
2022-08-29 18:02:25,268   Epoch = 1 iter 15199 step
2022-08-29 18:02:25,268   Num examples = 9815
2022-08-29 18:02:25,268   Batch size = 32
2022-08-29 18:02:25,269 ***** Eval results *****
2022-08-29 18:02:25,269   att_loss = 4.449426195090585
2022-08-29 18:02:25,269   cls_loss = 0.0
2022-08-29 18:02:25,269   global_step = 15199
2022-08-29 18:02:25,269   loss = 6.397392604846121
2022-08-29 18:02:25,270   rep_loss = 1.9479664056027521
2022-08-29 18:02:25,270 ***** Save model *****
2022-08-29 18:03:04,054 ***** Running evaluation *****
2022-08-29 18:03:04,054   Epoch = 6 iter 14399 step
2022-08-29 18:03:04,054   Num examples = 872
2022-08-29 18:03:04,054   Batch size = 32
2022-08-29 18:03:04,055 ***** Eval results *****
2022-08-29 18:03:04,055   att_loss = 1.0249163135004715
2022-08-29 18:03:04,055   cls_loss = 0.0
2022-08-29 18:03:04,055   global_step = 14399
2022-08-29 18:03:04,055   loss = 2.54536325495008
2022-08-29 18:03:04,056   rep_loss = 1.520446939199743
2022-08-29 18:03:04,056 ***** Save model *****
2022-08-29 18:03:39,344 ***** Running evaluation *****
2022-08-29 18:03:39,344   Epoch = 1 iter 15399 step
2022-08-29 18:03:39,344   Num examples = 9815
2022-08-29 18:03:39,344   Batch size = 32
2022-08-29 18:03:39,345 ***** Eval results *****
2022-08-29 18:03:39,345   att_loss = 4.445962675620833
2022-08-29 18:03:39,345   cls_loss = 0.0
2022-08-29 18:03:39,345   global_step = 15399
2022-08-29 18:03:39,346   loss = 6.392714087158213
2022-08-29 18:03:39,346   rep_loss = 1.9467514070403544
2022-08-29 18:03:39,346 ***** Save model *****
2022-08-29 18:04:22,440 ***** Running evaluation *****
2022-08-29 18:04:22,441   Epoch = 6 iter 14599 step
2022-08-29 18:04:22,441   Num examples = 872
2022-08-29 18:04:22,441   Batch size = 32
2022-08-29 18:04:22,442 ***** Eval results *****
2022-08-29 18:04:22,442   att_loss = 1.0262995213194739
2022-08-29 18:04:22,442   cls_loss = 0.0
2022-08-29 18:04:22,442   global_step = 14599
2022-08-29 18:04:22,442   loss = 2.546573112342931
2022-08-29 18:04:22,442   rep_loss = 1.5202735881563982
2022-08-29 18:04:22,442 ***** Save model *****
2022-08-29 18:04:53,764 ***** Running evaluation *****
2022-08-29 18:04:53,764   Epoch = 1 iter 15599 step
2022-08-29 18:04:53,764   Num examples = 9815
2022-08-29 18:04:53,765   Batch size = 32
2022-08-29 18:04:53,766 ***** Eval results *****
2022-08-29 18:04:53,766   att_loss = 4.44395084055857
2022-08-29 18:04:53,766   cls_loss = 0.0
2022-08-29 18:04:53,766   global_step = 15599
2022-08-29 18:04:53,766   loss = 6.389625695032569
2022-08-29 18:04:53,766   rep_loss = 1.9456748492800846
2022-08-29 18:04:53,766 ***** Save model *****
2022-08-29 18:05:40,413 ***** Running evaluation *****
2022-08-29 18:05:40,414   Epoch = 7 iter 14799 step
2022-08-29 18:05:40,414   Num examples = 872
2022-08-29 18:05:40,414   Batch size = 32
2022-08-29 18:05:40,415 ***** Eval results *****
2022-08-29 18:05:40,415   att_loss = 1.003325555526035
2022-08-29 18:05:40,415   cls_loss = 0.0
2022-08-29 18:05:40,415   global_step = 14799
2022-08-29 18:05:40,415   loss = 2.5080449681886483
2022-08-29 18:05:40,415   rep_loss = 1.504719420218132
2022-08-29 18:05:40,415 ***** Save model *****
2022-08-29 18:06:02,930 ***** Running evaluation *****
2022-08-29 18:06:02,931   Epoch = 1 iter 15799 step
2022-08-29 18:06:02,931   Num examples = 9815
2022-08-29 18:06:02,931   Batch size = 32
2022-08-29 18:06:02,932 ***** Eval results *****
2022-08-29 18:06:02,932   att_loss = 4.44180760280886
2022-08-29 18:06:02,932   cls_loss = 0.0
2022-08-29 18:06:02,932   global_step = 15799
2022-08-29 18:06:02,932   loss = 6.386451572517689
2022-08-29 18:06:02,932   rep_loss = 1.944643965518934
2022-08-29 18:06:02,932 ***** Save model *****
2022-08-29 18:06:58,402 ***** Running evaluation *****
2022-08-29 18:06:58,402   Epoch = 7 iter 14999 step
2022-08-29 18:06:58,402   Num examples = 872
2022-08-29 18:06:58,402   Batch size = 32
2022-08-29 18:06:58,404 ***** Eval results *****
2022-08-29 18:06:58,404   att_loss = 1.0179736684609164
2022-08-29 18:06:58,404   cls_loss = 0.0
2022-08-29 18:06:58,404   global_step = 14999
2022-08-29 18:06:58,404   loss = 2.5260594867692223
2022-08-29 18:06:58,404   rep_loss = 1.5080858147892124
2022-08-29 18:06:58,404 ***** Save model *****
2022-08-29 18:07:14,273 ***** Running evaluation *****
2022-08-29 18:07:14,274   Epoch = 1 iter 15999 step
2022-08-29 18:07:14,274   Num examples = 9815
2022-08-29 18:07:14,274   Batch size = 32
2022-08-29 18:07:14,275 ***** Eval results *****
2022-08-29 18:07:14,275   att_loss = 4.439696435572763
2022-08-29 18:07:14,275   cls_loss = 0.0
2022-08-29 18:07:14,275   global_step = 15999
2022-08-29 18:07:14,275   loss = 6.383169573519875
2022-08-29 18:07:14,276   rep_loss = 1.9434731339819953
2022-08-29 18:07:14,276 ***** Save model *****
2022-08-29 18:08:19,638 ***** Running evaluation *****
2022-08-29 18:08:19,639   Epoch = 7 iter 15199 step
2022-08-29 18:08:19,639   Num examples = 872
2022-08-29 18:08:19,639   Batch size = 32
2022-08-29 18:08:19,640 ***** Eval results *****
2022-08-29 18:08:19,640   att_loss = 1.015575381094736
2022-08-29 18:08:19,640   cls_loss = 0.0
2022-08-29 18:08:19,640   global_step = 15199
2022-08-29 18:08:19,640   loss = 2.5244340987721827
2022-08-29 18:08:19,640   rep_loss = 1.5088587164119551
2022-08-29 18:08:19,640 ***** Save model *****
2022-08-29 18:08:23,619 ***** Running evaluation *****
2022-08-29 18:08:23,619   Epoch = 1 iter 16199 step
2022-08-29 18:08:23,619   Num examples = 9815
2022-08-29 18:08:23,619   Batch size = 32
2022-08-29 18:08:23,620 ***** Eval results *****
2022-08-29 18:08:23,620   att_loss = 4.436035621190508
2022-08-29 18:08:23,621   cls_loss = 0.0
2022-08-29 18:08:23,621   global_step = 16199
2022-08-29 18:08:23,621   loss = 6.378298000017154
2022-08-29 18:08:23,621   rep_loss = 1.9422623751241175
2022-08-29 18:08:23,621 ***** Save model *****
2022-08-29 18:09:35,265 ***** Running evaluation *****
2022-08-29 18:09:35,265   Epoch = 1 iter 16399 step
2022-08-29 18:09:35,265   Num examples = 9815
2022-08-29 18:09:35,265   Batch size = 32
2022-08-29 18:09:35,266 ***** Eval results *****
2022-08-29 18:09:35,267   att_loss = 4.435822086452052
2022-08-29 18:09:35,267   cls_loss = 0.0
2022-08-29 18:09:35,267   global_step = 16399
2022-08-29 18:09:35,267   loss = 6.377241737620775
2022-08-29 18:09:35,267   rep_loss = 1.9414196478766064
2022-08-29 18:09:35,267 ***** Save model *****
2022-08-29 18:09:38,367 ***** Running evaluation *****
2022-08-29 18:09:38,367   Epoch = 7 iter 15399 step
2022-08-29 18:09:38,367   Num examples = 872
2022-08-29 18:09:38,367   Batch size = 32
2022-08-29 18:09:38,368 ***** Eval results *****
2022-08-29 18:09:38,369   att_loss = 1.0142274788523928
2022-08-29 18:09:38,369   cls_loss = 0.0
2022-08-29 18:09:38,369   global_step = 15399
2022-08-29 18:09:38,369   loss = 2.522401651220421
2022-08-29 18:09:38,369   rep_loss = 1.508174174944086
2022-08-29 18:09:38,369 ***** Save model *****
2022-08-29 18:10:46,912 ***** Running evaluation *****
2022-08-29 18:10:46,912   Epoch = 1 iter 16599 step
2022-08-29 18:10:46,912   Num examples = 9815
2022-08-29 18:10:46,912   Batch size = 32
2022-08-29 18:10:46,914 ***** Eval results *****
2022-08-29 18:10:46,914   att_loss = 4.435843863901502
2022-08-29 18:10:46,914   cls_loss = 0.0
2022-08-29 18:10:46,914   global_step = 16599
2022-08-29 18:10:46,914   loss = 6.3763477623793205
2022-08-29 18:10:46,914   rep_loss = 1.9405038951450266
2022-08-29 18:10:46,914 ***** Save model *****
2022-08-29 18:10:58,280 ***** Running evaluation *****
2022-08-29 18:10:58,280   Epoch = 7 iter 15599 step
2022-08-29 18:10:58,280   Num examples = 872
2022-08-29 18:10:58,280   Batch size = 32
2022-08-29 18:10:58,281 ***** Eval results *****
2022-08-29 18:10:58,281   att_loss = 1.0153177936233144
2022-08-29 18:10:58,281   cls_loss = 0.0
2022-08-29 18:10:58,282   global_step = 15599
2022-08-29 18:10:58,282   loss = 2.5224807136504985
2022-08-29 18:10:58,282   rep_loss = 1.507162922696049
2022-08-29 18:10:58,282 ***** Save model *****
2022-08-29 18:11:55,947 ***** Running evaluation *****
2022-08-29 18:11:55,947   Epoch = 1 iter 16799 step
2022-08-29 18:11:55,947   Num examples = 9815
2022-08-29 18:11:55,947   Batch size = 32
2022-08-29 18:11:55,948 ***** Eval results *****
2022-08-29 18:11:55,948   att_loss = 4.433719292335712
2022-08-29 18:11:55,948   cls_loss = 0.0
2022-08-29 18:11:55,949   global_step = 16799
2022-08-29 18:11:55,949   loss = 6.373178578097071
2022-08-29 18:11:55,949   rep_loss = 1.9394592825757742
2022-08-29 18:11:55,949 ***** Save model *****
2022-08-29 18:12:16,407 ***** Running evaluation *****
2022-08-29 18:12:16,407   Epoch = 7 iter 15799 step
2022-08-29 18:12:16,407   Num examples = 872
2022-08-29 18:12:16,408   Batch size = 32
2022-08-29 18:12:16,409 ***** Eval results *****
2022-08-29 18:12:16,409   att_loss = 1.013770817819875
2022-08-29 18:12:16,409   cls_loss = 0.0
2022-08-29 18:12:16,409   global_step = 15799
2022-08-29 18:12:16,409   loss = 2.519803070554546
2022-08-29 18:12:16,409   rep_loss = 1.5060322555173344
2022-08-29 18:12:16,409 ***** Save model *****
2022-08-29 18:13:04,789 ***** Running evaluation *****
2022-08-29 18:13:04,789   Epoch = 1 iter 16999 step
2022-08-29 18:13:04,789   Num examples = 9815
2022-08-29 18:13:04,789   Batch size = 32
2022-08-29 18:13:04,790 ***** Eval results *****
2022-08-29 18:13:04,790   att_loss = 4.430610874733949
2022-08-29 18:13:04,790   cls_loss = 0.0
2022-08-29 18:13:04,791   global_step = 16999
2022-08-29 18:13:04,791   loss = 6.3688341934870545
2022-08-29 18:13:04,791   rep_loss = 1.938223316105691
2022-08-29 18:13:04,791 ***** Save model *****
2022-08-29 18:13:34,325 ***** Running evaluation *****
2022-08-29 18:13:34,325   Epoch = 7 iter 15999 step
2022-08-29 18:13:34,326   Num examples = 872
2022-08-29 18:13:34,326   Batch size = 32
2022-08-29 18:13:34,327 ***** Eval results *****
2022-08-29 18:13:34,327   att_loss = 1.0107723203913421
2022-08-29 18:13:34,327   cls_loss = 0.0
2022-08-29 18:13:34,327   global_step = 15999
2022-08-29 18:13:34,327   loss = 2.5155699301853973
2022-08-29 18:13:34,327   rep_loss = 1.5047976128422866
2022-08-29 18:13:34,327 ***** Save model *****
2022-08-29 18:14:16,014 ***** Running evaluation *****
2022-08-29 18:14:16,015   Epoch = 1 iter 17199 step
2022-08-29 18:14:16,015   Num examples = 9815
2022-08-29 18:14:16,015   Batch size = 32
2022-08-29 18:14:16,016 ***** Eval results *****
2022-08-29 18:14:16,016   att_loss = 4.428065827188941
2022-08-29 18:14:16,016   cls_loss = 0.0
2022-08-29 18:14:16,016   global_step = 17199
2022-08-29 18:14:16,016   loss = 6.365303496842261
2022-08-29 18:14:16,016   rep_loss = 1.9372376667021156
2022-08-29 18:14:16,016 ***** Save model *****
2022-08-29 18:14:52,582 ***** Running evaluation *****
2022-08-29 18:14:52,583   Epoch = 7 iter 16199 step
2022-08-29 18:14:52,583   Num examples = 872
2022-08-29 18:14:52,583   Batch size = 32
2022-08-29 18:14:52,584 ***** Eval results *****
2022-08-29 18:14:52,584   att_loss = 1.0121137323142881
2022-08-29 18:14:52,584   cls_loss = 0.0
2022-08-29 18:14:52,584   global_step = 16199
2022-08-29 18:14:52,584   loss = 2.516350523782052
2022-08-29 18:14:52,584   rep_loss = 1.5042367936963537
2022-08-29 18:14:52,584 ***** Save model *****
2022-08-29 18:15:24,862 ***** Running evaluation *****
2022-08-29 18:15:24,862   Epoch = 1 iter 17399 step
2022-08-29 18:15:24,862   Num examples = 9815
2022-08-29 18:15:24,863   Batch size = 32
2022-08-29 18:15:24,864 ***** Eval results *****
2022-08-29 18:15:24,864   att_loss = 4.426600812796312
2022-08-29 18:15:24,864   cls_loss = 0.0
2022-08-29 18:15:24,864   global_step = 17399
2022-08-29 18:15:24,864   loss = 6.362837087159596
2022-08-29 18:15:24,864   rep_loss = 1.9362362716899089
2022-08-29 18:15:24,864 ***** Save model *****
2022-08-29 18:16:11,165 ***** Running evaluation *****
2022-08-29 18:16:11,165   Epoch = 7 iter 16399 step
2022-08-29 18:16:11,165   Num examples = 872
2022-08-29 18:16:11,165   Batch size = 32
2022-08-29 18:16:11,166 ***** Eval results *****
2022-08-29 18:16:11,166   att_loss = 1.009949246186257
2022-08-29 18:16:11,166   cls_loss = 0.0
2022-08-29 18:16:11,166   global_step = 16399
2022-08-29 18:16:11,166   loss = 2.513575770625235
2022-08-29 18:16:11,166   rep_loss = 1.503626526935881
2022-08-29 18:16:11,167 ***** Save model *****
2022-08-29 18:16:37,417 ***** Running evaluation *****
2022-08-29 18:16:37,417   Epoch = 1 iter 17599 step
2022-08-29 18:16:37,417   Num examples = 9815
2022-08-29 18:16:37,417   Batch size = 32
2022-08-29 18:16:37,418 ***** Eval results *****
2022-08-29 18:16:37,419   att_loss = 4.422088565545397
2022-08-29 18:16:37,419   cls_loss = 0.0
2022-08-29 18:16:37,419   global_step = 17599
2022-08-29 18:16:37,419   loss = 6.357261283082647
2022-08-29 18:16:37,419   rep_loss = 1.935172715232716
2022-08-29 18:16:37,419 ***** Save model *****
2022-08-29 18:17:29,519 ***** Running evaluation *****
2022-08-29 18:17:29,519   Epoch = 7 iter 16599 step
2022-08-29 18:17:29,519   Num examples = 872
2022-08-29 18:17:29,519   Batch size = 32
2022-08-29 18:17:29,520 ***** Eval results *****
2022-08-29 18:17:29,520   att_loss = 1.0080689450497935
2022-08-29 18:17:29,520   cls_loss = 0.0
2022-08-29 18:17:29,521   global_step = 16599
2022-08-29 18:17:29,521   loss = 2.510796433015025
2022-08-29 18:17:29,521   rep_loss = 1.5027274899403722
2022-08-29 18:17:29,521 ***** Save model *****
2022-08-29 18:17:49,838 ***** Running evaluation *****
2022-08-29 18:17:49,839   Epoch = 1 iter 17799 step
2022-08-29 18:17:49,839   Num examples = 9815
2022-08-29 18:17:49,839   Batch size = 32
2022-08-29 18:17:49,840 ***** Eval results *****
2022-08-29 18:17:49,840   att_loss = 4.419780808920108
2022-08-29 18:17:49,840   cls_loss = 0.0
2022-08-29 18:17:49,840   global_step = 17799
2022-08-29 18:17:49,840   loss = 6.354082545305989
2022-08-29 18:17:49,840   rep_loss = 1.9343017338843835
2022-08-29 18:17:49,840 ***** Save model *****
2022-08-29 18:18:47,811 ***** Running evaluation *****
2022-08-29 18:18:47,812   Epoch = 7 iter 16799 step
2022-08-29 18:18:47,812   Num examples = 872
2022-08-29 18:18:47,812   Batch size = 32
2022-08-29 18:18:47,813 ***** Eval results *****
2022-08-29 18:18:47,813   att_loss = 1.0092740407719927
2022-08-29 18:18:47,813   cls_loss = 0.0
2022-08-29 18:18:47,813   global_step = 16799
2022-08-29 18:18:47,813   loss = 2.511509715158084
2022-08-29 18:18:47,813   rep_loss = 1.502235675076826
2022-08-29 18:18:47,813 ***** Save model *****
2022-08-29 18:19:00,185 ***** Running evaluation *****
2022-08-29 18:19:00,185   Epoch = 1 iter 17999 step
2022-08-29 18:19:00,185   Num examples = 9815
2022-08-29 18:19:00,185   Batch size = 32
2022-08-29 18:19:00,187 ***** Eval results *****
2022-08-29 18:19:00,187   att_loss = 4.419503720903863
2022-08-29 18:19:00,187   cls_loss = 0.0
2022-08-29 18:19:00,187   global_step = 17999
2022-08-29 18:19:00,187   loss = 6.3529142219594075
2022-08-29 18:19:00,187   rep_loss = 1.933410498683013
2022-08-29 18:19:00,187 ***** Save model *****
2022-08-29 18:20:06,117 ***** Running evaluation *****
2022-08-29 18:20:06,117   Epoch = 8 iter 16999 step
2022-08-29 18:20:06,118   Num examples = 872
2022-08-29 18:20:06,118   Batch size = 32
2022-08-29 18:20:06,119 ***** Eval results *****
2022-08-29 18:20:06,119   att_loss = 0.992502009440325
2022-08-29 18:20:06,119   cls_loss = 0.0
2022-08-29 18:20:06,119   global_step = 16999
2022-08-29 18:20:06,119   loss = 2.479528572744952
2022-08-29 18:20:06,119   rep_loss = 1.4870265615200569
2022-08-29 18:20:06,119 ***** Save model *****
2022-08-29 18:20:12,515 ***** Running evaluation *****
2022-08-29 18:20:12,515   Epoch = 1 iter 18199 step
2022-08-29 18:20:12,515   Num examples = 9815
2022-08-29 18:20:12,516   Batch size = 32
2022-08-29 18:20:12,517 ***** Eval results *****
2022-08-29 18:20:12,517   att_loss = 4.416835533181022
2022-08-29 18:20:12,517   cls_loss = 0.0
2022-08-29 18:20:12,517   global_step = 18199
2022-08-29 18:20:12,517   loss = 6.349226618263564
2022-08-29 18:20:12,517   rep_loss = 1.9323910833531224
2022-08-29 18:20:12,517 ***** Save model *****
2022-08-29 18:21:24,057 ***** Running evaluation *****
2022-08-29 18:21:24,058   Epoch = 8 iter 17199 step
2022-08-29 18:21:24,058   Num examples = 872
2022-08-29 18:21:24,058   Batch size = 32
2022-08-29 18:21:24,059 ***** Eval results *****
2022-08-29 18:21:24,059   att_loss = 1.0055641949339198
2022-08-29 18:21:24,059   cls_loss = 0.0
2022-08-29 18:21:24,059   global_step = 17199
2022-08-29 18:21:24,059   loss = 2.4944417457164794
2022-08-29 18:21:24,059   rep_loss = 1.488877555654874
2022-08-29 18:21:24,059 ***** Save model *****
2022-08-29 18:21:24,962 ***** Running evaluation *****
2022-08-29 18:21:24,963   Epoch = 1 iter 18399 step
2022-08-29 18:21:24,963   Num examples = 9815
2022-08-29 18:21:24,963   Batch size = 32
2022-08-29 18:21:24,964 ***** Eval results *****
2022-08-29 18:21:24,965   att_loss = 4.412455021673643
2022-08-29 18:21:24,965   cls_loss = 0.0
2022-08-29 18:21:24,965   global_step = 18399
2022-08-29 18:21:24,965   loss = 6.343779472784958
2022-08-29 18:21:24,965   rep_loss = 1.9313244498274034
2022-08-29 18:21:24,965 ***** Save model *****
2022-08-29 18:22:36,960 ***** Running evaluation *****
2022-08-29 18:22:36,961   Epoch = 1 iter 18599 step
2022-08-29 18:22:36,961   Num examples = 9815
2022-08-29 18:22:36,961   Batch size = 32
2022-08-29 18:22:36,962 ***** Eval results *****
2022-08-29 18:22:36,962   att_loss = 4.411179280687976
2022-08-29 18:22:36,962   cls_loss = 0.0
2022-08-29 18:22:36,962   global_step = 18599
2022-08-29 18:22:36,962   loss = 6.341639848635562
2022-08-29 18:22:36,962   rep_loss = 1.9304605664405148
2022-08-29 18:22:36,962 ***** Save model *****
2022-08-29 18:22:44,735 ***** Running evaluation *****
2022-08-29 18:22:44,735   Epoch = 8 iter 17399 step
2022-08-29 18:22:44,735   Num examples = 872
2022-08-29 18:22:44,736   Batch size = 32
2022-08-29 18:22:44,737 ***** Eval results *****
2022-08-29 18:22:44,737   att_loss = 1.00653770245362
2022-08-29 18:22:44,737   cls_loss = 0.0
2022-08-29 18:22:44,737   global_step = 17399
2022-08-29 18:22:44,737   loss = 2.495657889300553
2022-08-29 18:22:44,737   rep_loss = 1.4891201890545127
2022-08-29 18:22:44,737 ***** Save model *****
2022-08-29 18:23:47,876 ***** Running evaluation *****
2022-08-29 18:23:47,877   Epoch = 1 iter 18799 step
2022-08-29 18:23:47,877   Num examples = 9815
2022-08-29 18:23:47,877   Batch size = 32
2022-08-29 18:23:47,878 ***** Eval results *****
2022-08-29 18:23:47,878   att_loss = 4.40778425869112
2022-08-29 18:23:47,878   cls_loss = 0.0
2022-08-29 18:23:47,879   global_step = 18799
2022-08-29 18:23:47,879   loss = 6.337174073153851
2022-08-29 18:23:47,879   rep_loss = 1.9293898130018337
2022-08-29 18:23:47,879 ***** Save model *****
2022-08-29 18:24:03,571 ***** Running evaluation *****
2022-08-29 18:24:03,572   Epoch = 8 iter 17599 step
2022-08-29 18:24:03,572   Num examples = 872
2022-08-29 18:24:03,572   Batch size = 32
2022-08-29 18:24:03,573 ***** Eval results *****
2022-08-29 18:24:03,573   att_loss = 1.000632162044381
2022-08-29 18:24:03,573   cls_loss = 0.0
2022-08-29 18:24:03,573   global_step = 17599
2022-08-29 18:24:03,573   loss = 2.4892941780164937
2022-08-29 18:24:03,573   rep_loss = 1.4886620198576825
2022-08-29 18:24:03,573 ***** Save model *****
2022-08-29 18:24:58,846 ***** Running evaluation *****
2022-08-29 18:24:58,846   Epoch = 1 iter 18999 step
2022-08-29 18:24:58,846   Num examples = 9815
2022-08-29 18:24:58,846   Batch size = 32
2022-08-29 18:24:58,848 ***** Eval results *****
2022-08-29 18:24:58,848   att_loss = 4.404970316555214
2022-08-29 18:24:58,848   cls_loss = 0.0
2022-08-29 18:24:58,848   global_step = 18999
2022-08-29 18:24:58,848   loss = 6.33338012940535
2022-08-29 18:24:58,848   rep_loss = 1.9284098114503834
2022-08-29 18:24:58,848 ***** Save model *****
2022-08-29 18:25:21,668 ***** Running evaluation *****
2022-08-29 18:25:21,669   Epoch = 8 iter 17799 step
2022-08-29 18:25:21,669   Num examples = 872
2022-08-29 18:25:21,669   Batch size = 32
2022-08-29 18:25:21,670 ***** Eval results *****
2022-08-29 18:25:21,670   att_loss = 1.004111021171558
2022-08-29 18:25:21,670   cls_loss = 0.0
2022-08-29 18:25:21,670   global_step = 17799
2022-08-29 18:25:21,670   loss = 2.4936420123604237
2022-08-29 18:25:21,670   rep_loss = 1.4895309961816023
2022-08-29 18:25:21,671 ***** Save model *****
2022-08-29 18:26:07,672 ***** Running evaluation *****
2022-08-29 18:26:07,673   Epoch = 1 iter 19199 step
2022-08-29 18:26:07,673   Num examples = 9815
2022-08-29 18:26:07,673   Batch size = 32
2022-08-29 18:26:07,674 ***** Eval results *****
2022-08-29 18:26:07,674   att_loss = 4.402291779292373
2022-08-29 18:26:07,674   cls_loss = 0.0
2022-08-29 18:26:07,674   global_step = 19199
2022-08-29 18:26:07,674   loss = 6.329796832786146
2022-08-29 18:26:07,674   rep_loss = 1.9275050528227045
2022-08-29 18:26:07,674 ***** Save model *****
2022-08-29 18:26:39,526 ***** Running evaluation *****
2022-08-29 18:26:39,526   Epoch = 8 iter 17999 step
2022-08-29 18:26:39,526   Num examples = 872
2022-08-29 18:26:39,526   Batch size = 32
2022-08-29 18:26:39,527 ***** Eval results *****
2022-08-29 18:26:39,528   att_loss = 1.0017351864644781
2022-08-29 18:26:39,528   cls_loss = 0.0
2022-08-29 18:26:39,528   global_step = 17999
2022-08-29 18:26:39,528   loss = 2.4907278736738845
2022-08-29 18:26:39,528   rep_loss = 1.4889926926744423
2022-08-29 18:26:39,528 ***** Save model *****
2022-08-29 18:27:17,985 ***** Running evaluation *****
2022-08-29 18:27:17,985   Epoch = 1 iter 19399 step
2022-08-29 18:27:17,985   Num examples = 9815
2022-08-29 18:27:17,985   Batch size = 32
2022-08-29 18:27:17,986 ***** Eval results *****
2022-08-29 18:27:17,986   att_loss = 4.399907779546432
2022-08-29 18:27:17,986   cls_loss = 0.0
2022-08-29 18:27:17,986   global_step = 19399
2022-08-29 18:27:17,986   loss = 6.326413697265214
2022-08-29 18:27:17,986   rep_loss = 1.9265059175013692
2022-08-29 18:27:17,987 ***** Save model *****
2022-08-29 18:27:57,915 ***** Running evaluation *****
2022-08-29 18:27:57,916   Epoch = 8 iter 18199 step
2022-08-29 18:27:57,916   Num examples = 872
2022-08-29 18:27:57,916   Batch size = 32
2022-08-29 18:27:57,917 ***** Eval results *****
2022-08-29 18:27:57,917   att_loss = 1.0006939228992002
2022-08-29 18:27:57,917   cls_loss = 0.0
2022-08-29 18:27:57,917   global_step = 18199
2022-08-29 18:27:57,917   loss = 2.489409845766571
2022-08-29 18:27:57,917   rep_loss = 1.488715928230481
2022-08-29 18:27:57,917 ***** Save model *****
2022-08-29 18:28:29,741 ***** Running evaluation *****
2022-08-29 18:28:29,741   Epoch = 1 iter 19599 step
2022-08-29 18:28:29,741   Num examples = 9815
2022-08-29 18:28:29,741   Batch size = 32
2022-08-29 18:28:29,742 ***** Eval results *****
2022-08-29 18:28:29,742   att_loss = 4.398220018398293
2022-08-29 18:28:29,743   cls_loss = 0.0
2022-08-29 18:28:29,743   global_step = 19599
2022-08-29 18:28:29,743   loss = 6.3237431066265275
2022-08-29 18:28:29,743   rep_loss = 1.92552308777274
2022-08-29 18:28:29,743 ***** Save model *****
2022-08-29 18:29:17,405 ***** Running evaluation *****
2022-08-29 18:29:17,405   Epoch = 8 iter 18399 step
2022-08-29 18:29:17,405   Num examples = 872
2022-08-29 18:29:17,405   Batch size = 32
2022-08-29 18:29:17,406 ***** Eval results *****
2022-08-29 18:29:17,407   att_loss = 0.9969906162972197
2022-08-29 18:29:17,407   cls_loss = 0.0
2022-08-29 18:29:17,407   global_step = 18399
2022-08-29 18:29:17,407   loss = 2.484509475838248
2022-08-29 18:29:17,407   rep_loss = 1.4875188649043052
2022-08-29 18:29:17,407 ***** Save model *****
2022-08-29 18:29:42,311 ***** Running evaluation *****
2022-08-29 18:29:42,311   Epoch = 1 iter 19799 step
2022-08-29 18:29:42,311   Num examples = 9815
2022-08-29 18:29:42,311   Batch size = 32
2022-08-29 18:29:42,312 ***** Eval results *****
2022-08-29 18:29:42,313   att_loss = 4.3967935454642735
2022-08-29 18:29:42,313   cls_loss = 0.0
2022-08-29 18:29:42,313   global_step = 19799
2022-08-29 18:29:42,313   loss = 6.321454796033509
2022-08-29 18:29:42,313   rep_loss = 1.924661250141678
2022-08-29 18:29:42,313 ***** Save model *****
2022-08-29 18:30:37,096 ***** Running evaluation *****
2022-08-29 18:30:37,097   Epoch = 8 iter 18599 step
2022-08-29 18:30:37,097   Num examples = 872
2022-08-29 18:30:37,097   Batch size = 32
2022-08-29 18:30:37,098 ***** Eval results *****
2022-08-29 18:30:37,098   att_loss = 0.9952430140520128
2022-08-29 18:30:37,098   cls_loss = 0.0
2022-08-29 18:30:37,098   global_step = 18599
2022-08-29 18:30:37,098   loss = 2.4819639817968087
2022-08-29 18:30:37,098   rep_loss = 1.486720973108202
2022-08-29 18:30:37,098 ***** Save model *****
2022-08-29 18:30:51,357 ***** Running evaluation *****
2022-08-29 18:30:51,357   Epoch = 1 iter 19999 step
2022-08-29 18:30:51,358   Num examples = 9815
2022-08-29 18:30:51,358   Batch size = 32
2022-08-29 18:30:51,359 ***** Eval results *****
2022-08-29 18:30:51,359   att_loss = 4.393087773651317
2022-08-29 18:30:51,359   cls_loss = 0.0
2022-08-29 18:30:51,359   global_step = 19999
2022-08-29 18:30:51,359   loss = 6.31669551609219
2022-08-29 18:30:51,359   rep_loss = 1.9236077420552324
2022-08-29 18:30:51,360 ***** Save model *****
2022-08-29 18:31:08,963 The args: Namespace(aug_train=False, cache_dir='', data_dir='glue_data/CoLA', data_url='', do_eval=False, do_lower_case=True, eval_batch_size=32, eval_step=200, gradient_accumulation_steps=1, hidden_act='quad', learning_rate=5e-05, log_path='tmp/distill/CoLA/quad_2quad/bert-base-uncased/log.txt', max_seq_length=128, no_cuda=False, num_train_epochs=3.0, output_dir='tmp/distill/CoLA/quad_2quad/bert-base-uncased/5e-05_1e-05_8', pred_distill=False, seed=42, softmax_act='2quad', student_model='/home/ubuntu/checkpoints/exp/CoLA', task_name='CoLA', teacher_model='/home/ubuntu/checkpoints/exp/CoLA', temperature=1.0, train_batch_size=8, warmup_proportion=0.1, weight_decay=0.0001)
2022-08-29 18:31:08,963 device: cuda n_gpu: 1
2022-08-29 18:31:09,010 The args: Namespace(aug_train=False, cache_dir='', data_dir='glue_data/SST2', data_url='', do_eval=False, do_lower_case=True, eval_batch_size=32, eval_step=200, gradient_accumulation_steps=1, hidden_act='quad', learning_rate=5e-05, log_path='tmp/distill/SST2/quad_2quad/bert-base-uncased/log.txt', max_seq_length=128, no_cuda=False, num_train_epochs=3.0, output_dir='tmp/distill/SST2/quad_2quad/bert-base-uncased/5e-05_1e-05_32', pred_distill=False, seed=42, softmax_act='2quad', student_model='/home/ubuntu/checkpoints/exp/SST2', task_name='SST2', teacher_model='/home/ubuntu/checkpoints/exp/SST2', temperature=1.0, train_batch_size=32, warmup_proportion=0.1, weight_decay=0.0001)
2022-08-29 18:31:09,010 device: cuda n_gpu: 1
2022-08-29 18:31:09,097 Writing example 0 of 8551
2022-08-29 18:31:09,097 *** Example ***
2022-08-29 18:31:09,097 guid: train-0
2022-08-29 18:31:09,097 tokens: [CLS] our friends won ' t buy this analysis , let alone the next one we propose . [SEP]
2022-08-29 18:31:09,097 input_ids: 101 2256 2814 2180 1005 1056 4965 2023 4106 1010 2292 2894 1996 2279 2028 2057 16599 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-08-29 18:31:09,098 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-08-29 18:31:09,098 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-08-29 18:31:09,098 label: 1
2022-08-29 18:31:09,098 label_id: 1
2022-08-29 18:31:10,637 Writing example 0 of 1043
2022-08-29 18:31:10,638 *** Example ***
2022-08-29 18:31:10,638 guid: dev-0
2022-08-29 18:31:10,638 tokens: [CLS] the sailors rode the breeze clear of the rocks . [SEP]
2022-08-29 18:31:10,638 input_ids: 101 1996 11279 8469 1996 9478 3154 1997 1996 5749 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-08-29 18:31:10,638 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-08-29 18:31:10,638 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-08-29 18:31:10,638 label: 1
2022-08-29 18:31:10,638 label_id: 1
2022-08-29 18:31:10,811 Model config {
  "_name_or_path": "bert-base-uncased",
  "architectures": [
    "BertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "crypten": false,
  "finetuning_task": "cola",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "unacceptable",
    "1": "acceptable"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "acceptable": 1,
    "unacceptable": 0
  },
  "layer_norm_eps": 1e-12,
  "log_path": "tmp/distill/CoLA/quad_2quad/bert-base-uncased/log.txt",
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "pre_trained": "",
  "problem_type": "single_label_classification",
  "softmax_act": "softmax",
  "torch_dtype": "float32",
  "training": "",
  "transformers_version": "4.20.0.dev0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

2022-08-29 18:31:10,934 The args: Namespace(aug_train=False, cache_dir='', data_dir='glue_data/SST2', data_url='', do_eval=False, do_lower_case=True, eval_batch_size=32, eval_step=100, gradient_accumulation_steps=1, hidden_act='quad', learning_rate=1e-05, log_path='tmp/distill/SST2/quad_2quad/bert-base-uncased/log.txt', max_seq_length=128, no_cuda=False, num_train_epochs=5.0, output_dir='tmp/distill/SST2/quad_2quad/bert-base-uncased/5e-05_1e-05_32_stage2', pred_distill=True, seed=42, softmax_act='2quad', student_model='tmp/distill/SST2/quad_2quad/bert-base-uncased/5e-05_1e-05_32', task_name='SST2', teacher_model='/home/ubuntu/checkpoints/exp/SST2', temperature=1.0, train_batch_size=32, warmup_proportion=0.1, weight_decay=0.0001)
2022-08-29 18:31:10,935 device: cuda n_gpu: 1
2022-08-29 18:31:11,303 Writing example 0 of 67349
2022-08-29 18:31:11,303 *** Example ***
2022-08-29 18:31:11,303 guid: train-1
2022-08-29 18:31:11,303 tokens: [CLS] hide new secret ##ions from the parental units [SEP]
2022-08-29 18:31:11,303 input_ids: 101 5342 2047 3595 8496 2013 1996 18643 3197 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-08-29 18:31:11,303 input_mask: 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-08-29 18:31:11,304 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-08-29 18:31:11,304 label: 0
2022-08-29 18:31:11,304 label_id: 0
2022-08-29 18:31:13,320 Writing example 10000 of 67349
2022-08-29 18:31:13,514 Loading model /home/ubuntu/checkpoints/exp/CoLA/pytorch_model.bin
2022-08-29 18:31:13,836 loading model...
2022-08-29 18:31:13,889 done!
2022-08-29 18:31:13,889 Weights from pretrained model not used in TinyBertForSequenceClassification: ['bert.embeddings.position_ids']
2022-08-29 18:31:15,401 Writing example 20000 of 67349
2022-08-29 18:31:17,291 Writing example 30000 of 67349
2022-08-29 18:31:19,401 Writing example 40000 of 67349
2022-08-29 18:31:20,099 ***** Teacher evaluation *****
2022-08-29 18:31:20,100 {'mcc': 0.5778590180299453, 'eval_loss': 0.5587321456070199}
2022-08-29 18:31:20,100 Model config {
  "_name_or_path": "bert-base-uncased",
  "architectures": [
    "BertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "crypten": false,
  "finetuning_task": "cola",
  "gradient_checkpointing": false,
  "hidden_act": "quad",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "unacceptable",
    "1": "acceptable"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "acceptable": 1,
    "unacceptable": 0
  },
  "layer_norm_eps": 1e-12,
  "log_path": "tmp/distill/CoLA/quad_2quad/bert-base-uncased/log.txt",
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "pre_trained": "",
  "problem_type": "single_label_classification",
  "softmax_act": "2quad",
  "torch_dtype": "float32",
  "training": "",
  "transformers_version": "4.20.0.dev0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

2022-08-29 18:31:21,573 Writing example 50000 of 67349
2022-08-29 18:31:22,741 Loading model /home/ubuntu/checkpoints/exp/CoLA/pytorch_model.bin
2022-08-29 18:31:23,050 loading model...
2022-08-29 18:31:23,095 done!
2022-08-29 18:31:23,095 Weights from pretrained model not used in TinyBertForSequenceClassification: ['bert.embeddings.position_ids']
2022-08-29 18:31:23,173 ***** Running training *****
2022-08-29 18:31:23,173   Num examples = 8551
2022-08-29 18:31:23,173   Batch size = 8
2022-08-29 18:31:23,173   Num steps = 53400
2022-08-29 18:31:23,174 n: bert.embeddings.word_embeddings.weight
2022-08-29 18:31:23,174 n: bert.embeddings.position_embeddings.weight
2022-08-29 18:31:23,174 n: bert.embeddings.token_type_embeddings.weight
2022-08-29 18:31:23,174 n: bert.embeddings.LayerNorm.weight
2022-08-29 18:31:23,175 n: bert.embeddings.LayerNorm.bias
2022-08-29 18:31:23,175 n: bert.encoder.layer.0.attention.self.query.weight
2022-08-29 18:31:23,175 n: bert.encoder.layer.0.attention.self.query.bias
2022-08-29 18:31:23,175 n: bert.encoder.layer.0.attention.self.key.weight
2022-08-29 18:31:23,175 n: bert.encoder.layer.0.attention.self.key.bias
2022-08-29 18:31:23,175 n: bert.encoder.layer.0.attention.self.value.weight
2022-08-29 18:31:23,175 n: bert.encoder.layer.0.attention.self.value.bias
2022-08-29 18:31:23,175 n: bert.encoder.layer.0.attention.output.dense.weight
2022-08-29 18:31:23,175 n: bert.encoder.layer.0.attention.output.dense.bias
2022-08-29 18:31:23,175 n: bert.encoder.layer.0.attention.output.LayerNorm.weight
2022-08-29 18:31:23,175 n: bert.encoder.layer.0.attention.output.LayerNorm.bias
2022-08-29 18:31:23,175 n: bert.encoder.layer.0.intermediate.dense.weight
2022-08-29 18:31:23,175 n: bert.encoder.layer.0.intermediate.dense.bias
2022-08-29 18:31:23,175 n: bert.encoder.layer.0.output.dense.weight
2022-08-29 18:31:23,175 n: bert.encoder.layer.0.output.dense.bias
2022-08-29 18:31:23,175 n: bert.encoder.layer.0.output.LayerNorm.weight
2022-08-29 18:31:23,175 n: bert.encoder.layer.0.output.LayerNorm.bias
2022-08-29 18:31:23,175 n: bert.encoder.layer.1.attention.self.query.weight
2022-08-29 18:31:23,176 n: bert.encoder.layer.1.attention.self.query.bias
2022-08-29 18:31:23,176 n: bert.encoder.layer.1.attention.self.key.weight
2022-08-29 18:31:23,176 n: bert.encoder.layer.1.attention.self.key.bias
2022-08-29 18:31:23,176 n: bert.encoder.layer.1.attention.self.value.weight
2022-08-29 18:31:23,176 n: bert.encoder.layer.1.attention.self.value.bias
2022-08-29 18:31:23,176 n: bert.encoder.layer.1.attention.output.dense.weight
2022-08-29 18:31:23,176 n: bert.encoder.layer.1.attention.output.dense.bias
2022-08-29 18:31:23,176 n: bert.encoder.layer.1.attention.output.LayerNorm.weight
2022-08-29 18:31:23,176 n: bert.encoder.layer.1.attention.output.LayerNorm.bias
2022-08-29 18:31:23,176 n: bert.encoder.layer.1.intermediate.dense.weight
2022-08-29 18:31:23,176 n: bert.encoder.layer.1.intermediate.dense.bias
2022-08-29 18:31:23,176 n: bert.encoder.layer.1.output.dense.weight
2022-08-29 18:31:23,176 n: bert.encoder.layer.1.output.dense.bias
2022-08-29 18:31:23,176 n: bert.encoder.layer.1.output.LayerNorm.weight
2022-08-29 18:31:23,176 n: bert.encoder.layer.1.output.LayerNorm.bias
2022-08-29 18:31:23,176 n: bert.encoder.layer.2.attention.self.query.weight
2022-08-29 18:31:23,176 n: bert.encoder.layer.2.attention.self.query.bias
2022-08-29 18:31:23,176 n: bert.encoder.layer.2.attention.self.key.weight
2022-08-29 18:31:23,176 n: bert.encoder.layer.2.attention.self.key.bias
2022-08-29 18:31:23,177 n: bert.encoder.layer.2.attention.self.value.weight
2022-08-29 18:31:23,177 n: bert.encoder.layer.2.attention.self.value.bias
2022-08-29 18:31:23,177 n: bert.encoder.layer.2.attention.output.dense.weight
2022-08-29 18:31:23,177 n: bert.encoder.layer.2.attention.output.dense.bias
2022-08-29 18:31:23,177 n: bert.encoder.layer.2.attention.output.LayerNorm.weight
2022-08-29 18:31:23,177 n: bert.encoder.layer.2.attention.output.LayerNorm.bias
2022-08-29 18:31:23,177 n: bert.encoder.layer.2.intermediate.dense.weight
2022-08-29 18:31:23,177 n: bert.encoder.layer.2.intermediate.dense.bias
2022-08-29 18:31:23,177 n: bert.encoder.layer.2.output.dense.weight
2022-08-29 18:31:23,177 n: bert.encoder.layer.2.output.dense.bias
2022-08-29 18:31:23,177 n: bert.encoder.layer.2.output.LayerNorm.weight
2022-08-29 18:31:23,177 n: bert.encoder.layer.2.output.LayerNorm.bias
2022-08-29 18:31:23,177 n: bert.encoder.layer.3.attention.self.query.weight
2022-08-29 18:31:23,177 n: bert.encoder.layer.3.attention.self.query.bias
2022-08-29 18:31:23,177 n: bert.encoder.layer.3.attention.self.key.weight
2022-08-29 18:31:23,177 n: bert.encoder.layer.3.attention.self.key.bias
2022-08-29 18:31:23,177 n: bert.encoder.layer.3.attention.self.value.weight
2022-08-29 18:31:23,177 n: bert.encoder.layer.3.attention.self.value.bias
2022-08-29 18:31:23,178 n: bert.encoder.layer.3.attention.output.dense.weight
2022-08-29 18:31:23,178 n: bert.encoder.layer.3.attention.output.dense.bias
2022-08-29 18:31:23,178 n: bert.encoder.layer.3.attention.output.LayerNorm.weight
2022-08-29 18:31:23,178 n: bert.encoder.layer.3.attention.output.LayerNorm.bias
2022-08-29 18:31:23,178 n: bert.encoder.layer.3.intermediate.dense.weight
2022-08-29 18:31:23,178 n: bert.encoder.layer.3.intermediate.dense.bias
2022-08-29 18:31:23,178 n: bert.encoder.layer.3.output.dense.weight
2022-08-29 18:31:23,178 n: bert.encoder.layer.3.output.dense.bias
2022-08-29 18:31:23,178 n: bert.encoder.layer.3.output.LayerNorm.weight
2022-08-29 18:31:23,178 n: bert.encoder.layer.3.output.LayerNorm.bias
2022-08-29 18:31:23,178 n: bert.encoder.layer.4.attention.self.query.weight
2022-08-29 18:31:23,178 n: bert.encoder.layer.4.attention.self.query.bias
2022-08-29 18:31:23,178 n: bert.encoder.layer.4.attention.self.key.weight
2022-08-29 18:31:23,178 n: bert.encoder.layer.4.attention.self.key.bias
2022-08-29 18:31:23,178 n: bert.encoder.layer.4.attention.self.value.weight
2022-08-29 18:31:23,178 n: bert.encoder.layer.4.attention.self.value.bias
2022-08-29 18:31:23,178 n: bert.encoder.layer.4.attention.output.dense.weight
2022-08-29 18:31:23,179 n: bert.encoder.layer.4.attention.output.dense.bias
2022-08-29 18:31:23,179 n: bert.encoder.layer.4.attention.output.LayerNorm.weight
2022-08-29 18:31:23,179 n: bert.encoder.layer.4.attention.output.LayerNorm.bias
2022-08-29 18:31:23,179 n: bert.encoder.layer.4.intermediate.dense.weight
2022-08-29 18:31:23,179 n: bert.encoder.layer.4.intermediate.dense.bias
2022-08-29 18:31:23,179 n: bert.encoder.layer.4.output.dense.weight
2022-08-29 18:31:23,179 n: bert.encoder.layer.4.output.dense.bias
2022-08-29 18:31:23,179 n: bert.encoder.layer.4.output.LayerNorm.weight
2022-08-29 18:31:23,179 n: bert.encoder.layer.4.output.LayerNorm.bias
2022-08-29 18:31:23,179 n: bert.encoder.layer.5.attention.self.query.weight
2022-08-29 18:31:23,179 n: bert.encoder.layer.5.attention.self.query.bias
2022-08-29 18:31:23,179 n: bert.encoder.layer.5.attention.self.key.weight
2022-08-29 18:31:23,179 n: bert.encoder.layer.5.attention.self.key.bias
2022-08-29 18:31:23,179 n: bert.encoder.layer.5.attention.self.value.weight
2022-08-29 18:31:23,179 n: bert.encoder.layer.5.attention.self.value.bias
2022-08-29 18:31:23,179 n: bert.encoder.layer.5.attention.output.dense.weight
2022-08-29 18:31:23,179 n: bert.encoder.layer.5.attention.output.dense.bias
2022-08-29 18:31:23,179 n: bert.encoder.layer.5.attention.output.LayerNorm.weight
2022-08-29 18:31:23,179 n: bert.encoder.layer.5.attention.output.LayerNorm.bias
2022-08-29 18:31:23,180 n: bert.encoder.layer.5.intermediate.dense.weight
2022-08-29 18:31:23,180 n: bert.encoder.layer.5.intermediate.dense.bias
2022-08-29 18:31:23,180 n: bert.encoder.layer.5.output.dense.weight
2022-08-29 18:31:23,180 n: bert.encoder.layer.5.output.dense.bias
2022-08-29 18:31:23,180 n: bert.encoder.layer.5.output.LayerNorm.weight
2022-08-29 18:31:23,180 n: bert.encoder.layer.5.output.LayerNorm.bias
2022-08-29 18:31:23,180 n: bert.encoder.layer.6.attention.self.query.weight
2022-08-29 18:31:23,180 n: bert.encoder.layer.6.attention.self.query.bias
2022-08-29 18:31:23,180 n: bert.encoder.layer.6.attention.self.key.weight
2022-08-29 18:31:23,180 n: bert.encoder.layer.6.attention.self.key.bias
2022-08-29 18:31:23,180 n: bert.encoder.layer.6.attention.self.value.weight
2022-08-29 18:31:23,180 n: bert.encoder.layer.6.attention.self.value.bias
2022-08-29 18:31:23,180 n: bert.encoder.layer.6.attention.output.dense.weight
2022-08-29 18:31:23,180 n: bert.encoder.layer.6.attention.output.dense.bias
2022-08-29 18:31:23,180 n: bert.encoder.layer.6.attention.output.LayerNorm.weight
2022-08-29 18:31:23,180 n: bert.encoder.layer.6.attention.output.LayerNorm.bias
2022-08-29 18:31:23,180 n: bert.encoder.layer.6.intermediate.dense.weight
2022-08-29 18:31:23,180 n: bert.encoder.layer.6.intermediate.dense.bias
2022-08-29 18:31:23,181 n: bert.encoder.layer.6.output.dense.weight
2022-08-29 18:31:23,181 n: bert.encoder.layer.6.output.dense.bias
2022-08-29 18:31:23,181 n: bert.encoder.layer.6.output.LayerNorm.weight
2022-08-29 18:31:23,181 n: bert.encoder.layer.6.output.LayerNorm.bias
2022-08-29 18:31:23,181 n: bert.encoder.layer.7.attention.self.query.weight
2022-08-29 18:31:23,181 n: bert.encoder.layer.7.attention.self.query.bias
2022-08-29 18:31:23,181 n: bert.encoder.layer.7.attention.self.key.weight
2022-08-29 18:31:23,181 n: bert.encoder.layer.7.attention.self.key.bias
2022-08-29 18:31:23,181 n: bert.encoder.layer.7.attention.self.value.weight
2022-08-29 18:31:23,181 n: bert.encoder.layer.7.attention.self.value.bias
2022-08-29 18:31:23,181 n: bert.encoder.layer.7.attention.output.dense.weight
2022-08-29 18:31:23,181 n: bert.encoder.layer.7.attention.output.dense.bias
2022-08-29 18:31:23,181 n: bert.encoder.layer.7.attention.output.LayerNorm.weight
2022-08-29 18:31:23,181 n: bert.encoder.layer.7.attention.output.LayerNorm.bias
2022-08-29 18:31:23,181 n: bert.encoder.layer.7.intermediate.dense.weight
2022-08-29 18:31:23,181 n: bert.encoder.layer.7.intermediate.dense.bias
2022-08-29 18:31:23,181 n: bert.encoder.layer.7.output.dense.weight
2022-08-29 18:31:23,181 n: bert.encoder.layer.7.output.dense.bias
2022-08-29 18:31:23,181 n: bert.encoder.layer.7.output.LayerNorm.weight
2022-08-29 18:31:23,182 n: bert.encoder.layer.7.output.LayerNorm.bias
2022-08-29 18:31:23,182 n: bert.encoder.layer.8.attention.self.query.weight
2022-08-29 18:31:23,182 n: bert.encoder.layer.8.attention.self.query.bias
2022-08-29 18:31:23,182 n: bert.encoder.layer.8.attention.self.key.weight
2022-08-29 18:31:23,182 n: bert.encoder.layer.8.attention.self.key.bias
2022-08-29 18:31:23,182 n: bert.encoder.layer.8.attention.self.value.weight
2022-08-29 18:31:23,182 n: bert.encoder.layer.8.attention.self.value.bias
2022-08-29 18:31:23,182 n: bert.encoder.layer.8.attention.output.dense.weight
2022-08-29 18:31:23,182 n: bert.encoder.layer.8.attention.output.dense.bias
2022-08-29 18:31:23,182 n: bert.encoder.layer.8.attention.output.LayerNorm.weight
2022-08-29 18:31:23,182 n: bert.encoder.layer.8.attention.output.LayerNorm.bias
2022-08-29 18:31:23,182 n: bert.encoder.layer.8.intermediate.dense.weight
2022-08-29 18:31:23,182 n: bert.encoder.layer.8.intermediate.dense.bias
2022-08-29 18:31:23,182 n: bert.encoder.layer.8.output.dense.weight
2022-08-29 18:31:23,182 n: bert.encoder.layer.8.output.dense.bias
2022-08-29 18:31:23,182 n: bert.encoder.layer.8.output.LayerNorm.weight
2022-08-29 18:31:23,182 n: bert.encoder.layer.8.output.LayerNorm.bias
2022-08-29 18:31:23,182 n: bert.encoder.layer.9.attention.self.query.weight
2022-08-29 18:31:23,183 n: bert.encoder.layer.9.attention.self.query.bias
2022-08-29 18:31:23,183 n: bert.encoder.layer.9.attention.self.key.weight
2022-08-29 18:31:23,183 n: bert.encoder.layer.9.attention.self.key.bias
2022-08-29 18:31:23,183 n: bert.encoder.layer.9.attention.self.value.weight
2022-08-29 18:31:23,183 n: bert.encoder.layer.9.attention.self.value.bias
2022-08-29 18:31:23,183 n: bert.encoder.layer.9.attention.output.dense.weight
2022-08-29 18:31:23,183 n: bert.encoder.layer.9.attention.output.dense.bias
2022-08-29 18:31:23,183 n: bert.encoder.layer.9.attention.output.LayerNorm.weight
2022-08-29 18:31:23,183 n: bert.encoder.layer.9.attention.output.LayerNorm.bias
2022-08-29 18:31:23,183 n: bert.encoder.layer.9.intermediate.dense.weight
2022-08-29 18:31:23,183 n: bert.encoder.layer.9.intermediate.dense.bias
2022-08-29 18:31:23,183 n: bert.encoder.layer.9.output.dense.weight
2022-08-29 18:31:23,183 n: bert.encoder.layer.9.output.dense.bias
2022-08-29 18:31:23,183 n: bert.encoder.layer.9.output.LayerNorm.weight
2022-08-29 18:31:23,183 n: bert.encoder.layer.9.output.LayerNorm.bias
2022-08-29 18:31:23,183 n: bert.encoder.layer.10.attention.self.query.weight
2022-08-29 18:31:23,183 n: bert.encoder.layer.10.attention.self.query.bias
2022-08-29 18:31:23,183 n: bert.encoder.layer.10.attention.self.key.weight
2022-08-29 18:31:23,184 n: bert.encoder.layer.10.attention.self.key.bias
2022-08-29 18:31:23,184 n: bert.encoder.layer.10.attention.self.value.weight
2022-08-29 18:31:23,184 n: bert.encoder.layer.10.attention.self.value.bias
2022-08-29 18:31:23,184 n: bert.encoder.layer.10.attention.output.dense.weight
2022-08-29 18:31:23,184 n: bert.encoder.layer.10.attention.output.dense.bias
2022-08-29 18:31:23,184 n: bert.encoder.layer.10.attention.output.LayerNorm.weight
2022-08-29 18:31:23,184 n: bert.encoder.layer.10.attention.output.LayerNorm.bias
2022-08-29 18:31:23,184 n: bert.encoder.layer.10.intermediate.dense.weight
2022-08-29 18:31:23,184 n: bert.encoder.layer.10.intermediate.dense.bias
2022-08-29 18:31:23,184 n: bert.encoder.layer.10.output.dense.weight
2022-08-29 18:31:23,184 n: bert.encoder.layer.10.output.dense.bias
2022-08-29 18:31:23,184 n: bert.encoder.layer.10.output.LayerNorm.weight
2022-08-29 18:31:23,184 n: bert.encoder.layer.10.output.LayerNorm.bias
2022-08-29 18:31:23,184 n: bert.encoder.layer.11.attention.self.query.weight
2022-08-29 18:31:23,184 n: bert.encoder.layer.11.attention.self.query.bias
2022-08-29 18:31:23,184 n: bert.encoder.layer.11.attention.self.key.weight
2022-08-29 18:31:23,184 n: bert.encoder.layer.11.attention.self.key.bias
2022-08-29 18:31:23,184 n: bert.encoder.layer.11.attention.self.value.weight
2022-08-29 18:31:23,184 n: bert.encoder.layer.11.attention.self.value.bias
2022-08-29 18:31:23,185 n: bert.encoder.layer.11.attention.output.dense.weight
2022-08-29 18:31:23,185 n: bert.encoder.layer.11.attention.output.dense.bias
2022-08-29 18:31:23,185 n: bert.encoder.layer.11.attention.output.LayerNorm.weight
2022-08-29 18:31:23,185 n: bert.encoder.layer.11.attention.output.LayerNorm.bias
2022-08-29 18:31:23,185 n: bert.encoder.layer.11.intermediate.dense.weight
2022-08-29 18:31:23,185 n: bert.encoder.layer.11.intermediate.dense.bias
2022-08-29 18:31:23,185 n: bert.encoder.layer.11.output.dense.weight
2022-08-29 18:31:23,185 n: bert.encoder.layer.11.output.dense.bias
2022-08-29 18:31:23,185 n: bert.encoder.layer.11.output.LayerNorm.weight
2022-08-29 18:31:23,185 n: bert.encoder.layer.11.output.LayerNorm.bias
2022-08-29 18:31:23,185 n: bert.pooler.dense.weight
2022-08-29 18:31:23,185 n: bert.pooler.dense.bias
2022-08-29 18:31:23,185 n: classifier.weight
2022-08-29 18:31:23,185 n: classifier.bias
2022-08-29 18:31:23,185 Total parameters: 109483778
2022-08-29 18:31:23,504 Writing example 60000 of 67349
2022-08-29 18:31:25,506 Writing example 0 of 872
2022-08-29 18:31:25,506 *** Example ***
2022-08-29 18:31:25,507 guid: dev-1
2022-08-29 18:31:25,507 tokens: [CLS] it ' s a charming and often affecting journey . [SEP]
2022-08-29 18:31:25,507 input_ids: 101 2009 1005 1055 1037 11951 1998 2411 12473 4990 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-08-29 18:31:25,507 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-08-29 18:31:25,507 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-08-29 18:31:25,507 label: 1
2022-08-29 18:31:25,507 label_id: 1
2022-08-29 18:31:25,826 Model config {
  "_name_or_path": "bert-base-uncased",
  "architectures": [
    "BertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "crypten": false,
  "finetuning_task": "sst2",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "negative",
    "1": "positive"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "negative": 0,
    "positive": 1
  },
  "layer_norm_eps": 1e-12,
  "log_path": "tmp/distill/SST2/quad_2quad/bert-base-uncased/log.txt",
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "pre_trained": "",
  "problem_type": "single_label_classification",
  "softmax_act": "softmax",
  "torch_dtype": "float32",
  "training": "",
  "transformers_version": "4.20.0.dev0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

2022-08-29 18:31:28,481 Loading model /home/ubuntu/checkpoints/exp/SST2/pytorch_model.bin
2022-08-29 18:31:28,783 loading model...
2022-08-29 18:31:28,833 done!
2022-08-29 18:31:28,833 Weights from pretrained model not used in TinyBertForSequenceClassification: ['bert.embeddings.position_ids']
2022-08-29 18:31:34,793 ***** Teacher evaluation *****
2022-08-29 18:31:34,793 {'acc': 0.9311926605504587, 'eval_loss': 0.2749780447089246}
2022-08-29 18:31:34,794 Model config {
  "_name_or_path": "bert-base-uncased",
  "architectures": [
    "BertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "crypten": false,
  "finetuning_task": "sst2",
  "gradient_checkpointing": false,
  "hidden_act": "quad",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "negative",
    "1": "positive"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "negative": 0,
    "positive": 1
  },
  "layer_norm_eps": 1e-12,
  "log_path": "tmp/distill/SST2/quad_2quad/bert-base-uncased/log.txt",
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "pre_trained": "",
  "problem_type": "single_label_classification",
  "softmax_act": "2quad",
  "torch_dtype": "float32",
  "training": "",
  "transformers_version": "4.20.0.dev0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

2022-08-29 18:31:37,441 Loading model tmp/distill/SST2/quad_2quad/bert-base-uncased/5e-05_1e-05_32/pytorch_model.bin
2022-08-29 18:31:37,729 loading model...
2022-08-29 18:31:37,775 done!
2022-08-29 18:31:37,857 ***** Running training *****
2022-08-29 18:31:37,857   Num examples = 67349
2022-08-29 18:31:37,857   Batch size = 32
2022-08-29 18:31:37,857   Num steps = 10520
2022-08-29 18:31:37,858 n: bert.embeddings.word_embeddings.weight
2022-08-29 18:31:37,858 n: bert.embeddings.position_embeddings.weight
2022-08-29 18:31:37,858 n: bert.embeddings.token_type_embeddings.weight
2022-08-29 18:31:37,859 n: bert.embeddings.LayerNorm.weight
2022-08-29 18:31:37,859 n: bert.embeddings.LayerNorm.bias
2022-08-29 18:31:37,859 n: bert.encoder.layer.0.attention.self.query.weight
2022-08-29 18:31:37,859 n: bert.encoder.layer.0.attention.self.query.bias
2022-08-29 18:31:37,859 n: bert.encoder.layer.0.attention.self.key.weight
2022-08-29 18:31:37,859 n: bert.encoder.layer.0.attention.self.key.bias
2022-08-29 18:31:37,859 n: bert.encoder.layer.0.attention.self.value.weight
2022-08-29 18:31:37,859 n: bert.encoder.layer.0.attention.self.value.bias
2022-08-29 18:31:37,859 n: bert.encoder.layer.0.attention.output.dense.weight
2022-08-29 18:31:37,859 n: bert.encoder.layer.0.attention.output.dense.bias
2022-08-29 18:31:37,859 n: bert.encoder.layer.0.attention.output.LayerNorm.weight
2022-08-29 18:31:37,859 n: bert.encoder.layer.0.attention.output.LayerNorm.bias
2022-08-29 18:31:37,859 n: bert.encoder.layer.0.intermediate.dense.weight
2022-08-29 18:31:37,859 n: bert.encoder.layer.0.intermediate.dense.bias
2022-08-29 18:31:37,859 n: bert.encoder.layer.0.output.dense.weight
2022-08-29 18:31:37,859 n: bert.encoder.layer.0.output.dense.bias
2022-08-29 18:31:37,859 n: bert.encoder.layer.0.output.LayerNorm.weight
2022-08-29 18:31:37,859 n: bert.encoder.layer.0.output.LayerNorm.bias
2022-08-29 18:31:37,860 n: bert.encoder.layer.1.attention.self.query.weight
2022-08-29 18:31:37,860 n: bert.encoder.layer.1.attention.self.query.bias
2022-08-29 18:31:37,860 n: bert.encoder.layer.1.attention.self.key.weight
2022-08-29 18:31:37,860 n: bert.encoder.layer.1.attention.self.key.bias
2022-08-29 18:31:37,860 n: bert.encoder.layer.1.attention.self.value.weight
2022-08-29 18:31:37,860 n: bert.encoder.layer.1.attention.self.value.bias
2022-08-29 18:31:37,860 n: bert.encoder.layer.1.attention.output.dense.weight
2022-08-29 18:31:37,860 n: bert.encoder.layer.1.attention.output.dense.bias
2022-08-29 18:31:37,860 n: bert.encoder.layer.1.attention.output.LayerNorm.weight
2022-08-29 18:31:37,860 n: bert.encoder.layer.1.attention.output.LayerNorm.bias
2022-08-29 18:31:37,860 n: bert.encoder.layer.1.intermediate.dense.weight
2022-08-29 18:31:37,860 n: bert.encoder.layer.1.intermediate.dense.bias
2022-08-29 18:31:37,860 n: bert.encoder.layer.1.output.dense.weight
2022-08-29 18:31:37,860 n: bert.encoder.layer.1.output.dense.bias
2022-08-29 18:31:37,860 n: bert.encoder.layer.1.output.LayerNorm.weight
2022-08-29 18:31:37,860 n: bert.encoder.layer.1.output.LayerNorm.bias
2022-08-29 18:31:37,860 n: bert.encoder.layer.2.attention.self.query.weight
2022-08-29 18:31:37,860 n: bert.encoder.layer.2.attention.self.query.bias
2022-08-29 18:31:37,861 n: bert.encoder.layer.2.attention.self.key.weight
2022-08-29 18:31:37,861 n: bert.encoder.layer.2.attention.self.key.bias
2022-08-29 18:31:37,861 n: bert.encoder.layer.2.attention.self.value.weight
2022-08-29 18:31:37,861 n: bert.encoder.layer.2.attention.self.value.bias
2022-08-29 18:31:37,861 n: bert.encoder.layer.2.attention.output.dense.weight
2022-08-29 18:31:37,861 n: bert.encoder.layer.2.attention.output.dense.bias
2022-08-29 18:31:37,861 n: bert.encoder.layer.2.attention.output.LayerNorm.weight
2022-08-29 18:31:37,861 n: bert.encoder.layer.2.attention.output.LayerNorm.bias
2022-08-29 18:31:37,861 n: bert.encoder.layer.2.intermediate.dense.weight
2022-08-29 18:31:37,861 n: bert.encoder.layer.2.intermediate.dense.bias
2022-08-29 18:31:37,861 n: bert.encoder.layer.2.output.dense.weight
2022-08-29 18:31:37,861 n: bert.encoder.layer.2.output.dense.bias
2022-08-29 18:31:37,861 n: bert.encoder.layer.2.output.LayerNorm.weight
2022-08-29 18:31:37,861 n: bert.encoder.layer.2.output.LayerNorm.bias
2022-08-29 18:31:37,861 n: bert.encoder.layer.3.attention.self.query.weight
2022-08-29 18:31:37,861 n: bert.encoder.layer.3.attention.self.query.bias
2022-08-29 18:31:37,861 n: bert.encoder.layer.3.attention.self.key.weight
2022-08-29 18:31:37,861 n: bert.encoder.layer.3.attention.self.key.bias
2022-08-29 18:31:37,861 n: bert.encoder.layer.3.attention.self.value.weight
2022-08-29 18:31:37,862 n: bert.encoder.layer.3.attention.self.value.bias
2022-08-29 18:31:37,862 n: bert.encoder.layer.3.attention.output.dense.weight
2022-08-29 18:31:37,862 n: bert.encoder.layer.3.attention.output.dense.bias
2022-08-29 18:31:37,862 n: bert.encoder.layer.3.attention.output.LayerNorm.weight
2022-08-29 18:31:37,862 n: bert.encoder.layer.3.attention.output.LayerNorm.bias
2022-08-29 18:31:37,862 n: bert.encoder.layer.3.intermediate.dense.weight
2022-08-29 18:31:37,862 n: bert.encoder.layer.3.intermediate.dense.bias
2022-08-29 18:31:37,862 n: bert.encoder.layer.3.output.dense.weight
2022-08-29 18:31:37,862 n: bert.encoder.layer.3.output.dense.bias
2022-08-29 18:31:37,862 n: bert.encoder.layer.3.output.LayerNorm.weight
2022-08-29 18:31:37,862 n: bert.encoder.layer.3.output.LayerNorm.bias
2022-08-29 18:31:37,862 n: bert.encoder.layer.4.attention.self.query.weight
2022-08-29 18:31:37,862 n: bert.encoder.layer.4.attention.self.query.bias
2022-08-29 18:31:37,862 n: bert.encoder.layer.4.attention.self.key.weight
2022-08-29 18:31:37,862 n: bert.encoder.layer.4.attention.self.key.bias
2022-08-29 18:31:37,862 n: bert.encoder.layer.4.attention.self.value.weight
2022-08-29 18:31:37,862 n: bert.encoder.layer.4.attention.self.value.bias
2022-08-29 18:31:37,862 n: bert.encoder.layer.4.attention.output.dense.weight
2022-08-29 18:31:37,863 n: bert.encoder.layer.4.attention.output.dense.bias
2022-08-29 18:31:37,863 n: bert.encoder.layer.4.attention.output.LayerNorm.weight
2022-08-29 18:31:37,863 n: bert.encoder.layer.4.attention.output.LayerNorm.bias
2022-08-29 18:31:37,863 n: bert.encoder.layer.4.intermediate.dense.weight
2022-08-29 18:31:37,863 n: bert.encoder.layer.4.intermediate.dense.bias
2022-08-29 18:31:37,863 n: bert.encoder.layer.4.output.dense.weight
2022-08-29 18:31:37,863 n: bert.encoder.layer.4.output.dense.bias
2022-08-29 18:31:37,863 n: bert.encoder.layer.4.output.LayerNorm.weight
2022-08-29 18:31:37,863 n: bert.encoder.layer.4.output.LayerNorm.bias
2022-08-29 18:31:37,863 n: bert.encoder.layer.5.attention.self.query.weight
2022-08-29 18:31:37,863 n: bert.encoder.layer.5.attention.self.query.bias
2022-08-29 18:31:37,863 n: bert.encoder.layer.5.attention.self.key.weight
2022-08-29 18:31:37,863 n: bert.encoder.layer.5.attention.self.key.bias
2022-08-29 18:31:37,863 n: bert.encoder.layer.5.attention.self.value.weight
2022-08-29 18:31:37,863 n: bert.encoder.layer.5.attention.self.value.bias
2022-08-29 18:31:37,863 n: bert.encoder.layer.5.attention.output.dense.weight
2022-08-29 18:31:37,863 n: bert.encoder.layer.5.attention.output.dense.bias
2022-08-29 18:31:37,863 n: bert.encoder.layer.5.attention.output.LayerNorm.weight
2022-08-29 18:31:37,863 n: bert.encoder.layer.5.attention.output.LayerNorm.bias
2022-08-29 18:31:37,864 n: bert.encoder.layer.5.intermediate.dense.weight
2022-08-29 18:31:37,864 n: bert.encoder.layer.5.intermediate.dense.bias
2022-08-29 18:31:37,864 n: bert.encoder.layer.5.output.dense.weight
2022-08-29 18:31:37,864 n: bert.encoder.layer.5.output.dense.bias
2022-08-29 18:31:37,864 n: bert.encoder.layer.5.output.LayerNorm.weight
2022-08-29 18:31:37,864 n: bert.encoder.layer.5.output.LayerNorm.bias
2022-08-29 18:31:37,864 n: bert.encoder.layer.6.attention.self.query.weight
2022-08-29 18:31:37,864 n: bert.encoder.layer.6.attention.self.query.bias
2022-08-29 18:31:37,864 n: bert.encoder.layer.6.attention.self.key.weight
2022-08-29 18:31:37,864 n: bert.encoder.layer.6.attention.self.key.bias
2022-08-29 18:31:37,864 n: bert.encoder.layer.6.attention.self.value.weight
2022-08-29 18:31:37,864 n: bert.encoder.layer.6.attention.self.value.bias
2022-08-29 18:31:37,864 n: bert.encoder.layer.6.attention.output.dense.weight
2022-08-29 18:31:37,864 n: bert.encoder.layer.6.attention.output.dense.bias
2022-08-29 18:31:37,864 n: bert.encoder.layer.6.attention.output.LayerNorm.weight
2022-08-29 18:31:37,864 n: bert.encoder.layer.6.attention.output.LayerNorm.bias
2022-08-29 18:31:37,864 n: bert.encoder.layer.6.intermediate.dense.weight
2022-08-29 18:31:37,864 n: bert.encoder.layer.6.intermediate.dense.bias
2022-08-29 18:31:37,864 n: bert.encoder.layer.6.output.dense.weight
2022-08-29 18:31:37,865 n: bert.encoder.layer.6.output.dense.bias
2022-08-29 18:31:37,865 n: bert.encoder.layer.6.output.LayerNorm.weight
2022-08-29 18:31:37,865 n: bert.encoder.layer.6.output.LayerNorm.bias
2022-08-29 18:31:37,865 n: bert.encoder.layer.7.attention.self.query.weight
2022-08-29 18:31:37,865 n: bert.encoder.layer.7.attention.self.query.bias
2022-08-29 18:31:37,865 n: bert.encoder.layer.7.attention.self.key.weight
2022-08-29 18:31:37,865 n: bert.encoder.layer.7.attention.self.key.bias
2022-08-29 18:31:37,865 n: bert.encoder.layer.7.attention.self.value.weight
2022-08-29 18:31:37,865 n: bert.encoder.layer.7.attention.self.value.bias
2022-08-29 18:31:37,865 n: bert.encoder.layer.7.attention.output.dense.weight
2022-08-29 18:31:37,865 n: bert.encoder.layer.7.attention.output.dense.bias
2022-08-29 18:31:37,865 n: bert.encoder.layer.7.attention.output.LayerNorm.weight
2022-08-29 18:31:37,865 n: bert.encoder.layer.7.attention.output.LayerNorm.bias
2022-08-29 18:31:37,865 n: bert.encoder.layer.7.intermediate.dense.weight
2022-08-29 18:31:37,865 n: bert.encoder.layer.7.intermediate.dense.bias
2022-08-29 18:31:37,865 n: bert.encoder.layer.7.output.dense.weight
2022-08-29 18:31:37,865 n: bert.encoder.layer.7.output.dense.bias
2022-08-29 18:31:37,865 n: bert.encoder.layer.7.output.LayerNorm.weight
2022-08-29 18:31:37,865 n: bert.encoder.layer.7.output.LayerNorm.bias
2022-08-29 18:31:37,866 n: bert.encoder.layer.8.attention.self.query.weight
2022-08-29 18:31:37,866 n: bert.encoder.layer.8.attention.self.query.bias
2022-08-29 18:31:37,866 n: bert.encoder.layer.8.attention.self.key.weight
2022-08-29 18:31:37,866 n: bert.encoder.layer.8.attention.self.key.bias
2022-08-29 18:31:37,866 n: bert.encoder.layer.8.attention.self.value.weight
2022-08-29 18:31:37,866 n: bert.encoder.layer.8.attention.self.value.bias
2022-08-29 18:31:37,866 n: bert.encoder.layer.8.attention.output.dense.weight
2022-08-29 18:31:37,866 n: bert.encoder.layer.8.attention.output.dense.bias
2022-08-29 18:31:37,866 n: bert.encoder.layer.8.attention.output.LayerNorm.weight
2022-08-29 18:31:37,866 n: bert.encoder.layer.8.attention.output.LayerNorm.bias
2022-08-29 18:31:37,866 n: bert.encoder.layer.8.intermediate.dense.weight
2022-08-29 18:31:37,866 n: bert.encoder.layer.8.intermediate.dense.bias
2022-08-29 18:31:37,866 n: bert.encoder.layer.8.output.dense.weight
2022-08-29 18:31:37,866 n: bert.encoder.layer.8.output.dense.bias
2022-08-29 18:31:37,866 n: bert.encoder.layer.8.output.LayerNorm.weight
2022-08-29 18:31:37,866 n: bert.encoder.layer.8.output.LayerNorm.bias
2022-08-29 18:31:37,866 n: bert.encoder.layer.9.attention.self.query.weight
2022-08-29 18:31:37,866 n: bert.encoder.layer.9.attention.self.query.bias
2022-08-29 18:31:37,867 n: bert.encoder.layer.9.attention.self.key.weight
2022-08-29 18:31:37,867 n: bert.encoder.layer.9.attention.self.key.bias
2022-08-29 18:31:37,867 n: bert.encoder.layer.9.attention.self.value.weight
2022-08-29 18:31:37,867 n: bert.encoder.layer.9.attention.self.value.bias
2022-08-29 18:31:37,867 n: bert.encoder.layer.9.attention.output.dense.weight
2022-08-29 18:31:37,867 n: bert.encoder.layer.9.attention.output.dense.bias
2022-08-29 18:31:37,867 n: bert.encoder.layer.9.attention.output.LayerNorm.weight
2022-08-29 18:31:37,867 n: bert.encoder.layer.9.attention.output.LayerNorm.bias
2022-08-29 18:31:37,867 n: bert.encoder.layer.9.intermediate.dense.weight
2022-08-29 18:31:37,867 n: bert.encoder.layer.9.intermediate.dense.bias
2022-08-29 18:31:37,867 n: bert.encoder.layer.9.output.dense.weight
2022-08-29 18:31:37,867 n: bert.encoder.layer.9.output.dense.bias
2022-08-29 18:31:37,867 n: bert.encoder.layer.9.output.LayerNorm.weight
2022-08-29 18:31:37,867 n: bert.encoder.layer.9.output.LayerNorm.bias
2022-08-29 18:31:37,867 n: bert.encoder.layer.10.attention.self.query.weight
2022-08-29 18:31:37,867 n: bert.encoder.layer.10.attention.self.query.bias
2022-08-29 18:31:37,867 n: bert.encoder.layer.10.attention.self.key.weight
2022-08-29 18:31:37,867 n: bert.encoder.layer.10.attention.self.key.bias
2022-08-29 18:31:37,867 n: bert.encoder.layer.10.attention.self.value.weight
2022-08-29 18:31:37,868 n: bert.encoder.layer.10.attention.self.value.bias
2022-08-29 18:31:37,868 n: bert.encoder.layer.10.attention.output.dense.weight
2022-08-29 18:31:37,868 n: bert.encoder.layer.10.attention.output.dense.bias
2022-08-29 18:31:37,868 n: bert.encoder.layer.10.attention.output.LayerNorm.weight
2022-08-29 18:31:37,868 n: bert.encoder.layer.10.attention.output.LayerNorm.bias
2022-08-29 18:31:37,868 n: bert.encoder.layer.10.intermediate.dense.weight
2022-08-29 18:31:37,868 n: bert.encoder.layer.10.intermediate.dense.bias
2022-08-29 18:31:37,868 n: bert.encoder.layer.10.output.dense.weight
2022-08-29 18:31:37,868 n: bert.encoder.layer.10.output.dense.bias
2022-08-29 18:31:37,868 n: bert.encoder.layer.10.output.LayerNorm.weight
2022-08-29 18:31:37,868 n: bert.encoder.layer.10.output.LayerNorm.bias
2022-08-29 18:31:37,868 n: bert.encoder.layer.11.attention.self.query.weight
2022-08-29 18:31:37,868 n: bert.encoder.layer.11.attention.self.query.bias
2022-08-29 18:31:37,868 n: bert.encoder.layer.11.attention.self.key.weight
2022-08-29 18:31:37,868 n: bert.encoder.layer.11.attention.self.key.bias
2022-08-29 18:31:37,868 n: bert.encoder.layer.11.attention.self.value.weight
2022-08-29 18:31:37,868 n: bert.encoder.layer.11.attention.self.value.bias
2022-08-29 18:31:37,868 n: bert.encoder.layer.11.attention.output.dense.weight
2022-08-29 18:31:37,869 n: bert.encoder.layer.11.attention.output.dense.bias
2022-08-29 18:31:37,869 n: bert.encoder.layer.11.attention.output.LayerNorm.weight
2022-08-29 18:31:37,869 n: bert.encoder.layer.11.attention.output.LayerNorm.bias
2022-08-29 18:31:37,869 n: bert.encoder.layer.11.intermediate.dense.weight
2022-08-29 18:31:37,869 n: bert.encoder.layer.11.intermediate.dense.bias
2022-08-29 18:31:37,869 n: bert.encoder.layer.11.output.dense.weight
2022-08-29 18:31:37,869 n: bert.encoder.layer.11.output.dense.bias
2022-08-29 18:31:37,869 n: bert.encoder.layer.11.output.LayerNorm.weight
2022-08-29 18:31:37,869 n: bert.encoder.layer.11.output.LayerNorm.bias
2022-08-29 18:31:37,869 n: bert.pooler.dense.weight
2022-08-29 18:31:37,869 n: bert.pooler.dense.bias
2022-08-29 18:31:37,869 n: classifier.weight
2022-08-29 18:31:37,869 n: classifier.bias
2022-08-29 18:31:37,869 Total parameters: 109483778
2022-08-29 18:31:56,058 ***** Running evaluation *****
2022-08-29 18:31:56,058   Epoch = 8 iter 18799 step
2022-08-29 18:31:56,058   Num examples = 872
2022-08-29 18:31:56,058   Batch size = 32
2022-08-29 18:31:56,060 ***** Eval results *****
2022-08-29 18:31:56,060   att_loss = 0.9977289033711836
2022-08-29 18:31:56,060   cls_loss = 0.0
2022-08-29 18:31:56,060   global_step = 18799
2022-08-29 18:31:56,060   loss = 2.484788852351295
2022-08-29 18:31:56,060   rep_loss = 1.4870599533739464
2022-08-29 18:31:56,060 ***** Save model *****
2022-08-29 18:31:59,703 ***** Running evaluation *****
2022-08-29 18:31:59,703   Epoch = 0 iter 199 step
2022-08-29 18:31:59,703   Num examples = 1043
2022-08-29 18:31:59,703   Batch size = 32
2022-08-29 18:31:59,705 ***** Eval results *****
2022-08-29 18:31:59,705   att_loss = 3.508555557859603
2022-08-29 18:31:59,705   cls_loss = 0.0
2022-08-29 18:31:59,705   global_step = 199
2022-08-29 18:31:59,705   loss = 7.314530250415131
2022-08-29 18:31:59,705   rep_loss = 3.80597469674882
2022-08-29 18:31:59,705 ***** Save model *****
2022-08-29 18:32:01,657 ***** Running evaluation *****
2022-08-29 18:32:01,657   Epoch = 1 iter 20199 step
2022-08-29 18:32:01,658   Num examples = 9815
2022-08-29 18:32:01,658   Batch size = 32
2022-08-29 18:32:01,659 ***** Eval results *****
2022-08-29 18:32:01,659   att_loss = 4.391694859497242
2022-08-29 18:32:01,659   cls_loss = 0.0
2022-08-29 18:32:01,659   global_step = 20199
2022-08-29 18:32:01,659   loss = 6.314465579483752
2022-08-29 18:32:01,659   rep_loss = 1.9227707195504522
2022-08-29 18:32:01,659 ***** Save model *****
2022-08-29 18:32:17,425 ***** Running evaluation *****
2022-08-29 18:32:17,426   Epoch = 0 iter 99 step
2022-08-29 18:32:17,426   Num examples = 872
2022-08-29 18:32:17,426   Batch size = 32
2022-08-29 18:32:19,601 ***** Eval results *****
2022-08-29 18:32:19,602   acc = 0.9208715596330275
2022-08-29 18:32:19,602   att_loss = 0.0
2022-08-29 18:32:19,602   cls_loss = 0.029638099314814263
2022-08-29 18:32:19,602   eval_loss = 0.29039201107142226
2022-08-29 18:32:19,602   global_step = 99
2022-08-29 18:32:19,602   loss = 0.029638099314814263
2022-08-29 18:32:19,602   rep_loss = 0.0
2022-08-29 18:32:19,602 ***** Save model *****
2022-08-29 18:32:35,774 ***** Running evaluation *****
2022-08-29 18:32:35,775   Epoch = 0 iter 399 step
2022-08-29 18:32:35,775   Num examples = 1043
2022-08-29 18:32:35,775   Batch size = 32
2022-08-29 18:32:35,776 ***** Eval results *****
2022-08-29 18:32:35,776   att_loss = 2.7136358617242418
2022-08-29 18:32:35,776   cls_loss = 0.0
2022-08-29 18:32:35,777   global_step = 399
2022-08-29 18:32:35,777   loss = 6.05637198163752
2022-08-29 18:32:35,777   rep_loss = 3.3427361163280365
2022-08-29 18:32:35,777 ***** Save model *****
2022-08-29 18:33:00,548 ***** Running evaluation *****
2022-08-29 18:33:00,549   Epoch = 0 iter 199 step
2022-08-29 18:33:00,549   Num examples = 872
2022-08-29 18:33:00,549   Batch size = 32
2022-08-29 18:33:02,735 ***** Eval results *****
2022-08-29 18:33:02,735   acc = 0.9208715596330275
2022-08-29 18:33:02,735   att_loss = 0.0
2022-08-29 18:33:02,735   cls_loss = 0.030135443150978443
2022-08-29 18:33:02,735   eval_loss = 0.26388863241299987
2022-08-29 18:33:02,735   global_step = 199
2022-08-29 18:33:02,735   loss = 0.030135443150978443
2022-08-29 18:33:02,735   rep_loss = 0.0
2022-08-29 18:33:13,181 ***** Running evaluation *****
2022-08-29 18:33:13,181   Epoch = 1 iter 20399 step
2022-08-29 18:33:13,181   Num examples = 9815
2022-08-29 18:33:13,181   Batch size = 32
2022-08-29 18:33:13,183 ***** Eval results *****
2022-08-29 18:33:13,183   att_loss = 4.390046266061584
2022-08-29 18:33:13,183   cls_loss = 0.0
2022-08-29 18:33:13,183   global_step = 20399
2022-08-29 18:33:13,183   loss = 6.3119168806615775
2022-08-29 18:33:13,183   rep_loss = 1.9218706144093294
2022-08-29 18:33:13,183 ***** Save model *****
2022-08-29 18:33:14,230 ***** Running evaluation *****
2022-08-29 18:33:14,231   Epoch = 0 iter 599 step
2022-08-29 18:33:14,231   Num examples = 1043
2022-08-29 18:33:14,231   Batch size = 32
2022-08-29 18:33:14,232 ***** Eval results *****
2022-08-29 18:33:14,232   att_loss = 2.3739114385813425
2022-08-29 18:33:14,232   cls_loss = 0.0
2022-08-29 18:33:14,232   global_step = 599
2022-08-29 18:33:14,233   loss = 5.494571867291636
2022-08-29 18:33:14,233   rep_loss = 3.12066042045122
2022-08-29 18:33:14,233 ***** Save model *****
2022-08-29 18:33:14,592 ***** Running evaluation *****
2022-08-29 18:33:14,593   Epoch = 9 iter 18999 step
2022-08-29 18:33:14,593   Num examples = 872
2022-08-29 18:33:14,593   Batch size = 32
2022-08-29 18:33:14,594 ***** Eval results *****
2022-08-29 18:33:14,594   att_loss = 0.9635649976276216
2022-08-29 18:33:14,594   cls_loss = 0.0
2022-08-29 18:33:14,594   global_step = 18999
2022-08-29 18:33:14,594   loss = 2.437735050443619
2022-08-29 18:33:14,594   rep_loss = 1.4741700528159973
2022-08-29 18:33:14,594 ***** Save model *****
2022-08-29 18:33:42,887 ***** Running evaluation *****
2022-08-29 18:33:42,888   Epoch = 0 iter 299 step
2022-08-29 18:33:42,888   Num examples = 872
2022-08-29 18:33:42,888   Batch size = 32
2022-08-29 18:33:45,081 ***** Eval results *****
2022-08-29 18:33:45,081   acc = 0.9162844036697247
2022-08-29 18:33:45,081   att_loss = 0.0
2022-08-29 18:33:45,081   cls_loss = 0.03131661586070988
2022-08-29 18:33:45,081   eval_loss = 0.2699630837222295
2022-08-29 18:33:45,081   global_step = 299
2022-08-29 18:33:45,081   loss = 0.03131661586070988
2022-08-29 18:33:45,081   rep_loss = 0.0
2022-08-29 18:33:57,764 ***** Running evaluation *****
2022-08-29 18:33:57,764   Epoch = 0 iter 799 step
2022-08-29 18:33:57,764   Num examples = 1043
2022-08-29 18:33:57,764   Batch size = 32
2022-08-29 18:33:57,766 ***** Eval results *****
2022-08-29 18:33:57,766   att_loss = 2.164828311143739
2022-08-29 18:33:57,766   cls_loss = 0.0
2022-08-29 18:33:57,766   global_step = 799
2022-08-29 18:33:57,766   loss = 5.134501120921817
2022-08-29 18:33:57,766   rep_loss = 2.969672804929139
2022-08-29 18:33:57,766 ***** Save model *****
2022-08-29 18:34:24,501 ***** Running evaluation *****
2022-08-29 18:34:24,502   Epoch = 1 iter 20599 step
2022-08-29 18:34:24,502   Num examples = 9815
2022-08-29 18:34:24,502   Batch size = 32
2022-08-29 18:34:24,503 ***** Eval results *****
2022-08-29 18:34:24,503   att_loss = 4.3882241885210895
2022-08-29 18:34:24,503   cls_loss = 0.0
2022-08-29 18:34:24,503   global_step = 20599
2022-08-29 18:34:24,503   loss = 6.309239485937168
2022-08-29 18:34:24,503   rep_loss = 1.9210152976021644
2022-08-29 18:34:24,503 ***** Save model *****
2022-08-29 18:34:25,276 ***** Running evaluation *****
2022-08-29 18:34:25,277   Epoch = 0 iter 399 step
2022-08-29 18:34:25,277   Num examples = 872
2022-08-29 18:34:25,277   Batch size = 32
2022-08-29 18:34:27,472 ***** Eval results *****
2022-08-29 18:34:27,472   acc = 0.9139908256880734
2022-08-29 18:34:27,472   att_loss = 0.0
2022-08-29 18:34:27,472   cls_loss = 0.03131367247971825
2022-08-29 18:34:27,472   eval_loss = 0.28947388300938265
2022-08-29 18:34:27,473   global_step = 399
2022-08-29 18:34:27,473   loss = 0.03131367247971825
2022-08-29 18:34:27,473   rep_loss = 0.0
2022-08-29 18:34:35,979 ***** Running evaluation *****
2022-08-29 18:34:35,979   Epoch = 0 iter 999 step
2022-08-29 18:34:35,979   Num examples = 1043
2022-08-29 18:34:35,979   Batch size = 32
2022-08-29 18:34:35,981 ***** Eval results *****
2022-08-29 18:34:35,981   att_loss = 2.0238504843549565
2022-08-29 18:34:35,981   cls_loss = 0.0
2022-08-29 18:34:35,981   global_step = 999
2022-08-29 18:34:35,981   loss = 4.883275621049516
2022-08-29 18:34:35,981   rep_loss = 2.8594251325777225
2022-08-29 18:34:35,981 ***** Save model *****
2022-08-29 18:34:38,214 ***** Running evaluation *****
2022-08-29 18:34:38,215   Epoch = 9 iter 19199 step
2022-08-29 18:34:38,215   Num examples = 872
2022-08-29 18:34:38,215   Batch size = 32
2022-08-29 18:34:38,216 ***** Eval results *****
2022-08-29 18:34:38,216   att_loss = 0.9899593734469251
2022-08-29 18:34:38,216   cls_loss = 0.0
2022-08-29 18:34:38,216   global_step = 19199
2022-08-29 18:34:38,216   loss = 2.4678196045835663
2022-08-29 18:34:38,217   rep_loss = 1.4778602381622836
2022-08-29 18:34:38,217 ***** Save model *****
2022-08-29 18:35:07,734 ***** Running evaluation *****
2022-08-29 18:35:07,735   Epoch = 0 iter 499 step
2022-08-29 18:35:07,735   Num examples = 872
2022-08-29 18:35:07,735   Batch size = 32
2022-08-29 18:35:09,931 ***** Eval results *****
2022-08-29 18:35:09,931   acc = 0.9094036697247706
2022-08-29 18:35:09,931   att_loss = 0.0
2022-08-29 18:35:09,931   cls_loss = 0.03124433517724813
2022-08-29 18:35:09,931   eval_loss = 0.3361691523875509
2022-08-29 18:35:09,931   global_step = 499
2022-08-29 18:35:09,931   loss = 0.03124433517724813
2022-08-29 18:35:09,932   rep_loss = 0.0
2022-08-29 18:35:14,101 ***** Running evaluation *****
2022-08-29 18:35:14,101   Epoch = 1 iter 1199 step
2022-08-29 18:35:14,101   Num examples = 1043
2022-08-29 18:35:14,102   Batch size = 32
2022-08-29 18:35:14,103 ***** Eval results *****
2022-08-29 18:35:14,103   att_loss = 1.346238060761954
2022-08-29 18:35:14,103   cls_loss = 0.0
2022-08-29 18:35:14,103   global_step = 1199
2022-08-29 18:35:14,103   loss = 3.6437856495835397
2022-08-29 18:35:14,103   rep_loss = 2.2975475942815535
2022-08-29 18:35:14,103 ***** Save model *****
2022-08-29 18:35:39,810 ***** Running evaluation *****
2022-08-29 18:35:39,811   Epoch = 1 iter 20799 step
2022-08-29 18:35:39,811   Num examples = 9815
2022-08-29 18:35:39,811   Batch size = 32
2022-08-29 18:35:39,812 ***** Eval results *****
2022-08-29 18:35:39,812   att_loss = 4.385626326191045
2022-08-29 18:35:39,812   cls_loss = 0.0
2022-08-29 18:35:39,812   global_step = 20799
2022-08-29 18:35:39,812   loss = 6.305820133851572
2022-08-29 18:35:39,812   rep_loss = 1.9201938081637555
2022-08-29 18:35:39,812 ***** Save model *****
2022-08-29 18:35:50,273 ***** Running evaluation *****
2022-08-29 18:35:50,274   Epoch = 0 iter 599 step
2022-08-29 18:35:50,274   Num examples = 872
2022-08-29 18:35:50,274   Batch size = 32
2022-08-29 18:35:52,138 ***** Running evaluation *****
2022-08-29 18:35:52,139   Epoch = 1 iter 1399 step
2022-08-29 18:35:52,139   Num examples = 1043
2022-08-29 18:35:52,139   Batch size = 32
2022-08-29 18:35:52,140 ***** Eval results *****
2022-08-29 18:35:52,140   att_loss = 1.3398431746621147
2022-08-29 18:35:52,140   cls_loss = 0.0
2022-08-29 18:35:52,140   global_step = 1399
2022-08-29 18:35:52,140   loss = 3.6041131747093087
2022-08-29 18:35:52,140   rep_loss = 2.264270002028013
2022-08-29 18:35:52,140 ***** Save model *****
2022-08-29 18:35:52,472 ***** Eval results *****
2022-08-29 18:35:52,472   acc = 0.9139908256880734
2022-08-29 18:35:52,472   att_loss = 0.0
2022-08-29 18:35:52,472   cls_loss = 0.03174003609883691
2022-08-29 18:35:52,473   eval_loss = 0.27608359505289365
2022-08-29 18:35:52,473   global_step = 599
2022-08-29 18:35:52,473   loss = 0.03174003609883691
2022-08-29 18:35:52,473   rep_loss = 0.0
2022-08-29 18:35:58,189 ***** Running evaluation *****
2022-08-29 18:35:58,190   Epoch = 9 iter 19399 step
2022-08-29 18:35:58,190   Num examples = 872
2022-08-29 18:35:58,190   Batch size = 32
2022-08-29 18:35:58,192 ***** Eval results *****
2022-08-29 18:35:58,192   att_loss = 0.9857634913586644
2022-08-29 18:35:58,192   cls_loss = 0.0
2022-08-29 18:35:58,193   global_step = 19399
2022-08-29 18:35:58,193   loss = 2.4618254506047292
2022-08-29 18:35:58,193   rep_loss = 1.4760619649104372
2022-08-29 18:35:58,193 ***** Save model *****
2022-08-29 18:36:30,176 ***** Running evaluation *****
2022-08-29 18:36:30,177   Epoch = 1 iter 1599 step
2022-08-29 18:36:30,177   Num examples = 1043
2022-08-29 18:36:30,177   Batch size = 32
2022-08-29 18:36:30,178 ***** Eval results *****
2022-08-29 18:36:30,178   att_loss = 1.3220643649217998
2022-08-29 18:36:30,178   cls_loss = 0.0
2022-08-29 18:36:30,178   global_step = 1599
2022-08-29 18:36:30,179   loss = 3.5561520393285373
2022-08-29 18:36:30,179   rep_loss = 2.234087675978235
2022-08-29 18:36:30,179 ***** Save model *****
2022-08-29 18:36:32,850 ***** Running evaluation *****
2022-08-29 18:36:32,850   Epoch = 0 iter 699 step
2022-08-29 18:36:32,851   Num examples = 872
2022-08-29 18:36:32,851   Batch size = 32
2022-08-29 18:36:35,049 ***** Eval results *****
2022-08-29 18:36:35,049   acc = 0.8979357798165137
2022-08-29 18:36:35,049   att_loss = 0.0
2022-08-29 18:36:35,049   cls_loss = 0.032010214487876804
2022-08-29 18:36:35,049   eval_loss = 0.36479704095316784
2022-08-29 18:36:35,049   global_step = 699
2022-08-29 18:36:35,049   loss = 0.032010214487876804
2022-08-29 18:36:35,050   rep_loss = 0.0
2022-08-29 18:36:51,348 ***** Running evaluation *****
2022-08-29 18:36:51,348   Epoch = 1 iter 20999 step
2022-08-29 18:36:51,348   Num examples = 9815
2022-08-29 18:36:51,348   Batch size = 32
2022-08-29 18:36:51,349 ***** Eval results *****
2022-08-29 18:36:51,349   att_loss = 4.383412752863904
2022-08-29 18:36:51,349   cls_loss = 0.0
2022-08-29 18:36:51,349   global_step = 20999
2022-08-29 18:36:51,349   loss = 6.302766149520437
2022-08-29 18:36:51,349   rep_loss = 1.9193533974213954
2022-08-29 18:36:51,350 ***** Save model *****
2022-08-29 18:37:08,311 ***** Running evaluation *****
2022-08-29 18:37:08,312   Epoch = 1 iter 1799 step
2022-08-29 18:37:08,312   Num examples = 1043
2022-08-29 18:37:08,312   Batch size = 32
2022-08-29 18:37:08,313 ***** Eval results *****
2022-08-29 18:37:08,313   att_loss = 1.308195410365119
2022-08-29 18:37:08,313   cls_loss = 0.0
2022-08-29 18:37:08,313   global_step = 1799
2022-08-29 18:37:08,313   loss = 3.5206777090576216
2022-08-29 18:37:08,313   rep_loss = 2.2124822994263487
2022-08-29 18:37:08,313 ***** Save model *****
2022-08-29 18:37:15,467 ***** Running evaluation *****
2022-08-29 18:37:15,467   Epoch = 0 iter 799 step
2022-08-29 18:37:15,467   Num examples = 872
2022-08-29 18:37:15,467   Batch size = 32
2022-08-29 18:37:17,061 ***** Running evaluation *****
2022-08-29 18:37:17,062   Epoch = 9 iter 19599 step
2022-08-29 18:37:17,062   Num examples = 872
2022-08-29 18:37:17,062   Batch size = 32
2022-08-29 18:37:17,063 ***** Eval results *****
2022-08-29 18:37:17,063   att_loss = 0.9841267823453762
2022-08-29 18:37:17,063   cls_loss = 0.0
2022-08-29 18:37:17,063   global_step = 19599
2022-08-29 18:37:17,063   loss = 2.4596854309870286
2022-08-29 18:37:17,063   rep_loss = 1.475558650259878
2022-08-29 18:37:17,064 ***** Save model *****
2022-08-29 18:37:17,670 ***** Eval results *****
2022-08-29 18:37:17,670   acc = 0.9094036697247706
2022-08-29 18:37:17,670   att_loss = 0.0
2022-08-29 18:37:17,670   cls_loss = 0.03233551696928985
2022-08-29 18:37:17,670   eval_loss = 0.3133795648547156
2022-08-29 18:37:17,670   global_step = 799
2022-08-29 18:37:17,670   loss = 0.03233551696928985
2022-08-29 18:37:17,670   rep_loss = 0.0
2022-08-29 18:37:46,410 ***** Running evaluation *****
2022-08-29 18:37:46,410   Epoch = 1 iter 1999 step
2022-08-29 18:37:46,410   Num examples = 1043
2022-08-29 18:37:46,410   Batch size = 32
2022-08-29 18:37:46,412 ***** Eval results *****
2022-08-29 18:37:46,412   att_loss = 1.2918305240179364
2022-08-29 18:37:46,412   cls_loss = 0.0
2022-08-29 18:37:46,412   global_step = 1999
2022-08-29 18:37:46,412   loss = 3.482985636345426
2022-08-29 18:37:46,412   rep_loss = 2.1911551107269354
2022-08-29 18:37:46,412 ***** Save model *****
2022-08-29 18:37:58,078 ***** Running evaluation *****
2022-08-29 18:37:58,079   Epoch = 0 iter 899 step
2022-08-29 18:37:58,079   Num examples = 872
2022-08-29 18:37:58,079   Batch size = 32
2022-08-29 18:38:00,284 ***** Eval results *****
2022-08-29 18:38:00,284   acc = 0.9059633027522935
2022-08-29 18:38:00,284   att_loss = 0.0
2022-08-29 18:38:00,284   cls_loss = 0.032721754001376856
2022-08-29 18:38:00,284   eval_loss = 0.27524236656193224
2022-08-29 18:38:00,284   global_step = 899
2022-08-29 18:38:00,285   loss = 0.032721754001376856
2022-08-29 18:38:00,285   rep_loss = 0.0
2022-08-29 18:38:00,672 ***** Running evaluation *****
2022-08-29 18:38:00,673   Epoch = 1 iter 21199 step
2022-08-29 18:38:00,673   Num examples = 9815
2022-08-29 18:38:00,673   Batch size = 32
2022-08-29 18:38:00,674 ***** Eval results *****
2022-08-29 18:38:00,674   att_loss = 4.381103335876405
2022-08-29 18:38:00,674   cls_loss = 0.0
2022-08-29 18:38:00,674   global_step = 21199
2022-08-29 18:38:00,674   loss = 6.299581811633161
2022-08-29 18:38:00,674   rep_loss = 1.918478476838292
2022-08-29 18:38:00,674 ***** Save model *****
2022-08-29 18:38:26,227 ***** Running evaluation *****
2022-08-29 18:38:26,227   Epoch = 2 iter 2199 step
2022-08-29 18:38:26,227   Num examples = 1043
2022-08-29 18:38:26,227   Batch size = 32
2022-08-29 18:38:26,229 ***** Eval results *****
2022-08-29 18:38:26,229   att_loss = 1.1867528141490997
2022-08-29 18:38:26,229   cls_loss = 0.0
2022-08-29 18:38:26,229   global_step = 2199
2022-08-29 18:38:26,229   loss = 3.2368591740017845
2022-08-29 18:38:26,229   rep_loss = 2.050106364583212
2022-08-29 18:38:26,229 ***** Save model *****
2022-08-29 18:38:35,626 ***** Running evaluation *****
2022-08-29 18:38:35,626   Epoch = 9 iter 19799 step
2022-08-29 18:38:35,626   Num examples = 872
2022-08-29 18:38:35,626   Batch size = 32
2022-08-29 18:38:35,628 ***** Eval results *****
2022-08-29 18:38:35,628   att_loss = 0.9850178496309946
2022-08-29 18:38:35,628   cls_loss = 0.0
2022-08-29 18:38:35,628   global_step = 19799
2022-08-29 18:38:35,628   loss = 2.4604514454634967
2022-08-29 18:38:35,628   rep_loss = 1.4754335986642406
2022-08-29 18:38:35,628 ***** Save model *****
2022-08-29 18:38:40,717 ***** Running evaluation *****
2022-08-29 18:38:40,717   Epoch = 0 iter 999 step
2022-08-29 18:38:40,717   Num examples = 872
2022-08-29 18:38:40,717   Batch size = 32
2022-08-29 18:38:42,920 ***** Eval results *****
2022-08-29 18:38:42,921   acc = 0.8990825688073395
2022-08-29 18:38:42,921   att_loss = 0.0
2022-08-29 18:38:42,921   cls_loss = 0.033815639950726575
2022-08-29 18:38:42,921   eval_loss = 0.3080236480704376
2022-08-29 18:38:42,921   global_step = 999
2022-08-29 18:38:42,921   loss = 0.033815639950726575
2022-08-29 18:38:42,921   rep_loss = 0.0
2022-08-29 18:39:06,022 ***** Running evaluation *****
2022-08-29 18:39:06,022   Epoch = 2 iter 2399 step
2022-08-29 18:39:06,022   Num examples = 1043
2022-08-29 18:39:06,022   Batch size = 32
2022-08-29 18:39:06,024 ***** Eval results *****
2022-08-29 18:39:06,024   att_loss = 1.176336455481134
2022-08-29 18:39:06,024   cls_loss = 0.0
2022-08-29 18:39:06,024   global_step = 2399
2022-08-29 18:39:06,024   loss = 3.230437859836187
2022-08-29 18:39:06,024   rep_loss = 2.054101408661092
2022-08-29 18:39:06,024 ***** Save model *****
2022-08-29 18:39:09,960 ***** Running evaluation *****
2022-08-29 18:39:09,960   Epoch = 1 iter 21399 step
2022-08-29 18:39:09,960   Num examples = 9815
2022-08-29 18:39:09,960   Batch size = 32
2022-08-29 18:39:09,961 ***** Eval results *****
2022-08-29 18:39:09,961   att_loss = 4.3797541619862095
2022-08-29 18:39:09,961   cls_loss = 0.0
2022-08-29 18:39:09,961   global_step = 21399
2022-08-29 18:39:09,961   loss = 6.297417101997957
2022-08-29 18:39:09,961   rep_loss = 1.9176629412001835
2022-08-29 18:39:09,961 ***** Save model *****
2022-08-29 18:39:23,375 ***** Running evaluation *****
2022-08-29 18:39:23,376   Epoch = 0 iter 1099 step
2022-08-29 18:39:23,376   Num examples = 872
2022-08-29 18:39:23,376   Batch size = 32
2022-08-29 18:39:25,582 ***** Eval results *****
2022-08-29 18:39:25,582   acc = 0.9094036697247706
2022-08-29 18:39:25,582   att_loss = 0.0
2022-08-29 18:39:25,582   cls_loss = 0.03439092730469426
2022-08-29 18:39:25,582   eval_loss = 0.3023133080984865
2022-08-29 18:39:25,582   global_step = 1099
2022-08-29 18:39:25,582   loss = 0.03439092730469426
2022-08-29 18:39:25,582   rep_loss = 0.0
2022-08-29 18:39:45,900 ***** Running evaluation *****
2022-08-29 18:39:45,900   Epoch = 2 iter 2599 step
2022-08-29 18:39:45,900   Num examples = 1043
2022-08-29 18:39:45,900   Batch size = 32
2022-08-29 18:39:45,902 ***** Eval results *****
2022-08-29 18:39:45,902   att_loss = 1.1721693467887895
2022-08-29 18:39:45,902   cls_loss = 0.0
2022-08-29 18:39:45,902   global_step = 2599
2022-08-29 18:39:45,902   loss = 3.2161647907858533
2022-08-29 18:39:45,902   rep_loss = 2.0439954499189077
2022-08-29 18:39:45,902 ***** Save model *****
2022-08-29 18:39:55,305 ***** Running evaluation *****
2022-08-29 18:39:55,306   Epoch = 9 iter 19999 step
2022-08-29 18:39:55,306   Num examples = 872
2022-08-29 18:39:55,306   Batch size = 32
2022-08-29 18:39:55,307 ***** Eval results *****
2022-08-29 18:39:55,307   att_loss = 0.9917827248012582
2022-08-29 18:39:55,307   cls_loss = 0.0
2022-08-29 18:39:55,307   global_step = 19999
2022-08-29 18:39:55,307   loss = 2.4676263635156297
2022-08-29 18:39:55,307   rep_loss = 1.4758436411815439
2022-08-29 18:39:55,307 ***** Save model *****
2022-08-29 18:40:06,072 ***** Running evaluation *****
2022-08-29 18:40:06,073   Epoch = 0 iter 1199 step
2022-08-29 18:40:06,073   Num examples = 872
2022-08-29 18:40:06,073   Batch size = 32
2022-08-29 18:40:08,277 ***** Eval results *****
2022-08-29 18:40:08,277   acc = 0.9002293577981652
2022-08-29 18:40:08,277   att_loss = 0.0
2022-08-29 18:40:08,277   cls_loss = 0.03462387805997244
2022-08-29 18:40:08,277   eval_loss = 0.35710643052256535
2022-08-29 18:40:08,277   global_step = 1199
2022-08-29 18:40:08,278   loss = 0.03462387805997244
2022-08-29 18:40:08,278   rep_loss = 0.0
2022-08-29 18:40:21,331 ***** Running evaluation *****
2022-08-29 18:40:21,331   Epoch = 1 iter 21599 step
2022-08-29 18:40:21,331   Num examples = 9815
2022-08-29 18:40:21,331   Batch size = 32
2022-08-29 18:40:21,333 ***** Eval results *****
2022-08-29 18:40:21,333   att_loss = 4.378419652453418
2022-08-29 18:40:21,333   cls_loss = 0.0
2022-08-29 18:40:21,333   global_step = 21599
2022-08-29 18:40:21,333   loss = 6.295338524144468
2022-08-29 18:40:21,333   rep_loss = 1.916918872726209
2022-08-29 18:40:21,333 ***** Save model *****
2022-08-29 18:40:25,547 ***** Running evaluation *****
2022-08-29 18:40:25,548   Epoch = 2 iter 2799 step
2022-08-29 18:40:25,548   Num examples = 1043
2022-08-29 18:40:25,548   Batch size = 32
2022-08-29 18:40:25,549 ***** Eval results *****
2022-08-29 18:40:25,549   att_loss = 1.168888291981785
2022-08-29 18:40:25,549   cls_loss = 0.0
2022-08-29 18:40:25,549   global_step = 2799
2022-08-29 18:40:25,549   loss = 3.206164768558462
2022-08-29 18:40:25,549   rep_loss = 2.037276482689974
2022-08-29 18:40:25,549 ***** Save model *****
2022-08-29 18:40:48,754 ***** Running evaluation *****
2022-08-29 18:40:48,755   Epoch = 0 iter 1299 step
2022-08-29 18:40:48,755   Num examples = 872
2022-08-29 18:40:48,755   Batch size = 32
2022-08-29 18:40:50,962 ***** Eval results *****
2022-08-29 18:40:50,962   acc = 0.9036697247706422
2022-08-29 18:40:50,962   att_loss = 0.0
2022-08-29 18:40:50,962   cls_loss = 0.03492608789485048
2022-08-29 18:40:50,962   eval_loss = 0.34284583000200136
2022-08-29 18:40:50,962   global_step = 1299
2022-08-29 18:40:50,962   loss = 0.03492608789485048
2022-08-29 18:40:50,962   rep_loss = 0.0
2022-08-29 18:41:03,993 ***** Running evaluation *****
2022-08-29 18:41:03,994   Epoch = 2 iter 2999 step
2022-08-29 18:41:03,994   Num examples = 1043
2022-08-29 18:41:03,994   Batch size = 32
2022-08-29 18:41:03,995 ***** Eval results *****
2022-08-29 18:41:03,996   att_loss = 1.1614470222872075
2022-08-29 18:41:03,996   cls_loss = 0.0
2022-08-29 18:41:03,996   global_step = 2999
2022-08-29 18:41:03,996   loss = 3.1912558360558396
2022-08-29 18:41:03,996   rep_loss = 2.029808819363043
2022-08-29 18:41:03,996 ***** Save model *****
2022-08-29 18:41:14,875 ***** Running evaluation *****
2022-08-29 18:41:14,875   Epoch = 9 iter 20199 step
2022-08-29 18:41:14,875   Num examples = 872
2022-08-29 18:41:14,876   Batch size = 32
2022-08-29 18:41:14,877 ***** Eval results *****
2022-08-29 18:41:14,877   att_loss = 0.9908127951791904
2022-08-29 18:41:14,877   cls_loss = 0.0
2022-08-29 18:41:14,877   global_step = 20199
2022-08-29 18:41:14,877   loss = 2.46671901538272
2022-08-29 18:41:14,877   rep_loss = 1.475906221524931
2022-08-29 18:41:14,877 ***** Save model *****
2022-08-29 18:41:30,540 ***** Running evaluation *****
2022-08-29 18:41:30,540   Epoch = 1 iter 21799 step
2022-08-29 18:41:30,541   Num examples = 9815
2022-08-29 18:41:30,541   Batch size = 32
2022-08-29 18:41:30,542 ***** Eval results *****
2022-08-29 18:41:30,542   att_loss = 4.376794528550805
2022-08-29 18:41:30,542   cls_loss = 0.0
2022-08-29 18:41:30,542   global_step = 21799
2022-08-29 18:41:30,542   loss = 6.292894158815757
2022-08-29 18:41:30,542   rep_loss = 1.9160996314034955
2022-08-29 18:41:30,542 ***** Save model *****
2022-08-29 18:41:31,437 ***** Running evaluation *****
2022-08-29 18:41:31,438   Epoch = 0 iter 1399 step
2022-08-29 18:41:31,438   Num examples = 872
2022-08-29 18:41:31,438   Batch size = 32
2022-08-29 18:41:33,643 ***** Eval results *****
2022-08-29 18:41:33,643   acc = 0.9185779816513762
2022-08-29 18:41:33,643   att_loss = 0.0
2022-08-29 18:41:33,643   cls_loss = 0.03486966156201694
2022-08-29 18:41:33,643   eval_loss = 0.30189075760011164
2022-08-29 18:41:33,643   global_step = 1399
2022-08-29 18:41:33,643   loss = 0.03486966156201694
2022-08-29 18:41:33,644   rep_loss = 0.0
2022-08-29 18:41:42,405 ***** Running evaluation *****
2022-08-29 18:41:42,406   Epoch = 2 iter 3199 step
2022-08-29 18:41:42,406   Num examples = 1043
2022-08-29 18:41:42,406   Batch size = 32
2022-08-29 18:41:42,407 ***** Eval results *****
2022-08-29 18:41:42,407   att_loss = 1.157832051433714
2022-08-29 18:41:42,407   cls_loss = 0.0
2022-08-29 18:41:42,407   global_step = 3199
2022-08-29 18:41:42,407   loss = 3.1795164823307815
2022-08-29 18:41:42,407   rep_loss = 2.0216844356631962
2022-08-29 18:41:42,407 ***** Save model *****
2022-08-29 18:42:14,130 ***** Running evaluation *****
2022-08-29 18:42:14,131   Epoch = 0 iter 1499 step
2022-08-29 18:42:14,131   Num examples = 872
2022-08-29 18:42:14,131   Batch size = 32
2022-08-29 18:42:16,337 ***** Eval results *****
2022-08-29 18:42:16,338   acc = 0.9162844036697247
2022-08-29 18:42:16,338   att_loss = 0.0
2022-08-29 18:42:16,338   cls_loss = 0.03501133982768112
2022-08-29 18:42:16,338   eval_loss = 0.26365820478115765
2022-08-29 18:42:16,338   global_step = 1499
2022-08-29 18:42:16,338   loss = 0.03501133982768112
2022-08-29 18:42:16,338   rep_loss = 0.0
2022-08-29 18:42:21,102 ***** Running evaluation *****
2022-08-29 18:42:21,102   Epoch = 3 iter 3399 step
2022-08-29 18:42:21,102   Num examples = 1043
2022-08-29 18:42:21,102   Batch size = 32
2022-08-29 18:42:21,104 ***** Eval results *****
2022-08-29 18:42:21,104   att_loss = 1.0707057543289966
2022-08-29 18:42:21,104   cls_loss = 0.0
2022-08-29 18:42:21,104   global_step = 3399
2022-08-29 18:42:21,104   loss = 2.9986337404984695
2022-08-29 18:42:21,104   rep_loss = 1.927927977610857
2022-08-29 18:42:21,104 ***** Save model *****
2022-08-29 18:42:34,656 ***** Running evaluation *****
2022-08-29 18:42:34,657   Epoch = 9 iter 20399 step
2022-08-29 18:42:34,657   Num examples = 872
2022-08-29 18:42:34,657   Batch size = 32
2022-08-29 18:42:34,658 ***** Eval results *****
2022-08-29 18:42:34,658   att_loss = 0.9893101497781822
2022-08-29 18:42:34,658   cls_loss = 0.0
2022-08-29 18:42:34,658   global_step = 20399
2022-08-29 18:42:34,658   loss = 2.4648100593199245
2022-08-29 18:42:34,659   rep_loss = 1.475499910601018
2022-08-29 18:42:34,659 ***** Save model *****
2022-08-29 18:42:42,890 ***** Running evaluation *****
2022-08-29 18:42:42,891   Epoch = 1 iter 21999 step
2022-08-29 18:42:42,891   Num examples = 9815
2022-08-29 18:42:42,891   Batch size = 32
2022-08-29 18:42:42,892 ***** Eval results *****
2022-08-29 18:42:42,892   att_loss = 4.374042622107816
2022-08-29 18:42:42,892   cls_loss = 0.0
2022-08-29 18:42:42,892   global_step = 21999
2022-08-29 18:42:42,892   loss = 6.289237477228437
2022-08-29 18:42:42,892   rep_loss = 1.9151948564685881
2022-08-29 18:42:42,892 ***** Save model *****
2022-08-29 18:42:56,866 ***** Running evaluation *****
2022-08-29 18:42:56,866   Epoch = 0 iter 1599 step
2022-08-29 18:42:56,866   Num examples = 872
2022-08-29 18:42:56,866   Batch size = 32
2022-08-29 18:42:59,070 ***** Eval results *****
2022-08-29 18:42:59,070   acc = 0.9025229357798165
2022-08-29 18:42:59,070   att_loss = 0.0
2022-08-29 18:42:59,070   cls_loss = 0.03520904148953866
2022-08-29 18:42:59,070   eval_loss = 0.2881106212735176
2022-08-29 18:42:59,070   global_step = 1599
2022-08-29 18:42:59,070   loss = 0.03520904148953866
2022-08-29 18:42:59,070   rep_loss = 0.0
2022-08-29 18:42:59,305 ***** Running evaluation *****
2022-08-29 18:42:59,305   Epoch = 3 iter 3599 step
2022-08-29 18:42:59,305   Num examples = 1043
2022-08-29 18:42:59,305   Batch size = 32
2022-08-29 18:42:59,307 ***** Eval results *****
2022-08-29 18:42:59,307   att_loss = 1.077038510238068
2022-08-29 18:42:59,307   cls_loss = 0.0
2022-08-29 18:42:59,307   global_step = 3599
2022-08-29 18:42:59,307   loss = 3.0112281461305255
2022-08-29 18:42:59,307   rep_loss = 1.9341896319691139
2022-08-29 18:42:59,307 ***** Save model *****
2022-08-29 18:43:37,509 ***** Running evaluation *****
2022-08-29 18:43:37,510   Epoch = 3 iter 3799 step
2022-08-29 18:43:37,510   Num examples = 1043
2022-08-29 18:43:37,510   Batch size = 32
2022-08-29 18:43:37,511 ***** Eval results *****
2022-08-29 18:43:37,511   att_loss = 1.0747305710776514
2022-08-29 18:43:37,511   cls_loss = 0.0
2022-08-29 18:43:37,511   global_step = 3799
2022-08-29 18:43:37,511   loss = 3.0065318019450213
2022-08-29 18:43:37,511   rep_loss = 1.9318012273612142
2022-08-29 18:43:37,511 ***** Save model *****
2022-08-29 18:43:39,608 ***** Running evaluation *****
2022-08-29 18:43:39,609   Epoch = 0 iter 1699 step
2022-08-29 18:43:39,609   Num examples = 872
2022-08-29 18:43:39,609   Batch size = 32
2022-08-29 18:43:41,819 ***** Eval results *****
2022-08-29 18:43:41,820   acc = 0.9036697247706422
2022-08-29 18:43:41,820   att_loss = 0.0
2022-08-29 18:43:41,820   cls_loss = 0.03522931219951818
2022-08-29 18:43:41,820   eval_loss = 0.28646479374063866
2022-08-29 18:43:41,820   global_step = 1699
2022-08-29 18:43:41,820   loss = 0.03522931219951818
2022-08-29 18:43:41,820   rep_loss = 0.0
2022-08-29 18:43:54,766 ***** Running evaluation *****
2022-08-29 18:43:54,766   Epoch = 9 iter 20599 step
2022-08-29 18:43:54,767   Num examples = 872
2022-08-29 18:43:54,767   Batch size = 32
2022-08-29 18:43:54,768 ***** Eval results *****
2022-08-29 18:43:54,768   att_loss = 0.9879534077873735
2022-08-29 18:43:54,768   cls_loss = 0.0
2022-08-29 18:43:54,768   global_step = 20599
2022-08-29 18:43:54,768   loss = 2.4629534158230686
2022-08-29 18:43:54,768   rep_loss = 1.4750000100069856
2022-08-29 18:43:54,768 ***** Save model *****
2022-08-29 18:43:55,571 ***** Running evaluation *****
2022-08-29 18:43:55,572   Epoch = 1 iter 22199 step
2022-08-29 18:43:55,572   Num examples = 9815
2022-08-29 18:43:55,572   Batch size = 32
2022-08-29 18:43:55,573 ***** Eval results *****
2022-08-29 18:43:55,573   att_loss = 4.372520785649682
2022-08-29 18:43:55,573   cls_loss = 0.0
2022-08-29 18:43:55,574   global_step = 22199
2022-08-29 18:43:55,574   loss = 6.286939913567567
2022-08-29 18:43:55,574   rep_loss = 1.9144191295148671
2022-08-29 18:43:55,574 ***** Save model *****
2022-08-29 18:44:15,665 ***** Running evaluation *****
2022-08-29 18:44:15,665   Epoch = 3 iter 3999 step
2022-08-29 18:44:15,665   Num examples = 1043
2022-08-29 18:44:15,665   Batch size = 32
2022-08-29 18:44:15,667 ***** Eval results *****
2022-08-29 18:44:15,667   att_loss = 1.080037750687989
2022-08-29 18:44:15,667   cls_loss = 0.0
2022-08-29 18:44:15,667   global_step = 3999
2022-08-29 18:44:15,667   loss = 3.0107219932964013
2022-08-29 18:44:15,667   rep_loss = 1.9306842397593853
2022-08-29 18:44:15,667 ***** Save model *****
2022-08-29 18:44:22,315 ***** Running evaluation *****
2022-08-29 18:44:22,316   Epoch = 0 iter 1799 step
2022-08-29 18:44:22,316   Num examples = 872
2022-08-29 18:44:22,316   Batch size = 32
2022-08-29 18:44:24,519 ***** Eval results *****
2022-08-29 18:44:24,519   acc = 0.9197247706422018
2022-08-29 18:44:24,519   att_loss = 0.0
2022-08-29 18:44:24,519   cls_loss = 0.03513842745279772
2022-08-29 18:44:24,519   eval_loss = 0.26990734812404427
2022-08-29 18:44:24,519   global_step = 1799
2022-08-29 18:44:24,519   loss = 0.03513842745279772
2022-08-29 18:44:24,519   rep_loss = 0.0
2022-08-29 18:44:53,824 ***** Running evaluation *****
2022-08-29 18:44:53,825   Epoch = 3 iter 4199 step
2022-08-29 18:44:53,825   Num examples = 1043
2022-08-29 18:44:53,825   Batch size = 32
2022-08-29 18:44:53,826 ***** Eval results *****
2022-08-29 18:44:53,827   att_loss = 1.0689135384320014
2022-08-29 18:44:53,827   cls_loss = 0.0
2022-08-29 18:44:53,827   global_step = 4199
2022-08-29 18:44:53,827   loss = 2.9923352943592936
2022-08-29 18:44:53,827   rep_loss = 1.9234217527523711
2022-08-29 18:44:53,827 ***** Save model *****
2022-08-29 18:45:05,036 ***** Running evaluation *****
2022-08-29 18:45:05,036   Epoch = 0 iter 1899 step
2022-08-29 18:45:05,036   Num examples = 872
2022-08-29 18:45:05,036   Batch size = 32
2022-08-29 18:45:07,246 ***** Eval results *****
2022-08-29 18:45:07,247   acc = 0.9059633027522935
2022-08-29 18:45:07,247   att_loss = 0.0
2022-08-29 18:45:07,247   cls_loss = 0.03506232493887094
2022-08-29 18:45:07,247   eval_loss = 0.27628385461866856
2022-08-29 18:45:07,247   global_step = 1899
2022-08-29 18:45:07,247   loss = 0.03506232493887094
2022-08-29 18:45:07,247   rep_loss = 0.0
2022-08-29 18:45:07,791 ***** Running evaluation *****
2022-08-29 18:45:07,791   Epoch = 1 iter 22399 step
2022-08-29 18:45:07,792   Num examples = 9815
2022-08-29 18:45:07,792   Batch size = 32
2022-08-29 18:45:07,793 ***** Eval results *****
2022-08-29 18:45:07,793   att_loss = 4.370909787961657
2022-08-29 18:45:07,793   cls_loss = 0.0
2022-08-29 18:45:07,793   global_step = 22399
2022-08-29 18:45:07,793   loss = 6.284493277988163
2022-08-29 18:45:07,793   rep_loss = 1.9135834915448704
2022-08-29 18:45:07,793 ***** Save model *****
2022-08-29 18:45:15,758 ***** Running evaluation *****
2022-08-29 18:45:15,759   Epoch = 9 iter 20799 step
2022-08-29 18:45:15,759   Num examples = 872
2022-08-29 18:45:15,759   Batch size = 32
2022-08-29 18:45:15,760 ***** Eval results *****
2022-08-29 18:45:15,760   att_loss = 0.9885527767676918
2022-08-29 18:45:15,760   cls_loss = 0.0
2022-08-29 18:45:15,761   global_step = 20799
2022-08-29 18:45:15,761   loss = 2.463213753073176
2022-08-29 18:45:15,761   rep_loss = 1.4746609766894108
2022-08-29 18:45:15,761 ***** Save model *****
2022-08-29 18:45:33,183 ***** Running evaluation *****
2022-08-29 18:45:33,184   Epoch = 4 iter 4399 step
2022-08-29 18:45:33,184   Num examples = 1043
2022-08-29 18:45:33,184   Batch size = 32
2022-08-29 18:45:33,185 ***** Eval results *****
2022-08-29 18:45:33,185   att_loss = 1.014284562407516
2022-08-29 18:45:33,186   cls_loss = 0.0
2022-08-29 18:45:33,186   global_step = 4399
2022-08-29 18:45:33,186   loss = 2.853520453445555
2022-08-29 18:45:33,186   rep_loss = 1.8392358933846782
2022-08-29 18:45:33,186 ***** Save model *****
2022-08-29 18:45:47,823 ***** Running evaluation *****
2022-08-29 18:45:47,823   Epoch = 0 iter 1999 step
2022-08-29 18:45:47,823   Num examples = 872
2022-08-29 18:45:47,823   Batch size = 32
2022-08-29 18:45:50,031 ***** Eval results *****
2022-08-29 18:45:50,031   acc = 0.9139908256880734
2022-08-29 18:45:50,031   att_loss = 0.0
2022-08-29 18:45:50,031   cls_loss = 0.034869079204372176
2022-08-29 18:45:50,031   eval_loss = 0.2774075686133334
2022-08-29 18:45:50,031   global_step = 1999
2022-08-29 18:45:50,031   loss = 0.034869079204372176
2022-08-29 18:45:50,031   rep_loss = 0.0
2022-08-29 18:46:13,276 ***** Running evaluation *****
2022-08-29 18:46:13,277   Epoch = 4 iter 4599 step
2022-08-29 18:46:13,277   Num examples = 1043
2022-08-29 18:46:13,277   Batch size = 32
2022-08-29 18:46:13,278 ***** Eval results *****
2022-08-29 18:46:13,279   att_loss = 1.0237653415866583
2022-08-29 18:46:13,279   cls_loss = 0.0
2022-08-29 18:46:13,279   global_step = 4599
2022-08-29 18:46:13,279   loss = 2.8748253077177464
2022-08-29 18:46:13,279   rep_loss = 1.8510599643083159
2022-08-29 18:46:13,279 ***** Save model *****
2022-08-29 18:46:16,799 ***** Running evaluation *****
2022-08-29 18:46:16,800   Epoch = 1 iter 22599 step
2022-08-29 18:46:16,800   Num examples = 9815
2022-08-29 18:46:16,800   Batch size = 32
2022-08-29 18:46:16,801 ***** Eval results *****
2022-08-29 18:46:16,801   att_loss = 4.369215068431163
2022-08-29 18:46:16,801   cls_loss = 0.0
2022-08-29 18:46:16,801   global_step = 22599
2022-08-29 18:46:16,801   loss = 6.281942410675894
2022-08-29 18:46:16,801   rep_loss = 1.912727343802947
2022-08-29 18:46:16,801 ***** Save model *****
2022-08-29 18:46:30,614 ***** Running evaluation *****
2022-08-29 18:46:30,615   Epoch = 0 iter 2099 step
2022-08-29 18:46:30,615   Num examples = 872
2022-08-29 18:46:30,615   Batch size = 32
2022-08-29 18:46:32,819 ***** Eval results *****
2022-08-29 18:46:32,819   acc = 0.9162844036697247
2022-08-29 18:46:32,819   att_loss = 0.0
2022-08-29 18:46:32,819   cls_loss = 0.03471024492231968
2022-08-29 18:46:32,819   eval_loss = 0.2456973615501608
2022-08-29 18:46:32,819   global_step = 2099
2022-08-29 18:46:32,819   loss = 0.03471024492231968
2022-08-29 18:46:32,819   rep_loss = 0.0
2022-08-29 18:46:34,029 ***** Running evaluation *****
2022-08-29 18:46:34,029   Epoch = 9 iter 20999 step
2022-08-29 18:46:34,029   Num examples = 872
2022-08-29 18:46:34,029   Batch size = 32
2022-08-29 18:46:34,030 ***** Eval results *****
2022-08-29 18:46:34,030   att_loss = 0.9863993751153285
2022-08-29 18:46:34,030   cls_loss = 0.0
2022-08-29 18:46:34,031   global_step = 20999
2022-08-29 18:46:34,031   loss = 2.4604490830721466
2022-08-29 18:46:34,031   rep_loss = 1.4740497073211891
2022-08-29 18:46:34,031 ***** Save model *****
2022-08-29 18:46:55,495 The args: Namespace(aug_train=False, cache_dir='', data_dir='glue_data/SST2', data_url='', do_eval=False, do_lower_case=True, eval_batch_size=32, eval_step=100, gradient_accumulation_steps=1, hidden_act='quad', learning_rate=1e-05, log_path='tmp/distill/SST2/quad_2quad/bert-base-uncased/log.txt', max_seq_length=128, no_cuda=False, num_train_epochs=5.0, output_dir='tmp/distill/SST2/quad_2quad/bert-base-uncased/5e-05_1e-05_32_stage2', pred_distill=True, seed=42, softmax_act='2quad', student_model='tmp/distill/SST2/quad_2quad/bert-base-uncased/5e-05_1e-05_32', task_name='SST2', teacher_model='/home/ubuntu/checkpoints/exp/SST2', temperature=1.0, train_batch_size=32, warmup_proportion=0.1, weight_decay=0.0001)
2022-08-29 18:46:55,495 device: cuda n_gpu: 2
2022-08-29 18:46:55,501 ***** Running evaluation *****
2022-08-29 18:46:55,502   Epoch = 4 iter 4799 step
2022-08-29 18:46:55,502   Num examples = 1043
2022-08-29 18:46:55,502   Batch size = 32
2022-08-29 18:46:55,503 ***** Eval results *****
2022-08-29 18:46:55,503   att_loss = 1.0194591074559902
2022-08-29 18:46:55,503   cls_loss = 0.0
2022-08-29 18:46:55,503   global_step = 4799
2022-08-29 18:46:55,503   loss = 2.8756094024122554
2022-08-29 18:46:55,503   rep_loss = 1.856150295069367
2022-08-29 18:46:55,503 ***** Save model *****
2022-08-29 18:47:13,410 ***** Running evaluation *****
2022-08-29 18:47:13,410   Epoch = 1 iter 2199 step
2022-08-29 18:47:13,410   Num examples = 872
2022-08-29 18:47:13,410   Batch size = 32
2022-08-29 18:47:15,619 ***** Eval results *****
2022-08-29 18:47:15,620   acc = 0.9128440366972477
2022-08-29 18:47:15,620   att_loss = 0.0
2022-08-29 18:47:15,620   cls_loss = 0.03122631473662822
2022-08-29 18:47:15,620   eval_loss = 0.26745487603225876
2022-08-29 18:47:15,620   global_step = 2199
2022-08-29 18:47:15,620   loss = 0.03122631473662822
2022-08-29 18:47:15,620   rep_loss = 0.0
2022-08-29 18:47:28,057 ***** Running evaluation *****
2022-08-29 18:47:28,058   Epoch = 1 iter 22799 step
2022-08-29 18:47:28,058   Num examples = 9815
2022-08-29 18:47:28,058   Batch size = 32
2022-08-29 18:47:28,059 ***** Eval results *****
2022-08-29 18:47:28,059   att_loss = 4.367582327530558
2022-08-29 18:47:28,059   cls_loss = 0.0
2022-08-29 18:47:28,059   global_step = 22799
2022-08-29 18:47:28,059   loss = 6.279531362344791
2022-08-29 18:47:28,059   rep_loss = 1.9119490362635863
2022-08-29 18:47:28,060 ***** Save model *****
2022-08-29 18:47:35,341 ***** Running evaluation *****
2022-08-29 18:47:35,342   Epoch = 4 iter 4999 step
2022-08-29 18:47:35,342   Num examples = 1043
2022-08-29 18:47:35,342   Batch size = 32
2022-08-29 18:47:35,344 ***** Eval results *****
2022-08-29 18:47:35,344   att_loss = 1.0208503305502081
2022-08-29 18:47:35,344   cls_loss = 0.0
2022-08-29 18:47:35,344   global_step = 4999
2022-08-29 18:47:35,344   loss = 2.8752074517114945
2022-08-29 18:47:35,344   rep_loss = 1.8543571192755823
2022-08-29 18:47:35,344 ***** Save model *****
2022-08-29 18:47:56,187 ***** Running evaluation *****
2022-08-29 18:47:56,188   Epoch = 1 iter 2299 step
2022-08-29 18:47:56,188   Num examples = 872
2022-08-29 18:47:56,188   Batch size = 32
2022-08-29 18:47:58,390 ***** Eval results *****
2022-08-29 18:47:58,391   acc = 0.9139908256880734
2022-08-29 18:47:58,391   att_loss = 0.0
2022-08-29 18:47:58,391   cls_loss = 0.032025732778203794
2022-08-29 18:47:58,391   eval_loss = 0.2855712731501886
2022-08-29 18:47:58,391   global_step = 2299
2022-08-29 18:47:58,391   loss = 0.032025732778203794
2022-08-29 18:47:58,391   rep_loss = 0.0
2022-08-29 18:48:20,092 ***** Running evaluation *****
2022-08-29 18:48:20,092   Epoch = 4 iter 5199 step
2022-08-29 18:48:20,092   Num examples = 1043
2022-08-29 18:48:20,092   Batch size = 32
2022-08-29 18:48:20,094 ***** Eval results *****
2022-08-29 18:48:20,094   att_loss = 1.01920071974027
2022-08-29 18:48:20,094   cls_loss = 0.0
2022-08-29 18:48:20,094   global_step = 5199
2022-08-29 18:48:20,094   loss = 2.8702282324320803
2022-08-29 18:48:20,094   rep_loss = 1.8510275123703184
2022-08-29 18:48:20,094 ***** Save model *****
2022-08-29 18:48:37,264 ***** Running evaluation *****
2022-08-29 18:48:37,265   Epoch = 1 iter 22999 step
2022-08-29 18:48:37,265   Num examples = 9815
2022-08-29 18:48:37,265   Batch size = 32
2022-08-29 18:48:37,266 ***** Eval results *****
2022-08-29 18:48:37,266   att_loss = 4.366079649891452
2022-08-29 18:48:37,266   cls_loss = 0.0
2022-08-29 18:48:37,266   global_step = 22999
2022-08-29 18:48:37,266   loss = 6.277216963986689
2022-08-29 18:48:37,266   rep_loss = 1.9111373155286826
2022-08-29 18:48:37,266 ***** Save model *****
2022-08-29 18:48:38,911 ***** Running evaluation *****
2022-08-29 18:48:38,912   Epoch = 1 iter 2399 step
2022-08-29 18:48:38,912   Num examples = 872
2022-08-29 18:48:38,912   Batch size = 32
2022-08-29 18:48:41,119 ***** Eval results *****
2022-08-29 18:48:41,119   acc = 0.9094036697247706
2022-08-29 18:48:41,119   att_loss = 0.0
2022-08-29 18:48:41,119   cls_loss = 0.031143813119348832
2022-08-29 18:48:41,119   eval_loss = 0.3067880042695573
2022-08-29 18:48:41,119   global_step = 2399
2022-08-29 18:48:41,119   loss = 0.031143813119348832
2022-08-29 18:48:41,119   rep_loss = 0.0
2022-08-29 18:48:59,966 ***** Running evaluation *****
2022-08-29 18:48:59,967   Epoch = 5 iter 5399 step
2022-08-29 18:48:59,967   Num examples = 1043
2022-08-29 18:48:59,967   Batch size = 32
2022-08-29 18:48:59,968 ***** Eval results *****
2022-08-29 18:48:59,968   att_loss = 1.033951223906824
2022-08-29 18:48:59,968   cls_loss = 0.0
2022-08-29 18:48:59,968   global_step = 5399
2022-08-29 18:48:59,968   loss = 2.868593454360962
2022-08-29 18:48:59,969   rep_loss = 1.8346422445976127
2022-08-29 18:48:59,969 ***** Save model *****
2022-08-29 18:49:21,641 ***** Running evaluation *****
2022-08-29 18:49:21,641   Epoch = 1 iter 2499 step
2022-08-29 18:49:21,642   Num examples = 872
2022-08-29 18:49:21,642   Batch size = 32
2022-08-29 18:49:23,845 ***** Eval results *****
2022-08-29 18:49:23,845   acc = 0.9128440366972477
2022-08-29 18:49:23,845   att_loss = 0.0
2022-08-29 18:49:23,845   cls_loss = 0.03094248836388505
2022-08-29 18:49:23,845   eval_loss = 0.258293586650065
2022-08-29 18:49:23,845   global_step = 2499
2022-08-29 18:49:23,845   loss = 0.03094248836388505
2022-08-29 18:49:23,845   rep_loss = 0.0
2022-08-29 18:49:39,816 ***** Running evaluation *****
2022-08-29 18:49:39,817   Epoch = 5 iter 5599 step
2022-08-29 18:49:39,817   Num examples = 1043
2022-08-29 18:49:39,817   Batch size = 32
2022-08-29 18:49:39,818 ***** Eval results *****
2022-08-29 18:49:39,818   att_loss = 0.9963697270997243
2022-08-29 18:49:39,818   cls_loss = 0.0
2022-08-29 18:49:39,818   global_step = 5599
2022-08-29 18:49:39,818   loss = 2.81169952020682
2022-08-29 18:49:39,818   rep_loss = 1.8153297933372292
2022-08-29 18:49:39,819 ***** Save model *****
2022-08-29 18:49:49,370 ***** Running evaluation *****
2022-08-29 18:49:49,371   Epoch = 1 iter 23199 step
2022-08-29 18:49:49,371   Num examples = 9815
2022-08-29 18:49:49,371   Batch size = 32
2022-08-29 18:49:49,372 ***** Eval results *****
2022-08-29 18:49:49,372   att_loss = 4.3640348296039795
2022-08-29 18:49:49,372   cls_loss = 0.0
2022-08-29 18:49:49,372   global_step = 23199
2022-08-29 18:49:49,372   loss = 6.274345948092236
2022-08-29 18:49:49,372   rep_loss = 1.9103111195572997
2022-08-29 18:49:49,373 ***** Save model *****
2022-08-29 18:50:04,405 ***** Running evaluation *****
2022-08-29 18:50:04,405   Epoch = 1 iter 2599 step
2022-08-29 18:50:04,405   Num examples = 872
2022-08-29 18:50:04,405   Batch size = 32
2022-08-29 18:50:06,611 ***** Eval results *****
2022-08-29 18:50:06,612   acc = 0.9094036697247706
2022-08-29 18:50:06,612   att_loss = 0.0
2022-08-29 18:50:06,612   cls_loss = 0.030976665279630458
2022-08-29 18:50:06,612   eval_loss = 0.29692238928484066
2022-08-29 18:50:06,612   global_step = 2599
2022-08-29 18:50:06,612   loss = 0.030976665279630458
2022-08-29 18:50:06,612   rep_loss = 0.0
2022-08-29 18:50:21,311 ***** Running evaluation *****
2022-08-29 18:50:21,311   Epoch = 5 iter 5799 step
2022-08-29 18:50:21,311   Num examples = 1043
2022-08-29 18:50:21,311   Batch size = 32
2022-08-29 18:50:21,313 ***** Eval results *****
2022-08-29 18:50:21,313   att_loss = 0.9853342065104732
2022-08-29 18:50:21,313   cls_loss = 0.0
2022-08-29 18:50:21,313   global_step = 5799
2022-08-29 18:50:21,313   loss = 2.791840577696923
2022-08-29 18:50:21,313   rep_loss = 1.8065063722253105
2022-08-29 18:50:21,313 ***** Save model *****
2022-08-29 18:50:47,133 ***** Running evaluation *****
2022-08-29 18:50:47,134   Epoch = 1 iter 2699 step
2022-08-29 18:50:47,134   Num examples = 872
2022-08-29 18:50:47,134   Batch size = 32
2022-08-29 18:50:49,340 ***** Eval results *****
2022-08-29 18:50:49,341   acc = 0.8864678899082569
2022-08-29 18:50:49,341   att_loss = 0.0
2022-08-29 18:50:49,341   cls_loss = 0.031086718991492727
2022-08-29 18:50:49,341   eval_loss = 0.3221288618764707
2022-08-29 18:50:49,341   global_step = 2699
2022-08-29 18:50:49,341   loss = 0.031086718991492727
2022-08-29 18:50:49,341   rep_loss = 0.0
2022-08-29 18:51:01,203 ***** Running evaluation *****
2022-08-29 18:51:01,204   Epoch = 5 iter 5999 step
2022-08-29 18:51:01,204   Num examples = 1043
2022-08-29 18:51:01,204   Batch size = 32
2022-08-29 18:51:01,205 ***** Eval results *****
2022-08-29 18:51:01,205   att_loss = 0.981866752160699
2022-08-29 18:51:01,205   cls_loss = 0.0
2022-08-29 18:51:01,205   global_step = 5999
2022-08-29 18:51:01,205   loss = 2.783540905878649
2022-08-29 18:51:01,205   rep_loss = 1.8016741565218102
2022-08-29 18:51:01,205 ***** Save model *****
2022-08-29 18:51:02,883 ***** Running evaluation *****
2022-08-29 18:51:02,883   Epoch = 1 iter 23399 step
2022-08-29 18:51:02,883   Num examples = 9815
2022-08-29 18:51:02,884   Batch size = 32
2022-08-29 18:51:02,885 ***** Eval results *****
2022-08-29 18:51:02,885   att_loss = 4.361667436202767
2022-08-29 18:51:02,885   cls_loss = 0.0
2022-08-29 18:51:02,885   global_step = 23399
2022-08-29 18:51:02,885   loss = 6.2712157915428675
2022-08-29 18:51:02,885   rep_loss = 1.909548356389931
2022-08-29 18:51:02,885 ***** Save model *****
2022-08-29 18:51:29,925 ***** Running evaluation *****
2022-08-29 18:51:29,925   Epoch = 1 iter 2799 step
2022-08-29 18:51:29,925   Num examples = 872
2022-08-29 18:51:29,925   Batch size = 32
2022-08-29 18:51:32,133 ***** Eval results *****
2022-08-29 18:51:32,133   acc = 0.8876146788990825
2022-08-29 18:51:32,133   att_loss = 0.0
2022-08-29 18:51:32,133   cls_loss = 0.030821110025423464
2022-08-29 18:51:32,133   eval_loss = 0.364802409229534
2022-08-29 18:51:32,134   global_step = 2799
2022-08-29 18:51:32,134   loss = 0.030821110025423464
2022-08-29 18:51:32,134   rep_loss = 0.0
2022-08-29 18:51:40,066 ***** Running evaluation *****
2022-08-29 18:51:40,067   Epoch = 5 iter 6199 step
2022-08-29 18:51:40,067   Num examples = 1043
2022-08-29 18:51:40,067   Batch size = 32
2022-08-29 18:51:40,068 ***** Eval results *****
2022-08-29 18:51:40,068   att_loss = 0.9772325675062862
2022-08-29 18:51:40,069   cls_loss = 0.0
2022-08-29 18:51:40,069   global_step = 6199
2022-08-29 18:51:40,069   loss = 2.777028711905163
2022-08-29 18:51:40,069   rep_loss = 1.7997961448152073
2022-08-29 18:51:40,069 ***** Save model *****
2022-08-29 18:52:12,670 ***** Running evaluation *****
2022-08-29 18:52:12,670   Epoch = 1 iter 2899 step
2022-08-29 18:52:12,670   Num examples = 872
2022-08-29 18:52:12,670   Batch size = 32
2022-08-29 18:52:13,705 ***** Running evaluation *****
2022-08-29 18:52:13,705   Epoch = 1 iter 23599 step
2022-08-29 18:52:13,705   Num examples = 9815
2022-08-29 18:52:13,705   Batch size = 32
2022-08-29 18:52:13,707 ***** Eval results *****
2022-08-29 18:52:13,707   att_loss = 4.360149719599781
2022-08-29 18:52:13,707   cls_loss = 0.0
2022-08-29 18:52:13,707   global_step = 23599
2022-08-29 18:52:13,707   loss = 6.26894313576868
2022-08-29 18:52:13,707   rep_loss = 1.9087934172738574
2022-08-29 18:52:13,707 ***** Save model *****
2022-08-29 18:52:14,882 ***** Eval results *****
2022-08-29 18:52:14,882   acc = 0.9162844036697247
2022-08-29 18:52:14,882   att_loss = 0.0
2022-08-29 18:52:14,882   cls_loss = 0.03081991631162804
2022-08-29 18:52:14,882   eval_loss = 0.2709245269319841
2022-08-29 18:52:14,882   global_step = 2899
2022-08-29 18:52:14,882   loss = 0.03081991631162804
2022-08-29 18:52:14,882   rep_loss = 0.0
2022-08-29 18:52:18,258 ***** Running evaluation *****
2022-08-29 18:52:18,258   Epoch = 5 iter 6399 step
2022-08-29 18:52:18,258   Num examples = 1043
2022-08-29 18:52:18,258   Batch size = 32
2022-08-29 18:52:18,260 ***** Eval results *****
2022-08-29 18:52:18,260   att_loss = 0.9771516467176371
2022-08-29 18:52:18,260   cls_loss = 0.0
2022-08-29 18:52:18,260   global_step = 6399
2022-08-29 18:52:18,260   loss = 2.77678000082713
2022-08-29 18:52:18,260   rep_loss = 1.7996283535466542
2022-08-29 18:52:18,260 ***** Save model *****
2022-08-29 18:52:55,421 ***** Running evaluation *****
2022-08-29 18:52:55,421   Epoch = 1 iter 2999 step
2022-08-29 18:52:55,421   Num examples = 872
2022-08-29 18:52:55,421   Batch size = 32
2022-08-29 18:52:56,289 ***** Running evaluation *****
2022-08-29 18:52:56,290   Epoch = 6 iter 6599 step
2022-08-29 18:52:56,290   Num examples = 1043
2022-08-29 18:52:56,290   Batch size = 32
2022-08-29 18:52:56,291 ***** Eval results *****
2022-08-29 18:52:56,291   att_loss = 0.9319084296051744
2022-08-29 18:52:56,292   cls_loss = 0.0
2022-08-29 18:52:56,292   global_step = 6599
2022-08-29 18:52:56,292   loss = 2.686027593013504
2022-08-29 18:52:56,292   rep_loss = 1.7541191596635348
2022-08-29 18:52:56,292 ***** Save model *****
2022-08-29 18:52:57,634 ***** Eval results *****
2022-08-29 18:52:57,634   acc = 0.9094036697247706
2022-08-29 18:52:57,634   att_loss = 0.0
2022-08-29 18:52:57,635   cls_loss = 0.030985991523065214
2022-08-29 18:52:57,635   eval_loss = 0.3148480713633554
2022-08-29 18:52:57,635   global_step = 2999
2022-08-29 18:52:57,635   loss = 0.030985991523065214
2022-08-29 18:52:57,635   rep_loss = 0.0
2022-08-29 18:53:22,764 ***** Running evaluation *****
2022-08-29 18:53:22,764   Epoch = 1 iter 23799 step
2022-08-29 18:53:22,765   Num examples = 9815
2022-08-29 18:53:22,765   Batch size = 32
2022-08-29 18:53:22,766 ***** Eval results *****
2022-08-29 18:53:22,766   att_loss = 4.359138932536659
2022-08-29 18:53:22,766   cls_loss = 0.0
2022-08-29 18:53:22,766   global_step = 23799
2022-08-29 18:53:22,766   loss = 6.267194916356528
2022-08-29 18:53:22,766   rep_loss = 1.9080559852572467
2022-08-29 18:53:22,766 ***** Save model *****
2022-08-29 18:53:38,186 ***** Running evaluation *****
2022-08-29 18:53:38,186   Epoch = 1 iter 3099 step
2022-08-29 18:53:38,186   Num examples = 872
2022-08-29 18:53:38,187   Batch size = 32
2022-08-29 18:53:40,393 ***** Eval results *****
2022-08-29 18:53:40,393   acc = 0.9151376146788991
2022-08-29 18:53:40,393   att_loss = 0.0
2022-08-29 18:53:40,394   cls_loss = 0.031245218329225204
2022-08-29 18:53:40,394   eval_loss = 0.27377609216741156
2022-08-29 18:53:40,394   global_step = 3099
2022-08-29 18:53:40,394   loss = 0.031245218329225204
2022-08-29 18:53:40,394   rep_loss = 0.0
2022-08-29 18:53:42,076 ***** Running evaluation *****
2022-08-29 18:53:42,077   Epoch = 6 iter 6799 step
2022-08-29 18:53:42,077   Num examples = 1043
2022-08-29 18:53:42,077   Batch size = 32
2022-08-29 18:53:42,078 ***** Eval results *****
2022-08-29 18:53:42,078   att_loss = 0.938928731109785
2022-08-29 18:53:42,079   cls_loss = 0.0
2022-08-29 18:53:42,079   global_step = 6799
2022-08-29 18:53:42,079   loss = 2.6998404669944587
2022-08-29 18:53:42,079   rep_loss = 1.7609117339029337
2022-08-29 18:53:42,079 ***** Save model *****
2022-08-29 18:54:20,955 ***** Running evaluation *****
2022-08-29 18:54:20,955   Epoch = 1 iter 3199 step
2022-08-29 18:54:20,955   Num examples = 872
2022-08-29 18:54:20,955   Batch size = 32
2022-08-29 18:54:23,003 ***** Running evaluation *****
2022-08-29 18:54:23,003   Epoch = 6 iter 6999 step
2022-08-29 18:54:23,004   Num examples = 1043
2022-08-29 18:54:23,004   Batch size = 32
2022-08-29 18:54:23,005 ***** Eval results *****
2022-08-29 18:54:23,005   att_loss = 0.9465051273202332
2022-08-29 18:54:23,005   cls_loss = 0.0
2022-08-29 18:54:23,005   global_step = 6999
2022-08-29 18:54:23,005   loss = 2.7105024550771954
2022-08-29 18:54:23,005   rep_loss = 1.763997328059524
2022-08-29 18:54:23,005 ***** Save model *****
2022-08-29 18:54:23,161 ***** Eval results *****
2022-08-29 18:54:23,161   acc = 0.9094036697247706
2022-08-29 18:54:23,161   att_loss = 0.0
2022-08-29 18:54:23,161   cls_loss = 0.031170320676290962
2022-08-29 18:54:23,161   eval_loss = 0.31420508213341236
2022-08-29 18:54:23,161   global_step = 3199
2022-08-29 18:54:23,162   loss = 0.031170320676290962
2022-08-29 18:54:23,162   rep_loss = 0.0
2022-08-29 18:54:33,935 ***** Running evaluation *****
2022-08-29 18:54:33,935   Epoch = 1 iter 23999 step
2022-08-29 18:54:33,935   Num examples = 9815
2022-08-29 18:54:33,936   Batch size = 32
2022-08-29 18:54:33,937 ***** Eval results *****
2022-08-29 18:54:33,937   att_loss = 4.356817846002228
2022-08-29 18:54:33,937   cls_loss = 0.0
2022-08-29 18:54:33,937   global_step = 23999
2022-08-29 18:54:33,937   loss = 6.264072872769458
2022-08-29 18:54:33,937   rep_loss = 1.907255028241084
2022-08-29 18:54:33,937 ***** Save model *****
2022-08-29 18:55:01,267 ***** Running evaluation *****
2022-08-29 18:55:01,268   Epoch = 6 iter 7199 step
2022-08-29 18:55:01,268   Num examples = 1043
2022-08-29 18:55:01,268   Batch size = 32
2022-08-29 18:55:01,270 ***** Eval results *****
2022-08-29 18:55:01,270   att_loss = 0.9445735641134675
2022-08-29 18:55:01,270   cls_loss = 0.0
2022-08-29 18:55:01,270   global_step = 7199
2022-08-29 18:55:01,270   loss = 2.705599364076945
2022-08-29 18:55:01,270   rep_loss = 1.7610257989085278
2022-08-29 18:55:01,270 ***** Save model *****
2022-08-29 18:55:03,711 ***** Running evaluation *****
2022-08-29 18:55:03,712   Epoch = 1 iter 3299 step
2022-08-29 18:55:03,712   Num examples = 872
2022-08-29 18:55:03,712   Batch size = 32
2022-08-29 18:55:05,914 ***** Eval results *****
2022-08-29 18:55:05,914   acc = 0.8956422018348624
2022-08-29 18:55:05,914   att_loss = 0.0
2022-08-29 18:55:05,914   cls_loss = 0.03107890249794996
2022-08-29 18:55:05,915   eval_loss = 0.31060069360371145
2022-08-29 18:55:05,915   global_step = 3299
2022-08-29 18:55:05,915   loss = 0.03107890249794996
2022-08-29 18:55:05,915   rep_loss = 0.0
2022-08-29 18:55:39,503 ***** Running evaluation *****
2022-08-29 18:55:39,503   Epoch = 6 iter 7399 step
2022-08-29 18:55:39,503   Num examples = 1043
2022-08-29 18:55:39,503   Batch size = 32
2022-08-29 18:55:39,504 ***** Eval results *****
2022-08-29 18:55:39,504   att_loss = 0.9451886508106343
2022-08-29 18:55:39,504   cls_loss = 0.0
2022-08-29 18:55:39,505   global_step = 7399
2022-08-29 18:55:39,505   loss = 2.707040697005394
2022-08-29 18:55:39,505   rep_loss = 1.7618520449918407
2022-08-29 18:55:39,505 ***** Save model *****
2022-08-29 18:55:42,878 ***** Running evaluation *****
2022-08-29 18:55:42,878   Epoch = 1 iter 24199 step
2022-08-29 18:55:42,878   Num examples = 9815
2022-08-29 18:55:42,878   Batch size = 32
2022-08-29 18:55:42,879 ***** Eval results *****
2022-08-29 18:55:42,879   att_loss = 4.354951550783046
2022-08-29 18:55:42,880   cls_loss = 0.0
2022-08-29 18:55:42,880   global_step = 24199
2022-08-29 18:55:42,880   loss = 6.261400089657283
2022-08-29 18:55:42,880   rep_loss = 1.9064485405032703
2022-08-29 18:55:42,880 ***** Save model *****
2022-08-29 18:55:46,459 ***** Running evaluation *****
2022-08-29 18:55:46,459   Epoch = 1 iter 3399 step
2022-08-29 18:55:46,459   Num examples = 872
2022-08-29 18:55:46,459   Batch size = 32
2022-08-29 18:55:48,665 ***** Eval results *****
2022-08-29 18:55:48,665   acc = 0.9174311926605505
2022-08-29 18:55:48,665   att_loss = 0.0
2022-08-29 18:55:48,665   cls_loss = 0.03087636146615313
2022-08-29 18:55:48,665   eval_loss = 0.2892898798787168
2022-08-29 18:55:48,665   global_step = 3399
2022-08-29 18:55:48,665   loss = 0.03087636146615313
2022-08-29 18:55:48,665   rep_loss = 0.0
2022-08-29 18:56:17,760 ***** Running evaluation *****
2022-08-29 18:56:17,760   Epoch = 7 iter 7599 step
2022-08-29 18:56:17,760   Num examples = 1043
2022-08-29 18:56:17,760   Batch size = 32
2022-08-29 18:56:17,762 ***** Eval results *****
2022-08-29 18:56:17,762   att_loss = 0.9435126621548723
2022-08-29 18:56:17,762   cls_loss = 0.0
2022-08-29 18:56:17,762   global_step = 7599
2022-08-29 18:56:17,762   loss = 2.680330268735808
2022-08-29 18:56:17,762   rep_loss = 1.7368175934969894
2022-08-29 18:56:17,762 ***** Save model *****
2022-08-29 18:56:29,239 ***** Running evaluation *****
2022-08-29 18:56:29,240   Epoch = 1 iter 3499 step
2022-08-29 18:56:29,240   Num examples = 872
2022-08-29 18:56:29,240   Batch size = 32
2022-08-29 18:56:31,446 ***** Eval results *****
2022-08-29 18:56:31,447   acc = 0.911697247706422
2022-08-29 18:56:31,447   att_loss = 0.0
2022-08-29 18:56:31,447   cls_loss = 0.030701645722739587
2022-08-29 18:56:31,447   eval_loss = 0.29152838533212033
2022-08-29 18:56:31,447   global_step = 3499
2022-08-29 18:56:31,447   loss = 0.030701645722739587
2022-08-29 18:56:31,447   rep_loss = 0.0
2022-08-29 18:56:55,621 ***** Running evaluation *****
2022-08-29 18:56:55,622   Epoch = 1 iter 24399 step
2022-08-29 18:56:55,622   Num examples = 9815
2022-08-29 18:56:55,622   Batch size = 32
2022-08-29 18:56:55,623 ***** Eval results *****
2022-08-29 18:56:55,623   att_loss = 4.352674483564881
2022-08-29 18:56:55,623   cls_loss = 0.0
2022-08-29 18:56:55,623   global_step = 24399
2022-08-29 18:56:55,623   loss = 6.258351430375532
2022-08-29 18:56:55,623   rep_loss = 1.9056769484423082
2022-08-29 18:56:55,623 ***** Save model *****
2022-08-29 18:57:00,364 ***** Running evaluation *****
2022-08-29 18:57:00,364   Epoch = 7 iter 7799 step
2022-08-29 18:57:00,364   Num examples = 1043
2022-08-29 18:57:00,364   Batch size = 32
2022-08-29 18:57:00,366 ***** Eval results *****
2022-08-29 18:57:00,366   att_loss = 0.9365823366693667
2022-08-29 18:57:00,366   cls_loss = 0.0
2022-08-29 18:57:00,366   global_step = 7799
2022-08-29 18:57:00,366   loss = 2.674343264139843
2022-08-29 18:57:00,366   rep_loss = 1.7377609228571134
2022-08-29 18:57:00,366 ***** Save model *****
2022-08-29 18:57:11,980 ***** Running evaluation *****
2022-08-29 18:57:11,981   Epoch = 1 iter 3599 step
2022-08-29 18:57:11,981   Num examples = 872
2022-08-29 18:57:11,981   Batch size = 32
2022-08-29 18:57:14,184 ***** Eval results *****
2022-08-29 18:57:14,184   acc = 0.9025229357798165
2022-08-29 18:57:14,184   att_loss = 0.0
2022-08-29 18:57:14,184   cls_loss = 0.03072231639407102
2022-08-29 18:57:14,184   eval_loss = 0.3036414756040488
2022-08-29 18:57:14,184   global_step = 3599
2022-08-29 18:57:14,185   loss = 0.03072231639407102
2022-08-29 18:57:14,185   rep_loss = 0.0
2022-08-29 18:57:38,589 ***** Running evaluation *****
2022-08-29 18:57:38,590   Epoch = 7 iter 7999 step
2022-08-29 18:57:38,590   Num examples = 1043
2022-08-29 18:57:38,590   Batch size = 32
2022-08-29 18:57:38,591 ***** Eval results *****
2022-08-29 18:57:38,591   att_loss = 0.9317850094452415
2022-08-29 18:57:38,591   cls_loss = 0.0
2022-08-29 18:57:38,591   global_step = 7999
2022-08-29 18:57:38,591   loss = 2.67014789216605
2022-08-29 18:57:38,591   rep_loss = 1.7383628791878374
2022-08-29 18:57:38,592 ***** Save model *****
2022-08-29 18:57:54,786 ***** Running evaluation *****
2022-08-29 18:57:54,787   Epoch = 1 iter 3699 step
2022-08-29 18:57:54,787   Num examples = 872
2022-08-29 18:57:54,787   Batch size = 32
2022-08-29 18:57:56,995 ***** Eval results *****
2022-08-29 18:57:56,996   acc = 0.9002293577981652
2022-08-29 18:57:56,996   att_loss = 0.0
2022-08-29 18:57:56,996   cls_loss = 0.03073313244615556
2022-08-29 18:57:56,996   eval_loss = 0.319761950190046
2022-08-29 18:57:56,996   global_step = 3699
2022-08-29 18:57:56,996   loss = 0.03073313244615556
2022-08-29 18:57:56,996   rep_loss = 0.0
2022-08-29 18:58:08,746 ***** Running evaluation *****
2022-08-29 18:58:08,746   Epoch = 2 iter 24599 step
2022-08-29 18:58:08,746   Num examples = 9815
2022-08-29 18:58:08,747   Batch size = 32
2022-08-29 18:58:08,748 ***** Eval results *****
2022-08-29 18:58:08,748   att_loss = 4.243582307246694
2022-08-29 18:58:08,748   cls_loss = 0.0
2022-08-29 18:58:08,748   global_step = 24599
2022-08-29 18:58:08,748   loss = 6.095264710878071
2022-08-29 18:58:08,748   rep_loss = 1.851682418271115
2022-08-29 18:58:08,748 ***** Save model *****
2022-08-29 18:58:17,513 ***** Running evaluation *****
2022-08-29 18:58:17,514   Epoch = 7 iter 8199 step
2022-08-29 18:58:17,514   Num examples = 1043
2022-08-29 18:58:17,514   Batch size = 32
2022-08-29 18:58:17,515 ***** Eval results *****
2022-08-29 18:58:17,515   att_loss = 0.9306262192389777
2022-08-29 18:58:17,515   cls_loss = 0.0
2022-08-29 18:58:17,515   global_step = 8199
2022-08-29 18:58:17,515   loss = 2.6691935840648897
2022-08-29 18:58:17,516   rep_loss = 1.7385673611985202
2022-08-29 18:58:17,516 ***** Save model *****
2022-08-29 18:58:37,750 ***** Running evaluation *****
2022-08-29 18:58:37,750   Epoch = 1 iter 3799 step
2022-08-29 18:58:37,750   Num examples = 872
2022-08-29 18:58:37,750   Batch size = 32
2022-08-29 18:58:39,959 ***** Eval results *****
2022-08-29 18:58:39,959   acc = 0.9151376146788991
2022-08-29 18:58:39,959   att_loss = 0.0
2022-08-29 18:58:39,959   cls_loss = 0.03057232935745324
2022-08-29 18:58:39,959   eval_loss = 0.26990384621811764
2022-08-29 18:58:39,959   global_step = 3799
2022-08-29 18:58:39,959   loss = 0.03057232935745324
2022-08-29 18:58:39,959   rep_loss = 0.0
2022-08-29 18:58:55,721 ***** Running evaluation *****
2022-08-29 18:58:55,722   Epoch = 7 iter 8399 step
2022-08-29 18:58:55,722   Num examples = 1043
2022-08-29 18:58:55,722   Batch size = 32
2022-08-29 18:58:55,723 ***** Eval results *****
2022-08-29 18:58:55,723   att_loss = 0.9266607953176984
2022-08-29 18:58:55,723   cls_loss = 0.0
2022-08-29 18:58:55,723   global_step = 8399
2022-08-29 18:58:55,723   loss = 2.6632666104793032
2022-08-29 18:58:55,724   rep_loss = 1.7366058123847903
2022-08-29 18:58:55,724 ***** Save model *****
2022-08-29 18:59:20,858 ***** Running evaluation *****
2022-08-29 18:59:20,858   Epoch = 1 iter 3899 step
2022-08-29 18:59:20,858   Num examples = 872
2022-08-29 18:59:20,858   Batch size = 32
2022-08-29 18:59:21,323 ***** Running evaluation *****
2022-08-29 18:59:21,324   Epoch = 2 iter 24799 step
2022-08-29 18:59:21,324   Num examples = 9815
2022-08-29 18:59:21,324   Batch size = 32
2022-08-29 18:59:21,325 ***** Eval results *****
2022-08-29 18:59:21,325   att_loss = 4.217676818602744
2022-08-29 18:59:21,325   cls_loss = 0.0
2022-08-29 18:59:21,325   global_step = 24799
2022-08-29 18:59:21,325   loss = 6.0718494315091736
2022-08-29 18:59:21,325   rep_loss = 1.854172616617225
2022-08-29 18:59:21,326 ***** Save model *****
2022-08-29 18:59:23,074 ***** Eval results *****
2022-08-29 18:59:23,074   acc = 0.8967889908256881
2022-08-29 18:59:23,074   att_loss = 0.0
2022-08-29 18:59:23,074   cls_loss = 0.030541972866058763
2022-08-29 18:59:23,074   eval_loss = 0.3236390832878117
2022-08-29 18:59:23,074   global_step = 3899
2022-08-29 18:59:23,075   loss = 0.030541972866058763
2022-08-29 18:59:23,075   rep_loss = 0.0
2022-08-29 18:59:34,109 ***** Running evaluation *****
2022-08-29 18:59:34,110   Epoch = 8 iter 8599 step
2022-08-29 18:59:34,110   Num examples = 1043
2022-08-29 18:59:34,110   Batch size = 32
2022-08-29 18:59:34,111 ***** Eval results *****
2022-08-29 18:59:34,111   att_loss = 0.9169659419493241
2022-08-29 18:59:34,111   cls_loss = 0.0
2022-08-29 18:59:34,111   global_step = 8599
2022-08-29 18:59:34,111   loss = 2.644609212875366
2022-08-29 18:59:34,111   rep_loss = 1.7276432600888338
2022-08-29 18:59:34,111 ***** Save model *****
2022-08-29 19:00:03,960 ***** Running evaluation *****
2022-08-29 19:00:03,960   Epoch = 1 iter 3999 step
2022-08-29 19:00:03,960   Num examples = 872
2022-08-29 19:00:03,960   Batch size = 32
2022-08-29 19:00:06,170 ***** Eval results *****
2022-08-29 19:00:06,170   acc = 0.9220183486238532
2022-08-29 19:00:06,171   att_loss = 0.0
2022-08-29 19:00:06,171   cls_loss = 0.03062368190618807
2022-08-29 19:00:06,171   eval_loss = 0.2576854641083628
2022-08-29 19:00:06,171   global_step = 3999
2022-08-29 19:00:06,171   loss = 0.03062368190618807
2022-08-29 19:00:06,171   rep_loss = 0.0
2022-08-29 19:00:06,171 ***** Save model *****
2022-08-29 19:00:15,105 ***** Running evaluation *****
2022-08-29 19:00:15,105   Epoch = 8 iter 8799 step
2022-08-29 19:00:15,106   Num examples = 1043
2022-08-29 19:00:15,106   Batch size = 32
2022-08-29 19:00:15,107 ***** Eval results *****
2022-08-29 19:00:15,107   att_loss = 0.8890714037652109
2022-08-29 19:00:15,107   cls_loss = 0.0
2022-08-29 19:00:15,107   global_step = 8799
2022-08-29 19:00:15,107   loss = 2.5944112459818522
2022-08-29 19:00:15,107   rep_loss = 1.705339847359003
2022-08-29 19:00:15,107 ***** Save model *****
2022-08-29 19:00:32,391 ***** Running evaluation *****
2022-08-29 19:00:32,392   Epoch = 2 iter 24999 step
2022-08-29 19:00:32,392   Num examples = 9815
2022-08-29 19:00:32,392   Batch size = 32
2022-08-29 19:00:32,393 ***** Eval results *****
2022-08-29 19:00:32,393   att_loss = 4.20764700432575
2022-08-29 19:00:32,393   cls_loss = 0.0
2022-08-29 19:00:32,393   global_step = 24999
2022-08-29 19:00:32,393   loss = 6.061242239219616
2022-08-29 19:00:32,393   rep_loss = 1.8535952393283468
2022-08-29 19:00:32,393 ***** Save model *****
2022-08-29 19:00:50,210 ***** Running evaluation *****
2022-08-29 19:00:50,210   Epoch = 1 iter 4099 step
2022-08-29 19:00:50,210   Num examples = 872
2022-08-29 19:00:50,210   Batch size = 32
2022-08-29 19:00:52,417 ***** Eval results *****
2022-08-29 19:00:52,417   acc = 0.9231651376146789
2022-08-29 19:00:52,417   att_loss = 0.0
2022-08-29 19:00:52,417   cls_loss = 0.03072010237597545
2022-08-29 19:00:52,418   eval_loss = 0.253194352013192
2022-08-29 19:00:52,418   global_step = 4099
2022-08-29 19:00:52,418   loss = 0.03072010237597545
2022-08-29 19:00:52,418   rep_loss = 0.0
2022-08-29 19:00:52,418 ***** Save model *****
2022-08-29 19:00:53,134 ***** Running evaluation *****
2022-08-29 19:00:53,135   Epoch = 8 iter 8999 step
2022-08-29 19:00:53,135   Num examples = 1043
2022-08-29 19:00:53,135   Batch size = 32
2022-08-29 19:00:53,136 ***** Eval results *****
2022-08-29 19:00:53,136   att_loss = 0.9035635595793252
2022-08-29 19:00:53,136   cls_loss = 0.0
2022-08-29 19:00:53,136   global_step = 8999
2022-08-29 19:00:53,137   loss = 2.618498543330601
2022-08-29 19:00:53,137   rep_loss = 1.7149349872882549
2022-08-29 19:00:53,137 ***** Save model *****
2022-08-29 19:01:34,182 ***** Running evaluation *****
2022-08-29 19:01:34,183   Epoch = 8 iter 9199 step
2022-08-29 19:01:34,183   Num examples = 1043
2022-08-29 19:01:34,183   Batch size = 32
2022-08-29 19:01:34,184 ***** Eval results *****
2022-08-29 19:01:34,184   att_loss = 0.9042432129838085
2022-08-29 19:01:34,184   cls_loss = 0.0
2022-08-29 19:01:34,184   global_step = 9199
2022-08-29 19:01:34,184   loss = 2.6163180118298714
2022-08-29 19:01:34,185   rep_loss = 1.7120748010300497
2022-08-29 19:01:34,185 ***** Save model *****
2022-08-29 19:01:38,032 ***** Running evaluation *****
2022-08-29 19:01:38,033   Epoch = 1 iter 4199 step
2022-08-29 19:01:38,033   Num examples = 872
2022-08-29 19:01:38,033   Batch size = 32
2022-08-29 19:01:40,239 ***** Eval results *****
2022-08-29 19:01:40,240   acc = 0.9162844036697247
2022-08-29 19:01:40,240   att_loss = 0.0
2022-08-29 19:01:40,240   cls_loss = 0.030643849242211883
2022-08-29 19:01:40,240   eval_loss = 0.2616356806829572
2022-08-29 19:01:40,240   global_step = 4199
2022-08-29 19:01:40,240   loss = 0.030643849242211883
2022-08-29 19:01:40,240   rep_loss = 0.0
2022-08-29 19:01:43,721 ***** Running evaluation *****
2022-08-29 19:01:43,722   Epoch = 2 iter 25199 step
2022-08-29 19:01:43,722   Num examples = 9815
2022-08-29 19:01:43,722   Batch size = 32
2022-08-29 19:01:43,723 ***** Eval results *****
2022-08-29 19:01:43,723   att_loss = 4.217107887137426
2022-08-29 19:01:43,723   cls_loss = 0.0
2022-08-29 19:01:43,723   global_step = 25199
2022-08-29 19:01:43,723   loss = 6.0714124276032
2022-08-29 19:01:43,723   rep_loss = 1.8543045457276761
2022-08-29 19:01:43,724 ***** Save model *****
2022-08-29 19:02:16,433 ***** Running evaluation *****
2022-08-29 19:02:16,434   Epoch = 8 iter 9399 step
2022-08-29 19:02:16,434   Num examples = 1043
2022-08-29 19:02:16,434   Batch size = 32
2022-08-29 19:02:16,435 ***** Eval results *****
2022-08-29 19:02:16,435   att_loss = 0.9076753854751587
2022-08-29 19:02:16,435   cls_loss = 0.0
2022-08-29 19:02:16,435   global_step = 9399
2022-08-29 19:02:16,435   loss = 2.620772023228874
2022-08-29 19:02:16,436   rep_loss = 1.7130966397056802
2022-08-29 19:02:16,436 ***** Save model *****
2022-08-29 19:02:20,712 ***** Running evaluation *****
2022-08-29 19:02:20,713   Epoch = 2 iter 4299 step
2022-08-29 19:02:20,713   Num examples = 872
2022-08-29 19:02:20,713   Batch size = 32
2022-08-29 19:02:22,914 ***** Eval results *****
2022-08-29 19:02:22,914   acc = 0.9151376146788991
2022-08-29 19:02:22,914   att_loss = 0.0
2022-08-29 19:02:22,915   cls_loss = 0.02871690370183397
2022-08-29 19:02:22,915   eval_loss = 0.2918126793312175
2022-08-29 19:02:22,915   global_step = 4299
2022-08-29 19:02:22,915   loss = 0.02871690370183397
2022-08-29 19:02:22,915   rep_loss = 0.0
2022-08-29 19:02:52,722 ***** Running evaluation *****
2022-08-29 19:02:52,722   Epoch = 2 iter 25399 step
2022-08-29 19:02:52,722   Num examples = 9815
2022-08-29 19:02:52,722   Batch size = 32
2022-08-29 19:02:52,724 ***** Eval results *****
2022-08-29 19:02:52,724   att_loss = 4.215376228267977
2022-08-29 19:02:52,724   cls_loss = 0.0
2022-08-29 19:02:52,724   global_step = 25399
2022-08-29 19:02:52,724   loss = 6.068572938511781
2022-08-29 19:02:52,724   rep_loss = 1.8531967135822203
2022-08-29 19:02:52,724 ***** Save model *****
2022-08-29 19:02:54,255 ***** Running evaluation *****
2022-08-29 19:02:54,255   Epoch = 8 iter 9599 step
2022-08-29 19:02:54,255   Num examples = 1043
2022-08-29 19:02:54,255   Batch size = 32
2022-08-29 19:02:54,257 ***** Eval results *****
2022-08-29 19:02:54,257   att_loss = 0.9100248406848637
2022-08-29 19:02:54,257   cls_loss = 0.0
2022-08-29 19:02:54,257   global_step = 9599
2022-08-29 19:02:54,257   loss = 2.6245889828668387
2022-08-29 19:02:54,257   rep_loss = 1.7145641447808506
2022-08-29 19:02:54,257 ***** Save model *****
2022-08-29 19:03:03,433 ***** Running evaluation *****
2022-08-29 19:03:03,434   Epoch = 2 iter 4399 step
2022-08-29 19:03:03,434   Num examples = 872
2022-08-29 19:03:03,434   Batch size = 32
2022-08-29 19:03:05,645 ***** Eval results *****
2022-08-29 19:03:05,645   acc = 0.908256880733945
2022-08-29 19:03:05,645   att_loss = 0.0
2022-08-29 19:03:05,645   cls_loss = 0.028176347035134493
2022-08-29 19:03:05,645   eval_loss = 0.29230782110244036
2022-08-29 19:03:05,645   global_step = 4399
2022-08-29 19:03:05,645   loss = 0.028176347035134493
2022-08-29 19:03:05,645   rep_loss = 0.0
2022-08-29 19:03:38,201 ***** Running evaluation *****
2022-08-29 19:03:38,202   Epoch = 9 iter 9799 step
2022-08-29 19:03:38,202   Num examples = 1043
2022-08-29 19:03:38,202   Batch size = 32
2022-08-29 19:03:38,203 ***** Eval results *****
2022-08-29 19:03:38,203   att_loss = 0.8952671152385161
2022-08-29 19:03:38,204   cls_loss = 0.0
2022-08-29 19:03:38,204   global_step = 9799
2022-08-29 19:03:38,204   loss = 2.5962862968444824
2022-08-29 19:03:38,204   rep_loss = 1.7010191793747764
2022-08-29 19:03:38,204 ***** Save model *****
2022-08-29 19:03:46,092 ***** Running evaluation *****
2022-08-29 19:03:46,093   Epoch = 2 iter 4499 step
2022-08-29 19:03:46,093   Num examples = 872
2022-08-29 19:03:46,093   Batch size = 32
2022-08-29 19:03:48,299 ***** Eval results *****
2022-08-29 19:03:48,299   acc = 0.9231651376146789
2022-08-29 19:03:48,299   att_loss = 0.0
2022-08-29 19:03:48,299   cls_loss = 0.028589776734728685
2022-08-29 19:03:48,299   eval_loss = 0.23998243029096297
2022-08-29 19:03:48,299   global_step = 4499
2022-08-29 19:03:48,299   loss = 0.028589776734728685
2022-08-29 19:03:48,299   rep_loss = 0.0
2022-08-29 19:04:10,196 ***** Running evaluation *****
2022-08-29 19:04:10,197   Epoch = 2 iter 25599 step
2022-08-29 19:04:10,197   Num examples = 9815
2022-08-29 19:04:10,197   Batch size = 32
2022-08-29 19:04:10,198 ***** Eval results *****
2022-08-29 19:04:10,198   att_loss = 4.214329706009763
2022-08-29 19:04:10,198   cls_loss = 0.0
2022-08-29 19:04:10,198   global_step = 25599
2022-08-29 19:04:10,198   loss = 6.066899884277171
2022-08-29 19:04:10,198   rep_loss = 1.8525701784929693
2022-08-29 19:04:10,198 ***** Save model *****
2022-08-29 19:04:16,555 ***** Running evaluation *****
2022-08-29 19:04:16,555   Epoch = 9 iter 9999 step
2022-08-29 19:04:16,555   Num examples = 1043
2022-08-29 19:04:16,555   Batch size = 32
2022-08-29 19:04:16,556 ***** Eval results *****
2022-08-29 19:04:16,556   att_loss = 0.8939278415315219
2022-08-29 19:04:16,556   cls_loss = 0.0
2022-08-29 19:04:16,556   global_step = 9999
2022-08-29 19:04:16,557   loss = 2.587230720569305
2022-08-29 19:04:16,557   rep_loss = 1.693302875341371
2022-08-29 19:04:16,557 ***** Save model *****
2022-08-29 19:04:28,752 ***** Running evaluation *****
2022-08-29 19:04:28,752   Epoch = 2 iter 4599 step
2022-08-29 19:04:28,752   Num examples = 872
2022-08-29 19:04:28,752   Batch size = 32
2022-08-29 19:04:30,960 ***** Eval results *****
2022-08-29 19:04:30,960   acc = 0.9002293577981652
2022-08-29 19:04:30,960   att_loss = 0.0
2022-08-29 19:04:30,960   cls_loss = 0.027718724313732762
2022-08-29 19:04:30,960   eval_loss = 0.34182260365092326
2022-08-29 19:04:30,960   global_step = 4599
2022-08-29 19:04:30,961   loss = 0.027718724313732762
2022-08-29 19:04:30,961   rep_loss = 0.0
2022-08-29 19:04:54,907 ***** Running evaluation *****
2022-08-29 19:04:54,907   Epoch = 9 iter 10199 step
2022-08-29 19:04:54,907   Num examples = 1043
2022-08-29 19:04:54,907   Batch size = 32
2022-08-29 19:04:54,909 ***** Eval results *****
2022-08-29 19:04:54,909   att_loss = 0.8911954029455299
2022-08-29 19:04:54,909   cls_loss = 0.0
2022-08-29 19:04:54,909   global_step = 10199
2022-08-29 19:04:54,909   loss = 2.585979736237144
2022-08-29 19:04:54,909   rep_loss = 1.6947843289253455
2022-08-29 19:04:54,909 ***** Save model *****
2022-08-29 19:05:11,403 ***** Running evaluation *****
2022-08-29 19:05:11,404   Epoch = 2 iter 4699 step
2022-08-29 19:05:11,404   Num examples = 872
2022-08-29 19:05:11,404   Batch size = 32
2022-08-29 19:05:13,610 ***** Eval results *****
2022-08-29 19:05:13,611   acc = 0.930045871559633
2022-08-29 19:05:13,611   att_loss = 0.0
2022-08-29 19:05:13,611   cls_loss = 0.028069657837884794
2022-08-29 19:05:13,611   eval_loss = 0.24323099372642382
2022-08-29 19:05:13,611   global_step = 4699
2022-08-29 19:05:13,611   loss = 0.028069657837884794
2022-08-29 19:05:13,611   rep_loss = 0.0
2022-08-29 19:05:13,611 ***** Save model *****
2022-08-29 19:05:20,890 ***** Running evaluation *****
2022-08-29 19:05:20,891   Epoch = 2 iter 25799 step
2022-08-29 19:05:20,891   Num examples = 9815
2022-08-29 19:05:20,891   Batch size = 32
2022-08-29 19:05:20,892 ***** Eval results *****
2022-08-29 19:05:20,892   att_loss = 4.218397650153464
2022-08-29 19:05:20,892   cls_loss = 0.0
2022-08-29 19:05:20,892   global_step = 25799
2022-08-29 19:05:20,892   loss = 6.0714948444776144
2022-08-29 19:05:20,892   rep_loss = 1.85309719745375
2022-08-29 19:05:20,893 ***** Save model *****
2022-08-29 19:05:37,346 ***** Running evaluation *****
2022-08-29 19:05:37,346   Epoch = 9 iter 10399 step
2022-08-29 19:05:37,346   Num examples = 1043
2022-08-29 19:05:37,346   Batch size = 32
2022-08-29 19:05:37,347 ***** Eval results *****
2022-08-29 19:05:37,347   att_loss = 0.892006889156041
2022-08-29 19:05:37,347   cls_loss = 0.0
2022-08-29 19:05:37,348   global_step = 10399
2022-08-29 19:05:37,348   loss = 2.587204441182041
2022-08-29 19:05:37,348   rep_loss = 1.6951975490722753
2022-08-29 19:05:37,348 ***** Save model *****
2022-08-29 19:05:57,228 ***** Running evaluation *****
2022-08-29 19:05:57,229   Epoch = 2 iter 4799 step
2022-08-29 19:05:57,229   Num examples = 872
2022-08-29 19:05:57,229   Batch size = 32
2022-08-29 19:05:59,437 ***** Eval results *****
2022-08-29 19:05:59,437   acc = 0.9254587155963303
2022-08-29 19:05:59,437   att_loss = 0.0
2022-08-29 19:05:59,437   cls_loss = 0.02815242056843593
2022-08-29 19:05:59,437   eval_loss = 0.23245068386729276
2022-08-29 19:05:59,437   global_step = 4799
2022-08-29 19:05:59,437   loss = 0.02815242056843593
2022-08-29 19:05:59,437   rep_loss = 0.0
2022-08-29 19:06:15,725 ***** Running evaluation *****
2022-08-29 19:06:15,726   Epoch = 9 iter 10599 step
2022-08-29 19:06:15,726   Num examples = 1043
2022-08-29 19:06:15,726   Batch size = 32
2022-08-29 19:06:15,727 ***** Eval results *****
2022-08-29 19:06:15,727   att_loss = 0.8936742676427661
2022-08-29 19:06:15,727   cls_loss = 0.0
2022-08-29 19:06:15,727   global_step = 10599
2022-08-29 19:06:15,727   loss = 2.5891546335867717
2022-08-29 19:06:15,727   rep_loss = 1.6954803644342626
2022-08-29 19:06:15,727 ***** Save model *****
2022-08-29 19:06:29,875 ***** Running evaluation *****
2022-08-29 19:06:29,875   Epoch = 2 iter 25999 step
2022-08-29 19:06:29,875   Num examples = 9815
2022-08-29 19:06:29,876   Batch size = 32
2022-08-29 19:06:29,877 ***** Eval results *****
2022-08-29 19:06:29,877   att_loss = 4.209423349586021
2022-08-29 19:06:29,877   cls_loss = 0.0
2022-08-29 19:06:29,877   global_step = 25999
2022-08-29 19:06:29,877   loss = 6.061489536898056
2022-08-29 19:06:29,877   rep_loss = 1.852066190584768
2022-08-29 19:06:29,877 ***** Save model *****
2022-08-29 19:06:39,881 ***** Running evaluation *****
2022-08-29 19:06:39,881   Epoch = 2 iter 4899 step
2022-08-29 19:06:39,881   Num examples = 872
2022-08-29 19:06:39,881   Batch size = 32
2022-08-29 19:06:42,085 ***** Eval results *****
2022-08-29 19:06:42,085   acc = 0.9197247706422018
2022-08-29 19:06:42,085   att_loss = 0.0
2022-08-29 19:06:42,085   cls_loss = 0.027818638980981667
2022-08-29 19:06:42,085   eval_loss = 0.28180606480288717
2022-08-29 19:06:42,085   global_step = 4899
2022-08-29 19:06:42,085   loss = 0.027818638980981667
2022-08-29 19:06:42,085   rep_loss = 0.0
2022-08-29 19:06:54,083 ***** Running evaluation *****
2022-08-29 19:06:54,084   Epoch = 10 iter 10799 step
2022-08-29 19:06:54,084   Num examples = 1043
2022-08-29 19:06:54,084   Batch size = 32
2022-08-29 19:06:54,085 ***** Eval results *****
2022-08-29 19:06:54,085   att_loss = 0.8742399626419324
2022-08-29 19:06:54,085   cls_loss = 0.0
2022-08-29 19:06:54,085   global_step = 10799
2022-08-29 19:06:54,085   loss = 2.550888169713381
2022-08-29 19:06:54,085   rep_loss = 1.6766482100767248
2022-08-29 19:06:54,085 ***** Save model *****
2022-08-29 19:07:22,514 ***** Running evaluation *****
2022-08-29 19:07:22,514   Epoch = 2 iter 4999 step
2022-08-29 19:07:22,515   Num examples = 872
2022-08-29 19:07:22,515   Batch size = 32
2022-08-29 19:07:24,726 ***** Eval results *****
2022-08-29 19:07:24,726   acc = 0.9231651376146789
2022-08-29 19:07:24,726   att_loss = 0.0
2022-08-29 19:07:24,726   cls_loss = 0.027542287054649457
2022-08-29 19:07:24,726   eval_loss = 0.2673998702583568
2022-08-29 19:07:24,726   global_step = 4999
2022-08-29 19:07:24,726   loss = 0.027542287054649457
2022-08-29 19:07:24,726   rep_loss = 0.0
2022-08-29 19:07:32,463 ***** Running evaluation *****
2022-08-29 19:07:32,463   Epoch = 10 iter 10999 step
2022-08-29 19:07:32,463   Num examples = 1043
2022-08-29 19:07:32,463   Batch size = 32
2022-08-29 19:07:32,465 ***** Eval results *****
2022-08-29 19:07:32,465   att_loss = 0.8826619459543856
2022-08-29 19:07:32,465   cls_loss = 0.0
2022-08-29 19:07:32,465   global_step = 10999
2022-08-29 19:07:32,465   loss = 2.5599191555035152
2022-08-29 19:07:32,465   rep_loss = 1.6772572104833716
2022-08-29 19:07:32,465 ***** Save model *****
2022-08-29 19:07:41,027 ***** Running evaluation *****
2022-08-29 19:07:41,027   Epoch = 2 iter 26199 step
2022-08-29 19:07:41,027   Num examples = 9815
2022-08-29 19:07:41,027   Batch size = 32
2022-08-29 19:07:41,028 ***** Eval results *****
2022-08-29 19:07:41,028   att_loss = 4.2049645534393365
2022-08-29 19:07:41,029   cls_loss = 0.0
2022-08-29 19:07:41,029   global_step = 26199
2022-08-29 19:07:41,029   loss = 6.056399765256376
2022-08-29 19:07:41,029   rep_loss = 1.8514352128961824
2022-08-29 19:07:41,029 ***** Save model *****
2022-08-29 19:08:05,191 ***** Running evaluation *****
2022-08-29 19:08:05,192   Epoch = 2 iter 5099 step
2022-08-29 19:08:05,192   Num examples = 872
2022-08-29 19:08:05,192   Batch size = 32
2022-08-29 19:08:07,401 ***** Eval results *****
2022-08-29 19:08:07,402   acc = 0.9128440366972477
2022-08-29 19:08:07,402   att_loss = 0.0
2022-08-29 19:08:07,402   cls_loss = 0.02747718179567093
2022-08-29 19:08:07,402   eval_loss = 0.2834177777570273
2022-08-29 19:08:07,402   global_step = 5099
2022-08-29 19:08:07,402   loss = 0.02747718179567093
2022-08-29 19:08:07,402   rep_loss = 0.0
2022-08-29 19:08:10,798 ***** Running evaluation *****
2022-08-29 19:08:10,798   Epoch = 10 iter 11199 step
2022-08-29 19:08:10,798   Num examples = 1043
2022-08-29 19:08:10,799   Batch size = 32
2022-08-29 19:08:10,800 ***** Eval results *****
2022-08-29 19:08:10,800   att_loss = 0.8794009360504518
2022-08-29 19:08:10,800   cls_loss = 0.0
2022-08-29 19:08:10,800   global_step = 11199
2022-08-29 19:08:10,800   loss = 2.5554756125962803
2022-08-29 19:08:10,800   rep_loss = 1.6760746773497448
2022-08-29 19:08:10,800 ***** Save model *****
2022-08-29 19:08:47,846 ***** Running evaluation *****
2022-08-29 19:08:47,846   Epoch = 2 iter 5199 step
2022-08-29 19:08:47,846   Num examples = 872
2022-08-29 19:08:47,846   Batch size = 32
2022-08-29 19:08:49,119 ***** Running evaluation *****
2022-08-29 19:08:49,120   Epoch = 10 iter 11399 step
2022-08-29 19:08:49,120   Num examples = 1043
2022-08-29 19:08:49,120   Batch size = 32
2022-08-29 19:08:49,121 ***** Eval results *****
2022-08-29 19:08:49,121   att_loss = 0.8804869815106187
2022-08-29 19:08:49,121   cls_loss = 0.0
2022-08-29 19:08:49,121   global_step = 11399
2022-08-29 19:08:49,121   loss = 2.556376761952428
2022-08-29 19:08:49,121   rep_loss = 1.6758897787009228
2022-08-29 19:08:49,121 ***** Save model *****
2022-08-29 19:08:50,059 ***** Eval results *****
2022-08-29 19:08:50,059   acc = 0.9128440366972477
2022-08-29 19:08:50,060   att_loss = 0.0
2022-08-29 19:08:50,060   cls_loss = 0.0273752301855259
2022-08-29 19:08:50,060   eval_loss = 0.283272421253579
2022-08-29 19:08:50,060   global_step = 5199
2022-08-29 19:08:50,060   loss = 0.0273752301855259
2022-08-29 19:08:50,060   rep_loss = 0.0
2022-08-29 19:08:50,502 ***** Running evaluation *****
2022-08-29 19:08:50,503   Epoch = 2 iter 26399 step
2022-08-29 19:08:50,503   Num examples = 9815
2022-08-29 19:08:50,503   Batch size = 32
2022-08-29 19:08:50,504 ***** Eval results *****
2022-08-29 19:08:50,504   att_loss = 4.202525917278161
2022-08-29 19:08:50,504   cls_loss = 0.0
2022-08-29 19:08:50,504   global_step = 26399
2022-08-29 19:08:50,504   loss = 6.053397097507984
2022-08-29 19:08:50,504   rep_loss = 1.8508711813853258
2022-08-29 19:08:50,505 ***** Save model *****
2022-08-29 19:09:28,335 ***** Running evaluation *****
2022-08-29 19:09:28,336   Epoch = 10 iter 11599 step
2022-08-29 19:09:28,336   Num examples = 1043
2022-08-29 19:09:28,336   Batch size = 32
2022-08-29 19:09:28,337 ***** Eval results *****
2022-08-29 19:09:28,337   att_loss = 0.8812893444876936
2022-08-29 19:09:28,337   cls_loss = 0.0
2022-08-29 19:09:28,338   global_step = 11599
2022-08-29 19:09:28,338   loss = 2.557265988890572
2022-08-29 19:09:28,338   rep_loss = 1.6759766440137296
2022-08-29 19:09:28,338 ***** Save model *****
2022-08-29 19:09:30,535 ***** Running evaluation *****
2022-08-29 19:09:30,536   Epoch = 2 iter 5299 step
2022-08-29 19:09:30,536   Num examples = 872
2022-08-29 19:09:30,536   Batch size = 32
2022-08-29 19:09:32,747 ***** Eval results *****
2022-08-29 19:09:32,747   acc = 0.9151376146788991
2022-08-29 19:09:32,747   att_loss = 0.0
2022-08-29 19:09:32,747   cls_loss = 0.027725981693547543
2022-08-29 19:09:32,747   eval_loss = 0.29366309438566013
2022-08-29 19:09:32,747   global_step = 5299
2022-08-29 19:09:32,747   loss = 0.027725981693547543
2022-08-29 19:09:32,747   rep_loss = 0.0
2022-08-29 19:10:05,019 ***** Running evaluation *****
2022-08-29 19:10:05,020   Epoch = 2 iter 26599 step
2022-08-29 19:10:05,020   Num examples = 9815
2022-08-29 19:10:05,020   Batch size = 32
2022-08-29 19:10:05,021 ***** Eval results *****
2022-08-29 19:10:05,021   att_loss = 4.199332531528909
2022-08-29 19:10:05,021   cls_loss = 0.0
2022-08-29 19:10:05,021   global_step = 26599
2022-08-29 19:10:05,021   loss = 6.049916506624941
2022-08-29 19:10:05,021   rep_loss = 1.8505839770664332
2022-08-29 19:10:05,021 ***** Save model *****
2022-08-29 19:10:06,854 ***** Running evaluation *****
2022-08-29 19:10:06,854   Epoch = 11 iter 11799 step
2022-08-29 19:10:06,854   Num examples = 1043
2022-08-29 19:10:06,854   Batch size = 32
2022-08-29 19:10:06,855 ***** Eval results *****
2022-08-29 19:10:06,855   att_loss = 0.8379048167490492
2022-08-29 19:10:06,855   cls_loss = 0.0
2022-08-29 19:10:06,855   global_step = 11799
2022-08-29 19:10:06,856   loss = 2.4891165471544454
2022-08-29 19:10:06,856   rep_loss = 1.6512117268992406
2022-08-29 19:10:06,856 ***** Save model *****
2022-08-29 19:10:13,247 ***** Running evaluation *****
2022-08-29 19:10:13,247   Epoch = 2 iter 5399 step
2022-08-29 19:10:13,247   Num examples = 872
2022-08-29 19:10:13,247   Batch size = 32
2022-08-29 19:10:15,454 ***** Eval results *****
2022-08-29 19:10:15,455   acc = 0.9220183486238532
2022-08-29 19:10:15,455   att_loss = 0.0
2022-08-29 19:10:15,455   cls_loss = 0.02789243169779491
2022-08-29 19:10:15,455   eval_loss = 0.26176000625959467
2022-08-29 19:10:15,455   global_step = 5399
2022-08-29 19:10:15,455   loss = 0.02789243169779491
2022-08-29 19:10:15,455   rep_loss = 0.0
2022-08-29 19:10:46,468 ***** Running evaluation *****
2022-08-29 19:10:46,469   Epoch = 11 iter 11999 step
2022-08-29 19:10:46,469   Num examples = 1043
2022-08-29 19:10:46,469   Batch size = 32
2022-08-29 19:10:46,470 ***** Eval results *****
2022-08-29 19:10:46,470   att_loss = 0.8722166328791128
2022-08-29 19:10:46,470   cls_loss = 0.0
2022-08-29 19:10:46,470   global_step = 11999
2022-08-29 19:10:46,470   loss = 2.532984542656705
2022-08-29 19:10:46,470   rep_loss = 1.660767912864685
2022-08-29 19:10:46,470 ***** Save model *****
2022-08-29 19:10:55,893 ***** Running evaluation *****
2022-08-29 19:10:55,894   Epoch = 2 iter 5499 step
2022-08-29 19:10:55,894   Num examples = 872
2022-08-29 19:10:55,894   Batch size = 32
2022-08-29 19:10:58,106 ***** Eval results *****
2022-08-29 19:10:58,106   acc = 0.9162844036697247
2022-08-29 19:10:58,106   att_loss = 0.0
2022-08-29 19:10:58,106   cls_loss = 0.027572600641320326
2022-08-29 19:10:58,106   eval_loss = 0.27290657854505945
2022-08-29 19:10:58,106   global_step = 5499
2022-08-29 19:10:58,106   loss = 0.027572600641320326
2022-08-29 19:10:58,106   rep_loss = 0.0
2022-08-29 19:11:15,599 ***** Running evaluation *****
2022-08-29 19:11:15,599   Epoch = 2 iter 26799 step
2022-08-29 19:11:15,599   Num examples = 9815
2022-08-29 19:11:15,599   Batch size = 32
2022-08-29 19:11:15,600 ***** Eval results *****
2022-08-29 19:11:15,601   att_loss = 4.200074802418964
2022-08-29 19:11:15,601   cls_loss = 0.0
2022-08-29 19:11:15,601   global_step = 26799
2022-08-29 19:11:15,601   loss = 6.049883758171396
2022-08-29 19:11:15,601   rep_loss = 1.8498089567031488
2022-08-29 19:11:15,601 ***** Save model *****
2022-08-29 19:11:24,821 ***** Running evaluation *****
2022-08-29 19:11:24,822   Epoch = 11 iter 12199 step
2022-08-29 19:11:24,822   Num examples = 1043
2022-08-29 19:11:24,822   Batch size = 32
2022-08-29 19:11:24,823 ***** Eval results *****
2022-08-29 19:11:24,823   att_loss = 0.8734667710877311
2022-08-29 19:11:24,823   cls_loss = 0.0
2022-08-29 19:11:24,823   global_step = 12199
2022-08-29 19:11:24,823   loss = 2.5364266932671455
2022-08-29 19:11:24,823   rep_loss = 1.6629599182145822
2022-08-29 19:11:24,823 ***** Save model *****
2022-08-29 19:11:38,580 ***** Running evaluation *****
2022-08-29 19:11:38,581   Epoch = 2 iter 5599 step
2022-08-29 19:11:38,581   Num examples = 872
2022-08-29 19:11:38,581   Batch size = 32
2022-08-29 19:11:40,786 ***** Eval results *****
2022-08-29 19:11:40,787   acc = 0.9197247706422018
2022-08-29 19:11:40,787   att_loss = 0.0
2022-08-29 19:11:40,787   cls_loss = 0.027738820711896626
2022-08-29 19:11:40,787   eval_loss = 0.24058763084134885
2022-08-29 19:11:40,787   global_step = 5599
2022-08-29 19:11:40,787   loss = 0.027738820711896626
2022-08-29 19:11:40,787   rep_loss = 0.0
2022-08-29 19:12:06,642 ***** Running evaluation *****
2022-08-29 19:12:06,642   Epoch = 11 iter 12399 step
2022-08-29 19:12:06,642   Num examples = 1043
2022-08-29 19:12:06,642   Batch size = 32
2022-08-29 19:12:06,643 ***** Eval results *****
2022-08-29 19:12:06,643   att_loss = 0.8694092467450143
2022-08-29 19:12:06,644   cls_loss = 0.0
2022-08-29 19:12:06,644   global_step = 12399
2022-08-29 19:12:06,644   loss = 2.5293838615241686
2022-08-29 19:12:06,644   rep_loss = 1.6599746125817483
2022-08-29 19:12:06,644 ***** Save model *****
2022-08-29 19:12:21,237 ***** Running evaluation *****
2022-08-29 19:12:21,238   Epoch = 2 iter 5699 step
2022-08-29 19:12:21,238   Num examples = 872
2022-08-29 19:12:21,238   Batch size = 32
2022-08-29 19:12:23,445 ***** Eval results *****
2022-08-29 19:12:23,445   acc = 0.926605504587156
2022-08-29 19:12:23,445   att_loss = 0.0
2022-08-29 19:12:23,445   cls_loss = 0.027748545144555315
2022-08-29 19:12:23,445   eval_loss = 0.2596040500460991
2022-08-29 19:12:23,445   global_step = 5699
2022-08-29 19:12:23,445   loss = 0.027748545144555315
2022-08-29 19:12:23,445   rep_loss = 0.0
2022-08-29 19:12:25,619 ***** Running evaluation *****
2022-08-29 19:12:25,619   Epoch = 2 iter 26999 step
2022-08-29 19:12:25,620   Num examples = 9815
2022-08-29 19:12:25,620   Batch size = 32
2022-08-29 19:12:25,621 ***** Eval results *****
2022-08-29 19:12:25,621   att_loss = 4.201168316381949
2022-08-29 19:12:25,621   cls_loss = 0.0
2022-08-29 19:12:25,621   global_step = 26999
2022-08-29 19:12:25,621   loss = 6.050796289292593
2022-08-29 19:12:25,621   rep_loss = 1.8496279741721178
2022-08-29 19:12:25,621 ***** Save model *****
2022-08-29 19:12:45,027 ***** Running evaluation *****
2022-08-29 19:12:45,027   Epoch = 11 iter 12599 step
2022-08-29 19:12:45,027   Num examples = 1043
2022-08-29 19:12:45,027   Batch size = 32
2022-08-29 19:12:45,029 ***** Eval results *****
2022-08-29 19:12:45,029   att_loss = 0.8716889044092628
2022-08-29 19:12:45,029   cls_loss = 0.0
2022-08-29 19:12:45,029   global_step = 12599
2022-08-29 19:12:45,029   loss = 2.533241388520399
2022-08-29 19:12:45,029   rep_loss = 1.6615524822900773
2022-08-29 19:12:45,029 ***** Save model *****
2022-08-29 19:13:03,903 ***** Running evaluation *****
2022-08-29 19:13:03,904   Epoch = 2 iter 5799 step
2022-08-29 19:13:03,904   Num examples = 872
2022-08-29 19:13:03,904   Batch size = 32
2022-08-29 19:13:06,109 ***** Eval results *****
2022-08-29 19:13:06,109   acc = 0.908256880733945
2022-08-29 19:13:06,109   att_loss = 0.0
2022-08-29 19:13:06,109   cls_loss = 0.027923578996443096
2022-08-29 19:13:06,109   eval_loss = 0.2834802955122931
2022-08-29 19:13:06,109   global_step = 5799
2022-08-29 19:13:06,109   loss = 0.027923578996443096
2022-08-29 19:13:06,109   rep_loss = 0.0
2022-08-29 19:13:23,356 ***** Running evaluation *****
2022-08-29 19:13:23,356   Epoch = 11 iter 12799 step
2022-08-29 19:13:23,356   Num examples = 1043
2022-08-29 19:13:23,356   Batch size = 32
2022-08-29 19:13:23,358 ***** Eval results *****
2022-08-29 19:13:23,358   att_loss = 0.8700250398534236
2022-08-29 19:13:23,358   cls_loss = 0.0
2022-08-29 19:13:23,358   global_step = 12799
2022-08-29 19:13:23,358   loss = 2.530809220986407
2022-08-29 19:13:23,358   rep_loss = 1.660784180679285
2022-08-29 19:13:23,358 ***** Save model *****
2022-08-29 19:13:36,493 ***** Running evaluation *****
2022-08-29 19:13:36,493   Epoch = 2 iter 27199 step
2022-08-29 19:13:36,493   Num examples = 9815
2022-08-29 19:13:36,493   Batch size = 32
2022-08-29 19:13:36,495 ***** Eval results *****
2022-08-29 19:13:36,495   att_loss = 4.198724142536886
2022-08-29 19:13:36,495   cls_loss = 0.0
2022-08-29 19:13:36,495   global_step = 27199
2022-08-29 19:13:36,495   loss = 6.047881218828023
2022-08-29 19:13:36,495   rep_loss = 1.849157077367924
2022-08-29 19:13:36,495 ***** Save model *****
2022-08-29 19:13:46,541 ***** Running evaluation *****
2022-08-29 19:13:46,541   Epoch = 2 iter 5899 step
2022-08-29 19:13:46,541   Num examples = 872
2022-08-29 19:13:46,542   Batch size = 32
2022-08-29 19:13:48,747 ***** Eval results *****
2022-08-29 19:13:48,747   acc = 0.926605504587156
2022-08-29 19:13:48,747   att_loss = 0.0
2022-08-29 19:13:48,747   cls_loss = 0.02796616546351447
2022-08-29 19:13:48,747   eval_loss = 0.24344572478107043
2022-08-29 19:13:48,747   global_step = 5899
2022-08-29 19:13:48,747   loss = 0.02796616546351447
2022-08-29 19:13:48,747   rep_loss = 0.0
2022-08-29 19:14:04,943 ***** Running evaluation *****
2022-08-29 19:14:04,944   Epoch = 12 iter 12999 step
2022-08-29 19:14:04,944   Num examples = 1043
2022-08-29 19:14:04,944   Batch size = 32
2022-08-29 19:14:04,945 ***** Eval results *****
2022-08-29 19:14:04,945   att_loss = 0.8641862116876196
2022-08-29 19:14:04,945   cls_loss = 0.0
2022-08-29 19:14:04,945   global_step = 12999
2022-08-29 19:14:04,945   loss = 2.513503320881578
2022-08-29 19:14:04,945   rep_loss = 1.6493171062625822
2022-08-29 19:14:04,945 ***** Save model *****
2022-08-29 19:14:29,245 ***** Running evaluation *****
2022-08-29 19:14:29,246   Epoch = 2 iter 5999 step
2022-08-29 19:14:29,246   Num examples = 872
2022-08-29 19:14:29,246   Batch size = 32
2022-08-29 19:14:31,456 ***** Eval results *****
2022-08-29 19:14:31,457   acc = 0.9243119266055045
2022-08-29 19:14:31,457   att_loss = 0.0
2022-08-29 19:14:31,457   cls_loss = 0.027979947479430323
2022-08-29 19:14:31,457   eval_loss = 0.2566060932752277
2022-08-29 19:14:31,457   global_step = 5999
2022-08-29 19:14:31,457   loss = 0.027979947479430323
2022-08-29 19:14:31,457   rep_loss = 0.0
2022-08-29 19:14:43,277 ***** Running evaluation *****
2022-08-29 19:14:43,278   Epoch = 12 iter 13199 step
2022-08-29 19:14:43,278   Num examples = 1043
2022-08-29 19:14:43,278   Batch size = 32
2022-08-29 19:14:43,279 ***** Eval results *****
2022-08-29 19:14:43,279   att_loss = 0.8523288577717216
2022-08-29 19:14:43,280   cls_loss = 0.0
2022-08-29 19:14:43,280   global_step = 13199
2022-08-29 19:14:43,280   loss = 2.494664583753979
2022-08-29 19:14:43,280   rep_loss = 1.6423357255153805
2022-08-29 19:14:43,280 ***** Save model *****
2022-08-29 19:14:45,780 ***** Running evaluation *****
2022-08-29 19:14:45,780   Epoch = 2 iter 27399 step
2022-08-29 19:14:45,780   Num examples = 9815
2022-08-29 19:14:45,780   Batch size = 32
2022-08-29 19:14:45,781 ***** Eval results *****
2022-08-29 19:14:45,781   att_loss = 4.19422050276794
2022-08-29 19:14:45,782   cls_loss = 0.0
2022-08-29 19:14:45,782   global_step = 27399
2022-08-29 19:14:45,782   loss = 6.042675002121116
2022-08-29 19:14:45,782   rep_loss = 1.8484545015228935
2022-08-29 19:14:45,782 ***** Save model *****
2022-08-29 19:15:12,020 ***** Running evaluation *****
2022-08-29 19:15:12,020   Epoch = 2 iter 6099 step
2022-08-29 19:15:12,021   Num examples = 872
2022-08-29 19:15:12,021   Batch size = 32
2022-08-29 19:15:14,229 ***** Eval results *****
2022-08-29 19:15:14,230   acc = 0.9243119266055045
2022-08-29 19:15:14,230   att_loss = 0.0
2022-08-29 19:15:14,230   cls_loss = 0.027811791531599777
2022-08-29 19:15:14,230   eval_loss = 0.2571046214018549
2022-08-29 19:15:14,230   global_step = 6099
2022-08-29 19:15:14,230   loss = 0.027811791531599777
2022-08-29 19:15:14,230   rep_loss = 0.0
2022-08-29 19:15:22,255 ***** Running evaluation *****
2022-08-29 19:15:22,256   Epoch = 12 iter 13399 step
2022-08-29 19:15:22,256   Num examples = 1043
2022-08-29 19:15:22,256   Batch size = 32
2022-08-29 19:15:22,257 ***** Eval results *****
2022-08-29 19:15:22,257   att_loss = 0.8612934932700561
2022-08-29 19:15:22,257   cls_loss = 0.0
2022-08-29 19:15:22,257   global_step = 13399
2022-08-29 19:15:22,257   loss = 2.507252154996424
2022-08-29 19:15:22,257   rep_loss = 1.6459586618286055
2022-08-29 19:15:22,257 ***** Save model *****
2022-08-29 19:15:54,752 ***** Running evaluation *****
2022-08-29 19:15:54,753   Epoch = 2 iter 6199 step
2022-08-29 19:15:54,753   Num examples = 872
2022-08-29 19:15:54,753   Batch size = 32
2022-08-29 19:15:56,961 ***** Eval results *****
2022-08-29 19:15:56,961   acc = 0.9128440366972477
2022-08-29 19:15:56,961   att_loss = 0.0
2022-08-29 19:15:56,961   cls_loss = 0.027836901512294485
2022-08-29 19:15:56,961   eval_loss = 0.27727659246219055
2022-08-29 19:15:56,961   global_step = 6199
2022-08-29 19:15:56,961   loss = 0.027836901512294485
2022-08-29 19:15:56,961   rep_loss = 0.0
2022-08-29 19:15:59,842 ***** Running evaluation *****
2022-08-29 19:15:59,842   Epoch = 2 iter 27599 step
2022-08-29 19:15:59,842   Num examples = 9815
2022-08-29 19:15:59,843   Batch size = 32
2022-08-29 19:15:59,844 ***** Eval results *****
2022-08-29 19:15:59,844   att_loss = 4.189829924467697
2022-08-29 19:15:59,844   cls_loss = 0.0
2022-08-29 19:15:59,844   global_step = 27599
2022-08-29 19:15:59,844   loss = 6.037461086781071
2022-08-29 19:15:59,844   rep_loss = 1.8476311646531054
2022-08-29 19:15:59,844 ***** Save model *****
2022-08-29 19:16:00,411 ***** Running evaluation *****
2022-08-29 19:16:00,412   Epoch = 12 iter 13599 step
2022-08-29 19:16:00,412   Num examples = 1043
2022-08-29 19:16:00,412   Batch size = 32
2022-08-29 19:16:00,413 ***** Eval results *****
2022-08-29 19:16:00,413   att_loss = 0.8642867238737767
2022-08-29 19:16:00,413   cls_loss = 0.0
2022-08-29 19:16:00,413   global_step = 13599
2022-08-29 19:16:00,413   loss = 2.5123792090482944
2022-08-29 19:16:00,413   rep_loss = 1.648092485783505
2022-08-29 19:16:00,414 ***** Save model *****
2022-08-29 19:16:37,512 ***** Running evaluation *****
2022-08-29 19:16:37,513   Epoch = 2 iter 6299 step
2022-08-29 19:16:37,513   Num examples = 872
2022-08-29 19:16:37,513   Batch size = 32
2022-08-29 19:16:39,717 ***** Eval results *****
2022-08-29 19:16:39,717   acc = 0.9162844036697247
2022-08-29 19:16:39,717   att_loss = 0.0
2022-08-29 19:16:39,717   cls_loss = 0.027758871220294537
2022-08-29 19:16:39,717   eval_loss = 0.2714426939907883
2022-08-29 19:16:39,717   global_step = 6299
2022-08-29 19:16:39,717   loss = 0.027758871220294537
2022-08-29 19:16:39,717   rep_loss = 0.0
2022-08-29 19:16:40,798 ***** Running evaluation *****
2022-08-29 19:16:40,799   Epoch = 12 iter 13799 step
2022-08-29 19:16:40,799   Num examples = 1043
2022-08-29 19:16:40,799   Batch size = 32
2022-08-29 19:16:40,800 ***** Eval results *****
2022-08-29 19:16:40,800   att_loss = 0.8589965205105251
2022-08-29 19:16:40,800   cls_loss = 0.0
2022-08-29 19:16:40,800   global_step = 13799
2022-08-29 19:16:40,800   loss = 2.5048702250662074
2022-08-29 19:16:40,800   rep_loss = 1.645873705343943
2022-08-29 19:16:40,800 ***** Save model *****
2022-08-29 19:17:13,667 ***** Running evaluation *****
2022-08-29 19:17:13,667   Epoch = 2 iter 27799 step
2022-08-29 19:17:13,667   Num examples = 9815
2022-08-29 19:17:13,667   Batch size = 32
2022-08-29 19:17:13,668 ***** Eval results *****
2022-08-29 19:17:13,668   att_loss = 4.188617410123952
2022-08-29 19:17:13,668   cls_loss = 0.0
2022-08-29 19:17:13,668   global_step = 27799
2022-08-29 19:17:13,668   loss = 6.035608959959298
2022-08-29 19:17:13,669   rep_loss = 1.8469915510797779
2022-08-29 19:17:13,669 ***** Save model *****
2022-08-29 19:17:20,126 ***** Running evaluation *****
2022-08-29 19:17:20,126   Epoch = 13 iter 13999 step
2022-08-29 19:17:20,126   Num examples = 1043
2022-08-29 19:17:20,126   Batch size = 32
2022-08-29 19:17:20,128 ***** Eval results *****
2022-08-29 19:17:20,128   att_loss = 0.8459036438361458
2022-08-29 19:17:20,128   cls_loss = 0.0
2022-08-29 19:17:20,128   global_step = 13999
2022-08-29 19:17:20,128   loss = 2.485609769821167
2022-08-29 19:17:20,128   rep_loss = 1.6397061389425527
2022-08-29 19:17:20,128 ***** Save model *****
2022-08-29 19:17:20,132 ***** Running evaluation *****
2022-08-29 19:17:20,133   Epoch = 3 iter 6399 step
2022-08-29 19:17:20,133   Num examples = 872
2022-08-29 19:17:20,133   Batch size = 32
2022-08-29 19:17:22,348 ***** Eval results *****
2022-08-29 19:17:22,348   acc = 0.9208715596330275
2022-08-29 19:17:22,348   att_loss = 0.0
2022-08-29 19:17:22,348   cls_loss = 0.024296562529929067
2022-08-29 19:17:22,348   eval_loss = 0.2647005880384573
2022-08-29 19:17:22,348   global_step = 6399
2022-08-29 19:17:22,348   loss = 0.024296562529929067
2022-08-29 19:17:22,348   rep_loss = 0.0
2022-08-29 19:17:59,418 ***** Running evaluation *****
2022-08-29 19:17:59,419   Epoch = 13 iter 14199 step
2022-08-29 19:17:59,419   Num examples = 1043
2022-08-29 19:17:59,419   Batch size = 32
2022-08-29 19:17:59,420 ***** Eval results *****
2022-08-29 19:17:59,420   att_loss = 0.8533708052029685
2022-08-29 19:17:59,421   cls_loss = 0.0
2022-08-29 19:17:59,421   global_step = 14199
2022-08-29 19:17:59,421   loss = 2.4938374072786362
2022-08-29 19:17:59,421   rep_loss = 1.640466604913984
2022-08-29 19:17:59,421 ***** Save model *****
2022-08-29 19:18:02,751 ***** Running evaluation *****
2022-08-29 19:18:02,752   Epoch = 3 iter 6499 step
2022-08-29 19:18:02,752   Num examples = 872
2022-08-29 19:18:02,752   Batch size = 32
2022-08-29 19:18:04,957 ***** Eval results *****
2022-08-29 19:18:04,957   acc = 0.9231651376146789
2022-08-29 19:18:04,957   att_loss = 0.0
2022-08-29 19:18:04,958   cls_loss = 0.024100815309560236
2022-08-29 19:18:04,958   eval_loss = 0.2638889702835253
2022-08-29 19:18:04,958   global_step = 6499
2022-08-29 19:18:04,958   loss = 0.024100815309560236
2022-08-29 19:18:04,958   rep_loss = 0.0
2022-08-29 19:18:24,260 ***** Running evaluation *****
2022-08-29 19:18:24,260   Epoch = 2 iter 27999 step
2022-08-29 19:18:24,260   Num examples = 9815
2022-08-29 19:18:24,260   Batch size = 32
2022-08-29 19:18:24,261 ***** Eval results *****
2022-08-29 19:18:24,261   att_loss = 4.1936363173869085
2022-08-29 19:18:24,261   cls_loss = 0.0
2022-08-29 19:18:24,261   global_step = 27999
2022-08-29 19:18:24,262   loss = 6.040474491240767
2022-08-29 19:18:24,262   rep_loss = 1.8468381752676795
2022-08-29 19:18:24,262 ***** Save model *****
2022-08-29 19:18:38,692 ***** Running evaluation *****
2022-08-29 19:18:38,692   Epoch = 13 iter 14399 step
2022-08-29 19:18:38,692   Num examples = 1043
2022-08-29 19:18:38,692   Batch size = 32
2022-08-29 19:18:38,693 ***** Eval results *****
2022-08-29 19:18:38,694   att_loss = 0.8572386664094277
2022-08-29 19:18:38,694   cls_loss = 0.0
2022-08-29 19:18:38,694   global_step = 14399
2022-08-29 19:18:38,694   loss = 2.4960673022038726
2022-08-29 19:18:38,694   rep_loss = 1.6388286393822975
2022-08-29 19:18:38,694 ***** Save model *****
2022-08-29 19:18:45,354 ***** Running evaluation *****
2022-08-29 19:18:45,355   Epoch = 3 iter 6599 step
2022-08-29 19:18:45,355   Num examples = 872
2022-08-29 19:18:45,355   Batch size = 32
2022-08-29 19:18:47,562 ***** Eval results *****
2022-08-29 19:18:47,562   acc = 0.9151376146788991
2022-08-29 19:18:47,562   att_loss = 0.0
2022-08-29 19:18:47,562   cls_loss = 0.02425783626793621
2022-08-29 19:18:47,562   eval_loss = 0.26776088029146194
2022-08-29 19:18:47,562   global_step = 6599
2022-08-29 19:18:47,562   loss = 0.02425783626793621
2022-08-29 19:18:47,562   rep_loss = 0.0
2022-08-29 19:19:16,923 ***** Running evaluation *****
2022-08-29 19:19:16,923   Epoch = 13 iter 14599 step
2022-08-29 19:19:16,923   Num examples = 1043
2022-08-29 19:19:16,923   Batch size = 32
2022-08-29 19:19:16,925 ***** Eval results *****
2022-08-29 19:19:16,925   att_loss = 0.85371526796501
2022-08-29 19:19:16,925   cls_loss = 0.0
2022-08-29 19:19:16,925   global_step = 14599
2022-08-29 19:19:16,925   loss = 2.490051156490833
2022-08-29 19:19:16,925   rep_loss = 1.6363358921104378
2022-08-29 19:19:16,925 ***** Save model *****
2022-08-29 19:19:27,956 ***** Running evaluation *****
2022-08-29 19:19:27,957   Epoch = 3 iter 6699 step
2022-08-29 19:19:27,957   Num examples = 872
2022-08-29 19:19:27,957   Batch size = 32
2022-08-29 19:19:30,164 ***** Eval results *****
2022-08-29 19:19:30,164   acc = 0.9220183486238532
2022-08-29 19:19:30,164   att_loss = 0.0
2022-08-29 19:19:30,164   cls_loss = 0.024786656936606543
2022-08-29 19:19:30,164   eval_loss = 0.25480848484273466
2022-08-29 19:19:30,164   global_step = 6699
2022-08-29 19:19:30,164   loss = 0.024786656936606543
2022-08-29 19:19:30,164   rep_loss = 0.0
2022-08-29 19:19:35,070 ***** Running evaluation *****
2022-08-29 19:19:35,071   Epoch = 2 iter 28199 step
2022-08-29 19:19:35,071   Num examples = 9815
2022-08-29 19:19:35,071   Batch size = 32
2022-08-29 19:19:35,072 ***** Eval results *****
2022-08-29 19:19:35,072   att_loss = 4.192936343901595
2022-08-29 19:19:35,072   cls_loss = 0.0
2022-08-29 19:19:35,072   global_step = 28199
2022-08-29 19:19:35,073   loss = 6.039361750065775
2022-08-29 19:19:35,073   rep_loss = 1.8464254064249608
2022-08-29 19:19:35,073 ***** Save model *****
2022-08-29 19:19:57,001 ***** Running evaluation *****
2022-08-29 19:19:57,001   Epoch = 13 iter 14799 step
2022-08-29 19:19:57,001   Num examples = 1043
2022-08-29 19:19:57,001   Batch size = 32
2022-08-29 19:19:57,003 ***** Eval results *****
2022-08-29 19:19:57,003   att_loss = 0.8509557450403933
2022-08-29 19:19:57,003   cls_loss = 0.0
2022-08-29 19:19:57,003   global_step = 14799
2022-08-29 19:19:57,003   loss = 2.4854140177450543
2022-08-29 19:19:57,003   rep_loss = 1.6344582736166449
2022-08-29 19:19:57,003 ***** Save model *****
2022-08-29 19:20:10,563 ***** Running evaluation *****
2022-08-29 19:20:10,564   Epoch = 3 iter 6799 step
2022-08-29 19:20:10,564   Num examples = 872
2022-08-29 19:20:10,564   Batch size = 32
2022-08-29 19:20:12,768 ***** Eval results *****
2022-08-29 19:20:12,768   acc = 0.9231651376146789
2022-08-29 19:20:12,768   att_loss = 0.0
2022-08-29 19:20:12,768   cls_loss = 0.02541091675091787
2022-08-29 19:20:12,768   eval_loss = 0.26571529222253176
2022-08-29 19:20:12,768   global_step = 6799
2022-08-29 19:20:12,768   loss = 0.02541091675091787
2022-08-29 19:20:12,768   rep_loss = 0.0
2022-08-29 19:20:35,537 ***** Running evaluation *****
2022-08-29 19:20:35,538   Epoch = 14 iter 14999 step
2022-08-29 19:20:35,538   Num examples = 1043
2022-08-29 19:20:35,538   Batch size = 32
2022-08-29 19:20:35,539 ***** Eval results *****
2022-08-29 19:20:35,539   att_loss = 0.8428009380685523
2022-08-29 19:20:35,539   cls_loss = 0.0
2022-08-29 19:20:35,539   global_step = 14999
2022-08-29 19:20:35,539   loss = 2.472428240674607
2022-08-29 19:20:35,539   rep_loss = 1.629627303874239
2022-08-29 19:20:35,539 ***** Save model *****
2022-08-29 19:20:47,382 ***** Running evaluation *****
2022-08-29 19:20:47,382   Epoch = 2 iter 28399 step
2022-08-29 19:20:47,382   Num examples = 9815
2022-08-29 19:20:47,382   Batch size = 32
2022-08-29 19:20:47,384 ***** Eval results *****
2022-08-29 19:20:47,384   att_loss = 4.193965616886375
2022-08-29 19:20:47,384   cls_loss = 0.0
2022-08-29 19:20:47,384   global_step = 28399
2022-08-29 19:20:47,384   loss = 6.040167429486117
2022-08-29 19:20:47,384   rep_loss = 1.846201812259762
2022-08-29 19:20:47,384 ***** Save model *****
2022-08-29 19:20:53,143 ***** Running evaluation *****
2022-08-29 19:20:53,144   Epoch = 3 iter 6899 step
2022-08-29 19:20:53,144   Num examples = 872
2022-08-29 19:20:53,144   Batch size = 32
2022-08-29 19:20:55,350 ***** Eval results *****
2022-08-29 19:20:55,350   acc = 0.9243119266055045
2022-08-29 19:20:55,350   att_loss = 0.0
2022-08-29 19:20:55,350   cls_loss = 0.025749276454831764
2022-08-29 19:20:55,350   eval_loss = 0.2625491211323866
2022-08-29 19:20:55,350   global_step = 6899
2022-08-29 19:20:55,350   loss = 0.025749276454831764
2022-08-29 19:20:55,350   rep_loss = 0.0
2022-08-29 19:21:06,532 The args: Namespace(aug_train=False, cache_dir='', data_dir='glue_data/STSB', data_url='', do_eval=False, do_lower_case=True, eval_batch_size=32, eval_step=200, gradient_accumulation_steps=1, hidden_act='quad', learning_rate=5e-05, log_path='tmp/distill/STSB/quad_2quad/bert-base-uncased/log.txt', max_seq_length=128, no_cuda=False, num_train_epochs=3.0, output_dir='tmp/distill/STSB/quad_2quad/bert-base-uncased/5e-05_1e-05_8', pred_distill=False, seed=42, softmax_act='2quad', student_model='/home/ubuntu/checkpoints/exp/STSB', task_name='STSB', teacher_model='/home/ubuntu/checkpoints/exp/STSB', temperature=1.0, train_batch_size=8, warmup_proportion=0.1, weight_decay=0.0001)
2022-08-29 19:21:06,532 device: cuda n_gpu: 1
2022-08-29 19:21:06,565 The args: Namespace(aug_train=False, cache_dir='', data_dir='glue_data/RTE', data_url='', do_eval=False, do_lower_case=True, eval_batch_size=32, eval_step=200, gradient_accumulation_steps=1, hidden_act='quad', learning_rate=5e-05, log_path='tmp/distill/RTE/quad_2quad/bert-base-uncased/log.txt', max_seq_length=128, no_cuda=False, num_train_epochs=3.0, output_dir='tmp/distill/RTE/quad_2quad/bert-base-uncased/5e-05_1e-05_8', pred_distill=False, seed=42, softmax_act='2quad', student_model='/home/ubuntu/checkpoints/exp/RTE', task_name='RTE', teacher_model='/home/ubuntu/checkpoints/exp/RTE', temperature=1.0, train_batch_size=8, warmup_proportion=0.1, weight_decay=0.0001)
2022-08-29 19:21:06,566 device: cuda n_gpu: 1
2022-08-29 19:21:06,635 Writing example 0 of 5749
2022-08-29 19:21:06,635 *** Example ***
2022-08-29 19:21:06,635 guid: train-0
2022-08-29 19:21:06,635 tokens: [CLS] a plane is taking off . [SEP] an air plane is taking off . [SEP]
2022-08-29 19:21:06,635 input_ids: 101 1037 4946 2003 2635 2125 1012 102 2019 2250 4946 2003 2635 2125 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-08-29 19:21:06,636 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-08-29 19:21:06,636 segment_ids: 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-08-29 19:21:06,636 label: 5.000
2022-08-29 19:21:06,636 label_id: 5.0
2022-08-29 19:21:06,655 Writing example 0 of 2490
2022-08-29 19:21:06,655 *** Example ***
2022-08-29 19:21:06,655 guid: train-0
2022-08-29 19:21:06,656 tokens: [CLS] no weapons of mass destruction found in iraq yet . [SEP] weapons of mass destruction found in iraq . [SEP]
2022-08-29 19:21:06,656 input_ids: 101 2053 4255 1997 3742 6215 2179 1999 5712 2664 1012 102 4255 1997 3742 6215 2179 1999 5712 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-08-29 19:21:06,656 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-08-29 19:21:06,656 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-08-29 19:21:06,656 label: not_entailment
2022-08-29 19:21:06,656 label_id: 1
2022-08-29 19:21:08,993 Writing example 0 of 1500
2022-08-29 19:21:08,994 *** Example ***
2022-08-29 19:21:08,994 guid: dev-0
2022-08-29 19:21:08,994 tokens: [CLS] a man with a hard hat is dancing . [SEP] a man wearing a hard hat is dancing . [SEP]
2022-08-29 19:21:08,994 input_ids: 101 1037 2158 2007 1037 2524 6045 2003 5613 1012 102 1037 2158 4147 1037 2524 6045 2003 5613 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-08-29 19:21:08,994 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-08-29 19:21:08,994 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-08-29 19:21:08,994 label: 5.000
2022-08-29 19:21:08,994 label_id: 5.0
2022-08-29 19:21:09,285 Writing example 0 of 277
2022-08-29 19:21:09,286 *** Example ***
2022-08-29 19:21:09,286 guid: dev-0
2022-08-29 19:21:09,287 tokens: [CLS] dana reeve , the widow of the actor christopher reeve , has died of lung cancer at age 44 , according to the christopher reeve foundation . [SEP] christopher reeve had an accident . [SEP]
2022-08-29 19:21:09,287 input_ids: 101 11271 20726 1010 1996 7794 1997 1996 3364 5696 20726 1010 2038 2351 1997 11192 4456 2012 2287 4008 1010 2429 2000 1996 5696 20726 3192 1012 102 5696 20726 2018 2019 4926 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-08-29 19:21:09,287 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-08-29 19:21:09,287 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-08-29 19:21:09,287 label: not_entailment
2022-08-29 19:21:09,287 label_id: 1
2022-08-29 19:21:09,567 Model config {
  "_name_or_path": "bert-base-uncased",
  "architectures": [
    "BertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "crypten": false,
  "finetuning_task": "rte",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "entailment",
    "1": "not_entailment"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "entailment": 0,
    "not_entailment": 1
  },
  "layer_norm_eps": 1e-12,
  "log_path": "tmp/distill/RTE/quad_2quad/bert-base-uncased/log.txt",
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "pre_trained": "",
  "problem_type": "single_label_classification",
  "softmax_act": "softmax",
  "torch_dtype": "float32",
  "training": "",
  "transformers_version": "4.20.0.dev0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

2022-08-29 19:21:09,673 Model config {
  "_name_or_path": "bert-base-uncased",
  "architectures": [
    "BertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "crypten": false,
  "finetuning_task": "stsb",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0
  },
  "layer_norm_eps": 1e-12,
  "log_path": "tmp/distill/STSB/quad_2quad/bert-base-uncased/log.txt",
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "pre_trained": "",
  "problem_type": "regression",
  "softmax_act": "softmax",
  "torch_dtype": "float32",
  "training": "",
  "transformers_version": "4.20.0.dev0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

2022-08-29 19:21:12,287 Loading model /home/ubuntu/checkpoints/exp/RTE/pytorch_model.bin
2022-08-29 19:21:12,384 Loading model /home/ubuntu/checkpoints/exp/STSB/pytorch_model.bin
2022-08-29 19:21:12,615 loading model...
2022-08-29 19:21:12,668 done!
2022-08-29 19:21:12,669 Weights from pretrained model not used in TinyBertForSequenceClassification: ['bert.embeddings.position_ids']
2022-08-29 19:21:12,741 loading model...
2022-08-29 19:21:12,795 done!
2022-08-29 19:21:12,796 Weights from pretrained model not used in TinyBertForSequenceClassification: ['bert.embeddings.position_ids']
2022-08-29 19:21:14,598 ***** Running evaluation *****
2022-08-29 19:21:14,599   Epoch = 14 iter 15199 step
2022-08-29 19:21:14,600   Num examples = 1043
2022-08-29 19:21:14,600   Batch size = 32
2022-08-29 19:21:14,601 ***** Eval results *****
2022-08-29 19:21:14,601   att_loss = 0.8562624599286902
2022-08-29 19:21:14,601   cls_loss = 0.0
2022-08-29 19:21:14,601   global_step = 15199
2022-08-29 19:21:14,601   loss = 2.4833338859110228
2022-08-29 19:21:14,601   rep_loss = 1.627071429843362
2022-08-29 19:21:14,601 ***** Save model *****
2022-08-29 19:21:17,593 ***** Teacher evaluation *****
2022-08-29 19:21:17,593 {'acc': 0.6967509025270758, 'eval_loss': 0.7074194153149923}
2022-08-29 19:21:17,594 Model config {
  "_name_or_path": "bert-base-uncased",
  "architectures": [
    "BertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "crypten": false,
  "finetuning_task": "rte",
  "gradient_checkpointing": false,
  "hidden_act": "quad",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "entailment",
    "1": "not_entailment"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "entailment": 0,
    "not_entailment": 1
  },
  "layer_norm_eps": 1e-12,
  "log_path": "tmp/distill/RTE/quad_2quad/bert-base-uncased/log.txt",
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "pre_trained": "",
  "problem_type": "single_label_classification",
  "softmax_act": "2quad",
  "torch_dtype": "float32",
  "training": "",
  "transformers_version": "4.20.0.dev0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

2022-08-29 19:21:20,258 Loading model /home/ubuntu/checkpoints/exp/RTE/pytorch_model.bin
2022-08-29 19:21:20,561 loading model...
2022-08-29 19:21:20,600 done!
2022-08-29 19:21:20,600 Weights from pretrained model not used in TinyBertForSequenceClassification: ['bert.embeddings.position_ids']
2022-08-29 19:21:20,680 ***** Running training *****
2022-08-29 19:21:20,680   Num examples = 2490
2022-08-29 19:21:20,680   Batch size = 8
2022-08-29 19:21:20,681   Num steps = 15550
2022-08-29 19:21:20,681 n: bert.embeddings.word_embeddings.weight
2022-08-29 19:21:20,681 n: bert.embeddings.position_embeddings.weight
2022-08-29 19:21:20,681 n: bert.embeddings.token_type_embeddings.weight
2022-08-29 19:21:20,682 n: bert.embeddings.LayerNorm.weight
2022-08-29 19:21:20,682 n: bert.embeddings.LayerNorm.bias
2022-08-29 19:21:20,682 n: bert.encoder.layer.0.attention.self.query.weight
2022-08-29 19:21:20,682 n: bert.encoder.layer.0.attention.self.query.bias
2022-08-29 19:21:20,682 n: bert.encoder.layer.0.attention.self.key.weight
2022-08-29 19:21:20,682 n: bert.encoder.layer.0.attention.self.key.bias
2022-08-29 19:21:20,682 n: bert.encoder.layer.0.attention.self.value.weight
2022-08-29 19:21:20,682 n: bert.encoder.layer.0.attention.self.value.bias
2022-08-29 19:21:20,682 n: bert.encoder.layer.0.attention.output.dense.weight
2022-08-29 19:21:20,682 n: bert.encoder.layer.0.attention.output.dense.bias
2022-08-29 19:21:20,682 n: bert.encoder.layer.0.attention.output.LayerNorm.weight
2022-08-29 19:21:20,682 n: bert.encoder.layer.0.attention.output.LayerNorm.bias
2022-08-29 19:21:20,682 n: bert.encoder.layer.0.intermediate.dense.weight
2022-08-29 19:21:20,682 n: bert.encoder.layer.0.intermediate.dense.bias
2022-08-29 19:21:20,682 n: bert.encoder.layer.0.output.dense.weight
2022-08-29 19:21:20,682 n: bert.encoder.layer.0.output.dense.bias
2022-08-29 19:21:20,682 n: bert.encoder.layer.0.output.LayerNorm.weight
2022-08-29 19:21:20,683 n: bert.encoder.layer.0.output.LayerNorm.bias
2022-08-29 19:21:20,683 n: bert.encoder.layer.1.attention.self.query.weight
2022-08-29 19:21:20,683 n: bert.encoder.layer.1.attention.self.query.bias
2022-08-29 19:21:20,683 n: bert.encoder.layer.1.attention.self.key.weight
2022-08-29 19:21:20,683 n: bert.encoder.layer.1.attention.self.key.bias
2022-08-29 19:21:20,683 n: bert.encoder.layer.1.attention.self.value.weight
2022-08-29 19:21:20,683 n: bert.encoder.layer.1.attention.self.value.bias
2022-08-29 19:21:20,683 n: bert.encoder.layer.1.attention.output.dense.weight
2022-08-29 19:21:20,683 n: bert.encoder.layer.1.attention.output.dense.bias
2022-08-29 19:21:20,683 n: bert.encoder.layer.1.attention.output.LayerNorm.weight
2022-08-29 19:21:20,683 n: bert.encoder.layer.1.attention.output.LayerNorm.bias
2022-08-29 19:21:20,683 n: bert.encoder.layer.1.intermediate.dense.weight
2022-08-29 19:21:20,683 n: bert.encoder.layer.1.intermediate.dense.bias
2022-08-29 19:21:20,683 n: bert.encoder.layer.1.output.dense.weight
2022-08-29 19:21:20,683 n: bert.encoder.layer.1.output.dense.bias
2022-08-29 19:21:20,683 n: bert.encoder.layer.1.output.LayerNorm.weight
2022-08-29 19:21:20,683 n: bert.encoder.layer.1.output.LayerNorm.bias
2022-08-29 19:21:20,683 n: bert.encoder.layer.2.attention.self.query.weight
2022-08-29 19:21:20,683 n: bert.encoder.layer.2.attention.self.query.bias
2022-08-29 19:21:20,684 n: bert.encoder.layer.2.attention.self.key.weight
2022-08-29 19:21:20,684 n: bert.encoder.layer.2.attention.self.key.bias
2022-08-29 19:21:20,684 n: bert.encoder.layer.2.attention.self.value.weight
2022-08-29 19:21:20,684 n: bert.encoder.layer.2.attention.self.value.bias
2022-08-29 19:21:20,684 n: bert.encoder.layer.2.attention.output.dense.weight
2022-08-29 19:21:20,684 n: bert.encoder.layer.2.attention.output.dense.bias
2022-08-29 19:21:20,684 n: bert.encoder.layer.2.attention.output.LayerNorm.weight
2022-08-29 19:21:20,684 n: bert.encoder.layer.2.attention.output.LayerNorm.bias
2022-08-29 19:21:20,684 n: bert.encoder.layer.2.intermediate.dense.weight
2022-08-29 19:21:20,684 n: bert.encoder.layer.2.intermediate.dense.bias
2022-08-29 19:21:20,684 n: bert.encoder.layer.2.output.dense.weight
2022-08-29 19:21:20,684 n: bert.encoder.layer.2.output.dense.bias
2022-08-29 19:21:20,684 n: bert.encoder.layer.2.output.LayerNorm.weight
2022-08-29 19:21:20,684 n: bert.encoder.layer.2.output.LayerNorm.bias
2022-08-29 19:21:20,684 n: bert.encoder.layer.3.attention.self.query.weight
2022-08-29 19:21:20,684 n: bert.encoder.layer.3.attention.self.query.bias
2022-08-29 19:21:20,685 n: bert.encoder.layer.3.attention.self.key.weight
2022-08-29 19:21:20,685 n: bert.encoder.layer.3.attention.self.key.bias
2022-08-29 19:21:20,685 n: bert.encoder.layer.3.attention.self.value.weight
2022-08-29 19:21:20,685 n: bert.encoder.layer.3.attention.self.value.bias
2022-08-29 19:21:20,685 n: bert.encoder.layer.3.attention.output.dense.weight
2022-08-29 19:21:20,685 n: bert.encoder.layer.3.attention.output.dense.bias
2022-08-29 19:21:20,685 n: bert.encoder.layer.3.attention.output.LayerNorm.weight
2022-08-29 19:21:20,685 n: bert.encoder.layer.3.attention.output.LayerNorm.bias
2022-08-29 19:21:20,685 n: bert.encoder.layer.3.intermediate.dense.weight
2022-08-29 19:21:20,685 n: bert.encoder.layer.3.intermediate.dense.bias
2022-08-29 19:21:20,685 n: bert.encoder.layer.3.output.dense.weight
2022-08-29 19:21:20,685 n: bert.encoder.layer.3.output.dense.bias
2022-08-29 19:21:20,686 n: bert.encoder.layer.3.output.LayerNorm.weight
2022-08-29 19:21:20,686 n: bert.encoder.layer.3.output.LayerNorm.bias
2022-08-29 19:21:20,686 n: bert.encoder.layer.4.attention.self.query.weight
2022-08-29 19:21:20,686 n: bert.encoder.layer.4.attention.self.query.bias
2022-08-29 19:21:20,686 n: bert.encoder.layer.4.attention.self.key.weight
2022-08-29 19:21:20,686 n: bert.encoder.layer.4.attention.self.key.bias
2022-08-29 19:21:20,686 n: bert.encoder.layer.4.attention.self.value.weight
2022-08-29 19:21:20,686 n: bert.encoder.layer.4.attention.self.value.bias
2022-08-29 19:21:20,686 n: bert.encoder.layer.4.attention.output.dense.weight
2022-08-29 19:21:20,686 n: bert.encoder.layer.4.attention.output.dense.bias
2022-08-29 19:21:20,686 n: bert.encoder.layer.4.attention.output.LayerNorm.weight
2022-08-29 19:21:20,687 n: bert.encoder.layer.4.attention.output.LayerNorm.bias
2022-08-29 19:21:20,687 n: bert.encoder.layer.4.intermediate.dense.weight
2022-08-29 19:21:20,687 n: bert.encoder.layer.4.intermediate.dense.bias
2022-08-29 19:21:20,687 n: bert.encoder.layer.4.output.dense.weight
2022-08-29 19:21:20,687 n: bert.encoder.layer.4.output.dense.bias
2022-08-29 19:21:20,687 n: bert.encoder.layer.4.output.LayerNorm.weight
2022-08-29 19:21:20,687 n: bert.encoder.layer.4.output.LayerNorm.bias
2022-08-29 19:21:20,687 n: bert.encoder.layer.5.attention.self.query.weight
2022-08-29 19:21:20,687 n: bert.encoder.layer.5.attention.self.query.bias
2022-08-29 19:21:20,687 n: bert.encoder.layer.5.attention.self.key.weight
2022-08-29 19:21:20,687 n: bert.encoder.layer.5.attention.self.key.bias
2022-08-29 19:21:20,687 n: bert.encoder.layer.5.attention.self.value.weight
2022-08-29 19:21:20,687 n: bert.encoder.layer.5.attention.self.value.bias
2022-08-29 19:21:20,687 n: bert.encoder.layer.5.attention.output.dense.weight
2022-08-29 19:21:20,687 n: bert.encoder.layer.5.attention.output.dense.bias
2022-08-29 19:21:20,687 n: bert.encoder.layer.5.attention.output.LayerNorm.weight
2022-08-29 19:21:20,687 n: bert.encoder.layer.5.attention.output.LayerNorm.bias
2022-08-29 19:21:20,687 n: bert.encoder.layer.5.intermediate.dense.weight
2022-08-29 19:21:20,688 n: bert.encoder.layer.5.intermediate.dense.bias
2022-08-29 19:21:20,688 n: bert.encoder.layer.5.output.dense.weight
2022-08-29 19:21:20,688 n: bert.encoder.layer.5.output.dense.bias
2022-08-29 19:21:20,688 n: bert.encoder.layer.5.output.LayerNorm.weight
2022-08-29 19:21:20,688 n: bert.encoder.layer.5.output.LayerNorm.bias
2022-08-29 19:21:20,688 n: bert.encoder.layer.6.attention.self.query.weight
2022-08-29 19:21:20,688 n: bert.encoder.layer.6.attention.self.query.bias
2022-08-29 19:21:20,688 n: bert.encoder.layer.6.attention.self.key.weight
2022-08-29 19:21:20,688 n: bert.encoder.layer.6.attention.self.key.bias
2022-08-29 19:21:20,688 n: bert.encoder.layer.6.attention.self.value.weight
2022-08-29 19:21:20,688 n: bert.encoder.layer.6.attention.self.value.bias
2022-08-29 19:21:20,688 n: bert.encoder.layer.6.attention.output.dense.weight
2022-08-29 19:21:20,688 n: bert.encoder.layer.6.attention.output.dense.bias
2022-08-29 19:21:20,688 n: bert.encoder.layer.6.attention.output.LayerNorm.weight
2022-08-29 19:21:20,688 n: bert.encoder.layer.6.attention.output.LayerNorm.bias
2022-08-29 19:21:20,688 n: bert.encoder.layer.6.intermediate.dense.weight
2022-08-29 19:21:20,688 n: bert.encoder.layer.6.intermediate.dense.bias
2022-08-29 19:21:20,688 n: bert.encoder.layer.6.output.dense.weight
2022-08-29 19:21:20,688 n: bert.encoder.layer.6.output.dense.bias
2022-08-29 19:21:20,689 n: bert.encoder.layer.6.output.LayerNorm.weight
2022-08-29 19:21:20,689 n: bert.encoder.layer.6.output.LayerNorm.bias
2022-08-29 19:21:20,689 n: bert.encoder.layer.7.attention.self.query.weight
2022-08-29 19:21:20,689 n: bert.encoder.layer.7.attention.self.query.bias
2022-08-29 19:21:20,689 n: bert.encoder.layer.7.attention.self.key.weight
2022-08-29 19:21:20,689 n: bert.encoder.layer.7.attention.self.key.bias
2022-08-29 19:21:20,689 n: bert.encoder.layer.7.attention.self.value.weight
2022-08-29 19:21:20,689 n: bert.encoder.layer.7.attention.self.value.bias
2022-08-29 19:21:20,689 n: bert.encoder.layer.7.attention.output.dense.weight
2022-08-29 19:21:20,689 n: bert.encoder.layer.7.attention.output.dense.bias
2022-08-29 19:21:20,689 n: bert.encoder.layer.7.attention.output.LayerNorm.weight
2022-08-29 19:21:20,689 n: bert.encoder.layer.7.attention.output.LayerNorm.bias
2022-08-29 19:21:20,689 n: bert.encoder.layer.7.intermediate.dense.weight
2022-08-29 19:21:20,689 n: bert.encoder.layer.7.intermediate.dense.bias
2022-08-29 19:21:20,689 n: bert.encoder.layer.7.output.dense.weight
2022-08-29 19:21:20,689 n: bert.encoder.layer.7.output.dense.bias
2022-08-29 19:21:20,689 n: bert.encoder.layer.7.output.LayerNorm.weight
2022-08-29 19:21:20,689 n: bert.encoder.layer.7.output.LayerNorm.bias
2022-08-29 19:21:20,689 n: bert.encoder.layer.8.attention.self.query.weight
2022-08-29 19:21:20,690 n: bert.encoder.layer.8.attention.self.query.bias
2022-08-29 19:21:20,690 n: bert.encoder.layer.8.attention.self.key.weight
2022-08-29 19:21:20,690 n: bert.encoder.layer.8.attention.self.key.bias
2022-08-29 19:21:20,690 n: bert.encoder.layer.8.attention.self.value.weight
2022-08-29 19:21:20,690 n: bert.encoder.layer.8.attention.self.value.bias
2022-08-29 19:21:20,690 n: bert.encoder.layer.8.attention.output.dense.weight
2022-08-29 19:21:20,690 n: bert.encoder.layer.8.attention.output.dense.bias
2022-08-29 19:21:20,690 n: bert.encoder.layer.8.attention.output.LayerNorm.weight
2022-08-29 19:21:20,690 n: bert.encoder.layer.8.attention.output.LayerNorm.bias
2022-08-29 19:21:20,690 n: bert.encoder.layer.8.intermediate.dense.weight
2022-08-29 19:21:20,690 n: bert.encoder.layer.8.intermediate.dense.bias
2022-08-29 19:21:20,690 n: bert.encoder.layer.8.output.dense.weight
2022-08-29 19:21:20,690 n: bert.encoder.layer.8.output.dense.bias
2022-08-29 19:21:20,690 n: bert.encoder.layer.8.output.LayerNorm.weight
2022-08-29 19:21:20,690 n: bert.encoder.layer.8.output.LayerNorm.bias
2022-08-29 19:21:20,690 n: bert.encoder.layer.9.attention.self.query.weight
2022-08-29 19:21:20,690 n: bert.encoder.layer.9.attention.self.query.bias
2022-08-29 19:21:20,691 n: bert.encoder.layer.9.attention.self.key.weight
2022-08-29 19:21:20,691 n: bert.encoder.layer.9.attention.self.key.bias
2022-08-29 19:21:20,691 n: bert.encoder.layer.9.attention.self.value.weight
2022-08-29 19:21:20,691 n: bert.encoder.layer.9.attention.self.value.bias
2022-08-29 19:21:20,691 n: bert.encoder.layer.9.attention.output.dense.weight
2022-08-29 19:21:20,691 n: bert.encoder.layer.9.attention.output.dense.bias
2022-08-29 19:21:20,691 n: bert.encoder.layer.9.attention.output.LayerNorm.weight
2022-08-29 19:21:20,691 n: bert.encoder.layer.9.attention.output.LayerNorm.bias
2022-08-29 19:21:20,691 n: bert.encoder.layer.9.intermediate.dense.weight
2022-08-29 19:21:20,691 n: bert.encoder.layer.9.intermediate.dense.bias
2022-08-29 19:21:20,691 n: bert.encoder.layer.9.output.dense.weight
2022-08-29 19:21:20,691 n: bert.encoder.layer.9.output.dense.bias
2022-08-29 19:21:20,691 n: bert.encoder.layer.9.output.LayerNorm.weight
2022-08-29 19:21:20,691 n: bert.encoder.layer.9.output.LayerNorm.bias
2022-08-29 19:21:20,691 n: bert.encoder.layer.10.attention.self.query.weight
2022-08-29 19:21:20,691 n: bert.encoder.layer.10.attention.self.query.bias
2022-08-29 19:21:20,691 n: bert.encoder.layer.10.attention.self.key.weight
2022-08-29 19:21:20,691 n: bert.encoder.layer.10.attention.self.key.bias
2022-08-29 19:21:20,691 n: bert.encoder.layer.10.attention.self.value.weight
2022-08-29 19:21:20,692 n: bert.encoder.layer.10.attention.self.value.bias
2022-08-29 19:21:20,692 n: bert.encoder.layer.10.attention.output.dense.weight
2022-08-29 19:21:20,692 n: bert.encoder.layer.10.attention.output.dense.bias
2022-08-29 19:21:20,692 n: bert.encoder.layer.10.attention.output.LayerNorm.weight
2022-08-29 19:21:20,692 n: bert.encoder.layer.10.attention.output.LayerNorm.bias
2022-08-29 19:21:20,692 n: bert.encoder.layer.10.intermediate.dense.weight
2022-08-29 19:21:20,692 n: bert.encoder.layer.10.intermediate.dense.bias
2022-08-29 19:21:20,692 n: bert.encoder.layer.10.output.dense.weight
2022-08-29 19:21:20,692 n: bert.encoder.layer.10.output.dense.bias
2022-08-29 19:21:20,692 n: bert.encoder.layer.10.output.LayerNorm.weight
2022-08-29 19:21:20,692 n: bert.encoder.layer.10.output.LayerNorm.bias
2022-08-29 19:21:20,692 n: bert.encoder.layer.11.attention.self.query.weight
2022-08-29 19:21:20,692 n: bert.encoder.layer.11.attention.self.query.bias
2022-08-29 19:21:20,692 n: bert.encoder.layer.11.attention.self.key.weight
2022-08-29 19:21:20,692 n: bert.encoder.layer.11.attention.self.key.bias
2022-08-29 19:21:20,692 n: bert.encoder.layer.11.attention.self.value.weight
2022-08-29 19:21:20,692 n: bert.encoder.layer.11.attention.self.value.bias
2022-08-29 19:21:20,692 n: bert.encoder.layer.11.attention.output.dense.weight
2022-08-29 19:21:20,692 n: bert.encoder.layer.11.attention.output.dense.bias
2022-08-29 19:21:20,693 n: bert.encoder.layer.11.attention.output.LayerNorm.weight
2022-08-29 19:21:20,693 n: bert.encoder.layer.11.attention.output.LayerNorm.bias
2022-08-29 19:21:20,693 n: bert.encoder.layer.11.intermediate.dense.weight
2022-08-29 19:21:20,693 n: bert.encoder.layer.11.intermediate.dense.bias
2022-08-29 19:21:20,693 n: bert.encoder.layer.11.output.dense.weight
2022-08-29 19:21:20,693 n: bert.encoder.layer.11.output.dense.bias
2022-08-29 19:21:20,693 n: bert.encoder.layer.11.output.LayerNorm.weight
2022-08-29 19:21:20,693 n: bert.encoder.layer.11.output.LayerNorm.bias
2022-08-29 19:21:20,693 n: bert.pooler.dense.weight
2022-08-29 19:21:20,693 n: bert.pooler.dense.bias
2022-08-29 19:21:20,693 n: classifier.weight
2022-08-29 19:21:20,693 n: classifier.bias
2022-08-29 19:21:20,693 Total parameters: 109483778
2022-08-29 19:21:20,698 ***** Teacher evaluation *****
2022-08-29 19:21:20,699 {'pearson': 0.8927170602068698, 'spearmanr': 0.8894144581995493, 'corr': 0.8910657592032096, 'eval_loss': 0.4806266178159004}
2022-08-29 19:21:20,699 Model config {
  "_name_or_path": "bert-base-uncased",
  "architectures": [
    "BertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "crypten": false,
  "finetuning_task": "stsb",
  "gradient_checkpointing": false,
  "hidden_act": "quad",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0
  },
  "layer_norm_eps": 1e-12,
  "log_path": "tmp/distill/STSB/quad_2quad/bert-base-uncased/log.txt",
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "pre_trained": "",
  "problem_type": "regression",
  "softmax_act": "2quad",
  "torch_dtype": "float32",
  "training": "",
  "transformers_version": "4.20.0.dev0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

2022-08-29 19:21:23,453 Loading model /home/ubuntu/checkpoints/exp/STSB/pytorch_model.bin
2022-08-29 19:21:23,782 loading model...
2022-08-29 19:21:23,829 done!
2022-08-29 19:21:23,829 Weights from pretrained model not used in TinyBertForSequenceClassification: ['bert.embeddings.position_ids']
2022-08-29 19:21:23,913 ***** Running training *****
2022-08-29 19:21:23,913   Num examples = 5749
2022-08-29 19:21:23,913   Batch size = 8
2022-08-29 19:21:23,913   Num steps = 35900
2022-08-29 19:21:23,914 n: bert.embeddings.word_embeddings.weight
2022-08-29 19:21:23,914 n: bert.embeddings.position_embeddings.weight
2022-08-29 19:21:23,914 n: bert.embeddings.token_type_embeddings.weight
2022-08-29 19:21:23,914 n: bert.embeddings.LayerNorm.weight
2022-08-29 19:21:23,914 n: bert.embeddings.LayerNorm.bias
2022-08-29 19:21:23,914 n: bert.encoder.layer.0.attention.self.query.weight
2022-08-29 19:21:23,914 n: bert.encoder.layer.0.attention.self.query.bias
2022-08-29 19:21:23,915 n: bert.encoder.layer.0.attention.self.key.weight
2022-08-29 19:21:23,915 n: bert.encoder.layer.0.attention.self.key.bias
2022-08-29 19:21:23,915 n: bert.encoder.layer.0.attention.self.value.weight
2022-08-29 19:21:23,915 n: bert.encoder.layer.0.attention.self.value.bias
2022-08-29 19:21:23,915 n: bert.encoder.layer.0.attention.output.dense.weight
2022-08-29 19:21:23,915 n: bert.encoder.layer.0.attention.output.dense.bias
2022-08-29 19:21:23,915 n: bert.encoder.layer.0.attention.output.LayerNorm.weight
2022-08-29 19:21:23,915 n: bert.encoder.layer.0.attention.output.LayerNorm.bias
2022-08-29 19:21:23,915 n: bert.encoder.layer.0.intermediate.dense.weight
2022-08-29 19:21:23,915 n: bert.encoder.layer.0.intermediate.dense.bias
2022-08-29 19:21:23,915 n: bert.encoder.layer.0.output.dense.weight
2022-08-29 19:21:23,915 n: bert.encoder.layer.0.output.dense.bias
2022-08-29 19:21:23,915 n: bert.encoder.layer.0.output.LayerNorm.weight
2022-08-29 19:21:23,915 n: bert.encoder.layer.0.output.LayerNorm.bias
2022-08-29 19:21:23,915 n: bert.encoder.layer.1.attention.self.query.weight
2022-08-29 19:21:23,915 n: bert.encoder.layer.1.attention.self.query.bias
2022-08-29 19:21:23,915 n: bert.encoder.layer.1.attention.self.key.weight
2022-08-29 19:21:23,916 n: bert.encoder.layer.1.attention.self.key.bias
2022-08-29 19:21:23,916 n: bert.encoder.layer.1.attention.self.value.weight
2022-08-29 19:21:23,916 n: bert.encoder.layer.1.attention.self.value.bias
2022-08-29 19:21:23,916 n: bert.encoder.layer.1.attention.output.dense.weight
2022-08-29 19:21:23,916 n: bert.encoder.layer.1.attention.output.dense.bias
2022-08-29 19:21:23,916 n: bert.encoder.layer.1.attention.output.LayerNorm.weight
2022-08-29 19:21:23,916 n: bert.encoder.layer.1.attention.output.LayerNorm.bias
2022-08-29 19:21:23,916 n: bert.encoder.layer.1.intermediate.dense.weight
2022-08-29 19:21:23,916 n: bert.encoder.layer.1.intermediate.dense.bias
2022-08-29 19:21:23,916 n: bert.encoder.layer.1.output.dense.weight
2022-08-29 19:21:23,916 n: bert.encoder.layer.1.output.dense.bias
2022-08-29 19:21:23,916 n: bert.encoder.layer.1.output.LayerNorm.weight
2022-08-29 19:21:23,916 n: bert.encoder.layer.1.output.LayerNorm.bias
2022-08-29 19:21:23,916 n: bert.encoder.layer.2.attention.self.query.weight
2022-08-29 19:21:23,916 n: bert.encoder.layer.2.attention.self.query.bias
2022-08-29 19:21:23,916 n: bert.encoder.layer.2.attention.self.key.weight
2022-08-29 19:21:23,917 n: bert.encoder.layer.2.attention.self.key.bias
2022-08-29 19:21:23,917 n: bert.encoder.layer.2.attention.self.value.weight
2022-08-29 19:21:23,917 n: bert.encoder.layer.2.attention.self.value.bias
2022-08-29 19:21:23,917 n: bert.encoder.layer.2.attention.output.dense.weight
2022-08-29 19:21:23,917 n: bert.encoder.layer.2.attention.output.dense.bias
2022-08-29 19:21:23,917 n: bert.encoder.layer.2.attention.output.LayerNorm.weight
2022-08-29 19:21:23,917 n: bert.encoder.layer.2.attention.output.LayerNorm.bias
2022-08-29 19:21:23,917 n: bert.encoder.layer.2.intermediate.dense.weight
2022-08-29 19:21:23,917 n: bert.encoder.layer.2.intermediate.dense.bias
2022-08-29 19:21:23,917 n: bert.encoder.layer.2.output.dense.weight
2022-08-29 19:21:23,917 n: bert.encoder.layer.2.output.dense.bias
2022-08-29 19:21:23,917 n: bert.encoder.layer.2.output.LayerNorm.weight
2022-08-29 19:21:23,917 n: bert.encoder.layer.2.output.LayerNorm.bias
2022-08-29 19:21:23,917 n: bert.encoder.layer.3.attention.self.query.weight
2022-08-29 19:21:23,917 n: bert.encoder.layer.3.attention.self.query.bias
2022-08-29 19:21:23,917 n: bert.encoder.layer.3.attention.self.key.weight
2022-08-29 19:21:23,917 n: bert.encoder.layer.3.attention.self.key.bias
2022-08-29 19:21:23,918 n: bert.encoder.layer.3.attention.self.value.weight
2022-08-29 19:21:23,918 n: bert.encoder.layer.3.attention.self.value.bias
2022-08-29 19:21:23,918 n: bert.encoder.layer.3.attention.output.dense.weight
2022-08-29 19:21:23,918 n: bert.encoder.layer.3.attention.output.dense.bias
2022-08-29 19:21:23,918 n: bert.encoder.layer.3.attention.output.LayerNorm.weight
2022-08-29 19:21:23,918 n: bert.encoder.layer.3.attention.output.LayerNorm.bias
2022-08-29 19:21:23,918 n: bert.encoder.layer.3.intermediate.dense.weight
2022-08-29 19:21:23,918 n: bert.encoder.layer.3.intermediate.dense.bias
2022-08-29 19:21:23,918 n: bert.encoder.layer.3.output.dense.weight
2022-08-29 19:21:23,918 n: bert.encoder.layer.3.output.dense.bias
2022-08-29 19:21:23,918 n: bert.encoder.layer.3.output.LayerNorm.weight
2022-08-29 19:21:23,918 n: bert.encoder.layer.3.output.LayerNorm.bias
2022-08-29 19:21:23,918 n: bert.encoder.layer.4.attention.self.query.weight
2022-08-29 19:21:23,918 n: bert.encoder.layer.4.attention.self.query.bias
2022-08-29 19:21:23,918 n: bert.encoder.layer.4.attention.self.key.weight
2022-08-29 19:21:23,918 n: bert.encoder.layer.4.attention.self.key.bias
2022-08-29 19:21:23,919 n: bert.encoder.layer.4.attention.self.value.weight
2022-08-29 19:21:23,919 n: bert.encoder.layer.4.attention.self.value.bias
2022-08-29 19:21:23,919 n: bert.encoder.layer.4.attention.output.dense.weight
2022-08-29 19:21:23,919 n: bert.encoder.layer.4.attention.output.dense.bias
2022-08-29 19:21:23,919 n: bert.encoder.layer.4.attention.output.LayerNorm.weight
2022-08-29 19:21:23,919 n: bert.encoder.layer.4.attention.output.LayerNorm.bias
2022-08-29 19:21:23,919 n: bert.encoder.layer.4.intermediate.dense.weight
2022-08-29 19:21:23,919 n: bert.encoder.layer.4.intermediate.dense.bias
2022-08-29 19:21:23,919 n: bert.encoder.layer.4.output.dense.weight
2022-08-29 19:21:23,919 n: bert.encoder.layer.4.output.dense.bias
2022-08-29 19:21:23,919 n: bert.encoder.layer.4.output.LayerNorm.weight
2022-08-29 19:21:23,919 n: bert.encoder.layer.4.output.LayerNorm.bias
2022-08-29 19:21:23,919 n: bert.encoder.layer.5.attention.self.query.weight
2022-08-29 19:21:23,919 n: bert.encoder.layer.5.attention.self.query.bias
2022-08-29 19:21:23,919 n: bert.encoder.layer.5.attention.self.key.weight
2022-08-29 19:21:23,919 n: bert.encoder.layer.5.attention.self.key.bias
2022-08-29 19:21:23,919 n: bert.encoder.layer.5.attention.self.value.weight
2022-08-29 19:21:23,920 n: bert.encoder.layer.5.attention.self.value.bias
2022-08-29 19:21:23,920 n: bert.encoder.layer.5.attention.output.dense.weight
2022-08-29 19:21:23,920 n: bert.encoder.layer.5.attention.output.dense.bias
2022-08-29 19:21:23,920 n: bert.encoder.layer.5.attention.output.LayerNorm.weight
2022-08-29 19:21:23,920 n: bert.encoder.layer.5.attention.output.LayerNorm.bias
2022-08-29 19:21:23,920 n: bert.encoder.layer.5.intermediate.dense.weight
2022-08-29 19:21:23,920 n: bert.encoder.layer.5.intermediate.dense.bias
2022-08-29 19:21:23,920 n: bert.encoder.layer.5.output.dense.weight
2022-08-29 19:21:23,920 n: bert.encoder.layer.5.output.dense.bias
2022-08-29 19:21:23,920 n: bert.encoder.layer.5.output.LayerNorm.weight
2022-08-29 19:21:23,920 n: bert.encoder.layer.5.output.LayerNorm.bias
2022-08-29 19:21:23,920 n: bert.encoder.layer.6.attention.self.query.weight
2022-08-29 19:21:23,920 n: bert.encoder.layer.6.attention.self.query.bias
2022-08-29 19:21:23,920 n: bert.encoder.layer.6.attention.self.key.weight
2022-08-29 19:21:23,920 n: bert.encoder.layer.6.attention.self.key.bias
2022-08-29 19:21:23,920 n: bert.encoder.layer.6.attention.self.value.weight
2022-08-29 19:21:23,920 n: bert.encoder.layer.6.attention.self.value.bias
2022-08-29 19:21:23,921 n: bert.encoder.layer.6.attention.output.dense.weight
2022-08-29 19:21:23,921 n: bert.encoder.layer.6.attention.output.dense.bias
2022-08-29 19:21:23,921 n: bert.encoder.layer.6.attention.output.LayerNorm.weight
2022-08-29 19:21:23,921 n: bert.encoder.layer.6.attention.output.LayerNorm.bias
2022-08-29 19:21:23,921 n: bert.encoder.layer.6.intermediate.dense.weight
2022-08-29 19:21:23,921 n: bert.encoder.layer.6.intermediate.dense.bias
2022-08-29 19:21:23,921 n: bert.encoder.layer.6.output.dense.weight
2022-08-29 19:21:23,921 n: bert.encoder.layer.6.output.dense.bias
2022-08-29 19:21:23,921 n: bert.encoder.layer.6.output.LayerNorm.weight
2022-08-29 19:21:23,921 n: bert.encoder.layer.6.output.LayerNorm.bias
2022-08-29 19:21:23,921 n: bert.encoder.layer.7.attention.self.query.weight
2022-08-29 19:21:23,921 n: bert.encoder.layer.7.attention.self.query.bias
2022-08-29 19:21:23,921 n: bert.encoder.layer.7.attention.self.key.weight
2022-08-29 19:21:23,921 n: bert.encoder.layer.7.attention.self.key.bias
2022-08-29 19:21:23,921 n: bert.encoder.layer.7.attention.self.value.weight
2022-08-29 19:21:23,921 n: bert.encoder.layer.7.attention.self.value.bias
2022-08-29 19:21:23,921 n: bert.encoder.layer.7.attention.output.dense.weight
2022-08-29 19:21:23,922 n: bert.encoder.layer.7.attention.output.dense.bias
2022-08-29 19:21:23,922 n: bert.encoder.layer.7.attention.output.LayerNorm.weight
2022-08-29 19:21:23,922 n: bert.encoder.layer.7.attention.output.LayerNorm.bias
2022-08-29 19:21:23,922 n: bert.encoder.layer.7.intermediate.dense.weight
2022-08-29 19:21:23,922 n: bert.encoder.layer.7.intermediate.dense.bias
2022-08-29 19:21:23,922 n: bert.encoder.layer.7.output.dense.weight
2022-08-29 19:21:23,922 n: bert.encoder.layer.7.output.dense.bias
2022-08-29 19:21:23,922 n: bert.encoder.layer.7.output.LayerNorm.weight
2022-08-29 19:21:23,922 n: bert.encoder.layer.7.output.LayerNorm.bias
2022-08-29 19:21:23,922 n: bert.encoder.layer.8.attention.self.query.weight
2022-08-29 19:21:23,922 n: bert.encoder.layer.8.attention.self.query.bias
2022-08-29 19:21:23,922 n: bert.encoder.layer.8.attention.self.key.weight
2022-08-29 19:21:23,922 n: bert.encoder.layer.8.attention.self.key.bias
2022-08-29 19:21:23,922 n: bert.encoder.layer.8.attention.self.value.weight
2022-08-29 19:21:23,922 n: bert.encoder.layer.8.attention.self.value.bias
2022-08-29 19:21:23,922 n: bert.encoder.layer.8.attention.output.dense.weight
2022-08-29 19:21:23,923 n: bert.encoder.layer.8.attention.output.dense.bias
2022-08-29 19:21:23,923 n: bert.encoder.layer.8.attention.output.LayerNorm.weight
2022-08-29 19:21:23,923 n: bert.encoder.layer.8.attention.output.LayerNorm.bias
2022-08-29 19:21:23,923 n: bert.encoder.layer.8.intermediate.dense.weight
2022-08-29 19:21:23,923 n: bert.encoder.layer.8.intermediate.dense.bias
2022-08-29 19:21:23,923 n: bert.encoder.layer.8.output.dense.weight
2022-08-29 19:21:23,923 n: bert.encoder.layer.8.output.dense.bias
2022-08-29 19:21:23,923 n: bert.encoder.layer.8.output.LayerNorm.weight
2022-08-29 19:21:23,923 n: bert.encoder.layer.8.output.LayerNorm.bias
2022-08-29 19:21:23,923 n: bert.encoder.layer.9.attention.self.query.weight
2022-08-29 19:21:23,923 n: bert.encoder.layer.9.attention.self.query.bias
2022-08-29 19:21:23,923 n: bert.encoder.layer.9.attention.self.key.weight
2022-08-29 19:21:23,923 n: bert.encoder.layer.9.attention.self.key.bias
2022-08-29 19:21:23,923 n: bert.encoder.layer.9.attention.self.value.weight
2022-08-29 19:21:23,923 n: bert.encoder.layer.9.attention.self.value.bias
2022-08-29 19:21:23,923 n: bert.encoder.layer.9.attention.output.dense.weight
2022-08-29 19:21:23,923 n: bert.encoder.layer.9.attention.output.dense.bias
2022-08-29 19:21:23,924 n: bert.encoder.layer.9.attention.output.LayerNorm.weight
2022-08-29 19:21:23,924 n: bert.encoder.layer.9.attention.output.LayerNorm.bias
2022-08-29 19:21:23,924 n: bert.encoder.layer.9.intermediate.dense.weight
2022-08-29 19:21:23,924 n: bert.encoder.layer.9.intermediate.dense.bias
2022-08-29 19:21:23,924 n: bert.encoder.layer.9.output.dense.weight
2022-08-29 19:21:23,924 n: bert.encoder.layer.9.output.dense.bias
2022-08-29 19:21:23,924 n: bert.encoder.layer.9.output.LayerNorm.weight
2022-08-29 19:21:23,924 n: bert.encoder.layer.9.output.LayerNorm.bias
2022-08-29 19:21:23,924 n: bert.encoder.layer.10.attention.self.query.weight
2022-08-29 19:21:23,924 n: bert.encoder.layer.10.attention.self.query.bias
2022-08-29 19:21:23,924 n: bert.encoder.layer.10.attention.self.key.weight
2022-08-29 19:21:23,924 n: bert.encoder.layer.10.attention.self.key.bias
2022-08-29 19:21:23,924 n: bert.encoder.layer.10.attention.self.value.weight
2022-08-29 19:21:23,924 n: bert.encoder.layer.10.attention.self.value.bias
2022-08-29 19:21:23,924 n: bert.encoder.layer.10.attention.output.dense.weight
2022-08-29 19:21:23,924 n: bert.encoder.layer.10.attention.output.dense.bias
2022-08-29 19:21:23,924 n: bert.encoder.layer.10.attention.output.LayerNorm.weight
2022-08-29 19:21:23,925 n: bert.encoder.layer.10.attention.output.LayerNorm.bias
2022-08-29 19:21:23,925 n: bert.encoder.layer.10.intermediate.dense.weight
2022-08-29 19:21:23,925 n: bert.encoder.layer.10.intermediate.dense.bias
2022-08-29 19:21:23,925 n: bert.encoder.layer.10.output.dense.weight
2022-08-29 19:21:23,925 n: bert.encoder.layer.10.output.dense.bias
2022-08-29 19:21:23,925 n: bert.encoder.layer.10.output.LayerNorm.weight
2022-08-29 19:21:23,925 n: bert.encoder.layer.10.output.LayerNorm.bias
2022-08-29 19:21:23,925 n: bert.encoder.layer.11.attention.self.query.weight
2022-08-29 19:21:23,925 n: bert.encoder.layer.11.attention.self.query.bias
2022-08-29 19:21:23,925 n: bert.encoder.layer.11.attention.self.key.weight
2022-08-29 19:21:23,925 n: bert.encoder.layer.11.attention.self.key.bias
2022-08-29 19:21:23,925 n: bert.encoder.layer.11.attention.self.value.weight
2022-08-29 19:21:23,925 n: bert.encoder.layer.11.attention.self.value.bias
2022-08-29 19:21:23,925 n: bert.encoder.layer.11.attention.output.dense.weight
2022-08-29 19:21:23,925 n: bert.encoder.layer.11.attention.output.dense.bias
2022-08-29 19:21:23,925 n: bert.encoder.layer.11.attention.output.LayerNorm.weight
2022-08-29 19:21:23,925 n: bert.encoder.layer.11.attention.output.LayerNorm.bias
2022-08-29 19:21:23,926 n: bert.encoder.layer.11.intermediate.dense.weight
2022-08-29 19:21:23,926 n: bert.encoder.layer.11.intermediate.dense.bias
2022-08-29 19:21:23,926 n: bert.encoder.layer.11.output.dense.weight
2022-08-29 19:21:23,926 n: bert.encoder.layer.11.output.dense.bias
2022-08-29 19:21:23,926 n: bert.encoder.layer.11.output.LayerNorm.weight
2022-08-29 19:21:23,926 n: bert.encoder.layer.11.output.LayerNorm.bias
2022-08-29 19:21:23,926 n: bert.pooler.dense.weight
2022-08-29 19:21:23,926 n: bert.pooler.dense.bias
2022-08-29 19:21:23,926 n: classifier.weight
2022-08-29 19:21:23,926 n: classifier.bias
2022-08-29 19:21:23,926 Total parameters: 109483009
2022-08-29 19:21:36,176 ***** Running evaluation *****
2022-08-29 19:21:36,176   Epoch = 3 iter 6999 step
2022-08-29 19:21:36,176   Num examples = 872
2022-08-29 19:21:36,176   Batch size = 32
2022-08-29 19:21:38,382 ***** Eval results *****
2022-08-29 19:21:38,382   acc = 0.9105504587155964
2022-08-29 19:21:38,382   att_loss = 0.0
2022-08-29 19:21:38,382   cls_loss = 0.02522549714914793
2022-08-29 19:21:38,382   eval_loss = 0.30524483123528107
2022-08-29 19:21:38,382   global_step = 6999
2022-08-29 19:21:38,382   loss = 0.02522549714914793
2022-08-29 19:21:38,382   rep_loss = 0.0
2022-08-29 19:21:53,152 ***** Running evaluation *****
2022-08-29 19:21:53,152   Epoch = 14 iter 15399 step
2022-08-29 19:21:53,152   Num examples = 1043
2022-08-29 19:21:53,152   Batch size = 32
2022-08-29 19:21:53,154 ***** Eval results *****
2022-08-29 19:21:53,154   att_loss = 0.8474568781970064
2022-08-29 19:21:53,154   cls_loss = 0.0
2022-08-29 19:21:53,154   global_step = 15399
2022-08-29 19:21:53,154   loss = 2.470641970367773
2022-08-29 19:21:53,154   rep_loss = 1.6231850911040167
2022-08-29 19:21:53,154 ***** Save model *****
2022-08-29 19:21:56,020 ***** Running evaluation *****
2022-08-29 19:21:56,020   Epoch = 0 iter 199 step
2022-08-29 19:21:56,020   Num examples = 277
2022-08-29 19:21:56,020   Batch size = 32
2022-08-29 19:21:56,022 ***** Eval results *****
2022-08-29 19:21:56,022   att_loss = 34.38896266898917
2022-08-29 19:21:56,022   cls_loss = 0.0
2022-08-29 19:21:56,022   global_step = 199
2022-08-29 19:21:56,022   loss = 40.603774008439416
2022-08-29 19:21:56,022   rep_loss = 6.214811327469409
2022-08-29 19:21:56,022 ***** Save model *****
2022-08-29 19:21:56,860 ***** Running evaluation *****
2022-08-29 19:21:56,861   Epoch = 2 iter 28599 step
2022-08-29 19:21:56,861   Num examples = 9815
2022-08-29 19:21:56,861   Batch size = 32
2022-08-29 19:21:56,862 ***** Eval results *****
2022-08-29 19:21:56,863   att_loss = 4.193993333410329
2022-08-29 19:21:56,863   cls_loss = 0.0
2022-08-29 19:21:56,863   global_step = 28599
2022-08-29 19:21:56,863   loss = 6.039813892283639
2022-08-29 19:21:56,863   rep_loss = 1.8458205580799516
2022-08-29 19:21:56,863 ***** Save model *****
2022-08-29 19:22:00,254 ***** Running evaluation *****
2022-08-29 19:22:00,255   Epoch = 0 iter 199 step
2022-08-29 19:22:00,255   Num examples = 1500
2022-08-29 19:22:00,255   Batch size = 32
2022-08-29 19:22:00,256 ***** Eval results *****
2022-08-29 19:22:00,256   att_loss = 10.942537886413497
2022-08-29 19:22:00,256   cls_loss = 0.0
2022-08-29 19:22:00,256   global_step = 199
2022-08-29 19:22:00,256   loss = 15.469557620772166
2022-08-29 19:22:00,256   rep_loss = 4.527019735556751
2022-08-29 19:22:00,256 ***** Save model *****
2022-08-29 19:22:18,890 ***** Running evaluation *****
2022-08-29 19:22:18,891   Epoch = 3 iter 7099 step
2022-08-29 19:22:18,891   Num examples = 872
2022-08-29 19:22:18,891   Batch size = 32
2022-08-29 19:22:21,094 ***** Eval results *****
2022-08-29 19:22:21,094   acc = 0.9254587155963303
2022-08-29 19:22:21,094   att_loss = 0.0
2022-08-29 19:22:21,094   cls_loss = 0.025280345495780777
2022-08-29 19:22:21,094   eval_loss = 0.26278930503342834
2022-08-29 19:22:21,094   global_step = 7099
2022-08-29 19:22:21,094   loss = 0.025280345495780777
2022-08-29 19:22:21,094   rep_loss = 0.0
2022-08-29 19:22:31,989 ***** Running evaluation *****
2022-08-29 19:22:31,989   Epoch = 1 iter 399 step
2022-08-29 19:22:31,989   Num examples = 277
2022-08-29 19:22:31,989   Batch size = 32
2022-08-29 19:22:31,991 ***** Eval results *****
2022-08-29 19:22:31,991   att_loss = 18.886920354583047
2022-08-29 19:22:31,991   cls_loss = 0.0
2022-08-29 19:22:31,991   global_step = 399
2022-08-29 19:22:31,991   loss = 23.872995723377574
2022-08-29 19:22:31,991   rep_loss = 4.986075282096863
2022-08-29 19:22:31,992 ***** Save model *****
2022-08-29 19:22:37,253 ***** Running evaluation *****
2022-08-29 19:22:37,254   Epoch = 0 iter 399 step
2022-08-29 19:22:37,254   Num examples = 1500
2022-08-29 19:22:37,254   Batch size = 32
2022-08-29 19:22:37,255 ***** Eval results *****
2022-08-29 19:22:37,255   att_loss = 8.629208764336761
2022-08-29 19:22:37,255   cls_loss = 0.0
2022-08-29 19:22:37,255   global_step = 399
2022-08-29 19:22:37,255   loss = 12.519053505775624
2022-08-29 19:22:37,255   rep_loss = 3.889844732475759
2022-08-29 19:22:37,255 ***** Save model *****
2022-08-29 19:22:37,346 ***** Running evaluation *****
2022-08-29 19:22:37,347   Epoch = 14 iter 15599 step
2022-08-29 19:22:37,347   Num examples = 1043
2022-08-29 19:22:37,347   Batch size = 32
2022-08-29 19:22:37,348 ***** Eval results *****
2022-08-29 19:22:37,348   att_loss = 0.8435978531100466
2022-08-29 19:22:37,348   cls_loss = 0.0
2022-08-29 19:22:37,349   global_step = 15599
2022-08-29 19:22:37,349   loss = 2.4666292321000256
2022-08-29 19:22:37,349   rep_loss = 1.623031378897854
2022-08-29 19:22:37,349 ***** Save model *****
2022-08-29 19:23:01,573 ***** Running evaluation *****
2022-08-29 19:23:01,574   Epoch = 3 iter 7199 step
2022-08-29 19:23:01,574   Num examples = 872
2022-08-29 19:23:01,574   Batch size = 32
2022-08-29 19:23:03,787 ***** Eval results *****
2022-08-29 19:23:03,787   acc = 0.9243119266055045
2022-08-29 19:23:03,787   att_loss = 0.0
2022-08-29 19:23:03,787   cls_loss = 0.025233737013503288
2022-08-29 19:23:03,787   eval_loss = 0.2791600202077201
2022-08-29 19:23:03,788   global_step = 7199
2022-08-29 19:23:03,788   loss = 0.025233737013503288
2022-08-29 19:23:03,788   rep_loss = 0.0
2022-08-29 19:23:12,703 ***** Running evaluation *****
2022-08-29 19:23:12,703   Epoch = 1 iter 599 step
2022-08-29 19:23:12,703   Num examples = 277
2022-08-29 19:23:12,703   Batch size = 32
2022-08-29 19:23:12,704 ***** Eval results *****
2022-08-29 19:23:12,704   att_loss = 17.44191826052136
2022-08-29 19:23:12,705   cls_loss = 0.0
2022-08-29 19:23:12,705   global_step = 599
2022-08-29 19:23:12,705   loss = 22.231562399201923
2022-08-29 19:23:12,705   rep_loss = 4.7896441370248795
2022-08-29 19:23:12,705 ***** Save model *****
2022-08-29 19:23:14,251 ***** Running evaluation *****
2022-08-29 19:23:14,251   Epoch = 2 iter 28799 step
2022-08-29 19:23:14,251   Num examples = 9815
2022-08-29 19:23:14,251   Batch size = 32
2022-08-29 19:23:14,253 ***** Eval results *****
2022-08-29 19:23:14,253   att_loss = 4.1941936031204445
2022-08-29 19:23:14,253   cls_loss = 0.0
2022-08-29 19:23:14,253   global_step = 28799
2022-08-29 19:23:14,253   loss = 6.039599422672587
2022-08-29 19:23:14,253   rep_loss = 1.8454058185160258
2022-08-29 19:23:14,253 ***** Save model *****
2022-08-29 19:23:19,714 ***** Running evaluation *****
2022-08-29 19:23:19,714   Epoch = 0 iter 599 step
2022-08-29 19:23:19,714   Num examples = 1500
2022-08-29 19:23:19,714   Batch size = 32
2022-08-29 19:23:19,716 ***** Eval results *****
2022-08-29 19:23:19,716   att_loss = 7.654062183950103
2022-08-29 19:23:19,716   cls_loss = 0.0
2022-08-29 19:23:19,716   global_step = 599
2022-08-29 19:23:19,716   loss = 11.256339204530287
2022-08-29 19:23:19,716   rep_loss = 3.6022770118235745
2022-08-29 19:23:19,716 ***** Save model *****
2022-08-29 19:23:20,841 ***** Running evaluation *****
2022-08-29 19:23:20,842   Epoch = 14 iter 15799 step
2022-08-29 19:23:20,842   Num examples = 1043
2022-08-29 19:23:20,842   Batch size = 32
2022-08-29 19:23:20,843 ***** Eval results *****
2022-08-29 19:23:20,843   att_loss = 0.8456717684530733
2022-08-29 19:23:20,843   cls_loss = 0.0
2022-08-29 19:23:20,843   global_step = 15799
2022-08-29 19:23:20,843   loss = 2.4710864145330724
2022-08-29 19:23:20,843   rep_loss = 1.6254146481911436
2022-08-29 19:23:20,844 ***** Save model *****
2022-08-29 19:23:44,249 ***** Running evaluation *****
2022-08-29 19:23:44,249   Epoch = 3 iter 7299 step
2022-08-29 19:23:44,250   Num examples = 872
2022-08-29 19:23:44,250   Batch size = 32
2022-08-29 19:23:46,455 ***** Eval results *****
2022-08-29 19:23:46,455   acc = 0.9185779816513762
2022-08-29 19:23:46,455   att_loss = 0.0
2022-08-29 19:23:46,455   cls_loss = 0.025461525509554776
2022-08-29 19:23:46,455   eval_loss = 0.2725762099559818
2022-08-29 19:23:46,455   global_step = 7299
2022-08-29 19:23:46,455   loss = 0.025461525509554776
2022-08-29 19:23:46,455   rep_loss = 0.0
2022-08-29 19:23:51,202 ***** Running evaluation *****
2022-08-29 19:23:51,203   Epoch = 2 iter 799 step
2022-08-29 19:23:51,203   Num examples = 277
2022-08-29 19:23:51,203   Batch size = 32
2022-08-29 19:23:51,204 ***** Eval results *****
2022-08-29 19:23:51,204   att_loss = 15.630669909008478
2022-08-29 19:23:51,204   cls_loss = 0.0
2022-08-29 19:23:51,204   global_step = 799
2022-08-29 19:23:51,204   loss = 20.04204031841903
2022-08-29 19:23:51,204   rep_loss = 4.411370422880529
2022-08-29 19:23:51,204 ***** Save model *****
2022-08-29 19:24:00,875 ***** Running evaluation *****
2022-08-29 19:24:00,876   Epoch = 1 iter 799 step
2022-08-29 19:24:00,876   Num examples = 1500
2022-08-29 19:24:00,876   Batch size = 32
2022-08-29 19:24:00,877 ***** Eval results *****
2022-08-29 19:24:00,877   att_loss = 4.953148459210808
2022-08-29 19:24:00,877   cls_loss = 0.0
2022-08-29 19:24:00,877   global_step = 799
2022-08-29 19:24:00,877   loss = 7.707244078318278
2022-08-29 19:24:00,877   rep_loss = 2.7540955837861993
2022-08-29 19:24:00,878 ***** Save model *****
2022-08-29 19:24:01,154 ***** Running evaluation *****
2022-08-29 19:24:01,155   Epoch = 14 iter 15999 step
2022-08-29 19:24:01,155   Num examples = 1043
2022-08-29 19:24:01,155   Batch size = 32
2022-08-29 19:24:01,156 ***** Eval results *****
2022-08-29 19:24:01,156   att_loss = 0.8454473907876948
2022-08-29 19:24:01,156   cls_loss = 0.0
2022-08-29 19:24:01,156   global_step = 15999
2022-08-29 19:24:01,156   loss = 2.469938039779663
2022-08-29 19:24:01,156   rep_loss = 1.6244906502444059
2022-08-29 19:24:01,156 ***** Save model *****
2022-08-29 19:24:26,934 ***** Running evaluation *****
2022-08-29 19:24:26,934   Epoch = 3 iter 7399 step
2022-08-29 19:24:26,934   Num examples = 872
2022-08-29 19:24:26,934   Batch size = 32
2022-08-29 19:24:27,699 ***** Running evaluation *****
2022-08-29 19:24:27,699   Epoch = 2 iter 28999 step
2022-08-29 19:24:27,699   Num examples = 9815
2022-08-29 19:24:27,699   Batch size = 32
2022-08-29 19:24:27,700 ***** Eval results *****
2022-08-29 19:24:27,700   att_loss = 4.19389386986948
2022-08-29 19:24:27,700   cls_loss = 0.0
2022-08-29 19:24:27,700   global_step = 28999
2022-08-29 19:24:27,700   loss = 6.0388472733381
2022-08-29 19:24:27,701   rep_loss = 1.8449534013288984
2022-08-29 19:24:27,701 ***** Save model *****
2022-08-29 19:24:29,000 ***** Running evaluation *****
2022-08-29 19:24:29,000   Epoch = 3 iter 999 step
2022-08-29 19:24:29,000   Num examples = 277
2022-08-29 19:24:29,000   Batch size = 32
2022-08-29 19:24:29,002 ***** Eval results *****
2022-08-29 19:24:29,002   att_loss = 14.616674481016217
2022-08-29 19:24:29,002   cls_loss = 0.0
2022-08-29 19:24:29,002   global_step = 999
2022-08-29 19:24:29,002   loss = 18.74496337139245
2022-08-29 19:24:29,002   rep_loss = 4.128288959011887
2022-08-29 19:24:29,002 ***** Save model *****
2022-08-29 19:24:29,144 ***** Eval results *****
2022-08-29 19:24:29,144   acc = 0.9243119266055045
2022-08-29 19:24:29,144   att_loss = 0.0
2022-08-29 19:24:29,144   cls_loss = 0.02573978975749375
2022-08-29 19:24:29,144   eval_loss = 0.2641304247413895
2022-08-29 19:24:29,144   global_step = 7399
2022-08-29 19:24:29,145   loss = 0.02573978975749375
2022-08-29 19:24:29,145   rep_loss = 0.0
2022-08-29 19:24:42,449 ***** Running evaluation *****
2022-08-29 19:24:42,450   Epoch = 15 iter 16199 step
2022-08-29 19:24:42,450   Num examples = 1043
2022-08-29 19:24:42,450   Batch size = 32
2022-08-29 19:24:42,451 ***** Eval results *****
2022-08-29 19:24:42,452   att_loss = 0.8349159403220235
2022-08-29 19:24:42,452   cls_loss = 0.0
2022-08-29 19:24:42,452   global_step = 16199
2022-08-29 19:24:42,452   loss = 2.4464314849683024
2022-08-29 19:24:42,452   rep_loss = 1.6115155439803055
2022-08-29 19:24:42,452 ***** Save model *****
2022-08-29 19:24:43,849 ***** Running evaluation *****
2022-08-29 19:24:43,850   Epoch = 1 iter 999 step
2022-08-29 19:24:43,850   Num examples = 1500
2022-08-29 19:24:43,850   Batch size = 32
2022-08-29 19:24:43,851 ***** Eval results *****
2022-08-29 19:24:43,851   att_loss = 4.832347298007843
2022-08-29 19:24:43,851   cls_loss = 0.0
2022-08-29 19:24:43,851   global_step = 999
2022-08-29 19:24:43,851   loss = 7.533165602497359
2022-08-29 19:24:43,851   rep_loss = 2.700818295156404
2022-08-29 19:24:43,851 ***** Save model *****
2022-08-29 19:25:08,908 ***** Running evaluation *****
2022-08-29 19:25:08,909   Epoch = 3 iter 1199 step
2022-08-29 19:25:08,909   Num examples = 277
2022-08-29 19:25:08,909   Batch size = 32
2022-08-29 19:25:08,910 ***** Eval results *****
2022-08-29 19:25:08,910   att_loss = 14.018164319203311
2022-08-29 19:25:08,910   cls_loss = 0.0
2022-08-29 19:25:08,910   global_step = 1199
2022-08-29 19:25:08,910   loss = 18.02811840960854
2022-08-29 19:25:08,910   rep_loss = 4.009954111020368
2022-08-29 19:25:08,911 ***** Save model *****
2022-08-29 19:25:09,619 ***** Running evaluation *****
2022-08-29 19:25:09,620   Epoch = 3 iter 7499 step
2022-08-29 19:25:09,620   Num examples = 872
2022-08-29 19:25:09,620   Batch size = 32
2022-08-29 19:25:11,830 ***** Eval results *****
2022-08-29 19:25:11,830   acc = 0.9220183486238532
2022-08-29 19:25:11,830   att_loss = 0.0
2022-08-29 19:25:11,830   cls_loss = 0.025563650305925067
2022-08-29 19:25:11,831   eval_loss = 0.2690652476490608
2022-08-29 19:25:11,831   global_step = 7499
2022-08-29 19:25:11,831   loss = 0.025563650305925067
2022-08-29 19:25:11,831   rep_loss = 0.0
2022-08-29 19:25:23,211 ***** Running evaluation *****
2022-08-29 19:25:23,211   Epoch = 15 iter 16399 step
2022-08-29 19:25:23,212   Num examples = 1043
2022-08-29 19:25:23,212   Batch size = 32
2022-08-29 19:25:23,213 ***** Eval results *****
2022-08-29 19:25:23,213   att_loss = 0.8442778340430247
2022-08-29 19:25:23,213   cls_loss = 0.0
2022-08-29 19:25:23,213   global_step = 16399
2022-08-29 19:25:23,213   loss = 2.460516967370831
2022-08-29 19:25:23,213   rep_loss = 1.6162391334850743
2022-08-29 19:25:23,213 ***** Save model *****
2022-08-29 19:25:25,964 ***** Running evaluation *****
2022-08-29 19:25:25,965   Epoch = 1 iter 1199 step
2022-08-29 19:25:25,965   Num examples = 1500
2022-08-29 19:25:25,965   Batch size = 32
2022-08-29 19:25:25,966 ***** Eval results *****
2022-08-29 19:25:25,966   att_loss = 4.780653882175375
2022-08-29 19:25:25,966   cls_loss = 0.0
2022-08-29 19:25:25,967   global_step = 1199
2022-08-29 19:25:25,967   loss = 7.42817832724716
2022-08-29 19:25:25,967   rep_loss = 2.647524435158331
2022-08-29 19:25:25,967 ***** Save model *****
2022-08-29 19:25:38,108 ***** Running evaluation *****
2022-08-29 19:25:38,109   Epoch = 2 iter 29199 step
2022-08-29 19:25:38,109   Num examples = 9815
2022-08-29 19:25:38,109   Batch size = 32
2022-08-29 19:25:38,110 ***** Eval results *****
2022-08-29 19:25:38,110   att_loss = 4.195224142033628
2022-08-29 19:25:38,110   cls_loss = 0.0
2022-08-29 19:25:38,110   global_step = 29199
2022-08-29 19:25:38,110   loss = 6.039825093677071
2022-08-29 19:25:38,110   rep_loss = 1.8446009494932214
2022-08-29 19:25:38,111 ***** Save model *****
2022-08-29 19:25:46,747 ***** Running evaluation *****
2022-08-29 19:25:46,747   Epoch = 4 iter 1399 step
2022-08-29 19:25:46,747   Num examples = 277
2022-08-29 19:25:46,747   Batch size = 32
2022-08-29 19:25:46,748 ***** Eval results *****
2022-08-29 19:25:46,748   att_loss = 12.91879112182125
2022-08-29 19:25:46,748   cls_loss = 0.0
2022-08-29 19:25:46,748   global_step = 1399
2022-08-29 19:25:46,748   loss = 16.680058190130417
2022-08-29 19:25:46,748   rep_loss = 3.761267122145622
2022-08-29 19:25:46,749 ***** Save model *****
2022-08-29 19:25:52,303 ***** Running evaluation *****
2022-08-29 19:25:52,304   Epoch = 3 iter 7599 step
2022-08-29 19:25:52,304   Num examples = 872
2022-08-29 19:25:52,304   Batch size = 32
2022-08-29 19:25:54,512 ***** Eval results *****
2022-08-29 19:25:54,512   acc = 0.9254587155963303
2022-08-29 19:25:54,512   att_loss = 0.0
2022-08-29 19:25:54,512   cls_loss = 0.02549946099438682
2022-08-29 19:25:54,512   eval_loss = 0.26514579841334907
2022-08-29 19:25:54,512   global_step = 7599
2022-08-29 19:25:54,512   loss = 0.02549946099438682
2022-08-29 19:25:54,512   rep_loss = 0.0
2022-08-29 19:26:03,149 ***** Running evaluation *****
2022-08-29 19:26:03,149   Epoch = 15 iter 16599 step
2022-08-29 19:26:03,150   Num examples = 1043
2022-08-29 19:26:03,150   Batch size = 32
2022-08-29 19:26:03,151 ***** Eval results *****
2022-08-29 19:26:03,151   att_loss = 0.8385294963041118
2022-08-29 19:26:03,151   cls_loss = 0.0
2022-08-29 19:26:03,151   global_step = 16599
2022-08-29 19:26:03,151   loss = 2.4519729581315692
2022-08-29 19:26:03,151   rep_loss = 1.6134434590479665
2022-08-29 19:26:03,151 ***** Save model *****
2022-08-29 19:26:05,379 ***** Running evaluation *****
2022-08-29 19:26:05,379   Epoch = 1 iter 1399 step
2022-08-29 19:26:05,379   Num examples = 1500
2022-08-29 19:26:05,379   Batch size = 32
2022-08-29 19:26:05,381 ***** Eval results *****
2022-08-29 19:26:05,381   att_loss = 4.634520880451286
2022-08-29 19:26:05,381   cls_loss = 0.0
2022-08-29 19:26:05,381   global_step = 1399
2022-08-29 19:26:05,381   loss = 7.2217059919662585
2022-08-29 19:26:05,381   rep_loss = 2.5871851006618507
2022-08-29 19:26:05,381 ***** Save model *****
2022-08-29 19:26:26,127 ***** Running evaluation *****
2022-08-29 19:26:26,128   Epoch = 5 iter 1599 step
2022-08-29 19:26:26,128   Num examples = 277
2022-08-29 19:26:26,128   Batch size = 32
2022-08-29 19:26:26,129 ***** Eval results *****
2022-08-29 19:26:26,129   att_loss = 11.719342751936479
2022-08-29 19:26:26,129   cls_loss = 0.0
2022-08-29 19:26:26,129   global_step = 1599
2022-08-29 19:26:26,129   loss = 15.231404759667136
2022-08-29 19:26:26,129   rep_loss = 3.512061991474845
2022-08-29 19:26:26,129 ***** Save model *****
2022-08-29 19:26:35,006 ***** Running evaluation *****
2022-08-29 19:26:35,007   Epoch = 3 iter 7699 step
2022-08-29 19:26:35,007   Num examples = 872
2022-08-29 19:26:35,007   Batch size = 32
2022-08-29 19:26:37,213 ***** Eval results *****
2022-08-29 19:26:37,213   acc = 0.9231651376146789
2022-08-29 19:26:37,213   att_loss = 0.0
2022-08-29 19:26:37,213   cls_loss = 0.025544856485280206
2022-08-29 19:26:37,213   eval_loss = 0.27160095390198485
2022-08-29 19:26:37,213   global_step = 7699
2022-08-29 19:26:37,213   loss = 0.025544856485280206
2022-08-29 19:26:37,213   rep_loss = 0.0
2022-08-29 19:26:45,206 ***** Running evaluation *****
2022-08-29 19:26:45,206   Epoch = 2 iter 1599 step
2022-08-29 19:26:45,207   Num examples = 1500
2022-08-29 19:26:45,207   Batch size = 32
2022-08-29 19:26:45,208 ***** Eval results *****
2022-08-29 19:26:45,208   att_loss = 4.003183527226828
2022-08-29 19:26:45,208   cls_loss = 0.0
2022-08-29 19:26:45,208   global_step = 1599
2022-08-29 19:26:45,208   loss = 6.32318036541617
2022-08-29 19:26:45,208   rep_loss = 2.3199968403833777
2022-08-29 19:26:45,208 ***** Save model *****
2022-08-29 19:26:47,546 ***** Running evaluation *****
2022-08-29 19:26:47,546   Epoch = 2 iter 29399 step
2022-08-29 19:26:47,546   Num examples = 9815
2022-08-29 19:26:47,546   Batch size = 32
2022-08-29 19:26:47,548 ***** Eval results *****
2022-08-29 19:26:47,548   att_loss = 4.193854811948981
2022-08-29 19:26:47,548   cls_loss = 0.0
2022-08-29 19:26:47,548   global_step = 29399
2022-08-29 19:26:47,548   loss = 6.038085474996428
2022-08-29 19:26:47,548   rep_loss = 1.8442306610839416
2022-08-29 19:26:47,548 ***** Save model *****
2022-08-29 19:26:47,642 ***** Running evaluation *****
2022-08-29 19:26:47,643   Epoch = 15 iter 16799 step
2022-08-29 19:26:47,643   Num examples = 1043
2022-08-29 19:26:47,643   Batch size = 32
2022-08-29 19:26:47,645 ***** Eval results *****
2022-08-29 19:26:47,645   att_loss = 0.8394803616752673
2022-08-29 19:26:47,645   cls_loss = 0.0
2022-08-29 19:26:47,645   global_step = 16799
2022-08-29 19:26:47,645   loss = 2.4555074467738573
2022-08-29 19:26:47,645   rep_loss = 1.6160270837213322
2022-08-29 19:26:47,645 ***** Save model *****
2022-08-29 19:27:07,098 ***** Running evaluation *****
2022-08-29 19:27:07,098   Epoch = 5 iter 1799 step
2022-08-29 19:27:07,098   Num examples = 277
2022-08-29 19:27:07,098   Batch size = 32
2022-08-29 19:27:07,099 ***** Eval results *****
2022-08-29 19:27:07,099   att_loss = 12.132776164617695
2022-08-29 19:27:07,100   cls_loss = 0.0
2022-08-29 19:27:07,100   global_step = 1799
2022-08-29 19:27:07,100   loss = 15.623114062137292
2022-08-29 19:27:07,100   rep_loss = 3.4903378955653457
2022-08-29 19:27:07,100 ***** Save model *****
2022-08-29 19:27:17,716 ***** Running evaluation *****
2022-08-29 19:27:17,717   Epoch = 3 iter 7799 step
2022-08-29 19:27:17,717   Num examples = 872
2022-08-29 19:27:17,717   Batch size = 32
2022-08-29 19:27:19,923 ***** Eval results *****
2022-08-29 19:27:19,923   acc = 0.9243119266055045
2022-08-29 19:27:19,924   att_loss = 0.0
2022-08-29 19:27:19,924   cls_loss = 0.025508555683552238
2022-08-29 19:27:19,924   eval_loss = 0.25742459343746305
2022-08-29 19:27:19,924   global_step = 7799
2022-08-29 19:27:19,924   loss = 0.025508555683552238
2022-08-29 19:27:19,924   rep_loss = 0.0
2022-08-29 19:27:24,000 ***** Running evaluation *****
2022-08-29 19:27:24,000   Epoch = 2 iter 1799 step
2022-08-29 19:27:24,000   Num examples = 1500
2022-08-29 19:27:24,000   Batch size = 32
2022-08-29 19:27:24,002 ***** Eval results *****
2022-08-29 19:27:24,002   att_loss = 3.865331174913517
2022-08-29 19:27:24,002   cls_loss = 0.0
2022-08-29 19:27:24,002   global_step = 1799
2022-08-29 19:27:24,002   loss = 6.151666526952066
2022-08-29 19:27:24,002   rep_loss = 2.2863353579497536
2022-08-29 19:27:24,002 ***** Save model *****
2022-08-29 19:27:36,546 ***** Running evaluation *****
2022-08-29 19:27:36,547   Epoch = 15 iter 16999 step
2022-08-29 19:27:36,547   Num examples = 1043
2022-08-29 19:27:36,547   Batch size = 32
2022-08-29 19:27:36,548 ***** Eval results *****
2022-08-29 19:27:36,548   att_loss = 0.839574339440943
2022-08-29 19:27:36,548   cls_loss = 0.0
2022-08-29 19:27:36,548   global_step = 16999
2022-08-29 19:27:36,548   loss = 2.4562184155535283
2022-08-29 19:27:36,548   rep_loss = 1.6166440736772578
2022-08-29 19:27:36,548 ***** Save model *****
2022-08-29 19:27:48,485 ***** Running evaluation *****
2022-08-29 19:27:48,485   Epoch = 6 iter 1999 step
2022-08-29 19:27:48,485   Num examples = 277
2022-08-29 19:27:48,485   Batch size = 32
2022-08-29 19:27:48,486 ***** Eval results *****
2022-08-29 19:27:48,486   att_loss = 11.615706135455827
2022-08-29 19:27:48,487   cls_loss = 0.0
2022-08-29 19:27:48,487   global_step = 1999
2022-08-29 19:27:48,487   loss = 14.944205843416372
2022-08-29 19:27:48,487   rep_loss = 3.3284996774859894
2022-08-29 19:27:48,487 ***** Save model *****
2022-08-29 19:28:00,498 ***** Running evaluation *****
2022-08-29 19:28:00,498   Epoch = 3 iter 7899 step
2022-08-29 19:28:00,498   Num examples = 872
2022-08-29 19:28:00,498   Batch size = 32
2022-08-29 19:28:02,708 ***** Eval results *****
2022-08-29 19:28:02,708   acc = 0.9197247706422018
2022-08-29 19:28:02,708   att_loss = 0.0
2022-08-29 19:28:02,708   cls_loss = 0.02559005030798805
2022-08-29 19:28:02,708   eval_loss = 0.258646165286856
2022-08-29 19:28:02,708   global_step = 7899
2022-08-29 19:28:02,708   loss = 0.02559005030798805
2022-08-29 19:28:02,708   rep_loss = 0.0
2022-08-29 19:28:02,958 ***** Running evaluation *****
2022-08-29 19:28:02,959   Epoch = 2 iter 1999 step
2022-08-29 19:28:02,959   Num examples = 1500
2022-08-29 19:28:02,959   Batch size = 32
2022-08-29 19:28:02,960 ***** Eval results *****
2022-08-29 19:28:02,960   att_loss = 3.7828213562973865
2022-08-29 19:28:02,960   cls_loss = 0.0
2022-08-29 19:28:02,960   global_step = 1999
2022-08-29 19:28:02,960   loss = 6.039214539485545
2022-08-29 19:28:02,960   rep_loss = 2.2563931859407713
2022-08-29 19:28:02,960 ***** Save model *****
2022-08-29 19:28:03,137 ***** Running evaluation *****
2022-08-29 19:28:03,138   Epoch = 2 iter 29599 step
2022-08-29 19:28:03,138   Num examples = 9815
2022-08-29 19:28:03,138   Batch size = 32
2022-08-29 19:28:03,140 ***** Eval results *****
2022-08-29 19:28:03,140   att_loss = 4.1936034886102
2022-08-29 19:28:03,140   cls_loss = 0.0
2022-08-29 19:28:03,140   global_step = 29599
2022-08-29 19:28:03,140   loss = 6.037528724945744
2022-08-29 19:28:03,140   rep_loss = 1.8439252349683035
2022-08-29 19:28:03,140 ***** Save model *****
2022-08-29 19:28:20,672 ***** Running evaluation *****
2022-08-29 19:28:20,673   Epoch = 16 iter 17199 step
2022-08-29 19:28:20,673   Num examples = 1043
2022-08-29 19:28:20,673   Batch size = 32
2022-08-29 19:28:20,674 ***** Eval results *****
2022-08-29 19:28:20,674   att_loss = 0.8335373654021873
2022-08-29 19:28:20,674   cls_loss = 0.0
2022-08-29 19:28:20,674   global_step = 17199
2022-08-29 19:28:20,674   loss = 2.442179991318299
2022-08-29 19:28:20,674   rep_loss = 1.6086426232312176
2022-08-29 19:28:20,674 ***** Save model *****
2022-08-29 19:28:29,187 ***** Running evaluation *****
2022-08-29 19:28:29,188   Epoch = 7 iter 2199 step
2022-08-29 19:28:29,188   Num examples = 277
2022-08-29 19:28:29,188   Batch size = 32
2022-08-29 19:28:29,189 ***** Eval results *****
2022-08-29 19:28:29,189   att_loss = 11.041950420899825
2022-08-29 19:28:29,189   cls_loss = 0.0
2022-08-29 19:28:29,189   global_step = 2199
2022-08-29 19:28:29,189   loss = 14.215607296336781
2022-08-29 19:28:29,189   rep_loss = 3.1736568212509155
2022-08-29 19:28:29,189 ***** Save model *****
2022-08-29 19:28:43,233 ***** Running evaluation *****
2022-08-29 19:28:43,233   Epoch = 3 iter 7999 step
2022-08-29 19:28:43,233   Num examples = 872
2022-08-29 19:28:43,233   Batch size = 32
2022-08-29 19:28:44,510 ***** Running evaluation *****
2022-08-29 19:28:44,511   Epoch = 3 iter 2199 step
2022-08-29 19:28:44,511   Num examples = 1500
2022-08-29 19:28:44,511   Batch size = 32
2022-08-29 19:28:44,512 ***** Eval results *****
2022-08-29 19:28:44,512   att_loss = 3.231999635696411
2022-08-29 19:28:44,512   cls_loss = 0.0
2022-08-29 19:28:44,512   global_step = 2199
2022-08-29 19:28:44,512   loss = 5.302173142962985
2022-08-29 19:28:44,512   rep_loss = 2.070173480775621
2022-08-29 19:28:44,512 ***** Save model *****
2022-08-29 19:28:45,444 ***** Eval results *****
2022-08-29 19:28:45,444   acc = 0.9254587155963303
2022-08-29 19:28:45,444   att_loss = 0.0
2022-08-29 19:28:45,444   cls_loss = 0.02561755447273618
2022-08-29 19:28:45,444   eval_loss = 0.2628145381542189
2022-08-29 19:28:45,444   global_step = 7999
2022-08-29 19:28:45,444   loss = 0.02561755447273618
2022-08-29 19:28:45,444   rep_loss = 0.0
2022-08-29 19:28:58,786 ***** Running evaluation *****
2022-08-29 19:28:58,787   Epoch = 16 iter 17399 step
2022-08-29 19:28:58,787   Num examples = 1043
2022-08-29 19:28:58,787   Batch size = 32
2022-08-29 19:28:58,788 ***** Eval results *****
2022-08-29 19:28:58,788   att_loss = 0.827205956173863
2022-08-29 19:28:58,788   cls_loss = 0.0
2022-08-29 19:28:58,789   global_step = 17399
2022-08-29 19:28:58,789   loss = 2.4319426154783685
2022-08-29 19:28:58,789   rep_loss = 1.6047366619876726
2022-08-29 19:28:58,789 ***** Save model *****
2022-08-29 19:29:07,818 ***** Running evaluation *****
2022-08-29 19:29:07,818   Epoch = 7 iter 2399 step
2022-08-29 19:29:07,818   Num examples = 277
2022-08-29 19:29:07,818   Batch size = 32
2022-08-29 19:29:07,820 ***** Eval results *****
2022-08-29 19:29:07,820   att_loss = 10.915530705237174
2022-08-29 19:29:07,820   cls_loss = 0.0
2022-08-29 19:29:07,820   global_step = 2399
2022-08-29 19:29:07,820   loss = 14.046392213116896
2022-08-29 19:29:07,820   rep_loss = 3.130861493918273
2022-08-29 19:29:07,820 ***** Save model *****
2022-08-29 19:29:15,659 ***** Running evaluation *****
2022-08-29 19:29:15,660   Epoch = 2 iter 29799 step
2022-08-29 19:29:15,660   Num examples = 9815
2022-08-29 19:29:15,660   Batch size = 32
2022-08-29 19:29:15,661 ***** Eval results *****
2022-08-29 19:29:15,661   att_loss = 4.193572190181143
2022-08-29 19:29:15,661   cls_loss = 0.0
2022-08-29 19:29:15,661   global_step = 29799
2022-08-29 19:29:15,661   loss = 6.037266242451964
2022-08-29 19:29:15,662   rep_loss = 1.843694051068977
2022-08-29 19:29:15,662 ***** Save model *****
2022-08-29 19:29:23,171 ***** Running evaluation *****
2022-08-29 19:29:23,172   Epoch = 3 iter 2399 step
2022-08-29 19:29:23,172   Num examples = 1500
2022-08-29 19:29:23,172   Batch size = 32
2022-08-29 19:29:23,173 ***** Eval results *****
2022-08-29 19:29:23,173   att_loss = 3.386079151776372
2022-08-29 19:29:23,173   cls_loss = 0.0
2022-08-29 19:29:23,173   global_step = 2399
2022-08-29 19:29:23,173   loss = 5.4670796462467735
2022-08-29 19:29:23,174   rep_loss = 2.0810005027420666
2022-08-29 19:29:23,174 ***** Save model *****
2022-08-29 19:29:25,973 ***** Running evaluation *****
2022-08-29 19:29:25,973   Epoch = 3 iter 8099 step
2022-08-29 19:29:25,973   Num examples = 872
2022-08-29 19:29:25,973   Batch size = 32
2022-08-29 19:29:28,181 ***** Eval results *****
2022-08-29 19:29:28,181   acc = 0.9220183486238532
2022-08-29 19:29:28,181   att_loss = 0.0
2022-08-29 19:29:28,181   cls_loss = 0.025722377972388626
2022-08-29 19:29:28,181   eval_loss = 0.23685591282056911
2022-08-29 19:29:28,181   global_step = 8099
2022-08-29 19:29:28,181   loss = 0.025722377972388626
2022-08-29 19:29:28,181   rep_loss = 0.0
2022-08-29 19:29:36,959 ***** Running evaluation *****
2022-08-29 19:29:36,960   Epoch = 16 iter 17599 step
2022-08-29 19:29:36,960   Num examples = 1043
2022-08-29 19:29:36,960   Batch size = 32
2022-08-29 19:29:36,961 ***** Eval results *****
2022-08-29 19:29:36,961   att_loss = 0.832242570277996
2022-08-29 19:29:36,961   cls_loss = 0.0
2022-08-29 19:29:36,961   global_step = 17599
2022-08-29 19:29:36,961   loss = 2.43732830642954
2022-08-29 19:29:36,961   rep_loss = 1.6050857389509794
2022-08-29 19:29:36,961 ***** Save model *****
2022-08-29 19:29:46,643 ***** Running evaluation *****
2022-08-29 19:29:46,643   Epoch = 8 iter 2599 step
2022-08-29 19:29:46,643   Num examples = 277
2022-08-29 19:29:46,643   Batch size = 32
2022-08-29 19:29:46,644 ***** Eval results *****
2022-08-29 19:29:46,645   att_loss = 10.933995092237318
2022-08-29 19:29:46,645   cls_loss = 0.0
2022-08-29 19:29:46,645   global_step = 2599
2022-08-29 19:29:46,645   loss = 13.988428081478084
2022-08-29 19:29:46,645   rep_loss = 3.054432980649106
2022-08-29 19:29:46,645 ***** Save model *****
2022-08-29 19:30:01,857 ***** Running evaluation *****
2022-08-29 19:30:01,858   Epoch = 3 iter 2599 step
2022-08-29 19:30:01,858   Num examples = 1500
2022-08-29 19:30:01,858   Batch size = 32
2022-08-29 19:30:01,859 ***** Eval results *****
2022-08-29 19:30:01,859   att_loss = 3.380914180466298
2022-08-29 19:30:01,859   cls_loss = 0.0
2022-08-29 19:30:01,859   global_step = 2599
2022-08-29 19:30:01,860   loss = 5.448314868198352
2022-08-29 19:30:01,860   rep_loss = 2.0674006906788
2022-08-29 19:30:01,860 ***** Save model *****
2022-08-29 19:30:08,711 ***** Running evaluation *****
2022-08-29 19:30:08,712   Epoch = 3 iter 8199 step
2022-08-29 19:30:08,712   Num examples = 872
2022-08-29 19:30:08,712   Batch size = 32
2022-08-29 19:30:10,918 ***** Eval results *****
2022-08-29 19:30:10,918   acc = 0.9197247706422018
2022-08-29 19:30:10,918   att_loss = 0.0
2022-08-29 19:30:10,918   cls_loss = 0.02569055864607168
2022-08-29 19:30:10,919   eval_loss = 0.24947026744484901
2022-08-29 19:30:10,919   global_step = 8199
2022-08-29 19:30:10,919   loss = 0.02569055864607168
2022-08-29 19:30:10,919   rep_loss = 0.0
2022-08-29 19:30:16,903 ***** Running evaluation *****
2022-08-29 19:30:16,904   Epoch = 16 iter 17799 step
2022-08-29 19:30:16,904   Num examples = 1043
2022-08-29 19:30:16,904   Batch size = 32
2022-08-29 19:30:16,905 ***** Eval results *****
2022-08-29 19:30:16,905   att_loss = 0.831774260172864
2022-08-29 19:30:16,905   cls_loss = 0.0
2022-08-29 19:30:16,905   global_step = 17799
2022-08-29 19:30:16,905   loss = 2.4376774853627223
2022-08-29 19:30:16,905   rep_loss = 1.6059032256090189
2022-08-29 19:30:16,905 ***** Save model *****
2022-08-29 19:30:24,763 ***** Running evaluation *****
2022-08-29 19:30:24,763   Epoch = 8 iter 2799 step
2022-08-29 19:30:24,763   Num examples = 277
2022-08-29 19:30:24,763   Batch size = 32
2022-08-29 19:30:24,764 ***** Eval results *****
2022-08-29 19:30:24,764   att_loss = 10.54126846521997
2022-08-29 19:30:24,765   cls_loss = 0.0
2022-08-29 19:30:24,765   global_step = 2799
2022-08-29 19:30:24,765   loss = 13.541958115875145
2022-08-29 19:30:24,765   rep_loss = 3.0006896629210837
2022-08-29 19:30:24,765 ***** Save model *****
2022-08-29 19:30:27,073 ***** Running evaluation *****
2022-08-29 19:30:27,073   Epoch = 2 iter 29999 step
2022-08-29 19:30:27,073   Num examples = 9815
2022-08-29 19:30:27,073   Batch size = 32
2022-08-29 19:30:27,075 ***** Eval results *****
2022-08-29 19:30:27,075   att_loss = 4.1916191722089735
2022-08-29 19:30:27,075   cls_loss = 0.0
2022-08-29 19:30:27,075   global_step = 29999
2022-08-29 19:30:27,075   loss = 6.034830371080049
2022-08-29 19:30:27,075   rep_loss = 1.843211197909886
2022-08-29 19:30:27,075 ***** Save model *****
2022-08-29 19:30:42,494 ***** Running evaluation *****
2022-08-29 19:30:42,495   Epoch = 3 iter 2799 step
2022-08-29 19:30:42,495   Num examples = 1500
2022-08-29 19:30:42,495   Batch size = 32
2022-08-29 19:30:42,496 ***** Eval results *****
2022-08-29 19:30:42,496   att_loss = 3.3295926447062527
2022-08-29 19:30:42,496   cls_loss = 0.0
2022-08-29 19:30:42,496   global_step = 2799
2022-08-29 19:30:42,496   loss = 5.378382338664329
2022-08-29 19:30:42,496   rep_loss = 2.0487897006116174
2022-08-29 19:30:42,496 ***** Save model *****
2022-08-29 19:30:51,418 ***** Running evaluation *****
2022-08-29 19:30:51,418   Epoch = 3 iter 8299 step
2022-08-29 19:30:51,418   Num examples = 872
2022-08-29 19:30:51,418   Batch size = 32
2022-08-29 19:30:53,624 ***** Eval results *****
2022-08-29 19:30:53,625   acc = 0.9243119266055045
2022-08-29 19:30:53,625   att_loss = 0.0
2022-08-29 19:30:53,625   cls_loss = 0.025688161806563862
2022-08-29 19:30:53,625   eval_loss = 0.2645154806253101
2022-08-29 19:30:53,625   global_step = 8299
2022-08-29 19:30:53,625   loss = 0.025688161806563862
2022-08-29 19:30:53,625   rep_loss = 0.0
2022-08-29 19:31:01,062 ***** Running evaluation *****
2022-08-29 19:31:01,062   Epoch = 16 iter 17999 step
2022-08-29 19:31:01,062   Num examples = 1043
2022-08-29 19:31:01,062   Batch size = 32
2022-08-29 19:31:01,064 ***** Eval results *****
2022-08-29 19:31:01,064   att_loss = 0.8310406594061825
2022-08-29 19:31:01,064   cls_loss = 0.0
2022-08-29 19:31:01,064   global_step = 17999
2022-08-29 19:31:01,064   loss = 2.4342226683766337
2022-08-29 19:31:01,064   rep_loss = 1.60318201014815
2022-08-29 19:31:01,064 ***** Save model *****
2022-08-29 19:31:06,222 ***** Running evaluation *****
2022-08-29 19:31:06,222   Epoch = 9 iter 2999 step
2022-08-29 19:31:06,222   Num examples = 277
2022-08-29 19:31:06,222   Batch size = 32
2022-08-29 19:31:06,224 ***** Eval results *****
2022-08-29 19:31:06,224   att_loss = 10.175891447067261
2022-08-29 19:31:06,224   cls_loss = 0.0
2022-08-29 19:31:06,224   global_step = 2999
2022-08-29 19:31:06,224   loss = 13.083264088630676
2022-08-29 19:31:06,224   rep_loss = 2.90737264752388
2022-08-29 19:31:06,224 ***** Save model *****
2022-08-29 19:31:23,085 ***** Running evaluation *****
2022-08-29 19:31:23,086   Epoch = 4 iter 2999 step
2022-08-29 19:31:23,086   Num examples = 1500
2022-08-29 19:31:23,086   Batch size = 32
2022-08-29 19:31:23,087 ***** Eval results *****
2022-08-29 19:31:23,087   att_loss = 3.10385232178245
2022-08-29 19:31:23,087   cls_loss = 0.0
2022-08-29 19:31:23,087   global_step = 2999
2022-08-29 19:31:23,087   loss = 5.063295784897692
2022-08-29 19:31:23,087   rep_loss = 1.95944344997406
2022-08-29 19:31:23,087 ***** Save model *****
2022-08-29 19:31:34,135 ***** Running evaluation *****
2022-08-29 19:31:34,135   Epoch = 3 iter 8399 step
2022-08-29 19:31:34,136   Num examples = 872
2022-08-29 19:31:34,136   Batch size = 32
2022-08-29 19:31:36,340 ***** Eval results *****
2022-08-29 19:31:36,340   acc = 0.9231651376146789
2022-08-29 19:31:36,340   att_loss = 0.0
2022-08-29 19:31:36,340   cls_loss = 0.025565253872563594
2022-08-29 19:31:36,340   eval_loss = 0.26496030766117784
2022-08-29 19:31:36,341   global_step = 8399
2022-08-29 19:31:36,341   loss = 0.025565253872563594
2022-08-29 19:31:36,341   rep_loss = 0.0
2022-08-29 19:31:41,060 ***** Running evaluation *****
2022-08-29 19:31:41,061   Epoch = 17 iter 18199 step
2022-08-29 19:31:41,061   Num examples = 1043
2022-08-29 19:31:41,061   Batch size = 32
2022-08-29 19:31:41,062 ***** Eval results *****
2022-08-29 19:31:41,062   att_loss = 0.7902523667313331
2022-08-29 19:31:41,062   cls_loss = 0.0
2022-08-29 19:31:41,062   global_step = 18199
2022-08-29 19:31:41,062   loss = 2.3797104414119277
2022-08-29 19:31:41,062   rep_loss = 1.5894580635913582
2022-08-29 19:31:41,063 ***** Save model *****
2022-08-29 19:31:41,180 ***** Running evaluation *****
2022-08-29 19:31:41,180   Epoch = 2 iter 30199 step
2022-08-29 19:31:41,180   Num examples = 9815
2022-08-29 19:31:41,180   Batch size = 32
2022-08-29 19:31:41,182 ***** Eval results *****
2022-08-29 19:31:41,182   att_loss = 4.190730525807945
2022-08-29 19:31:41,182   cls_loss = 0.0
2022-08-29 19:31:41,182   global_step = 30199
2022-08-29 19:31:41,182   loss = 6.033592416727229
2022-08-29 19:31:41,182   rep_loss = 1.842861889844567
2022-08-29 19:31:41,182 ***** Save model *****
2022-08-29 19:31:44,279 ***** Running evaluation *****
2022-08-29 19:31:44,279   Epoch = 10 iter 3199 step
2022-08-29 19:31:44,279   Num examples = 277
2022-08-29 19:31:44,279   Batch size = 32
2022-08-29 19:31:44,280 ***** Eval results *****
2022-08-29 19:31:44,280   att_loss = 9.683445148253709
2022-08-29 19:31:44,280   cls_loss = 0.0
2022-08-29 19:31:44,281   global_step = 3199
2022-08-29 19:31:44,281   loss = 12.503502572520395
2022-08-29 19:31:44,281   rep_loss = 2.8200575073113603
2022-08-29 19:31:44,281 ***** Save model *****
2022-08-29 19:32:03,730 ***** Running evaluation *****
2022-08-29 19:32:03,730   Epoch = 4 iter 3199 step
2022-08-29 19:32:03,730   Num examples = 1500
2022-08-29 19:32:03,730   Batch size = 32
2022-08-29 19:32:03,732 ***** Eval results *****
2022-08-29 19:32:03,732   att_loss = 3.1333641630429376
2022-08-29 19:32:03,732   cls_loss = 0.0
2022-08-29 19:32:03,732   global_step = 3199
2022-08-29 19:32:03,732   loss = 5.089026716506445
2022-08-29 19:32:03,732   rep_loss = 1.9556625472660822
2022-08-29 19:32:03,732 ***** Save model *****
2022-08-29 19:32:16,874 ***** Running evaluation *****
2022-08-29 19:32:16,875   Epoch = 4 iter 8499 step
2022-08-29 19:32:16,875   Num examples = 872
2022-08-29 19:32:16,875   Batch size = 32
2022-08-29 19:32:19,084 ***** Eval results *****
2022-08-29 19:32:19,084   acc = 0.9208715596330275
2022-08-29 19:32:19,084   att_loss = 0.0
2022-08-29 19:32:19,084   cls_loss = 0.02489646203277341
2022-08-29 19:32:19,084   eval_loss = 0.26407773330408546
2022-08-29 19:32:19,084   global_step = 8499
2022-08-29 19:32:19,084   loss = 0.02489646203277341
2022-08-29 19:32:19,084   rep_loss = 0.0
2022-08-29 19:32:23,917 ***** Running evaluation *****
2022-08-29 19:32:23,917   Epoch = 17 iter 18399 step
2022-08-29 19:32:23,917   Num examples = 1043
2022-08-29 19:32:23,917   Batch size = 32
2022-08-29 19:32:23,919 ***** Eval results *****
2022-08-29 19:32:23,919   att_loss = 0.8284765020810037
2022-08-29 19:32:23,919   cls_loss = 0.0
2022-08-29 19:32:23,919   global_step = 18399
2022-08-29 19:32:23,919   loss = 2.4227534851419583
2022-08-29 19:32:23,919   rep_loss = 1.5942769825703813
2022-08-29 19:32:23,919 ***** Save model *****
2022-08-29 19:32:25,854 ***** Running evaluation *****
2022-08-29 19:32:25,854   Epoch = 10 iter 3399 step
2022-08-29 19:32:25,854   Num examples = 277
2022-08-29 19:32:25,854   Batch size = 32
2022-08-29 19:32:25,856 ***** Eval results *****
2022-08-29 19:32:25,856   att_loss = 9.911594016329227
2022-08-29 19:32:25,856   cls_loss = 0.0
2022-08-29 19:32:25,856   global_step = 3399
2022-08-29 19:32:25,856   loss = 12.737153419573827
2022-08-29 19:32:25,856   rep_loss = 2.8255594337687775
2022-08-29 19:32:25,856 ***** Save model *****
2022-08-29 19:32:44,312 ***** Running evaluation *****
2022-08-29 19:32:44,312   Epoch = 4 iter 3399 step
2022-08-29 19:32:44,312   Num examples = 1500
2022-08-29 19:32:44,312   Batch size = 32
2022-08-29 19:32:44,313 ***** Eval results *****
2022-08-29 19:32:44,313   att_loss = 3.1577026113387077
2022-08-29 19:32:44,313   cls_loss = 0.0
2022-08-29 19:32:44,314   global_step = 3399
2022-08-29 19:32:44,314   loss = 5.101036416511608
2022-08-29 19:32:44,314   rep_loss = 1.9433338024584572
2022-08-29 19:32:44,314 ***** Save model *****
2022-08-29 19:32:54,695 ***** Running evaluation *****
2022-08-29 19:32:54,696   Epoch = 2 iter 30399 step
2022-08-29 19:32:54,696   Num examples = 9815
2022-08-29 19:32:54,696   Batch size = 32
2022-08-29 19:32:54,697 ***** Eval results *****
2022-08-29 19:32:54,697   att_loss = 4.18825098616242
2022-08-29 19:32:54,697   cls_loss = 0.0
2022-08-29 19:32:54,697   global_step = 30399
2022-08-29 19:32:54,697   loss = 6.030537606344017
2022-08-29 19:32:54,697   rep_loss = 1.842286619082518
2022-08-29 19:32:54,698 ***** Save model *****
2022-08-29 19:32:59,583 ***** Running evaluation *****
2022-08-29 19:32:59,584   Epoch = 4 iter 8599 step
2022-08-29 19:32:59,584   Num examples = 872
2022-08-29 19:32:59,584   Batch size = 32
2022-08-29 19:33:01,792 ***** Eval results *****
2022-08-29 19:33:01,792   acc = 0.9220183486238532
2022-08-29 19:33:01,792   att_loss = 0.0
2022-08-29 19:33:01,792   cls_loss = 0.024862366491837092
2022-08-29 19:33:01,792   eval_loss = 0.25617231568321586
2022-08-29 19:33:01,792   global_step = 8599
2022-08-29 19:33:01,792   loss = 0.024862366491837092
2022-08-29 19:33:01,792   rep_loss = 0.0
2022-08-29 19:33:02,567 ***** Running evaluation *****
2022-08-29 19:33:02,567   Epoch = 17 iter 18599 step
2022-08-29 19:33:02,567   Num examples = 1043
2022-08-29 19:33:02,567   Batch size = 32
2022-08-29 19:33:02,568 ***** Eval results *****
2022-08-29 19:33:02,568   att_loss = 0.8263976646599867
2022-08-29 19:33:02,568   cls_loss = 0.0
2022-08-29 19:33:02,568   global_step = 18599
2022-08-29 19:33:02,568   loss = 2.422998922942185
2022-08-29 19:33:02,568   rep_loss = 1.5966012582821987
2022-08-29 19:33:02,569 ***** Save model *****
2022-08-29 19:33:05,381 ***** Running evaluation *****
2022-08-29 19:33:05,382   Epoch = 11 iter 3599 step
2022-08-29 19:33:05,382   Num examples = 277
2022-08-29 19:33:05,382   Batch size = 32
2022-08-29 19:33:05,383 ***** Eval results *****
2022-08-29 19:33:05,383   att_loss = 9.640748701738508
2022-08-29 19:33:05,383   cls_loss = 0.0
2022-08-29 19:33:05,383   global_step = 3599
2022-08-29 19:33:05,383   loss = 12.407559668080191
2022-08-29 19:33:05,383   rep_loss = 2.7668109248193464
2022-08-29 19:33:05,383 ***** Save model *****
2022-08-29 19:33:23,298 ***** Running evaluation *****
2022-08-29 19:33:23,298   Epoch = 5 iter 3599 step
2022-08-29 19:33:23,298   Num examples = 1500
2022-08-29 19:33:23,298   Batch size = 32
2022-08-29 19:33:23,300 ***** Eval results *****
2022-08-29 19:33:23,300   att_loss = 3.345769749747382
2022-08-29 19:33:23,300   cls_loss = 0.0
2022-08-29 19:33:23,300   global_step = 3599
2022-08-29 19:33:23,300   loss = 5.296389606263903
2022-08-29 19:33:23,300   rep_loss = 1.9506198697619967
2022-08-29 19:33:23,300 ***** Save model *****
2022-08-29 19:33:40,826 ***** Running evaluation *****
2022-08-29 19:33:40,827   Epoch = 17 iter 18799 step
2022-08-29 19:33:40,827   Num examples = 1043
2022-08-29 19:33:40,827   Batch size = 32
2022-08-29 19:33:40,828 ***** Eval results *****
2022-08-29 19:33:40,829   att_loss = 0.8242901547885794
2022-08-29 19:33:40,829   cls_loss = 0.0
2022-08-29 19:33:40,829   global_step = 18799
2022-08-29 19:33:40,829   loss = 2.4214242016360696
2022-08-29 19:33:40,829   rep_loss = 1.5971340463840016
2022-08-29 19:33:40,829 ***** Save model *****
2022-08-29 19:33:42,291 ***** Running evaluation *****
2022-08-29 19:33:42,292   Epoch = 4 iter 8699 step
2022-08-29 19:33:42,292   Num examples = 872
2022-08-29 19:33:42,292   Batch size = 32
2022-08-29 19:33:44,031 ***** Running evaluation *****
2022-08-29 19:33:44,032   Epoch = 12 iter 3799 step
2022-08-29 19:33:44,032   Num examples = 277
2022-08-29 19:33:44,032   Batch size = 32
2022-08-29 19:33:44,033 ***** Eval results *****
2022-08-29 19:33:44,033   att_loss = 9.890377073145624
2022-08-29 19:33:44,033   cls_loss = 0.0
2022-08-29 19:33:44,033   global_step = 3799
2022-08-29 19:33:44,033   loss = 12.631500500351635
2022-08-29 19:33:44,033   rep_loss = 2.7411234058550935
2022-08-29 19:33:44,033 ***** Save model *****
2022-08-29 19:33:44,502 ***** Eval results *****
2022-08-29 19:33:44,502   acc = 0.9197247706422018
2022-08-29 19:33:44,502   att_loss = 0.0
2022-08-29 19:33:44,502   cls_loss = 0.024178999178309006
2022-08-29 19:33:44,502   eval_loss = 0.25799966590212925
2022-08-29 19:33:44,502   global_step = 8699
2022-08-29 19:33:44,502   loss = 0.024178999178309006
2022-08-29 19:33:44,502   rep_loss = 0.0
2022-08-29 19:34:02,078 ***** Running evaluation *****
2022-08-29 19:34:02,078   Epoch = 5 iter 3799 step
2022-08-29 19:34:02,078   Num examples = 1500
2022-08-29 19:34:02,079   Batch size = 32
2022-08-29 19:34:02,080 ***** Eval results *****
2022-08-29 19:34:02,080   att_loss = 2.98028763629603
2022-08-29 19:34:02,080   cls_loss = 0.0
2022-08-29 19:34:02,080   global_step = 3799
2022-08-29 19:34:02,080   loss = 4.848513322583797
2022-08-29 19:34:02,080   rep_loss = 1.8682256914211803
2022-08-29 19:34:02,080 ***** Save model *****
2022-08-29 19:34:03,810 ***** Running evaluation *****
2022-08-29 19:34:03,810   Epoch = 2 iter 30599 step
2022-08-29 19:34:03,811   Num examples = 9815
2022-08-29 19:34:03,811   Batch size = 32
2022-08-29 19:34:03,812 ***** Eval results *****
2022-08-29 19:34:03,812   att_loss = 4.186744990119727
2022-08-29 19:34:03,812   cls_loss = 0.0
2022-08-29 19:34:03,812   global_step = 30599
2022-08-29 19:34:03,812   loss = 6.02867881807337
2022-08-29 19:34:03,812   rep_loss = 1.8419338274419317
2022-08-29 19:34:03,812 ***** Save model *****
2022-08-29 19:34:24,039 ***** Running evaluation *****
2022-08-29 19:34:24,040   Epoch = 17 iter 18999 step
2022-08-29 19:34:24,040   Num examples = 1043
2022-08-29 19:34:24,040   Batch size = 32
2022-08-29 19:34:24,041 ***** Eval results *****
2022-08-29 19:34:24,041   att_loss = 0.8254667354365401
2022-08-29 19:34:24,041   cls_loss = 0.0
2022-08-29 19:34:24,041   global_step = 18999
2022-08-29 19:34:24,041   loss = 2.4221832950202877
2022-08-29 19:34:24,041   rep_loss = 1.5967165602200961
2022-08-29 19:34:24,041 ***** Save model *****
2022-08-29 19:34:25,147 ***** Running evaluation *****
2022-08-29 19:34:25,148   Epoch = 4 iter 8799 step
2022-08-29 19:34:25,148   Num examples = 872
2022-08-29 19:34:25,148   Batch size = 32
2022-08-29 19:34:25,700 ***** Running evaluation *****
2022-08-29 19:34:25,701   Epoch = 12 iter 3999 step
2022-08-29 19:34:25,701   Num examples = 277
2022-08-29 19:34:25,701   Batch size = 32
2022-08-29 19:34:25,702 ***** Eval results *****
2022-08-29 19:34:25,702   att_loss = 9.488910792918688
2022-08-29 19:34:25,702   cls_loss = 0.0
2022-08-29 19:34:25,702   global_step = 3999
2022-08-29 19:34:25,702   loss = 12.198890868197665
2022-08-29 19:34:25,702   rep_loss = 2.709980052062188
2022-08-29 19:34:25,703 ***** Save model *****
2022-08-29 19:34:27,368 ***** Eval results *****
2022-08-29 19:34:27,368   acc = 0.9208715596330275
2022-08-29 19:34:27,368   att_loss = 0.0
2022-08-29 19:34:27,368   cls_loss = 0.024545425769213987
2022-08-29 19:34:27,368   eval_loss = 0.24767912032880954
2022-08-29 19:34:27,368   global_step = 8799
2022-08-29 19:34:27,368   loss = 0.024545425769213987
2022-08-29 19:34:27,368   rep_loss = 0.0
2022-08-29 19:34:42,403 ***** Running evaluation *****
2022-08-29 19:34:42,403   Epoch = 5 iter 3999 step
2022-08-29 19:34:42,403   Num examples = 1500
2022-08-29 19:34:42,403   Batch size = 32
2022-08-29 19:34:42,405 ***** Eval results *****
2022-08-29 19:34:42,405   att_loss = 2.9763566756889697
2022-08-29 19:34:42,405   cls_loss = 0.0
2022-08-29 19:34:42,405   global_step = 3999
2022-08-29 19:34:42,405   loss = 4.835208862509879
2022-08-29 19:34:42,405   rep_loss = 1.8588521949819363
2022-08-29 19:34:42,405 ***** Save model *****
2022-08-29 19:35:03,012 ***** Running evaluation *****
2022-08-29 19:35:03,012   Epoch = 17 iter 19199 step
2022-08-29 19:35:03,012   Num examples = 1043
2022-08-29 19:35:03,012   Batch size = 32
2022-08-29 19:35:03,014 ***** Eval results *****
2022-08-29 19:35:03,014   att_loss = 0.8265573443525247
2022-08-29 19:35:03,014   cls_loss = 0.0
2022-08-29 19:35:03,014   global_step = 19199
2022-08-29 19:35:03,014   loss = 2.42434791407672
2022-08-29 19:35:03,014   rep_loss = 1.5977905690384278
2022-08-29 19:35:03,014 ***** Save model *****
2022-08-29 19:35:05,821 ***** Running evaluation *****
2022-08-29 19:35:05,821   Epoch = 13 iter 4199 step
2022-08-29 19:35:05,821   Num examples = 277
2022-08-29 19:35:05,821   Batch size = 32
2022-08-29 19:35:05,822 ***** Eval results *****
2022-08-29 19:35:05,823   att_loss = 9.280404280393551
2022-08-29 19:35:05,823   cls_loss = 0.0
2022-08-29 19:35:05,823   global_step = 4199
2022-08-29 19:35:05,823   loss = 11.939381156212244
2022-08-29 19:35:05,823   rep_loss = 2.6589768635920987
2022-08-29 19:35:05,823 ***** Save model *****
2022-08-29 19:35:08,576 ***** Running evaluation *****
2022-08-29 19:35:08,577   Epoch = 4 iter 8899 step
2022-08-29 19:35:08,577   Num examples = 872
2022-08-29 19:35:08,577   Batch size = 32
2022-08-29 19:35:10,801 ***** Eval results *****
2022-08-29 19:35:10,801   acc = 0.9254587155963303
2022-08-29 19:35:10,802   att_loss = 0.0
2022-08-29 19:35:10,802   cls_loss = 0.02444569472663151
2022-08-29 19:35:10,802   eval_loss = 0.25393251881801654
2022-08-29 19:35:10,802   global_step = 8899
2022-08-29 19:35:10,802   loss = 0.02444569472663151
2022-08-29 19:35:10,802   rep_loss = 0.0
2022-08-29 19:35:16,557 ***** Running evaluation *****
2022-08-29 19:35:16,557   Epoch = 2 iter 30799 step
2022-08-29 19:35:16,557   Num examples = 9815
2022-08-29 19:35:16,557   Batch size = 32
2022-08-29 19:35:16,558 ***** Eval results *****
2022-08-29 19:35:16,559   att_loss = 4.185539296131765
2022-08-29 19:35:16,559   cls_loss = 0.0
2022-08-29 19:35:16,559   global_step = 30799
2022-08-29 19:35:16,559   loss = 6.026988844911911
2022-08-29 19:35:16,559   rep_loss = 1.8414495485324687
2022-08-29 19:35:16,559 ***** Save model *****
2022-08-29 19:35:21,820 ***** Running evaluation *****
2022-08-29 19:35:21,821   Epoch = 5 iter 4199 step
2022-08-29 19:35:21,821   Num examples = 1500
2022-08-29 19:35:21,821   Batch size = 32
2022-08-29 19:35:21,822 ***** Eval results *****
2022-08-29 19:35:21,822   att_loss = 2.9550951163365533
2022-08-29 19:35:21,822   cls_loss = 0.0
2022-08-29 19:35:21,822   global_step = 4199
2022-08-29 19:35:21,822   loss = 4.806233864503932
2022-08-29 19:35:21,822   rep_loss = 1.8511387571716935
2022-08-29 19:35:21,822 ***** Save model *****
2022-08-29 19:35:41,286 ***** Running evaluation *****
2022-08-29 19:35:41,287   Epoch = 18 iter 19399 step
2022-08-29 19:35:41,287   Num examples = 1043
2022-08-29 19:35:41,287   Batch size = 32
2022-08-29 19:35:41,288 ***** Eval results *****
2022-08-29 19:35:41,288   att_loss = 0.8186974719592504
2022-08-29 19:35:41,288   cls_loss = 0.0
2022-08-29 19:35:41,288   global_step = 19399
2022-08-29 19:35:41,289   loss = 2.4113825130462647
2022-08-29 19:35:41,289   rep_loss = 1.5926850407464164
2022-08-29 19:35:41,289 ***** Save model *****
2022-08-29 19:35:44,449 ***** Running evaluation *****
2022-08-29 19:35:44,449   Epoch = 14 iter 4399 step
2022-08-29 19:35:44,449   Num examples = 277
2022-08-29 19:35:44,449   Batch size = 32
2022-08-29 19:35:44,450 ***** Eval results *****
2022-08-29 19:35:44,451   att_loss = 9.168727811177572
2022-08-29 19:35:44,451   cls_loss = 0.0
2022-08-29 19:35:44,451   global_step = 4399
2022-08-29 19:35:44,451   loss = 11.792416201697456
2022-08-29 19:35:44,451   rep_loss = 2.623688401116265
2022-08-29 19:35:44,451 ***** Save model *****
2022-08-29 19:35:51,921 ***** Running evaluation *****
2022-08-29 19:35:51,922   Epoch = 4 iter 8999 step
2022-08-29 19:35:51,922   Num examples = 872
2022-08-29 19:35:51,922   Batch size = 32
2022-08-29 19:35:54,134 ***** Eval results *****
2022-08-29 19:35:54,134   acc = 0.9254587155963303
2022-08-29 19:35:54,135   att_loss = 0.0
2022-08-29 19:35:54,135   cls_loss = 0.024995654056209744
2022-08-29 19:35:54,135   eval_loss = 0.24082921990858658
2022-08-29 19:35:54,135   global_step = 8999
2022-08-29 19:35:54,135   loss = 0.024995654056209744
2022-08-29 19:35:54,135   rep_loss = 0.0
2022-08-29 19:36:05,026 ***** Running evaluation *****
2022-08-29 19:36:05,026   Epoch = 6 iter 4399 step
2022-08-29 19:36:05,026   Num examples = 1500
2022-08-29 19:36:05,026   Batch size = 32
2022-08-29 19:36:05,027 ***** Eval results *****
2022-08-29 19:36:05,027   att_loss = 2.8292980849087894
2022-08-29 19:36:05,028   cls_loss = 0.0
2022-08-29 19:36:05,028   global_step = 4399
2022-08-29 19:36:05,028   loss = 4.639822076965165
2022-08-29 19:36:05,028   rep_loss = 1.8105240103962656
2022-08-29 19:36:05,028 ***** Save model *****
2022-08-29 19:36:19,565 ***** Running evaluation *****
2022-08-29 19:36:19,566   Epoch = 18 iter 19599 step
2022-08-29 19:36:19,566   Num examples = 1043
2022-08-29 19:36:19,566   Batch size = 32
2022-08-29 19:36:19,567 ***** Eval results *****
2022-08-29 19:36:19,567   att_loss = 0.8299770895640055
2022-08-29 19:36:19,567   cls_loss = 0.0
2022-08-29 19:36:19,567   global_step = 19599
2022-08-29 19:36:19,567   loss = 2.4217337169647215
2022-08-29 19:36:19,567   rep_loss = 1.5917566293080647
2022-08-29 19:36:19,568 ***** Save model *****
2022-08-29 19:36:22,755 ***** Running evaluation *****
2022-08-29 19:36:22,755   Epoch = 14 iter 4599 step
2022-08-29 19:36:22,756   Num examples = 277
2022-08-29 19:36:22,756   Batch size = 32
2022-08-29 19:36:22,757 ***** Eval results *****
2022-08-29 19:36:22,757   att_loss = 9.066241494003608
2022-08-29 19:36:22,757   cls_loss = 0.0
2022-08-29 19:36:22,757   global_step = 4599
2022-08-29 19:36:22,757   loss = 11.680149804329385
2022-08-29 19:36:22,757   rep_loss = 2.6139082986481337
2022-08-29 19:36:22,757 ***** Save model *****
2022-08-29 19:36:26,168 ***** Running evaluation *****
2022-08-29 19:36:26,169   Epoch = 2 iter 30999 step
2022-08-29 19:36:26,169   Num examples = 9815
2022-08-29 19:36:26,169   Batch size = 32
2022-08-29 19:36:26,170 ***** Eval results *****
2022-08-29 19:36:26,171   att_loss = 4.184643121885961
2022-08-29 19:36:26,171   cls_loss = 0.0
2022-08-29 19:36:26,171   global_step = 30999
2022-08-29 19:36:26,171   loss = 6.025671088475828
2022-08-29 19:36:26,171   rep_loss = 1.841027966368322
2022-08-29 19:36:26,171 ***** Save model *****
2022-08-29 19:36:35,760 ***** Running evaluation *****
2022-08-29 19:36:35,761   Epoch = 4 iter 9099 step
2022-08-29 19:36:35,761   Num examples = 872
2022-08-29 19:36:35,761   Batch size = 32
2022-08-29 19:36:37,978 ***** Eval results *****
2022-08-29 19:36:37,978   acc = 0.9243119266055045
2022-08-29 19:36:37,978   att_loss = 0.0
2022-08-29 19:36:37,978   cls_loss = 0.02511258907057072
2022-08-29 19:36:37,978   eval_loss = 0.24626946236406053
2022-08-29 19:36:37,978   global_step = 9099
2022-08-29 19:36:37,978   loss = 0.02511258907057072
2022-08-29 19:36:37,978   rep_loss = 0.0
2022-08-29 19:36:45,223 ***** Running evaluation *****
2022-08-29 19:36:45,224   Epoch = 6 iter 4599 step
2022-08-29 19:36:45,224   Num examples = 1500
2022-08-29 19:36:45,224   Batch size = 32
2022-08-29 19:36:45,226 ***** Eval results *****
2022-08-29 19:36:45,226   att_loss = 2.8652144498431804
2022-08-29 19:36:45,226   cls_loss = 0.0
2022-08-29 19:36:45,226   global_step = 4599
2022-08-29 19:36:45,226   loss = 4.668605999438624
2022-08-29 19:36:45,226   rep_loss = 1.803391553691982
2022-08-29 19:36:45,226 ***** Save model *****
2022-08-29 19:36:58,758 ***** Running evaluation *****
2022-08-29 19:36:58,758   Epoch = 18 iter 19799 step
2022-08-29 19:36:58,758   Num examples = 1043
2022-08-29 19:36:58,759   Batch size = 32
2022-08-29 19:36:58,760 ***** Eval results *****
2022-08-29 19:36:58,760   att_loss = 0.8247152629105942
2022-08-29 19:36:58,760   cls_loss = 0.0
2022-08-29 19:36:58,760   global_step = 19799
2022-08-29 19:36:58,760   loss = 2.4119774441097093
2022-08-29 19:36:58,760   rep_loss = 1.5872621789185897
2022-08-29 19:36:58,760 ***** Save model *****
2022-08-29 19:37:01,235 ***** Running evaluation *****
2022-08-29 19:37:01,236   Epoch = 15 iter 4799 step
2022-08-29 19:37:01,236   Num examples = 277
2022-08-29 19:37:01,236   Batch size = 32
2022-08-29 19:37:01,238 ***** Eval results *****
2022-08-29 19:37:01,238   att_loss = 8.94954188546138
2022-08-29 19:37:01,238   cls_loss = 0.0
2022-08-29 19:37:01,238   global_step = 4799
2022-08-29 19:37:01,238   loss = 11.529178612267794
2022-08-29 19:37:01,238   rep_loss = 2.5796367072347386
2022-08-29 19:37:01,238 ***** Save model *****
2022-08-29 19:37:19,514 ***** Running evaluation *****
2022-08-29 19:37:19,515   Epoch = 4 iter 9199 step
2022-08-29 19:37:19,515   Num examples = 872
2022-08-29 19:37:19,515   Batch size = 32
2022-08-29 19:37:21,729 ***** Eval results *****
2022-08-29 19:37:21,729   acc = 0.9220183486238532
2022-08-29 19:37:21,729   att_loss = 0.0
2022-08-29 19:37:21,729   cls_loss = 0.024785277117305406
2022-08-29 19:37:21,729   eval_loss = 0.26387582792501363
2022-08-29 19:37:21,729   global_step = 9199
2022-08-29 19:37:21,729   loss = 0.024785277117305406
2022-08-29 19:37:21,729   rep_loss = 0.0
2022-08-29 19:37:25,450 ***** Running evaluation *****
2022-08-29 19:37:25,451   Epoch = 6 iter 4799 step
2022-08-29 19:37:25,451   Num examples = 1500
2022-08-29 19:37:25,451   Batch size = 32
2022-08-29 19:37:25,452 ***** Eval results *****
2022-08-29 19:37:25,452   att_loss = 2.852887880049509
2022-08-29 19:37:25,452   cls_loss = 0.0
2022-08-29 19:37:25,453   global_step = 4799
2022-08-29 19:37:25,453   loss = 4.649469561100977
2022-08-29 19:37:25,453   rep_loss = 1.7965816834793558
2022-08-29 19:37:25,453 ***** Save model *****
2022-08-29 19:37:38,082 ***** Running evaluation *****
2022-08-29 19:37:38,082   Epoch = 2 iter 31199 step
2022-08-29 19:37:38,082   Num examples = 9815
2022-08-29 19:37:38,082   Batch size = 32
2022-08-29 19:37:38,083 ***** Eval results *****
2022-08-29 19:37:38,083   att_loss = 4.18305492598104
2022-08-29 19:37:38,083   cls_loss = 0.0
2022-08-29 19:37:38,083   global_step = 31199
2022-08-29 19:37:38,084   loss = 6.023705158646228
2022-08-29 19:37:38,084   rep_loss = 1.8406502323786704
2022-08-29 19:37:38,084 ***** Save model *****
2022-08-29 19:37:41,504 ***** Running evaluation *****
2022-08-29 19:37:41,505   Epoch = 18 iter 19999 step
2022-08-29 19:37:41,505   Num examples = 1043
2022-08-29 19:37:41,505   Batch size = 32
2022-08-29 19:37:41,506 ***** Eval results *****
2022-08-29 19:37:41,507   att_loss = 0.8224984513559649
2022-08-29 19:37:41,507   cls_loss = 0.0
2022-08-29 19:37:41,507   global_step = 19999
2022-08-29 19:37:41,507   loss = 2.4078459573561144
2022-08-29 19:37:41,507   rep_loss = 1.5853475055386943
2022-08-29 19:37:41,507 ***** Save model *****
2022-08-29 19:37:44,551 ***** Running evaluation *****
2022-08-29 19:37:44,551   Epoch = 16 iter 4999 step
2022-08-29 19:37:44,551   Num examples = 277
2022-08-29 19:37:44,551   Batch size = 32
2022-08-29 19:37:44,553 ***** Eval results *****
2022-08-29 19:37:44,553   att_loss = 8.85095963270768
2022-08-29 19:37:44,553   cls_loss = 0.0
2022-08-29 19:37:44,553   global_step = 4999
2022-08-29 19:37:44,553   loss = 11.396013508672299
2022-08-29 19:37:44,553   rep_loss = 2.5450540107229482
2022-08-29 19:37:44,553 ***** Save model *****
2022-08-29 19:38:03,196 ***** Running evaluation *****
2022-08-29 19:38:03,197   Epoch = 4 iter 9299 step
2022-08-29 19:38:03,197   Num examples = 872
2022-08-29 19:38:03,197   Batch size = 32
2022-08-29 19:38:05,412 ***** Eval results *****
2022-08-29 19:38:05,413   acc = 0.9231651376146789
2022-08-29 19:38:05,413   att_loss = 0.0
2022-08-29 19:38:05,413   cls_loss = 0.02478795456262316
2022-08-29 19:38:05,413   eval_loss = 0.24937628202938608
2022-08-29 19:38:05,413   global_step = 9299
2022-08-29 19:38:05,413   loss = 0.02478795456262316
2022-08-29 19:38:05,413   rep_loss = 0.0
2022-08-29 19:38:07,280 ***** Running evaluation *****
2022-08-29 19:38:07,281   Epoch = 6 iter 4999 step
2022-08-29 19:38:07,281   Num examples = 1500
2022-08-29 19:38:07,281   Batch size = 32
2022-08-29 19:38:07,282 ***** Eval results *****
2022-08-29 19:38:07,282   att_loss = 2.8510782120024243
2022-08-29 19:38:07,282   cls_loss = 0.0
2022-08-29 19:38:07,282   global_step = 4999
2022-08-29 19:38:07,282   loss = 4.642422196488995
2022-08-29 19:38:07,282   rep_loss = 1.7913439851766384
2022-08-29 19:38:07,282 ***** Save model *****
2022-08-29 19:38:19,815 ***** Running evaluation *****
2022-08-29 19:38:19,816   Epoch = 18 iter 20199 step
2022-08-29 19:38:19,816   Num examples = 1043
2022-08-29 19:38:19,816   Batch size = 32
2022-08-29 19:38:19,817 ***** Eval results *****
2022-08-29 19:38:19,817   att_loss = 0.8200646891960731
2022-08-29 19:38:19,818   cls_loss = 0.0
2022-08-29 19:38:19,818   global_step = 20199
2022-08-29 19:38:19,818   loss = 2.4066205110305394
2022-08-29 19:38:19,818   rep_loss = 1.5865558204895411
2022-08-29 19:38:19,818 ***** Save model *****
2022-08-29 19:38:23,051 ***** Running evaluation *****
2022-08-29 19:38:23,052   Epoch = 16 iter 5199 step
2022-08-29 19:38:23,052   Num examples = 277
2022-08-29 19:38:23,052   Batch size = 32
2022-08-29 19:38:23,053 ***** Eval results *****
2022-08-29 19:38:23,053   att_loss = 8.856538259395036
2022-08-29 19:38:23,053   cls_loss = 0.0
2022-08-29 19:38:23,053   global_step = 5199
2022-08-29 19:38:23,053   loss = 11.401581276692617
2022-08-29 19:38:23,053   rep_loss = 2.5450430269198567
2022-08-29 19:38:23,053 ***** Save model *****
2022-08-29 19:38:46,879 ***** Running evaluation *****
2022-08-29 19:38:46,880   Epoch = 4 iter 9399 step
2022-08-29 19:38:46,880   Num examples = 872
2022-08-29 19:38:46,880   Batch size = 32
2022-08-29 19:38:47,291 ***** Running evaluation *****
2022-08-29 19:38:47,291   Epoch = 7 iter 5199 step
2022-08-29 19:38:47,291   Num examples = 1500
2022-08-29 19:38:47,291   Batch size = 32
2022-08-29 19:38:47,292 ***** Eval results *****
2022-08-29 19:38:47,292   att_loss = 2.7576141474563953
2022-08-29 19:38:47,292   cls_loss = 0.0
2022-08-29 19:38:47,292   global_step = 5199
2022-08-29 19:38:47,292   loss = 4.51719338356415
2022-08-29 19:38:47,292   rep_loss = 1.7595792388640388
2022-08-29 19:38:47,293 ***** Save model *****
2022-08-29 19:38:49,101 ***** Eval results *****
2022-08-29 19:38:49,101   acc = 0.9174311926605505
2022-08-29 19:38:49,101   att_loss = 0.0
2022-08-29 19:38:49,101   cls_loss = 0.024607137231665948
2022-08-29 19:38:49,101   eval_loss = 0.2618542110015239
2022-08-29 19:38:49,101   global_step = 9399
2022-08-29 19:38:49,101   loss = 0.024607137231665948
2022-08-29 19:38:49,101   rep_loss = 0.0
2022-08-29 19:38:49,798 ***** Running evaluation *****
2022-08-29 19:38:49,798   Epoch = 2 iter 31399 step
2022-08-29 19:38:49,798   Num examples = 9815
2022-08-29 19:38:49,798   Batch size = 32
2022-08-29 19:38:49,800 ***** Eval results *****
2022-08-29 19:38:49,800   att_loss = 4.181298265551728
2022-08-29 19:38:49,800   cls_loss = 0.0
2022-08-29 19:38:49,800   global_step = 31399
2022-08-29 19:38:49,800   loss = 6.0214790894579435
2022-08-29 19:38:49,800   rep_loss = 1.8401808232977384
2022-08-29 19:38:49,800 ***** Save model *****
2022-08-29 19:38:58,084 ***** Running evaluation *****
2022-08-29 19:38:58,084   Epoch = 19 iter 20399 step
2022-08-29 19:38:58,084   Num examples = 1043
2022-08-29 19:38:58,085   Batch size = 32
2022-08-29 19:38:58,086 ***** Eval results *****
2022-08-29 19:38:58,086   att_loss = 0.8168226146252355
2022-08-29 19:38:58,086   cls_loss = 0.0
2022-08-29 19:38:58,086   global_step = 20399
2022-08-29 19:38:58,086   loss = 2.406887154712855
2022-08-29 19:38:58,086   rep_loss = 1.5900645456581473
2022-08-29 19:38:58,086 ***** Save model *****
2022-08-29 19:39:01,436 ***** Running evaluation *****
2022-08-29 19:39:01,436   Epoch = 17 iter 5399 step
2022-08-29 19:39:01,436   Num examples = 277
2022-08-29 19:39:01,436   Batch size = 32
2022-08-29 19:39:01,437 ***** Eval results *****
2022-08-29 19:39:01,437   att_loss = 8.551904320716858
2022-08-29 19:39:01,437   cls_loss = 0.0
2022-08-29 19:39:01,437   global_step = 5399
2022-08-29 19:39:01,437   loss = 11.065910641636167
2022-08-29 19:39:01,438   rep_loss = 2.5140063038894107
2022-08-29 19:39:01,438 ***** Save model *****
2022-08-29 19:39:26,108 ***** Running evaluation *****
2022-08-29 19:39:26,109   Epoch = 7 iter 5399 step
2022-08-29 19:39:26,109   Num examples = 1500
2022-08-29 19:39:26,109   Batch size = 32
2022-08-29 19:39:26,110 ***** Eval results *****
2022-08-29 19:39:26,110   att_loss = 2.748383026979564
2022-08-29 19:39:26,110   cls_loss = 0.0
2022-08-29 19:39:26,111   global_step = 5399
2022-08-29 19:39:26,111   loss = 4.494969653699737
2022-08-29 19:39:26,111   rep_loss = 1.746586626400577
2022-08-29 19:39:26,111 ***** Save model *****
2022-08-29 19:39:30,491 ***** Running evaluation *****
2022-08-29 19:39:30,492   Epoch = 4 iter 9499 step
2022-08-29 19:39:30,492   Num examples = 872
2022-08-29 19:39:30,492   Batch size = 32
2022-08-29 19:39:32,705 ***** Eval results *****
2022-08-29 19:39:32,706   acc = 0.9174311926605505
2022-08-29 19:39:32,706   att_loss = 0.0
2022-08-29 19:39:32,706   cls_loss = 0.024506981242086914
2022-08-29 19:39:32,706   eval_loss = 0.25712577040706364
2022-08-29 19:39:32,706   global_step = 9499
2022-08-29 19:39:32,706   loss = 0.024506981242086914
2022-08-29 19:39:32,706   rep_loss = 0.0
2022-08-29 19:39:36,293 ***** Running evaluation *****
2022-08-29 19:39:36,294   Epoch = 19 iter 20599 step
2022-08-29 19:39:36,294   Num examples = 1043
2022-08-29 19:39:36,294   Batch size = 32
2022-08-29 19:39:36,295 ***** Eval results *****
2022-08-29 19:39:36,295   att_loss = 0.814348119477884
2022-08-29 19:39:36,295   cls_loss = 0.0
2022-08-29 19:39:36,295   global_step = 20599
2022-08-29 19:39:36,295   loss = 2.3982771996178145
2022-08-29 19:39:36,295   rep_loss = 1.5839290844112732
2022-08-29 19:39:36,295 ***** Save model *****
2022-08-29 19:39:39,654 ***** Running evaluation *****
2022-08-29 19:39:39,655   Epoch = 18 iter 5599 step
2022-08-29 19:39:39,655   Num examples = 277
2022-08-29 19:39:39,655   Batch size = 32
2022-08-29 19:39:39,656 ***** Eval results *****
2022-08-29 19:39:39,656   att_loss = 6.681612014770508
2022-08-29 19:39:39,656   cls_loss = 0.0
2022-08-29 19:39:39,656   global_step = 5599
2022-08-29 19:39:39,657   loss = 8.985763549804688
2022-08-29 19:39:39,657   rep_loss = 2.3041510581970215
2022-08-29 19:39:39,657 ***** Save model *****
2022-08-29 19:40:01,808 ***** Running evaluation *****
2022-08-29 19:40:01,808   Epoch = 2 iter 31599 step
2022-08-29 19:40:01,808   Num examples = 9815
2022-08-29 19:40:01,808   Batch size = 32
2022-08-29 19:40:01,810 ***** Eval results *****
2022-08-29 19:40:01,810   att_loss = 4.181132703187599
2022-08-29 19:40:01,810   cls_loss = 0.0
2022-08-29 19:40:01,810   global_step = 31599
2022-08-29 19:40:01,810   loss = 6.020961997021489
2022-08-29 19:40:01,810   rep_loss = 1.8398292928710274
2022-08-29 19:40:01,810 ***** Save model *****
2022-08-29 19:40:05,458 ***** Running evaluation *****
2022-08-29 19:40:05,458   Epoch = 7 iter 5599 step
2022-08-29 19:40:05,458   Num examples = 1500
2022-08-29 19:40:05,458   Batch size = 32
2022-08-29 19:40:05,460 ***** Eval results *****
2022-08-29 19:40:05,460   att_loss = 2.7482853010686905
2022-08-29 19:40:05,460   cls_loss = 0.0
2022-08-29 19:40:05,460   global_step = 5599
2022-08-29 19:40:05,460   loss = 4.492597007834682
2022-08-29 19:40:05,460   rep_loss = 1.7443117017729328
2022-08-29 19:40:05,460 ***** Save model *****
2022-08-29 19:40:13,983 ***** Running evaluation *****
2022-08-29 19:40:13,983   Epoch = 4 iter 9599 step
2022-08-29 19:40:13,983   Num examples = 872
2022-08-29 19:40:13,983   Batch size = 32
2022-08-29 19:40:14,520 ***** Running evaluation *****
2022-08-29 19:40:14,520   Epoch = 19 iter 20799 step
2022-08-29 19:40:14,520   Num examples = 1043
2022-08-29 19:40:14,520   Batch size = 32
2022-08-29 19:40:14,522 ***** Eval results *****
2022-08-29 19:40:14,522   att_loss = 0.8171376948055666
2022-08-29 19:40:14,522   cls_loss = 0.0
2022-08-29 19:40:14,522   global_step = 20799
2022-08-29 19:40:14,522   loss = 2.3999049021647525
2022-08-29 19:40:14,522   rep_loss = 1.5827672119441587
2022-08-29 19:40:14,522 ***** Save model *****
2022-08-29 19:40:16,213 ***** Eval results *****
2022-08-29 19:40:16,214   acc = 0.9174311926605505
2022-08-29 19:40:16,214   att_loss = 0.0
2022-08-29 19:40:16,214   cls_loss = 0.024754696444939706
2022-08-29 19:40:16,214   eval_loss = 0.24592906582568372
2022-08-29 19:40:16,214   global_step = 9599
2022-08-29 19:40:16,214   loss = 0.024754696444939706
2022-08-29 19:40:16,214   rep_loss = 0.0
2022-08-29 19:40:17,880 ***** Running evaluation *****
2022-08-29 19:40:17,880   Epoch = 18 iter 5799 step
2022-08-29 19:40:17,880   Num examples = 277
2022-08-29 19:40:17,880   Batch size = 32
2022-08-29 19:40:17,881 ***** Eval results *****
2022-08-29 19:40:17,881   att_loss = 8.695570651571549
2022-08-29 19:40:17,881   cls_loss = 0.0
2022-08-29 19:40:17,881   global_step = 5799
2022-08-29 19:40:17,881   loss = 11.193620162223702
2022-08-29 19:40:17,881   rep_loss = 2.4980495047213425
2022-08-29 19:40:17,882 ***** Save model *****
2022-08-29 19:40:45,399 ***** Running evaluation *****
2022-08-29 19:40:45,399   Epoch = 8 iter 5799 step
2022-08-29 19:40:45,399   Num examples = 1500
2022-08-29 19:40:45,399   Batch size = 32
2022-08-29 19:40:45,401 ***** Eval results *****
2022-08-29 19:40:45,401   att_loss = 2.7820201657035133
2022-08-29 19:40:45,401   cls_loss = 0.0
2022-08-29 19:40:45,401   global_step = 5799
2022-08-29 19:40:45,401   loss = 4.518767222491178
2022-08-29 19:40:45,401   rep_loss = 1.7367470372806897
2022-08-29 19:40:45,401 ***** Save model *****
2022-08-29 19:40:54,647 ***** Running evaluation *****
2022-08-29 19:40:54,647   Epoch = 19 iter 20999 step
2022-08-29 19:40:54,647   Num examples = 1043
2022-08-29 19:40:54,647   Batch size = 32
2022-08-29 19:40:54,649 ***** Eval results *****
2022-08-29 19:40:54,649   att_loss = 0.8142465904376126
2022-08-29 19:40:54,649   cls_loss = 0.0
2022-08-29 19:40:54,649   global_step = 20999
2022-08-29 19:40:54,649   loss = 2.3957304841558194
2022-08-29 19:40:54,649   rep_loss = 1.5814838954043355
2022-08-29 19:40:54,649 ***** Save model *****
2022-08-29 19:40:57,715 ***** Running evaluation *****
2022-08-29 19:40:57,716   Epoch = 4 iter 9699 step
2022-08-29 19:40:57,716   Num examples = 872
2022-08-29 19:40:57,716   Batch size = 32
2022-08-29 19:40:59,945 ***** Eval results *****
2022-08-29 19:40:59,946   acc = 0.9197247706422018
2022-08-29 19:40:59,946   att_loss = 0.0
2022-08-29 19:40:59,946   cls_loss = 0.02495481784978848
2022-08-29 19:40:59,946   eval_loss = 0.2536481473860996
2022-08-29 19:40:59,946   global_step = 9699
2022-08-29 19:40:59,946   loss = 0.02495481784978848
2022-08-29 19:40:59,946   rep_loss = 0.0
2022-08-29 19:41:00,069 ***** Running evaluation *****
2022-08-29 19:41:00,070   Epoch = 19 iter 5999 step
2022-08-29 19:41:00,070   Num examples = 277
2022-08-29 19:41:00,070   Batch size = 32
2022-08-29 19:41:00,071 ***** Eval results *****
2022-08-29 19:41:00,071   att_loss = 8.4853827741411
2022-08-29 19:41:00,071   cls_loss = 0.0
2022-08-29 19:41:00,071   global_step = 5999
2022-08-29 19:41:00,071   loss = 10.948617675569322
2022-08-29 19:41:00,072   rep_loss = 2.4632349120246038
2022-08-29 19:41:00,072 ***** Save model *****
2022-08-29 19:41:13,724 ***** Running evaluation *****
2022-08-29 19:41:13,724   Epoch = 2 iter 31799 step
2022-08-29 19:41:13,724   Num examples = 9815
2022-08-29 19:41:13,724   Batch size = 32
2022-08-29 19:41:13,725 ***** Eval results *****
2022-08-29 19:41:13,725   att_loss = 4.179804858876207
2022-08-29 19:41:13,725   cls_loss = 0.0
2022-08-29 19:41:13,725   global_step = 31799
2022-08-29 19:41:13,725   loss = 6.019239763854143
2022-08-29 19:41:13,726   rep_loss = 1.839434903992328
2022-08-29 19:41:13,726 ***** Save model *****
2022-08-29 19:41:24,287 ***** Running evaluation *****
2022-08-29 19:41:24,287   Epoch = 8 iter 5999 step
2022-08-29 19:41:24,287   Num examples = 1500
2022-08-29 19:41:24,287   Batch size = 32
2022-08-29 19:41:24,289 ***** Eval results *****
2022-08-29 19:41:24,289   att_loss = 2.663929139399061
2022-08-29 19:41:24,289   cls_loss = 0.0
2022-08-29 19:41:24,289   global_step = 5999
2022-08-29 19:41:24,289   loss = 4.37919121910544
2022-08-29 19:41:24,289   rep_loss = 1.7152620750315049
2022-08-29 19:41:24,289 ***** Save model *****
2022-08-29 19:41:35,522 ***** Running evaluation *****
2022-08-29 19:41:35,523   Epoch = 19 iter 21199 step
2022-08-29 19:41:35,523   Num examples = 1043
2022-08-29 19:41:35,523   Batch size = 32
2022-08-29 19:41:35,524 ***** Eval results *****
2022-08-29 19:41:35,524   att_loss = 0.8137802142555421
2022-08-29 19:41:35,524   cls_loss = 0.0
2022-08-29 19:41:35,525   global_step = 21199
2022-08-29 19:41:35,525   loss = 2.396144325047107
2022-08-29 19:41:35,525   rep_loss = 1.5823641123030388
2022-08-29 19:41:35,525 ***** Save model *****
2022-08-29 19:41:37,904 ***** Running evaluation *****
2022-08-29 19:41:37,905   Epoch = 19 iter 6199 step
2022-08-29 19:41:37,905   Num examples = 277
2022-08-29 19:41:37,905   Batch size = 32
2022-08-29 19:41:37,906 ***** Eval results *****
2022-08-29 19:41:37,906   att_loss = 8.45636512986545
2022-08-29 19:41:37,906   cls_loss = 0.0
2022-08-29 19:41:37,906   global_step = 6199
2022-08-29 19:41:37,906   loss = 10.914760512319104
2022-08-29 19:41:37,906   rep_loss = 2.458395379165123
2022-08-29 19:41:37,906 ***** Save model *****
2022-08-29 19:41:41,175 ***** Running evaluation *****
2022-08-29 19:41:41,176   Epoch = 4 iter 9799 step
2022-08-29 19:41:41,176   Num examples = 872
2022-08-29 19:41:41,176   Batch size = 32
2022-08-29 19:41:43,383 ***** Eval results *****
2022-08-29 19:41:43,383   acc = 0.9231651376146789
2022-08-29 19:41:43,383   att_loss = 0.0
2022-08-29 19:41:43,383   cls_loss = 0.02478452914630804
2022-08-29 19:41:43,383   eval_loss = 0.2648352029999452
2022-08-29 19:41:43,383   global_step = 9799
2022-08-29 19:41:43,383   loss = 0.02478452914630804
2022-08-29 19:41:43,383   rep_loss = 0.0
2022-08-29 19:42:03,334 ***** Running evaluation *****
2022-08-29 19:42:03,335   Epoch = 8 iter 6199 step
2022-08-29 19:42:03,335   Num examples = 1500
2022-08-29 19:42:03,335   Batch size = 32
2022-08-29 19:42:03,336 ***** Eval results *****
2022-08-29 19:42:03,336   att_loss = 2.6954908559610558
2022-08-29 19:42:03,336   cls_loss = 0.0
2022-08-29 19:42:03,337   global_step = 6199
2022-08-29 19:42:03,337   loss = 4.409359115558666
2022-08-29 19:42:03,337   rep_loss = 1.7138682525236528
2022-08-29 19:42:03,337 ***** Save model *****
2022-08-29 19:42:14,303 ***** Running evaluation *****
2022-08-29 19:42:14,304   Epoch = 20 iter 21399 step
2022-08-29 19:42:14,304   Num examples = 1043
2022-08-29 19:42:14,304   Batch size = 32
2022-08-29 19:42:14,305 ***** Eval results *****
2022-08-29 19:42:14,305   att_loss = 0.8050087675070151
2022-08-29 19:42:14,305   cls_loss = 0.0
2022-08-29 19:42:14,306   global_step = 21399
2022-08-29 19:42:14,306   loss = 2.3871146471072464
2022-08-29 19:42:14,306   rep_loss = 1.582105884185204
2022-08-29 19:42:14,306 ***** Save model *****
2022-08-29 19:42:18,056 ***** Running evaluation *****
2022-08-29 19:42:18,056   Epoch = 20 iter 6399 step
2022-08-29 19:42:18,056   Num examples = 277
2022-08-29 19:42:18,056   Batch size = 32
2022-08-29 19:42:18,057 ***** Eval results *****
2022-08-29 19:42:18,058   att_loss = 8.312535408488865
2022-08-29 19:42:18,058   cls_loss = 0.0
2022-08-29 19:42:18,058   global_step = 6399
2022-08-29 19:42:18,058   loss = 10.752386825710701
2022-08-29 19:42:18,058   rep_loss = 2.439851418553784
2022-08-29 19:42:18,058 ***** Save model *****
2022-08-29 19:42:24,504 ***** Running evaluation *****
2022-08-29 19:42:24,505   Epoch = 4 iter 9899 step
2022-08-29 19:42:24,505   Num examples = 872
2022-08-29 19:42:24,505   Batch size = 32
2022-08-29 19:42:26,710 ***** Eval results *****
2022-08-29 19:42:26,710   acc = 0.9197247706422018
2022-08-29 19:42:26,711   att_loss = 0.0
2022-08-29 19:42:26,711   cls_loss = 0.02483226465022371
2022-08-29 19:42:26,711   eval_loss = 0.2581038838917656
2022-08-29 19:42:26,711   global_step = 9899
2022-08-29 19:42:26,711   loss = 0.02483226465022371
2022-08-29 19:42:26,711   rep_loss = 0.0
2022-08-29 19:42:27,077 ***** Running evaluation *****
2022-08-29 19:42:27,078   Epoch = 2 iter 31999 step
2022-08-29 19:42:27,078   Num examples = 9815
2022-08-29 19:42:27,078   Batch size = 32
2022-08-29 19:42:27,079 ***** Eval results *****
2022-08-29 19:42:27,079   att_loss = 4.179042545003739
2022-08-29 19:42:27,079   cls_loss = 0.0
2022-08-29 19:42:27,079   global_step = 31999
2022-08-29 19:42:27,079   loss = 6.018129249448445
2022-08-29 19:42:27,079   rep_loss = 1.8390867033736291
2022-08-29 19:42:27,079 ***** Save model *****
2022-08-29 19:42:46,858 ***** Running evaluation *****
2022-08-29 19:42:46,859   Epoch = 8 iter 6399 step
2022-08-29 19:42:46,859   Num examples = 1500
2022-08-29 19:42:46,859   Batch size = 32
2022-08-29 19:42:46,860 ***** Eval results *****
2022-08-29 19:42:46,860   att_loss = 2.6741111023735455
2022-08-29 19:42:46,860   cls_loss = 0.0
2022-08-29 19:42:46,860   global_step = 6399
2022-08-29 19:42:46,860   loss = 4.380996067833355
2022-08-29 19:42:46,860   rep_loss = 1.706884959089847
2022-08-29 19:42:46,860 ***** Save model *****
2022-08-29 19:42:53,367 ***** Running evaluation *****
2022-08-29 19:42:53,368   Epoch = 20 iter 21599 step
2022-08-29 19:42:53,368   Num examples = 1043
2022-08-29 19:42:53,368   Batch size = 32
2022-08-29 19:42:53,369 ***** Eval results *****
2022-08-29 19:42:53,369   att_loss = 0.8015760561162957
2022-08-29 19:42:53,369   cls_loss = 0.0
2022-08-29 19:42:53,369   global_step = 21599
2022-08-29 19:42:53,369   loss = 2.3747280277467673
2022-08-29 19:42:53,369   rep_loss = 1.5731519759948283
2022-08-29 19:42:53,369 ***** Save model *****
2022-08-29 19:42:58,419 ***** Running evaluation *****
2022-08-29 19:42:58,420   Epoch = 21 iter 6599 step
2022-08-29 19:42:58,420   Num examples = 277
2022-08-29 19:42:58,420   Batch size = 32
2022-08-29 19:42:58,421 ***** Eval results *****
2022-08-29 19:42:58,421   att_loss = 8.405733094495886
2022-08-29 19:42:58,421   cls_loss = 0.0
2022-08-29 19:42:58,421   global_step = 6599
2022-08-29 19:42:58,422   loss = 10.838690322988173
2022-08-29 19:42:58,422   rep_loss = 2.4329572740723107
2022-08-29 19:42:58,422 ***** Save model *****
2022-08-29 19:43:07,810 ***** Running evaluation *****
2022-08-29 19:43:07,811   Epoch = 4 iter 9999 step
2022-08-29 19:43:07,811   Num examples = 872
2022-08-29 19:43:07,811   Batch size = 32
2022-08-29 19:43:10,020 ***** Eval results *****
2022-08-29 19:43:10,020   acc = 0.9185779816513762
2022-08-29 19:43:10,020   att_loss = 0.0
2022-08-29 19:43:10,020   cls_loss = 0.024972813656648937
2022-08-29 19:43:10,020   eval_loss = 0.24877696071884461
2022-08-29 19:43:10,020   global_step = 9999
2022-08-29 19:43:10,021   loss = 0.024972813656648937
2022-08-29 19:43:10,021   rep_loss = 0.0
2022-08-29 19:43:25,942 ***** Running evaluation *****
2022-08-29 19:43:25,942   Epoch = 9 iter 6599 step
2022-08-29 19:43:25,942   Num examples = 1500
2022-08-29 19:43:25,943   Batch size = 32
2022-08-29 19:43:25,944 ***** Eval results *****
2022-08-29 19:43:25,944   att_loss = 2.61129902749166
2022-08-29 19:43:25,944   cls_loss = 0.0
2022-08-29 19:43:25,944   global_step = 6599
2022-08-29 19:43:25,944   loss = 4.300088795432209
2022-08-29 19:43:25,944   rep_loss = 1.6887897540182963
2022-08-29 19:43:25,944 ***** Save model *****
2022-08-29 19:43:32,328 ***** Running evaluation *****
2022-08-29 19:43:32,329   Epoch = 20 iter 21799 step
2022-08-29 19:43:32,329   Num examples = 1043
2022-08-29 19:43:32,329   Batch size = 32
2022-08-29 19:43:32,330 ***** Eval results *****
2022-08-29 19:43:32,330   att_loss = 0.8061884115524336
2022-08-29 19:43:32,330   cls_loss = 0.0
2022-08-29 19:43:32,330   global_step = 21799
2022-08-29 19:43:32,330   loss = 2.3810850528486767
2022-08-29 19:43:32,330   rep_loss = 1.5748966391917514
2022-08-29 19:43:32,330 ***** Save model *****
2022-08-29 19:43:37,545 ***** Running evaluation *****
2022-08-29 19:43:37,546   Epoch = 21 iter 6799 step
2022-08-29 19:43:37,546   Num examples = 277
2022-08-29 19:43:37,546   Batch size = 32
2022-08-29 19:43:37,547 ***** Eval results *****
2022-08-29 19:43:37,547   att_loss = 8.44775928846046
2022-08-29 19:43:37,547   cls_loss = 0.0
2022-08-29 19:43:37,547   global_step = 6799
2022-08-29 19:43:37,547   loss = 10.873650557959257
2022-08-29 19:43:37,547   rep_loss = 2.4258912872912277
2022-08-29 19:43:37,547 ***** Save model *****
2022-08-29 19:43:39,016 ***** Running evaluation *****
2022-08-29 19:43:39,016   Epoch = 2 iter 32199 step
2022-08-29 19:43:39,016   Num examples = 9815
2022-08-29 19:43:39,017   Batch size = 32
2022-08-29 19:43:39,018 ***** Eval results *****
2022-08-29 19:43:39,018   att_loss = 4.177182675622315
2022-08-29 19:43:39,018   cls_loss = 0.0
2022-08-29 19:43:39,018   global_step = 32199
2022-08-29 19:43:39,018   loss = 6.015832406789185
2022-08-29 19:43:39,018   rep_loss = 1.8386497295477284
2022-08-29 19:43:39,018 ***** Save model *****
2022-08-29 19:43:51,110 ***** Running evaluation *****
2022-08-29 19:43:51,110   Epoch = 4 iter 10099 step
2022-08-29 19:43:51,110   Num examples = 872
2022-08-29 19:43:51,110   Batch size = 32
2022-08-29 19:43:53,318 ***** Eval results *****
2022-08-29 19:43:53,319   acc = 0.9220183486238532
2022-08-29 19:43:53,319   att_loss = 0.0
2022-08-29 19:43:53,319   cls_loss = 0.024902946954572035
2022-08-29 19:43:53,319   eval_loss = 0.2584214490572257
2022-08-29 19:43:53,319   global_step = 10099
2022-08-29 19:43:53,319   loss = 0.024902946954572035
2022-08-29 19:43:53,319   rep_loss = 0.0
2022-08-29 19:44:05,379 ***** Running evaluation *****
2022-08-29 19:44:05,380   Epoch = 9 iter 6799 step
2022-08-29 19:44:05,380   Num examples = 1500
2022-08-29 19:44:05,380   Batch size = 32
2022-08-29 19:44:05,381 ***** Eval results *****
2022-08-29 19:44:05,381   att_loss = 2.5823572240880406
2022-08-29 19:44:05,381   cls_loss = 0.0
2022-08-29 19:44:05,381   global_step = 6799
2022-08-29 19:44:05,381   loss = 4.260987433198651
2022-08-29 19:44:05,381   rep_loss = 1.6786302009746654
2022-08-29 19:44:05,382 ***** Save model *****
2022-08-29 19:44:16,853 ***** Running evaluation *****
2022-08-29 19:44:16,854   Epoch = 20 iter 21999 step
2022-08-29 19:44:16,854   Num examples = 1043
2022-08-29 19:44:16,854   Batch size = 32
2022-08-29 19:44:16,855 ***** Eval results *****
2022-08-29 19:44:16,855   att_loss = 0.8035222453783749
2022-08-29 19:44:16,855   cls_loss = 0.0
2022-08-29 19:44:16,855   global_step = 21999
2022-08-29 19:44:16,855   loss = 2.3754446163237186
2022-08-29 19:44:16,856   rep_loss = 1.5719223684734769
2022-08-29 19:44:16,856 ***** Save model *****
2022-08-29 19:44:25,537 ***** Running evaluation *****
2022-08-29 19:44:25,538   Epoch = 22 iter 6999 step
2022-08-29 19:44:25,538   Num examples = 277
2022-08-29 19:44:25,538   Batch size = 32
2022-08-29 19:44:25,539 ***** Eval results *****
2022-08-29 19:44:25,539   att_loss = 8.373724153846693
2022-08-29 19:44:25,539   cls_loss = 0.0
2022-08-29 19:44:25,539   global_step = 6999
2022-08-29 19:44:25,539   loss = 10.780873863560378
2022-08-29 19:44:25,539   rep_loss = 2.407149686934842
2022-08-29 19:44:25,539 ***** Save model *****
2022-08-29 19:44:34,408 ***** Running evaluation *****
2022-08-29 19:44:34,408   Epoch = 4 iter 10199 step
2022-08-29 19:44:34,408   Num examples = 872
2022-08-29 19:44:34,409   Batch size = 32
2022-08-29 19:44:36,618 ***** Eval results *****
2022-08-29 19:44:36,618   acc = 0.9197247706422018
2022-08-29 19:44:36,618   att_loss = 0.0
2022-08-29 19:44:36,618   cls_loss = 0.024841930918589827
2022-08-29 19:44:36,618   eval_loss = 0.26104486610607375
2022-08-29 19:44:36,618   global_step = 10199
2022-08-29 19:44:36,618   loss = 0.024841930918589827
2022-08-29 19:44:36,618   rep_loss = 0.0
2022-08-29 19:44:44,440 ***** Running evaluation *****
2022-08-29 19:44:44,441   Epoch = 9 iter 6999 step
2022-08-29 19:44:44,441   Num examples = 1500
2022-08-29 19:44:44,441   Batch size = 32
2022-08-29 19:44:44,442 ***** Eval results *****
2022-08-29 19:44:44,442   att_loss = 2.6376118124974286
2022-08-29 19:44:44,442   cls_loss = 0.0
2022-08-29 19:44:44,442   global_step = 6999
2022-08-29 19:44:44,442   loss = 4.321485010605285
2022-08-29 19:44:44,442   rep_loss = 1.6838731958879438
2022-08-29 19:44:44,442 ***** Save model *****
2022-08-29 19:44:55,677 ***** Running evaluation *****
2022-08-29 19:44:55,678   Epoch = 20 iter 22199 step
2022-08-29 19:44:55,678   Num examples = 1043
2022-08-29 19:44:55,678   Batch size = 32
2022-08-29 19:44:55,679 ***** Eval results *****
2022-08-29 19:44:55,679   att_loss = 0.8102224733818132
2022-08-29 19:44:55,679   cls_loss = 0.0
2022-08-29 19:44:55,679   global_step = 22199
2022-08-29 19:44:55,679   loss = 2.383432664803015
2022-08-29 19:44:55,679   rep_loss = 1.5732101879756413
2022-08-29 19:44:55,679 ***** Save model *****
2022-08-29 19:44:55,753 ***** Running evaluation *****
2022-08-29 19:44:55,754   Epoch = 2 iter 32399 step
2022-08-29 19:44:55,754   Num examples = 9815
2022-08-29 19:44:55,754   Batch size = 32
2022-08-29 19:44:55,755 ***** Eval results *****
2022-08-29 19:44:55,755   att_loss = 4.176267711719029
2022-08-29 19:44:55,756   cls_loss = 0.0
2022-08-29 19:44:55,756   global_step = 32399
2022-08-29 19:44:55,756   loss = 6.014521771356079
2022-08-29 19:44:55,756   rep_loss = 1.8382540582411917
2022-08-29 19:44:55,756 ***** Save model *****
2022-08-29 19:45:04,619 ***** Running evaluation *****
2022-08-29 19:45:04,620   Epoch = 23 iter 7199 step
2022-08-29 19:45:04,620   Num examples = 277
2022-08-29 19:45:04,620   Batch size = 32
2022-08-29 19:45:04,621 ***** Eval results *****
2022-08-29 19:45:04,621   att_loss = 8.06477631693301
2022-08-29 19:45:04,621   cls_loss = 0.0
2022-08-29 19:45:04,621   global_step = 7199
2022-08-29 19:45:04,621   loss = 10.449332817741062
2022-08-29 19:45:04,621   rep_loss = 2.3845564904420273
2022-08-29 19:45:04,622 ***** Save model *****
2022-08-29 19:45:17,701 ***** Running evaluation *****
2022-08-29 19:45:17,702   Epoch = 4 iter 10299 step
2022-08-29 19:45:17,702   Num examples = 872
2022-08-29 19:45:17,702   Batch size = 32
2022-08-29 19:45:19,908 ***** Eval results *****
2022-08-29 19:45:19,909   acc = 0.9197247706422018
2022-08-29 19:45:19,909   att_loss = 0.0
2022-08-29 19:45:19,909   cls_loss = 0.024781631834535953
2022-08-29 19:45:19,909   eval_loss = 0.2579878196120262
2022-08-29 19:45:19,909   global_step = 10299
2022-08-29 19:45:19,909   loss = 0.024781631834535953
2022-08-29 19:45:19,909   rep_loss = 0.0
2022-08-29 19:45:23,463 ***** Running evaluation *****
2022-08-29 19:45:23,463   Epoch = 10 iter 7199 step
2022-08-29 19:45:23,463   Num examples = 1500
2022-08-29 19:45:23,463   Batch size = 32
2022-08-29 19:45:23,464 ***** Eval results *****
2022-08-29 19:45:23,464   att_loss = 2.5005768349296167
2022-08-29 19:45:23,464   cls_loss = 0.0
2022-08-29 19:45:23,465   global_step = 7199
2022-08-29 19:45:23,465   loss = 4.148894096675672
2022-08-29 19:45:23,465   rep_loss = 1.6483172805685746
2022-08-29 19:45:23,465 ***** Save model *****
2022-08-29 19:45:37,782 ***** Running evaluation *****
2022-08-29 19:45:37,782   Epoch = 20 iter 22399 step
2022-08-29 19:45:37,783   Num examples = 1043
2022-08-29 19:45:37,783   Batch size = 32
2022-08-29 19:45:37,784 ***** Eval results *****
2022-08-29 19:45:37,784   att_loss = 0.811770373089481
2022-08-29 19:45:37,784   cls_loss = 0.0
2022-08-29 19:45:37,784   global_step = 22399
2022-08-29 19:45:37,784   loss = 2.3850861201502016
2022-08-29 19:45:37,784   rep_loss = 1.573315741467407
2022-08-29 19:45:37,784 ***** Save model *****
2022-08-29 19:45:43,749 ***** Running evaluation *****
2022-08-29 19:45:43,750   Epoch = 23 iter 7399 step
2022-08-29 19:45:43,750   Num examples = 277
2022-08-29 19:45:43,750   Batch size = 32
2022-08-29 19:45:43,751 ***** Eval results *****
2022-08-29 19:45:43,751   att_loss = 8.28609065311711
2022-08-29 19:45:43,751   cls_loss = 0.0
2022-08-29 19:45:43,751   global_step = 7399
2022-08-29 19:45:43,751   loss = 10.678703585291297
2022-08-29 19:45:43,751   rep_loss = 2.3926129554345357
2022-08-29 19:45:43,751 ***** Save model *****
2022-08-29 19:46:00,988 ***** Running evaluation *****
2022-08-29 19:46:00,988   Epoch = 4 iter 10399 step
2022-08-29 19:46:00,988   Num examples = 872
2022-08-29 19:46:00,988   Batch size = 32
2022-08-29 19:46:03,203 ***** Eval results *****
2022-08-29 19:46:03,203   acc = 0.9208715596330275
2022-08-29 19:46:03,203   att_loss = 0.0
2022-08-29 19:46:03,203   cls_loss = 0.024686831134158708
2022-08-29 19:46:03,203   eval_loss = 0.2569735835838531
2022-08-29 19:46:03,203   global_step = 10399
2022-08-29 19:46:03,203   loss = 0.024686831134158708
2022-08-29 19:46:03,204   rep_loss = 0.0
2022-08-29 19:46:06,625 ***** Running evaluation *****
2022-08-29 19:46:06,625   Epoch = 10 iter 7399 step
2022-08-29 19:46:06,625   Num examples = 1500
2022-08-29 19:46:06,625   Batch size = 32
2022-08-29 19:46:06,627 ***** Eval results *****
2022-08-29 19:46:06,627   att_loss = 2.5978011018065015
2022-08-29 19:46:06,627   cls_loss = 0.0
2022-08-29 19:46:06,627   global_step = 7399
2022-08-29 19:46:06,627   loss = 4.2585634303419555
2022-08-29 19:46:06,627   rep_loss = 1.6607623410551515
2022-08-29 19:46:06,627 ***** Save model *****
2022-08-29 19:46:07,771 ***** Running evaluation *****
2022-08-29 19:46:07,772   Epoch = 2 iter 32599 step
2022-08-29 19:46:07,772   Num examples = 9815
2022-08-29 19:46:07,772   Batch size = 32
2022-08-29 19:46:07,773 ***** Eval results *****
2022-08-29 19:46:07,774   att_loss = 4.175021777218708
2022-08-29 19:46:07,774   cls_loss = 0.0
2022-08-29 19:46:07,774   global_step = 32599
2022-08-29 19:46:07,774   loss = 6.0128746533402735
2022-08-29 19:46:07,774   rep_loss = 1.837852874775153
2022-08-29 19:46:07,774 ***** Save model *****
2022-08-29 19:46:16,566 ***** Running evaluation *****
2022-08-29 19:46:16,567   Epoch = 21 iter 22599 step
2022-08-29 19:46:16,567   Num examples = 1043
2022-08-29 19:46:16,567   Batch size = 32
2022-08-29 19:46:16,568 ***** Eval results *****
2022-08-29 19:46:16,568   att_loss = 0.8142135781154298
2022-08-29 19:46:16,568   cls_loss = 0.0
2022-08-29 19:46:16,568   global_step = 22599
2022-08-29 19:46:16,568   loss = 2.3844335720553036
2022-08-29 19:46:16,568   rep_loss = 1.5702199838314836
2022-08-29 19:46:16,568 ***** Save model *****
2022-08-29 19:46:22,888 ***** Running evaluation *****
2022-08-29 19:46:22,889   Epoch = 24 iter 7599 step
2022-08-29 19:46:22,889   Num examples = 277
2022-08-29 19:46:22,889   Batch size = 32
2022-08-29 19:46:22,890 ***** Eval results *****
2022-08-29 19:46:22,890   att_loss = 8.198002882356997
2022-08-29 19:46:22,890   cls_loss = 0.0
2022-08-29 19:46:22,890   global_step = 7599
2022-08-29 19:46:22,890   loss = 10.567581042536982
2022-08-29 19:46:22,891   rep_loss = 2.3695782131618923
2022-08-29 19:46:22,891 ***** Save model *****
2022-08-29 19:46:44,299 ***** Running evaluation *****
2022-08-29 19:46:44,300   Epoch = 4 iter 10499 step
2022-08-29 19:46:44,300   Num examples = 872
2022-08-29 19:46:44,300   Batch size = 32
2022-08-29 19:46:46,509 ***** Eval results *****
2022-08-29 19:46:46,509   acc = 0.9197247706422018
2022-08-29 19:46:46,510   att_loss = 0.0
2022-08-29 19:46:46,510   cls_loss = 0.024717348777973845
2022-08-29 19:46:46,510   eval_loss = 0.2558608389393027
2022-08-29 19:46:46,510   global_step = 10499
2022-08-29 19:46:46,510   loss = 0.024717348777973845
2022-08-29 19:46:46,510   rep_loss = 0.0
2022-08-29 19:46:46,837 ***** Running evaluation *****
2022-08-29 19:46:46,838   Epoch = 10 iter 7599 step
2022-08-29 19:46:46,838   Num examples = 1500
2022-08-29 19:46:46,838   Batch size = 32
2022-08-29 19:46:46,839 ***** Eval results *****
2022-08-29 19:46:46,839   att_loss = 2.581239122195005
2022-08-29 19:46:46,839   cls_loss = 0.0
2022-08-29 19:46:46,839   global_step = 7599
2022-08-29 19:46:46,839   loss = 4.237908336598435
2022-08-29 19:46:46,839   rep_loss = 1.656669220093611
2022-08-29 19:46:46,840 ***** Save model *****
2022-08-29 19:46:55,407 ***** Running evaluation *****
2022-08-29 19:46:55,407   Epoch = 21 iter 22799 step
2022-08-29 19:46:55,407   Num examples = 1043
2022-08-29 19:46:55,408   Batch size = 32
2022-08-29 19:46:55,409 ***** Eval results *****
2022-08-29 19:46:55,409   att_loss = 0.8032899278032812
2022-08-29 19:46:55,409   cls_loss = 0.0
2022-08-29 19:46:55,409   global_step = 22799
2022-08-29 19:46:55,409   loss = 2.366388319637576
2022-08-29 19:46:55,409   rep_loss = 1.5630983840423132
2022-08-29 19:46:55,409 ***** Save model *****
2022-08-29 19:47:00,735 ***** Running evaluation *****
2022-08-29 19:47:00,735   Epoch = 25 iter 7799 step
2022-08-29 19:47:00,735   Num examples = 277
2022-08-29 19:47:00,735   Batch size = 32
2022-08-29 19:47:00,737 ***** Eval results *****
2022-08-29 19:47:00,737   att_loss = 8.311697125434875
2022-08-29 19:47:00,737   cls_loss = 0.0
2022-08-29 19:47:00,737   global_step = 7799
2022-08-29 19:47:00,737   loss = 10.673952301343283
2022-08-29 19:47:00,737   rep_loss = 2.3622552156448364
2022-08-29 19:47:00,737 ***** Save model *****
2022-08-29 19:47:21,025 ***** Running evaluation *****
2022-08-29 19:47:21,025   Epoch = 2 iter 32799 step
2022-08-29 19:47:21,025   Num examples = 9815
2022-08-29 19:47:21,025   Batch size = 32
2022-08-29 19:47:21,027 ***** Eval results *****
2022-08-29 19:47:21,027   att_loss = 4.175084456759907
2022-08-29 19:47:21,027   cls_loss = 0.0
2022-08-29 19:47:21,027   global_step = 32799
2022-08-29 19:47:21,027   loss = 6.012561681730574
2022-08-29 19:47:21,027   rep_loss = 1.8374772235124932
2022-08-29 19:47:21,027 ***** Save model *****
2022-08-29 19:47:25,865 ***** Running evaluation *****
2022-08-29 19:47:25,865   Epoch = 10 iter 7799 step
2022-08-29 19:47:25,865   Num examples = 1500
2022-08-29 19:47:25,865   Batch size = 32
2022-08-29 19:47:25,866 ***** Eval results *****
2022-08-29 19:47:25,866   att_loss = 2.5576589344391336
2022-08-29 19:47:25,866   cls_loss = 0.0
2022-08-29 19:47:25,866   global_step = 7799
2022-08-29 19:47:25,866   loss = 4.208468456068023
2022-08-29 19:47:25,866   rep_loss = 1.6508095291396527
2022-08-29 19:47:25,867 ***** Save model *****
2022-08-29 19:47:34,253 ***** Running evaluation *****
2022-08-29 19:47:34,254   Epoch = 21 iter 22999 step
2022-08-29 19:47:34,254   Num examples = 1043
2022-08-29 19:47:34,254   Batch size = 32
2022-08-29 19:47:34,255 ***** Eval results *****
2022-08-29 19:47:34,256   att_loss = 0.8054347001211865
2022-08-29 19:47:34,256   cls_loss = 0.0
2022-08-29 19:47:34,256   global_step = 22999
2022-08-29 19:47:34,256   loss = 2.370156680848678
2022-08-29 19:47:34,256   rep_loss = 1.5647219759779094
2022-08-29 19:47:34,256 ***** Save model *****
2022-08-29 19:47:42,752 ***** Running evaluation *****
2022-08-29 19:47:42,753   Epoch = 25 iter 7999 step
2022-08-29 19:47:42,753   Num examples = 277
2022-08-29 19:47:42,753   Batch size = 32
2022-08-29 19:47:42,754 ***** Eval results *****
2022-08-29 19:47:42,754   att_loss = 7.990229183009693
2022-08-29 19:47:42,754   cls_loss = 0.0
2022-08-29 19:47:42,754   global_step = 7999
2022-08-29 19:47:42,755   loss = 10.337723844817706
2022-08-29 19:47:42,755   rep_loss = 2.347494670322963
2022-08-29 19:47:42,755 ***** Save model *****
2022-08-29 19:48:05,048 ***** Running evaluation *****
2022-08-29 19:48:05,048   Epoch = 11 iter 7999 step
2022-08-29 19:48:05,048   Num examples = 1500
2022-08-29 19:48:05,048   Batch size = 32
2022-08-29 19:48:05,049 ***** Eval results *****
2022-08-29 19:48:05,049   att_loss = 2.62967856331627
2022-08-29 19:48:05,050   cls_loss = 0.0
2022-08-29 19:48:05,050   global_step = 7999
2022-08-29 19:48:05,050   loss = 4.282309284304628
2022-08-29 19:48:05,050   rep_loss = 1.6526307068248787
2022-08-29 19:48:05,050 ***** Save model *****
2022-08-29 19:48:13,306 ***** Running evaluation *****
2022-08-29 19:48:13,306   Epoch = 21 iter 23199 step
2022-08-29 19:48:13,306   Num examples = 1043
2022-08-29 19:48:13,306   Batch size = 32
2022-08-29 19:48:13,308 ***** Eval results *****
2022-08-29 19:48:13,308   att_loss = 0.8037271704887138
2022-08-29 19:48:13,308   cls_loss = 0.0
2022-08-29 19:48:13,308   global_step = 23199
2022-08-29 19:48:13,308   loss = 2.367271293616635
2022-08-29 19:48:13,308   rep_loss = 1.5635441207700203
2022-08-29 19:48:13,308 ***** Save model *****
2022-08-29 19:48:20,769 ***** Running evaluation *****
2022-08-29 19:48:20,769   Epoch = 26 iter 8199 step
2022-08-29 19:48:20,769   Num examples = 277
2022-08-29 19:48:20,770   Batch size = 32
2022-08-29 19:48:20,771 ***** Eval results *****
2022-08-29 19:48:20,771   att_loss = 8.093869787401857
2022-08-29 19:48:20,771   cls_loss = 0.0
2022-08-29 19:48:20,771   global_step = 8199
2022-08-29 19:48:20,771   loss = 10.433142417300063
2022-08-29 19:48:20,771   rep_loss = 2.3392726214586106
2022-08-29 19:48:20,771 ***** Save model *****
2022-08-29 19:48:30,881 ***** Running evaluation *****
2022-08-29 19:48:30,882   Epoch = 2 iter 32999 step
2022-08-29 19:48:30,882   Num examples = 9815
2022-08-29 19:48:30,882   Batch size = 32
2022-08-29 19:48:30,883 ***** Eval results *****
2022-08-29 19:48:30,883   att_loss = 4.172982111204963
2022-08-29 19:48:30,883   cls_loss = 0.0
2022-08-29 19:48:30,883   global_step = 32999
2022-08-29 19:48:30,883   loss = 6.010047257587929
2022-08-29 19:48:30,883   rep_loss = 1.837065144860605
2022-08-29 19:48:30,884 ***** Save model *****
2022-08-29 19:48:45,143 ***** Running evaluation *****
2022-08-29 19:48:45,144   Epoch = 11 iter 8199 step
2022-08-29 19:48:45,144   Num examples = 1500
2022-08-29 19:48:45,144   Batch size = 32
2022-08-29 19:48:45,145 ***** Eval results *****
2022-08-29 19:48:45,145   att_loss = 2.54698632999116
2022-08-29 19:48:45,145   cls_loss = 0.0
2022-08-29 19:48:45,145   global_step = 8199
2022-08-29 19:48:45,145   loss = 4.181574942661678
2022-08-29 19:48:45,145   rep_loss = 1.6345886019773261
2022-08-29 19:48:45,146 ***** Save model *****
2022-08-29 19:48:52,004 ***** Running evaluation *****
2022-08-29 19:48:52,005   Epoch = 21 iter 23399 step
2022-08-29 19:48:52,005   Num examples = 1043
2022-08-29 19:48:52,005   Batch size = 32
2022-08-29 19:48:52,006 ***** Eval results *****
2022-08-29 19:48:52,006   att_loss = 0.8042295727069297
2022-08-29 19:48:52,006   cls_loss = 0.0
2022-08-29 19:48:52,006   global_step = 23399
2022-08-29 19:48:52,006   loss = 2.3686992751336615
2022-08-29 19:48:52,006   rep_loss = 1.5644697013524975
2022-08-29 19:48:52,006 ***** Save model *****
2022-08-29 19:49:00,470 ***** Running evaluation *****
2022-08-29 19:49:00,471   Epoch = 27 iter 8399 step
2022-08-29 19:49:00,471   Num examples = 277
2022-08-29 19:49:00,471   Batch size = 32
2022-08-29 19:49:00,472 ***** Eval results *****
2022-08-29 19:49:00,472   att_loss = 5.2874380350112915
2022-08-29 19:49:00,472   cls_loss = 0.0
2022-08-29 19:49:00,472   global_step = 8399
2022-08-29 19:49:00,472   loss = 7.4488444328308105
2022-08-29 19:49:00,472   rep_loss = 2.161406397819519
2022-08-29 19:49:00,472 ***** Save model *****
2022-08-29 19:49:29,863 ***** Running evaluation *****
2022-08-29 19:49:29,863   Epoch = 11 iter 8399 step
2022-08-29 19:49:29,864   Num examples = 1500
2022-08-29 19:49:29,864   Batch size = 32
2022-08-29 19:49:29,865 ***** Eval results *****
2022-08-29 19:49:29,865   att_loss = 2.5155569364924633
2022-08-29 19:49:29,865   cls_loss = 0.0
2022-08-29 19:49:29,865   global_step = 8399
2022-08-29 19:49:29,865   loss = 4.145436060404825
2022-08-29 19:49:29,865   rep_loss = 1.6298791212949924
2022-08-29 19:49:29,865 ***** Save model *****
2022-08-29 19:49:34,451 ***** Running evaluation *****
2022-08-29 19:49:34,451   Epoch = 22 iter 23599 step
2022-08-29 19:49:34,451   Num examples = 1043
2022-08-29 19:49:34,451   Batch size = 32
2022-08-29 19:49:34,452 ***** Eval results *****
2022-08-29 19:49:34,452   att_loss = 0.808569298785867
2022-08-29 19:49:34,452   cls_loss = 0.0
2022-08-29 19:49:34,452   global_step = 23599
2022-08-29 19:49:34,452   loss = 2.3687952282359297
2022-08-29 19:49:34,452   rep_loss = 1.5602259392877227
2022-08-29 19:49:34,453 ***** Save model *****
2022-08-29 19:49:40,593 ***** Running evaluation *****
2022-08-29 19:49:40,593   Epoch = 27 iter 8599 step
2022-08-29 19:49:40,593   Num examples = 277
2022-08-29 19:49:40,593   Batch size = 32
2022-08-29 19:49:40,595 ***** Eval results *****
2022-08-29 19:49:40,595   att_loss = 8.064454385549714
2022-08-29 19:49:40,595   cls_loss = 0.0
2022-08-29 19:49:40,595   global_step = 8599
2022-08-29 19:49:40,595   loss = 10.3941493600902
2022-08-29 19:49:40,595   rep_loss = 2.329694960377004
2022-08-29 19:49:40,595 ***** Save model *****
2022-08-29 19:49:47,527 ***** Running evaluation *****
2022-08-29 19:49:47,527   Epoch = 2 iter 33199 step
2022-08-29 19:49:47,528   Num examples = 9815
2022-08-29 19:49:47,528   Batch size = 32
2022-08-29 19:49:47,530 ***** Eval results *****
2022-08-29 19:49:47,530   att_loss = 4.172877287994126
2022-08-29 19:49:47,530   cls_loss = 0.0
2022-08-29 19:49:47,531   global_step = 33199
2022-08-29 19:49:47,531   loss = 6.009610784842839
2022-08-29 19:49:47,531   rep_loss = 1.8367334951549696
2022-08-29 19:49:47,531 ***** Save model *****
2022-08-29 19:50:10,340 ***** Running evaluation *****
2022-08-29 19:50:10,341   Epoch = 11 iter 8599 step
2022-08-29 19:50:10,341   Num examples = 1500
2022-08-29 19:50:10,341   Batch size = 32
2022-08-29 19:50:10,342 ***** Eval results *****
2022-08-29 19:50:10,342   att_loss = 2.5142969214457076
2022-08-29 19:50:10,343   cls_loss = 0.0
2022-08-29 19:50:10,343   global_step = 8599
2022-08-29 19:50:10,343   loss = 4.141846959499081
2022-08-29 19:50:10,343   rep_loss = 1.6275500394138223
2022-08-29 19:50:10,343 ***** Save model *****
2022-08-29 19:50:12,693 ***** Running evaluation *****
2022-08-29 19:50:12,694   Epoch = 22 iter 23799 step
2022-08-29 19:50:12,694   Num examples = 1043
2022-08-29 19:50:12,694   Batch size = 32
2022-08-29 19:50:12,696 ***** Eval results *****
2022-08-29 19:50:12,696   att_loss = 0.799949313941175
2022-08-29 19:50:12,696   cls_loss = 0.0
2022-08-29 19:50:12,696   global_step = 23799
2022-08-29 19:50:12,696   loss = 2.3562360069539285
2022-08-29 19:50:12,696   rep_loss = 1.5562866922258938
2022-08-29 19:50:12,696 ***** Save model *****
2022-08-29 19:50:20,132 ***** Running evaluation *****
2022-08-29 19:50:20,133   Epoch = 28 iter 8799 step
2022-08-29 19:50:20,133   Num examples = 277
2022-08-29 19:50:20,133   Batch size = 32
2022-08-29 19:50:20,134 ***** Eval results *****
2022-08-29 19:50:20,135   att_loss = 7.754270912526728
2022-08-29 19:50:20,135   cls_loss = 0.0
2022-08-29 19:50:20,135   global_step = 8799
2022-08-29 19:50:20,135   loss = 10.05795588860145
2022-08-29 19:50:20,135   rep_loss = 2.3036849970345967
2022-08-29 19:50:20,135 ***** Save model *****
2022-08-29 19:50:50,906 ***** Running evaluation *****
2022-08-29 19:50:50,906   Epoch = 12 iter 8799 step
2022-08-29 19:50:50,906   Num examples = 1500
2022-08-29 19:50:50,906   Batch size = 32
2022-08-29 19:50:50,908 ***** Eval results *****
2022-08-29 19:50:50,908   att_loss = 2.5023864318764275
2022-08-29 19:50:50,908   cls_loss = 0.0
2022-08-29 19:50:50,908   global_step = 8799
2022-08-29 19:50:50,908   loss = 4.124215674530613
2022-08-29 19:50:50,908   rep_loss = 1.621829240048518
2022-08-29 19:50:50,908 ***** Save model *****
2022-08-29 19:50:51,852 ***** Running evaluation *****
2022-08-29 19:50:51,853   Epoch = 22 iter 23999 step
2022-08-29 19:50:51,853   Num examples = 1043
2022-08-29 19:50:51,853   Batch size = 32
2022-08-29 19:50:51,854 ***** Eval results *****
2022-08-29 19:50:51,854   att_loss = 0.799215883312832
2022-08-29 19:50:51,854   cls_loss = 0.0
2022-08-29 19:50:51,854   global_step = 23999
2022-08-29 19:50:51,854   loss = 2.3599990438040397
2022-08-29 19:50:51,855   rep_loss = 1.5607831622686823
2022-08-29 19:50:51,855 ***** Save model *****
2022-08-29 19:50:58,192 ***** Running evaluation *****
2022-08-29 19:50:58,193   Epoch = 28 iter 8999 step
2022-08-29 19:50:58,193   Num examples = 277
2022-08-29 19:50:58,193   Batch size = 32
2022-08-29 19:50:58,194 ***** Eval results *****
2022-08-29 19:50:58,194   att_loss = 7.914637450090389
2022-08-29 19:50:58,194   cls_loss = 0.0
2022-08-29 19:50:58,194   global_step = 8999
2022-08-29 19:50:58,194   loss = 10.227072215981499
2022-08-29 19:50:58,195   rep_loss = 2.31243476998765
2022-08-29 19:50:58,195 ***** Save model *****
2022-08-29 19:51:04,221 ***** Running evaluation *****
2022-08-29 19:51:04,221   Epoch = 2 iter 33399 step
2022-08-29 19:51:04,222   Num examples = 9815
2022-08-29 19:51:04,222   Batch size = 32
2022-08-29 19:51:04,223 ***** Eval results *****
2022-08-29 19:51:04,223   att_loss = 4.172182802692606
2022-08-29 19:51:04,223   cls_loss = 0.0
2022-08-29 19:51:04,223   global_step = 33399
2022-08-29 19:51:04,223   loss = 6.00849162206513
2022-08-29 19:51:04,223   rep_loss = 1.8363088180265914
2022-08-29 19:51:04,223 ***** Save model *****
2022-08-29 19:51:32,409 ***** Running evaluation *****
2022-08-29 19:51:32,410   Epoch = 22 iter 24199 step
2022-08-29 19:51:32,410   Num examples = 1043
2022-08-29 19:51:32,410   Batch size = 32
2022-08-29 19:51:32,411 ***** Eval results *****
2022-08-29 19:51:32,411   att_loss = 0.7977404697012596
2022-08-29 19:51:32,411   cls_loss = 0.0
2022-08-29 19:51:32,411   global_step = 24199
2022-08-29 19:51:32,412   loss = 2.3598246435352612
2022-08-29 19:51:32,412   rep_loss = 1.5620841749362213
2022-08-29 19:51:32,412 ***** Save model *****
2022-08-29 19:51:33,962 ***** Running evaluation *****
2022-08-29 19:51:33,963   Epoch = 12 iter 8999 step
2022-08-29 19:51:33,963   Num examples = 1500
2022-08-29 19:51:33,963   Batch size = 32
2022-08-29 19:51:33,964 ***** Eval results *****
2022-08-29 19:51:33,964   att_loss = 2.4663386257759274
2022-08-29 19:51:33,964   cls_loss = 0.0
2022-08-29 19:51:33,964   global_step = 8999
2022-08-29 19:51:33,964   loss = 4.080384911818542
2022-08-29 19:51:33,964   rep_loss = 1.6140462779500777
2022-08-29 19:51:33,964 ***** Save model *****
2022-08-29 19:51:36,059 ***** Running evaluation *****
2022-08-29 19:51:36,059   Epoch = 29 iter 9199 step
2022-08-29 19:51:36,059   Num examples = 277
2022-08-29 19:51:36,060   Batch size = 32
2022-08-29 19:51:36,061 ***** Eval results *****
2022-08-29 19:51:36,061   att_loss = 7.913531586858961
2022-08-29 19:51:36,061   cls_loss = 0.0
2022-08-29 19:51:36,061   global_step = 9199
2022-08-29 19:51:36,061   loss = 10.217098919550578
2022-08-29 19:51:36,061   rep_loss = 2.3035673439502715
2022-08-29 19:51:36,061 ***** Save model *****
2022-08-29 19:52:10,628 ***** Running evaluation *****
2022-08-29 19:52:10,629   Epoch = 22 iter 24399 step
2022-08-29 19:52:10,629   Num examples = 1043
2022-08-29 19:52:10,629   Batch size = 32
2022-08-29 19:52:10,630 ***** Eval results *****
2022-08-29 19:52:10,630   att_loss = 0.8023054116588099
2022-08-29 19:52:10,630   cls_loss = 0.0
2022-08-29 19:52:10,630   global_step = 24399
2022-08-29 19:52:10,630   loss = 2.36539997584532
2022-08-29 19:52:10,631   rep_loss = 1.563094564318525
2022-08-29 19:52:10,631 ***** Save model *****
2022-08-29 19:52:16,561 ***** Running evaluation *****
2022-08-29 19:52:16,562   Epoch = 30 iter 9399 step
2022-08-29 19:52:16,562   Num examples = 277
2022-08-29 19:52:16,562   Batch size = 32
2022-08-29 19:52:16,563 ***** Eval results *****
2022-08-29 19:52:16,563   att_loss = 7.834330579508906
2022-08-29 19:52:16,563   cls_loss = 0.0
2022-08-29 19:52:16,563   global_step = 9399
2022-08-29 19:52:16,563   loss = 10.123072251029637
2022-08-29 19:52:16,564   rep_loss = 2.2887416887974394
2022-08-29 19:52:16,564 ***** Save model *****
2022-08-29 19:52:16,905 ***** Running evaluation *****
2022-08-29 19:52:16,906   Epoch = 2 iter 33599 step
2022-08-29 19:52:16,906   Num examples = 9815
2022-08-29 19:52:16,906   Batch size = 32
2022-08-29 19:52:16,908 ***** Eval results *****
2022-08-29 19:52:16,908   att_loss = 4.170474261022422
2022-08-29 19:52:16,908   cls_loss = 0.0
2022-08-29 19:52:16,908   global_step = 33599
2022-08-29 19:52:16,908   loss = 6.006359109976466
2022-08-29 19:52:16,908   rep_loss = 1.8358848474009142
2022-08-29 19:52:16,908 ***** Save model *****
2022-08-29 19:52:17,955 ***** Running evaluation *****
2022-08-29 19:52:17,956   Epoch = 12 iter 9199 step
2022-08-29 19:52:17,956   Num examples = 1500
2022-08-29 19:52:17,956   Batch size = 32
2022-08-29 19:52:17,957 ***** Eval results *****
2022-08-29 19:52:17,957   att_loss = 2.465977578179505
2022-08-29 19:52:17,957   cls_loss = 0.0
2022-08-29 19:52:17,957   global_step = 9199
2022-08-29 19:52:17,957   loss = 4.078284320455063
2022-08-29 19:52:17,957   rep_loss = 1.6123067408442293
2022-08-29 19:52:17,958 ***** Save model *****
2022-08-29 19:52:53,884 ***** Running evaluation *****
2022-08-29 19:52:53,885   Epoch = 23 iter 24599 step
2022-08-29 19:52:53,885   Num examples = 1043
2022-08-29 19:52:53,885   Batch size = 32
2022-08-29 19:52:53,886 ***** Eval results *****
2022-08-29 19:52:53,886   att_loss = 0.8192983814648219
2022-08-29 19:52:53,886   cls_loss = 0.0
2022-08-29 19:52:53,886   global_step = 24599
2022-08-29 19:52:53,886   loss = 2.3684671946934293
2022-08-29 19:52:53,886   rep_loss = 1.549168804713658
2022-08-29 19:52:53,886 ***** Save model *****
2022-08-29 19:53:06,402 ***** Running evaluation *****
2022-08-29 19:53:06,402   Epoch = 30 iter 9599 step
2022-08-29 19:53:06,402   Num examples = 277
2022-08-29 19:53:06,402   Batch size = 32
2022-08-29 19:53:06,404 ***** Eval results *****
2022-08-29 19:53:06,404   att_loss = 7.836020700107277
2022-08-29 19:53:06,404   cls_loss = 0.0
2022-08-29 19:53:06,404   global_step = 9599
2022-08-29 19:53:06,404   loss = 10.12421469351616
2022-08-29 19:53:06,404   rep_loss = 2.288194000499399
2022-08-29 19:53:06,404 ***** Save model *****
2022-08-29 19:53:08,068 ***** Running evaluation *****
2022-08-29 19:53:08,069   Epoch = 13 iter 9399 step
2022-08-29 19:53:08,069   Num examples = 1500
2022-08-29 19:53:08,069   Batch size = 32
2022-08-29 19:53:08,070 ***** Eval results *****
2022-08-29 19:53:08,070   att_loss = 2.576968943155729
2022-08-29 19:53:08,071   cls_loss = 0.0
2022-08-29 19:53:08,071   global_step = 9399
2022-08-29 19:53:08,071   loss = 4.196296105018029
2022-08-29 19:53:08,071   rep_loss = 1.6193271471903874
2022-08-29 19:53:08,071 ***** Save model *****
2022-08-29 19:53:32,114 ***** Running evaluation *****
2022-08-29 19:53:32,114   Epoch = 23 iter 24799 step
2022-08-29 19:53:32,114   Num examples = 1043
2022-08-29 19:53:32,115   Batch size = 32
2022-08-29 19:53:32,116 ***** Eval results *****
2022-08-29 19:53:32,116   att_loss = 0.8066699766098184
2022-08-29 19:53:32,116   cls_loss = 0.0
2022-08-29 19:53:32,116   global_step = 24799
2022-08-29 19:53:32,116   loss = 2.3615562165037116
2022-08-29 19:53:32,116   rep_loss = 1.5548862376111618
2022-08-29 19:53:32,116 ***** Save model *****
2022-08-29 19:53:37,437 ***** Running evaluation *****
2022-08-29 19:53:37,438   Epoch = 2 iter 33799 step
2022-08-29 19:53:37,438   Num examples = 9815
2022-08-29 19:53:37,438   Batch size = 32
2022-08-29 19:53:37,439 ***** Eval results *****
2022-08-29 19:53:37,439   att_loss = 4.170791411819656
2022-08-29 19:53:37,440   cls_loss = 0.0
2022-08-29 19:53:37,440   global_step = 33799
2022-08-29 19:53:37,440   loss = 6.006386080042672
2022-08-29 19:53:37,440   rep_loss = 1.835594667076897
2022-08-29 19:53:37,440 ***** Save model *****
2022-08-29 19:53:44,723 ***** Running evaluation *****
2022-08-29 19:53:44,724   Epoch = 31 iter 9799 step
2022-08-29 19:53:44,724   Num examples = 277
2022-08-29 19:53:44,724   Batch size = 32
2022-08-29 19:53:44,725 ***** Eval results *****
2022-08-29 19:53:44,725   att_loss = 7.858757427976101
2022-08-29 19:53:44,725   cls_loss = 0.0
2022-08-29 19:53:44,725   global_step = 9799
2022-08-29 19:53:44,725   loss = 10.141047891182236
2022-08-29 19:53:44,725   rep_loss = 2.282290417936784
2022-08-29 19:53:44,725 ***** Save model *****
2022-08-29 19:53:49,982 ***** Running evaluation *****
2022-08-29 19:53:49,983   Epoch = 13 iter 9599 step
2022-08-29 19:53:49,983   Num examples = 1500
2022-08-29 19:53:49,983   Batch size = 32
2022-08-29 19:53:49,984 ***** Eval results *****
2022-08-29 19:53:49,984   att_loss = 2.4712132764312456
2022-08-29 19:53:49,984   cls_loss = 0.0
2022-08-29 19:53:49,984   global_step = 9599
2022-08-29 19:53:49,984   loss = 4.073294888802295
2022-08-29 19:53:49,985   rep_loss = 1.602081607872585
2022-08-29 19:53:49,985 ***** Save model *****
2022-08-29 19:54:10,392 ***** Running evaluation *****
2022-08-29 19:54:10,392   Epoch = 23 iter 24999 step
2022-08-29 19:54:10,393   Num examples = 1043
2022-08-29 19:54:10,393   Batch size = 32
2022-08-29 19:54:10,394 ***** Eval results *****
2022-08-29 19:54:10,394   att_loss = 0.7906567154259517
2022-08-29 19:54:10,394   cls_loss = 0.0
2022-08-29 19:54:10,394   global_step = 24999
2022-08-29 19:54:10,394   loss = 2.343939989188622
2022-08-29 19:54:10,394   rep_loss = 1.5532832702000936
2022-08-29 19:54:10,394 ***** Save model *****
2022-08-29 19:54:22,876 ***** Running evaluation *****
2022-08-29 19:54:22,877   Epoch = 32 iter 9999 step
2022-08-29 19:54:22,877   Num examples = 277
2022-08-29 19:54:22,877   Batch size = 32
2022-08-29 19:54:22,878 ***** Eval results *****
2022-08-29 19:54:22,878   att_loss = 7.446271744180233
2022-08-29 19:54:22,878   cls_loss = 0.0
2022-08-29 19:54:22,878   global_step = 9999
2022-08-29 19:54:22,878   loss = 9.695659566432871
2022-08-29 19:54:22,878   rep_loss = 2.249387822252639
2022-08-29 19:54:22,878 ***** Save model *****
2022-08-29 19:54:36,204 ***** Running evaluation *****
2022-08-29 19:54:36,205   Epoch = 13 iter 9799 step
2022-08-29 19:54:36,205   Num examples = 1500
2022-08-29 19:54:36,205   Batch size = 32
2022-08-29 19:54:36,206 ***** Eval results *****
2022-08-29 19:54:36,206   att_loss = 2.4418507527279596
2022-08-29 19:54:36,206   cls_loss = 0.0
2022-08-29 19:54:36,207   global_step = 9799
2022-08-29 19:54:36,207   loss = 4.037098066781157
2022-08-29 19:54:36,207   rep_loss = 1.5952473043113626
2022-08-29 19:54:36,207 ***** Save model *****
2022-08-29 19:54:48,632 ***** Running evaluation *****
2022-08-29 19:54:48,633   Epoch = 23 iter 25199 step
2022-08-29 19:54:48,633   Num examples = 1043
2022-08-29 19:54:48,633   Batch size = 32
2022-08-29 19:54:48,634 ***** Eval results *****
2022-08-29 19:54:48,634   att_loss = 0.7918260721709784
2022-08-29 19:54:48,634   cls_loss = 0.0
2022-08-29 19:54:48,635   global_step = 25199
2022-08-29 19:54:48,635   loss = 2.3449020496503574
2022-08-29 19:54:48,635   rep_loss = 1.5530759741940836
2022-08-29 19:54:48,635 ***** Save model *****
2022-08-29 19:54:52,083 ***** Running evaluation *****
2022-08-29 19:54:52,084   Epoch = 2 iter 33999 step
2022-08-29 19:54:52,084   Num examples = 9815
2022-08-29 19:54:52,084   Batch size = 32
2022-08-29 19:54:52,085 ***** Eval results *****
2022-08-29 19:54:52,085   att_loss = 4.169863901166542
2022-08-29 19:54:52,085   cls_loss = 0.0
2022-08-29 19:54:52,085   global_step = 33999
2022-08-29 19:54:52,085   loss = 6.005093790076305
2022-08-29 19:54:52,085   rep_loss = 1.835229887674434
2022-08-29 19:54:52,085 ***** Save model *****
2022-08-29 19:55:02,070 ***** Running evaluation *****
2022-08-29 19:55:02,071   Epoch = 32 iter 10199 step
2022-08-29 19:55:02,071   Num examples = 277
2022-08-29 19:55:02,071   Batch size = 32
2022-08-29 19:55:02,072 ***** Eval results *****
2022-08-29 19:55:02,073   att_loss = 7.756939606145326
2022-08-29 19:55:02,073   cls_loss = 0.0
2022-08-29 19:55:02,073   global_step = 10199
2022-08-29 19:55:02,073   loss = 10.02320695985184
2022-08-29 19:55:02,073   rep_loss = 2.266267367220118
2022-08-29 19:55:02,073 ***** Save model *****
2022-08-29 19:55:17,953 ***** Running evaluation *****
2022-08-29 19:55:17,954   Epoch = 13 iter 9999 step
2022-08-29 19:55:17,954   Num examples = 1500
2022-08-29 19:55:17,954   Batch size = 32
2022-08-29 19:55:17,955 ***** Eval results *****
2022-08-29 19:55:17,955   att_loss = 2.4516275454284555
2022-08-29 19:55:17,955   cls_loss = 0.0
2022-08-29 19:55:17,955   global_step = 9999
2022-08-29 19:55:17,955   loss = 4.046360724312919
2022-08-29 19:55:17,955   rep_loss = 1.5947331688457862
2022-08-29 19:55:17,956 ***** Save model *****
2022-08-29 19:55:26,828 ***** Running evaluation *****
2022-08-29 19:55:26,829   Epoch = 23 iter 25399 step
2022-08-29 19:55:26,829   Num examples = 1043
2022-08-29 19:55:26,829   Batch size = 32
2022-08-29 19:55:26,830 ***** Eval results *****
2022-08-29 19:55:26,830   att_loss = 0.794414269495867
2022-08-29 19:55:26,830   cls_loss = 0.0
2022-08-29 19:55:26,830   global_step = 25399
2022-08-29 19:55:26,830   loss = 2.3492653131484986
2022-08-29 19:55:26,831   rep_loss = 1.5548510405831708
2022-08-29 19:55:26,831 ***** Save model *****
2022-08-29 19:55:41,384 ***** Running evaluation *****
2022-08-29 19:55:41,385   Epoch = 33 iter 10399 step
2022-08-29 19:55:41,385   Num examples = 277
2022-08-29 19:55:41,385   Batch size = 32
2022-08-29 19:55:41,386 ***** Eval results *****
2022-08-29 19:55:41,386   att_loss = 7.707356498521917
2022-08-29 19:55:41,386   cls_loss = 0.0
2022-08-29 19:55:41,386   global_step = 10399
2022-08-29 19:55:41,386   loss = 9.965703904628754
2022-08-29 19:55:41,386   rep_loss = 2.258347393835292
2022-08-29 19:55:41,387 ***** Save model *****
2022-08-29 19:55:59,774 ***** Running evaluation *****
2022-08-29 19:55:59,774   Epoch = 14 iter 10199 step
2022-08-29 19:55:59,774   Num examples = 1500
2022-08-29 19:55:59,774   Batch size = 32
2022-08-29 19:55:59,776 ***** Eval results *****
2022-08-29 19:55:59,776   att_loss = 2.3799026920681907
2022-08-29 19:55:59,776   cls_loss = 0.0
2022-08-29 19:55:59,776   global_step = 10199
2022-08-29 19:55:59,776   loss = 3.9532425176529657
2022-08-29 19:55:59,776   rep_loss = 1.5733398263957226
2022-08-29 19:55:59,776 ***** Save model *****
2022-08-29 19:56:05,139 ***** Running evaluation *****
2022-08-29 19:56:05,140   Epoch = 2 iter 34199 step
2022-08-29 19:56:05,140   Num examples = 9815
2022-08-29 19:56:05,140   Batch size = 32
2022-08-29 19:56:05,142 ***** Eval results *****
2022-08-29 19:56:05,142   att_loss = 4.169032497016913
2022-08-29 19:56:05,142   cls_loss = 0.0
2022-08-29 19:56:05,142   global_step = 34199
2022-08-29 19:56:05,143   loss = 6.003930230460157
2022-08-29 19:56:05,143   rep_loss = 1.834897732196467
2022-08-29 19:56:05,143 ***** Save model *****
2022-08-29 19:56:06,955 ***** Running evaluation *****
2022-08-29 19:56:06,955   Epoch = 23 iter 25599 step
2022-08-29 19:56:06,955   Num examples = 1043
2022-08-29 19:56:06,955   Batch size = 32
2022-08-29 19:56:06,957 ***** Eval results *****
2022-08-29 19:56:06,957   att_loss = 0.7978111850466705
2022-08-29 19:56:06,957   cls_loss = 0.0
2022-08-29 19:56:06,957   global_step = 25599
2022-08-29 19:56:06,957   loss = 2.352133912510342
2022-08-29 19:56:06,957   rep_loss = 1.5543227248721652
2022-08-29 19:56:06,957 ***** Save model *****
2022-08-29 19:56:20,610 ***** Running evaluation *****
2022-08-29 19:56:20,611   Epoch = 34 iter 10599 step
2022-08-29 19:56:20,611   Num examples = 277
2022-08-29 19:56:20,611   Batch size = 32
2022-08-29 19:56:20,613 ***** Eval results *****
2022-08-29 19:56:20,613   att_loss = 7.372037868499756
2022-08-29 19:56:20,613   cls_loss = 0.0
2022-08-29 19:56:20,613   global_step = 10599
2022-08-29 19:56:20,613   loss = 9.59854497909546
2022-08-29 19:56:20,613   rep_loss = 2.2265071201324464
2022-08-29 19:56:20,613 ***** Save model *****
2022-08-29 19:56:40,421 ***** Running evaluation *****
2022-08-29 19:56:40,421   Epoch = 14 iter 10399 step
2022-08-29 19:56:40,422   Num examples = 1500
2022-08-29 19:56:40,422   Batch size = 32
2022-08-29 19:56:40,423 ***** Eval results *****
2022-08-29 19:56:40,423   att_loss = 2.3819283185156346
2022-08-29 19:56:40,423   cls_loss = 0.0
2022-08-29 19:56:40,423   global_step = 10399
2022-08-29 19:56:40,423   loss = 3.956299132503762
2022-08-29 19:56:40,423   rep_loss = 1.5743708150187556
2022-08-29 19:56:40,423 ***** Save model *****
2022-08-29 19:56:48,229 ***** Running evaluation *****
2022-08-29 19:56:48,229   Epoch = 24 iter 25799 step
2022-08-29 19:56:48,229   Num examples = 1043
2022-08-29 19:56:48,229   Batch size = 32
2022-08-29 19:56:48,231 ***** Eval results *****
2022-08-29 19:56:48,231   att_loss = 0.7929207543413083
2022-08-29 19:56:48,231   cls_loss = 0.0
2022-08-29 19:56:48,231   global_step = 25799
2022-08-29 19:56:48,231   loss = 2.345190239523699
2022-08-29 19:56:48,231   rep_loss = 1.552269486610047
2022-08-29 19:56:48,231 ***** Save model *****
2022-08-29 19:56:59,874 ***** Running evaluation *****
2022-08-29 19:56:59,874   Epoch = 34 iter 10799 step
2022-08-29 19:56:59,874   Num examples = 277
2022-08-29 19:56:59,874   Batch size = 32
2022-08-29 19:56:59,875 ***** Eval results *****
2022-08-29 19:56:59,875   att_loss = 7.742643684811062
2022-08-29 19:56:59,875   cls_loss = 0.0
2022-08-29 19:56:59,876   global_step = 10799
2022-08-29 19:56:59,876   loss = 9.995147683885362
2022-08-29 19:56:59,876   rep_loss = 2.252504014968872
2022-08-29 19:56:59,876 ***** Save model *****
2022-08-29 19:57:17,510 ***** Running evaluation *****
2022-08-29 19:57:17,510   Epoch = 2 iter 34399 step
2022-08-29 19:57:17,510   Num examples = 9815
2022-08-29 19:57:17,510   Batch size = 32
2022-08-29 19:57:17,511 ***** Eval results *****
2022-08-29 19:57:17,512   att_loss = 4.168546961011033
2022-08-29 19:57:17,512   cls_loss = 0.0
2022-08-29 19:57:17,512   global_step = 34399
2022-08-29 19:57:17,512   loss = 6.003141401166472
2022-08-29 19:57:17,512   rep_loss = 1.8345944388493014
2022-08-29 19:57:17,512 ***** Save model *****
2022-08-29 19:57:19,685 ***** Running evaluation *****
2022-08-29 19:57:19,686   Epoch = 14 iter 10599 step
2022-08-29 19:57:19,686   Num examples = 1500
2022-08-29 19:57:19,686   Batch size = 32
2022-08-29 19:57:19,687 ***** Eval results *****
2022-08-29 19:57:19,687   att_loss = 2.383371826935512
2022-08-29 19:57:19,687   cls_loss = 0.0
2022-08-29 19:57:19,687   global_step = 10599
2022-08-29 19:57:19,687   loss = 3.9583101573330395
2022-08-29 19:57:19,687   rep_loss = 1.5749383317051249
2022-08-29 19:57:19,688 ***** Save model *****
2022-08-29 19:57:32,364 ***** Running evaluation *****
2022-08-29 19:57:32,365   Epoch = 24 iter 25999 step
2022-08-29 19:57:32,365   Num examples = 1043
2022-08-29 19:57:32,365   Batch size = 32
2022-08-29 19:57:32,366 ***** Eval results *****
2022-08-29 19:57:32,366   att_loss = 0.7969665379550217
2022-08-29 19:57:32,366   cls_loss = 0.0
2022-08-29 19:57:32,366   global_step = 25999
2022-08-29 19:57:32,366   loss = 2.349014620040353
2022-08-29 19:57:32,366   rep_loss = 1.5520480822477418
2022-08-29 19:57:32,367 ***** Save model *****
2022-08-29 19:57:39,235 ***** Running evaluation *****
2022-08-29 19:57:39,236   Epoch = 35 iter 10999 step
2022-08-29 19:57:39,236   Num examples = 277
2022-08-29 19:57:39,236   Batch size = 32
2022-08-29 19:57:39,237 ***** Eval results *****
2022-08-29 19:57:39,237   att_loss = 7.5511417012465625
2022-08-29 19:57:39,237   cls_loss = 0.0
2022-08-29 19:57:39,237   global_step = 10999
2022-08-29 19:57:39,237   loss = 9.783553069097954
2022-08-29 19:57:39,237   rep_loss = 2.232411349028872
2022-08-29 19:57:39,238 ***** Save model *****
2022-08-29 19:57:59,500 ***** Running evaluation *****
2022-08-29 19:57:59,500   Epoch = 15 iter 10799 step
2022-08-29 19:57:59,500   Num examples = 1500
2022-08-29 19:57:59,501   Batch size = 32
2022-08-29 19:57:59,502 ***** Eval results *****
2022-08-29 19:57:59,502   att_loss = 2.4663790094441382
2022-08-29 19:57:59,502   cls_loss = 0.0
2022-08-29 19:57:59,502   global_step = 10799
2022-08-29 19:57:59,502   loss = 4.048414789397141
2022-08-29 19:57:59,502   rep_loss = 1.582035755289012
2022-08-29 19:57:59,502 ***** Save model *****
2022-08-29 19:58:12,245 ***** Running evaluation *****
2022-08-29 19:58:12,246   Epoch = 24 iter 26199 step
2022-08-29 19:58:12,246   Num examples = 1043
2022-08-29 19:58:12,246   Batch size = 32
2022-08-29 19:58:12,248 ***** Eval results *****
2022-08-29 19:58:12,248   att_loss = 0.7961521342633262
2022-08-29 19:58:12,248   cls_loss = 0.0
2022-08-29 19:58:12,248   global_step = 26199
2022-08-29 19:58:12,248   loss = 2.3480342094650135
2022-08-29 19:58:12,248   rep_loss = 1.5518820733620375
2022-08-29 19:58:12,248 ***** Save model *****
2022-08-29 19:58:18,481 ***** Running evaluation *****
2022-08-29 19:58:18,482   Epoch = 36 iter 11199 step
2022-08-29 19:58:18,482   Num examples = 277
2022-08-29 19:58:18,482   Batch size = 32
2022-08-29 19:58:18,483 ***** Eval results *****
2022-08-29 19:58:18,483   att_loss = 6.413988908131917
2022-08-29 19:58:18,483   cls_loss = 0.0
2022-08-29 19:58:18,484   global_step = 11199
2022-08-29 19:58:18,484   loss = 8.574760437011719
2022-08-29 19:58:18,484   rep_loss = 2.160771608352661
2022-08-29 19:58:18,484 ***** Save model *****
2022-08-29 19:58:29,042 ***** Running evaluation *****
2022-08-29 19:58:29,042   Epoch = 2 iter 34599 step
2022-08-29 19:58:29,042   Num examples = 9815
2022-08-29 19:58:29,042   Batch size = 32
2022-08-29 19:58:29,043 ***** Eval results *****
2022-08-29 19:58:29,043   att_loss = 4.166959465433094
2022-08-29 19:58:29,043   cls_loss = 0.0
2022-08-29 19:58:29,043   global_step = 34599
2022-08-29 19:58:29,043   loss = 6.001129312923963
2022-08-29 19:58:29,043   rep_loss = 1.834169846068465
2022-08-29 19:58:29,044 ***** Save model *****
2022-08-29 19:58:38,165 ***** Running evaluation *****
2022-08-29 19:58:38,166   Epoch = 15 iter 10999 step
2022-08-29 19:58:38,166   Num examples = 1500
2022-08-29 19:58:38,166   Batch size = 32
2022-08-29 19:58:38,167 ***** Eval results *****
2022-08-29 19:58:38,167   att_loss = 2.386215321882323
2022-08-29 19:58:38,167   cls_loss = 0.0
2022-08-29 19:58:38,167   global_step = 10999
2022-08-29 19:58:38,167   loss = 3.955152321070042
2022-08-29 19:58:38,167   rep_loss = 1.5689369945026381
2022-08-29 19:58:38,167 ***** Save model *****
2022-08-29 19:58:50,537 ***** Running evaluation *****
2022-08-29 19:58:50,538   Epoch = 24 iter 26399 step
2022-08-29 19:58:50,538   Num examples = 1043
2022-08-29 19:58:50,538   Batch size = 32
2022-08-29 19:58:50,539 ***** Eval results *****
2022-08-29 19:58:50,539   att_loss = 0.7924445406321285
2022-08-29 19:58:50,540   cls_loss = 0.0
2022-08-29 19:58:50,540   global_step = 26399
2022-08-29 19:58:50,540   loss = 2.341176000381231
2022-08-29 19:58:50,540   rep_loss = 1.5487314577674618
2022-08-29 19:58:50,540 ***** Save model *****
2022-08-29 19:58:57,757 ***** Running evaluation *****
2022-08-29 19:58:57,757   Epoch = 36 iter 11399 step
2022-08-29 19:58:57,757   Num examples = 277
2022-08-29 19:58:57,758   Batch size = 32
2022-08-29 19:58:57,759 ***** Eval results *****
2022-08-29 19:58:57,759   att_loss = 7.592996787555112
2022-08-29 19:58:57,759   cls_loss = 0.0
2022-08-29 19:58:57,759   global_step = 11399
2022-08-29 19:58:57,759   loss = 9.825703137026633
2022-08-29 19:58:57,759   rep_loss = 2.2327063535821847
2022-08-29 19:58:57,759 ***** Save model *****
2022-08-29 19:59:16,863 ***** Running evaluation *****
2022-08-29 19:59:16,863   Epoch = 15 iter 11199 step
2022-08-29 19:59:16,863   Num examples = 1500
2022-08-29 19:59:16,863   Batch size = 32
2022-08-29 19:59:16,864 ***** Eval results *****
2022-08-29 19:59:16,864   att_loss = 2.376232078025391
2022-08-29 19:59:16,864   cls_loss = 0.0
2022-08-29 19:59:16,864   global_step = 11199
2022-08-29 19:59:16,865   loss = 3.942270292824521
2022-08-29 19:59:16,865   rep_loss = 1.5660382125761125
2022-08-29 19:59:16,865 ***** Save model *****
2022-08-29 19:59:32,837 ***** Running evaluation *****
2022-08-29 19:59:32,837   Epoch = 24 iter 26599 step
2022-08-29 19:59:32,838   Num examples = 1043
2022-08-29 19:59:32,838   Batch size = 32
2022-08-29 19:59:32,839 ***** Eval results *****
2022-08-29 19:59:32,839   att_loss = 0.7929046930670122
2022-08-29 19:59:32,839   cls_loss = 0.0
2022-08-29 19:59:32,839   global_step = 26599
2022-08-29 19:59:32,839   loss = 2.3412572156557006
2022-08-29 19:59:32,839   rep_loss = 1.5483525211093592
2022-08-29 19:59:32,839 ***** Save model *****
2022-08-29 19:59:39,192 ***** Running evaluation *****
2022-08-29 19:59:39,193   Epoch = 2 iter 34799 step
2022-08-29 19:59:39,193   Num examples = 9815
2022-08-29 19:59:39,193   Batch size = 32
2022-08-29 19:59:39,194 ***** Eval results *****
2022-08-29 19:59:39,194   att_loss = 4.1655145461556735
2022-08-29 19:59:39,194   cls_loss = 0.0
2022-08-29 19:59:39,194   global_step = 34799
2022-08-29 19:59:39,194   loss = 5.999245940866068
2022-08-29 19:59:39,194   rep_loss = 1.8337313935016812
2022-08-29 19:59:39,194 ***** Save model *****
2022-08-29 19:59:41,315 ***** Running evaluation *****
2022-08-29 19:59:41,315   Epoch = 37 iter 11599 step
2022-08-29 19:59:41,315   Num examples = 277
2022-08-29 19:59:41,315   Batch size = 32
2022-08-29 19:59:41,317 ***** Eval results *****
2022-08-29 19:59:41,317   att_loss = 7.673447733340056
2022-08-29 19:59:41,317   cls_loss = 0.0
2022-08-29 19:59:41,317   global_step = 11599
2022-08-29 19:59:41,317   loss = 9.904349938682888
2022-08-29 19:59:41,317   rep_loss = 2.230902137963668
2022-08-29 19:59:41,317 ***** Save model *****
2022-08-29 19:59:57,742 ***** Running evaluation *****
2022-08-29 19:59:57,742   Epoch = 15 iter 11399 step
2022-08-29 19:59:57,742   Num examples = 1500
2022-08-29 19:59:57,742   Batch size = 32
2022-08-29 19:59:57,744 ***** Eval results *****
2022-08-29 19:59:57,744   att_loss = 2.380784248509733
2022-08-29 19:59:57,744   cls_loss = 0.0
2022-08-29 19:59:57,744   global_step = 11399
2022-08-29 19:59:57,744   loss = 3.946302191441692
2022-08-29 19:59:57,744   rep_loss = 1.5655179438795688
2022-08-29 19:59:57,744 ***** Save model *****
2022-08-29 20:00:11,086 ***** Running evaluation *****
2022-08-29 20:00:11,086   Epoch = 25 iter 26799 step
2022-08-29 20:00:11,087   Num examples = 1043
2022-08-29 20:00:11,087   Batch size = 32
2022-08-29 20:00:11,088 ***** Eval results *****
2022-08-29 20:00:11,088   att_loss = 0.7792625270708643
2022-08-29 20:00:11,088   cls_loss = 0.0
2022-08-29 20:00:11,088   global_step = 26799
2022-08-29 20:00:11,088   loss = 2.3209829294320308
2022-08-29 20:00:11,088   rep_loss = 1.5417204119942405
2022-08-29 20:00:11,088 ***** Save model *****
2022-08-29 20:00:21,745 ***** Running evaluation *****
2022-08-29 20:00:21,745   Epoch = 37 iter 11799 step
2022-08-29 20:00:21,745   Num examples = 277
2022-08-29 20:00:21,745   Batch size = 32
2022-08-29 20:00:21,746 ***** Eval results *****
2022-08-29 20:00:21,747   att_loss = 7.535942212359546
2022-08-29 20:00:21,747   cls_loss = 0.0
2022-08-29 20:00:21,747   global_step = 11799
2022-08-29 20:00:21,747   loss = 9.755948411275263
2022-08-29 20:00:21,747   rep_loss = 2.2200061621731275
2022-08-29 20:00:21,747 ***** Save model *****
2022-08-29 20:00:38,915 ***** Running evaluation *****
2022-08-29 20:00:38,915   Epoch = 16 iter 11599 step
2022-08-29 20:00:38,915   Num examples = 1500
2022-08-29 20:00:38,915   Batch size = 32
2022-08-29 20:00:38,917 ***** Eval results *****
2022-08-29 20:00:38,917   att_loss = 2.229581198176822
2022-08-29 20:00:38,917   cls_loss = 0.0
2022-08-29 20:00:38,917   global_step = 11599
2022-08-29 20:00:38,917   loss = 3.765402097959776
2022-08-29 20:00:38,917   rep_loss = 1.5358209040787842
2022-08-29 20:00:38,917 ***** Save model *****
2022-08-29 20:00:48,577 ***** Running evaluation *****
2022-08-29 20:00:48,578   Epoch = 2 iter 34999 step
2022-08-29 20:00:48,578   Num examples = 9815
2022-08-29 20:00:48,578   Batch size = 32
2022-08-29 20:00:48,579 ***** Eval results *****
2022-08-29 20:00:48,579   att_loss = 4.164650140739346
2022-08-29 20:00:48,579   cls_loss = 0.0
2022-08-29 20:00:48,580   global_step = 34999
2022-08-29 20:00:48,580   loss = 5.998018587115528
2022-08-29 20:00:48,580   rep_loss = 1.833368445350185
2022-08-29 20:00:48,580 ***** Save model *****
2022-08-29 20:00:49,269 ***** Running evaluation *****
2022-08-29 20:00:49,270   Epoch = 25 iter 26999 step
2022-08-29 20:00:49,270   Num examples = 1043
2022-08-29 20:00:49,270   Batch size = 32
2022-08-29 20:00:49,271 ***** Eval results *****
2022-08-29 20:00:49,272   att_loss = 0.7980646022586121
2022-08-29 20:00:49,272   cls_loss = 0.0
2022-08-29 20:00:49,272   global_step = 26999
2022-08-29 20:00:49,272   loss = 2.3448274586113
2022-08-29 20:00:49,272   rep_loss = 1.5467628539605285
2022-08-29 20:00:49,272 ***** Save model *****
2022-08-29 20:01:01,020 ***** Running evaluation *****
2022-08-29 20:01:01,021   Epoch = 38 iter 11999 step
2022-08-29 20:01:01,021   Num examples = 277
2022-08-29 20:01:01,021   Batch size = 32
2022-08-29 20:01:01,022 ***** Eval results *****
2022-08-29 20:01:01,022   att_loss = 7.36532866757219
2022-08-29 20:01:01,022   cls_loss = 0.0
2022-08-29 20:01:01,022   global_step = 11999
2022-08-29 20:01:01,022   loss = 9.567153738348523
2022-08-29 20:01:01,022   rep_loss = 2.201825085924475
2022-08-29 20:01:01,022 ***** Save model *****
2022-08-29 20:01:21,407 ***** Running evaluation *****
2022-08-29 20:01:21,408   Epoch = 16 iter 11799 step
2022-08-29 20:01:21,408   Num examples = 1500
2022-08-29 20:01:21,408   Batch size = 32
2022-08-29 20:01:21,409 ***** Eval results *****
2022-08-29 20:01:21,409   att_loss = 2.3415382000411054
2022-08-29 20:01:21,409   cls_loss = 0.0
2022-08-29 20:01:21,409   global_step = 11799
2022-08-29 20:01:21,409   loss = 3.8953348195054525
2022-08-29 20:01:21,409   rep_loss = 1.5537966175477989
2022-08-29 20:01:21,409 ***** Save model *****
2022-08-29 20:01:29,944 ***** Running evaluation *****
2022-08-29 20:01:29,945   Epoch = 25 iter 27199 step
2022-08-29 20:01:29,945   Num examples = 1043
2022-08-29 20:01:29,945   Batch size = 32
2022-08-29 20:01:29,946 ***** Eval results *****
2022-08-29 20:01:29,946   att_loss = 0.792974926665694
2022-08-29 20:01:29,946   cls_loss = 0.0
2022-08-29 20:01:29,946   global_step = 27199
2022-08-29 20:01:29,946   loss = 2.33828948590464
2022-08-29 20:01:29,946   rep_loss = 1.5453145573277751
2022-08-29 20:01:29,947 ***** Save model *****
2022-08-29 20:01:41,139 ***** Running evaluation *****
2022-08-29 20:01:41,140   Epoch = 39 iter 12199 step
2022-08-29 20:01:41,140   Num examples = 277
2022-08-29 20:01:41,140   Batch size = 32
2022-08-29 20:01:41,141 ***** Eval results *****
2022-08-29 20:01:41,141   att_loss = 7.612671933855329
2022-08-29 20:01:41,141   cls_loss = 0.0
2022-08-29 20:01:41,142   global_step = 12199
2022-08-29 20:01:41,142   loss = 9.828403976985387
2022-08-29 20:01:41,142   rep_loss = 2.215732032912118
2022-08-29 20:01:41,142 ***** Save model *****
2022-08-29 20:02:00,374 ***** Running evaluation *****
2022-08-29 20:02:00,375   Epoch = 16 iter 11999 step
2022-08-29 20:02:00,375   Num examples = 1500
2022-08-29 20:02:00,375   Batch size = 32
2022-08-29 20:02:00,376 ***** Eval results *****
2022-08-29 20:02:00,376   att_loss = 2.327475158202438
2022-08-29 20:02:00,376   cls_loss = 0.0
2022-08-29 20:02:00,376   global_step = 11999
2022-08-29 20:02:00,377   loss = 3.8740728503337345
2022-08-29 20:02:00,377   rep_loss = 1.5465976907315786
2022-08-29 20:02:00,377 ***** Save model *****
2022-08-29 20:02:00,975 ***** Running evaluation *****
2022-08-29 20:02:00,976   Epoch = 2 iter 35199 step
2022-08-29 20:02:00,976   Num examples = 9815
2022-08-29 20:02:00,976   Batch size = 32
2022-08-29 20:02:00,977 ***** Eval results *****
2022-08-29 20:02:00,977   att_loss = 4.163806058591578
2022-08-29 20:02:00,977   cls_loss = 0.0
2022-08-29 20:02:00,978   global_step = 35199
2022-08-29 20:02:00,978   loss = 5.996819619200876
2022-08-29 20:02:00,978   rep_loss = 1.833013559658487
2022-08-29 20:02:00,978 ***** Save model *****
2022-08-29 20:02:08,045 ***** Running evaluation *****
2022-08-29 20:02:08,045   Epoch = 25 iter 27399 step
2022-08-29 20:02:08,045   Num examples = 1043
2022-08-29 20:02:08,045   Batch size = 32
2022-08-29 20:02:08,047 ***** Eval results *****
2022-08-29 20:02:08,047   att_loss = 0.7878798574678205
2022-08-29 20:02:08,047   cls_loss = 0.0
2022-08-29 20:02:08,047   global_step = 27399
2022-08-29 20:02:08,047   loss = 2.3315340937121913
2022-08-29 20:02:08,047   rep_loss = 1.543654232407162
2022-08-29 20:02:08,047 ***** Save model *****
2022-08-29 20:02:20,399 ***** Running evaluation *****
2022-08-29 20:02:20,400   Epoch = 39 iter 12399 step
2022-08-29 20:02:20,400   Num examples = 277
2022-08-29 20:02:20,400   Batch size = 32
2022-08-29 20:02:20,401 ***** Eval results *****
2022-08-29 20:02:20,401   att_loss = 7.428070650277315
2022-08-29 20:02:20,401   cls_loss = 0.0
2022-08-29 20:02:20,401   global_step = 12399
2022-08-29 20:02:20,401   loss = 9.628375180562337
2022-08-29 20:02:20,401   rep_loss = 2.2003045267528956
2022-08-29 20:02:20,401 ***** Save model *****
2022-08-29 20:02:41,296 ***** Running evaluation *****
2022-08-29 20:02:41,296   Epoch = 16 iter 12199 step
2022-08-29 20:02:41,296   Num examples = 1500
2022-08-29 20:02:41,296   Batch size = 32
2022-08-29 20:02:41,298 ***** Eval results *****
2022-08-29 20:02:41,298   att_loss = 2.3593624825216044
2022-08-29 20:02:41,298   cls_loss = 0.0
2022-08-29 20:02:41,298   global_step = 12199
2022-08-29 20:02:41,298   loss = 3.9094340184234535
2022-08-29 20:02:41,298   rep_loss = 1.5500715384168129
2022-08-29 20:02:41,298 ***** Save model *****
2022-08-29 20:02:46,495 ***** Running evaluation *****
2022-08-29 20:02:46,495   Epoch = 25 iter 27599 step
2022-08-29 20:02:46,495   Num examples = 1043
2022-08-29 20:02:46,495   Batch size = 32
2022-08-29 20:02:46,496 ***** Eval results *****
2022-08-29 20:02:46,497   att_loss = 0.7915992913177202
2022-08-29 20:02:46,497   cls_loss = 0.0
2022-08-29 20:02:46,497   global_step = 27599
2022-08-29 20:02:46,497   loss = 2.3354286774910067
2022-08-29 20:02:46,497   rep_loss = 1.543829382990836
2022-08-29 20:02:46,497 ***** Save model *****
2022-08-29 20:03:05,144 ***** Running evaluation *****
2022-08-29 20:03:05,145   Epoch = 40 iter 12599 step
2022-08-29 20:03:05,145   Num examples = 277
2022-08-29 20:03:05,145   Batch size = 32
2022-08-29 20:03:05,147 ***** Eval results *****
2022-08-29 20:03:05,147   att_loss = 7.31494565729825
2022-08-29 20:03:05,147   cls_loss = 0.0
2022-08-29 20:03:05,147   global_step = 12599
2022-08-29 20:03:05,147   loss = 9.506363277915138
2022-08-29 20:03:05,147   rep_loss = 2.191417599624058
2022-08-29 20:03:05,147 ***** Save model *****
2022-08-29 20:03:14,317 ***** Running evaluation *****
2022-08-29 20:03:14,318   Epoch = 2 iter 35399 step
2022-08-29 20:03:14,318   Num examples = 9815
2022-08-29 20:03:14,318   Batch size = 32
2022-08-29 20:03:14,319 ***** Eval results *****
2022-08-29 20:03:14,319   att_loss = 4.163557185623389
2022-08-29 20:03:14,319   cls_loss = 0.0
2022-08-29 20:03:14,319   global_step = 35399
2022-08-29 20:03:14,319   loss = 5.996259301694989
2022-08-29 20:03:14,319   rep_loss = 1.8327021150285063
2022-08-29 20:03:14,319 ***** Save model *****
2022-08-29 20:03:20,166 ***** Running evaluation *****
2022-08-29 20:03:20,166   Epoch = 17 iter 12399 step
2022-08-29 20:03:20,166   Num examples = 1500
2022-08-29 20:03:20,166   Batch size = 32
2022-08-29 20:03:20,168 ***** Eval results *****
2022-08-29 20:03:20,168   att_loss = 2.3558439146051753
2022-08-29 20:03:20,168   cls_loss = 0.0
2022-08-29 20:03:20,168   global_step = 12399
2022-08-29 20:03:20,168   loss = 3.8993485072733827
2022-08-29 20:03:20,168   rep_loss = 1.5435045957565308
2022-08-29 20:03:20,168 ***** Save model *****
2022-08-29 20:03:26,076 ***** Running evaluation *****
2022-08-29 20:03:26,077   Epoch = 26 iter 27799 step
2022-08-29 20:03:26,077   Num examples = 1043
2022-08-29 20:03:26,077   Batch size = 32
2022-08-29 20:03:26,078 ***** Eval results *****
2022-08-29 20:03:26,078   att_loss = 0.8214590780196651
2022-08-29 20:03:26,078   cls_loss = 0.0
2022-08-29 20:03:26,078   global_step = 27799
2022-08-29 20:03:26,078   loss = 2.368762400842482
2022-08-29 20:03:26,078   rep_loss = 1.5473033266682779
2022-08-29 20:03:26,078 ***** Save model *****
2022-08-29 20:03:43,504 ***** Running evaluation *****
2022-08-29 20:03:43,505   Epoch = 41 iter 12799 step
2022-08-29 20:03:43,505   Num examples = 277
2022-08-29 20:03:43,505   Batch size = 32
2022-08-29 20:03:43,506 ***** Eval results *****
2022-08-29 20:03:43,506   att_loss = 7.422700434923172
2022-08-29 20:03:43,506   cls_loss = 0.0
2022-08-29 20:03:43,506   global_step = 12799
2022-08-29 20:03:43,506   loss = 9.608394354581833
2022-08-29 20:03:43,506   rep_loss = 2.1856938749551773
2022-08-29 20:03:43,506 ***** Save model *****
2022-08-29 20:04:01,154 ***** Running evaluation *****
2022-08-29 20:04:01,154   Epoch = 17 iter 12599 step
2022-08-29 20:04:01,154   Num examples = 1500
2022-08-29 20:04:01,154   Batch size = 32
2022-08-29 20:04:01,155 ***** Eval results *****
2022-08-29 20:04:01,156   att_loss = 2.3342094251521064
2022-08-29 20:04:01,156   cls_loss = 0.0
2022-08-29 20:04:01,156   global_step = 12599
2022-08-29 20:04:01,156   loss = 3.874317263828889
2022-08-29 20:04:01,156   rep_loss = 1.5401078304868314
2022-08-29 20:04:01,156 ***** Save model *****
2022-08-29 20:04:04,418 ***** Running evaluation *****
2022-08-29 20:04:04,418   Epoch = 26 iter 27999 step
2022-08-29 20:04:04,418   Num examples = 1043
2022-08-29 20:04:04,418   Batch size = 32
2022-08-29 20:04:04,420 ***** Eval results *****
2022-08-29 20:04:04,420   att_loss = 0.789717051115903
2022-08-29 20:04:04,420   cls_loss = 0.0
2022-08-29 20:04:04,420   global_step = 27999
2022-08-29 20:04:04,420   loss = 2.322993097367225
2022-08-29 20:04:04,420   rep_loss = 1.5332760434130053
2022-08-29 20:04:04,420 ***** Save model *****
2022-08-29 20:04:21,320 ***** Running evaluation *****
2022-08-29 20:04:21,321   Epoch = 41 iter 12999 step
2022-08-29 20:04:21,321   Num examples = 277
2022-08-29 20:04:21,321   Batch size = 32
2022-08-29 20:04:21,322 ***** Eval results *****
2022-08-29 20:04:21,322   att_loss = 7.420233040086685
2022-08-29 20:04:21,323   cls_loss = 0.0
2022-08-29 20:04:21,323   global_step = 12999
2022-08-29 20:04:21,323   loss = 9.60675077092263
2022-08-29 20:04:21,323   rep_loss = 2.186517704879084
2022-08-29 20:04:21,323 ***** Save model *****
2022-08-29 20:04:26,715 ***** Running evaluation *****
2022-08-29 20:04:26,715   Epoch = 2 iter 35599 step
2022-08-29 20:04:26,716   Num examples = 9815
2022-08-29 20:04:26,716   Batch size = 32
2022-08-29 20:04:26,717 ***** Eval results *****
2022-08-29 20:04:26,717   att_loss = 4.16305237999335
2022-08-29 20:04:26,717   cls_loss = 0.0
2022-08-29 20:04:26,717   global_step = 35599
2022-08-29 20:04:26,717   loss = 5.995440376918614
2022-08-29 20:04:26,717   rep_loss = 1.8323879962244765
2022-08-29 20:04:26,717 ***** Save model *****
2022-08-29 20:04:44,937 ***** Running evaluation *****
2022-08-29 20:04:44,937   Epoch = 17 iter 12799 step
2022-08-29 20:04:44,937   Num examples = 1500
2022-08-29 20:04:44,937   Batch size = 32
2022-08-29 20:04:44,939 ***** Eval results *****
2022-08-29 20:04:44,939   att_loss = 2.325624752607555
2022-08-29 20:04:44,939   cls_loss = 0.0
2022-08-29 20:04:44,939   global_step = 12799
2022-08-29 20:04:44,939   loss = 3.8627942094915433
2022-08-29 20:04:44,939   rep_loss = 1.5371694502500821
2022-08-29 20:04:44,939 ***** Save model *****
2022-08-29 20:04:45,845 ***** Running evaluation *****
2022-08-29 20:04:45,846   Epoch = 26 iter 28199 step
2022-08-29 20:04:45,846   Num examples = 1043
2022-08-29 20:04:45,847   Batch size = 32
2022-08-29 20:04:45,848 ***** Eval results *****
2022-08-29 20:04:45,848   att_loss = 0.7877699568902257
2022-08-29 20:04:45,848   cls_loss = 0.0
2022-08-29 20:04:45,848   global_step = 28199
2022-08-29 20:04:45,848   loss = 2.3218203539638123
2022-08-29 20:04:45,848   rep_loss = 1.5340503968661456
2022-08-29 20:04:45,848 ***** Save model *****
2022-08-29 20:04:59,121 ***** Running evaluation *****
2022-08-29 20:04:59,122   Epoch = 42 iter 13199 step
2022-08-29 20:04:59,122   Num examples = 277
2022-08-29 20:04:59,122   Batch size = 32
2022-08-29 20:04:59,123 ***** Eval results *****
2022-08-29 20:04:59,123   att_loss = 7.335486961977325
2022-08-29 20:04:59,123   cls_loss = 0.0
2022-08-29 20:04:59,123   global_step = 13199
2022-08-29 20:04:59,123   loss = 9.51772920580676
2022-08-29 20:04:59,123   rep_loss = 2.1822422681933773
2022-08-29 20:04:59,124 ***** Save model *****
2022-08-29 20:05:26,480 ***** Running evaluation *****
2022-08-29 20:05:26,481   Epoch = 26 iter 28399 step
2022-08-29 20:05:26,481   Num examples = 1043
2022-08-29 20:05:26,481   Batch size = 32
2022-08-29 20:05:26,482 ***** Eval results *****
2022-08-29 20:05:26,482   att_loss = 0.7883514134233992
2022-08-29 20:05:26,482   cls_loss = 0.0
2022-08-29 20:05:26,483   global_step = 28399
2022-08-29 20:05:26,483   loss = 2.324960909063578
2022-08-29 20:05:26,483   rep_loss = 1.5366094975766214
2022-08-29 20:05:26,483 ***** Save model *****
2022-08-29 20:05:27,070 ***** Running evaluation *****
2022-08-29 20:05:27,071   Epoch = 18 iter 12999 step
2022-08-29 20:05:27,071   Num examples = 1500
2022-08-29 20:05:27,071   Batch size = 32
2022-08-29 20:05:27,072 ***** Eval results *****
2022-08-29 20:05:27,073   att_loss = 2.2577139043807986
2022-08-29 20:05:27,073   cls_loss = 0.0
2022-08-29 20:05:27,073   global_step = 12999
2022-08-29 20:05:27,073   loss = 3.7814478302001953
2022-08-29 20:05:27,073   rep_loss = 1.523733951250712
2022-08-29 20:05:27,073 ***** Save model *****
2022-08-29 20:05:38,268 ***** Running evaluation *****
2022-08-29 20:05:38,269   Epoch = 43 iter 13399 step
2022-08-29 20:05:38,269   Num examples = 277
2022-08-29 20:05:38,269   Batch size = 32
2022-08-29 20:05:38,270 ***** Eval results *****
2022-08-29 20:05:38,271   att_loss = 7.4563203408167915
2022-08-29 20:05:38,271   cls_loss = 0.0
2022-08-29 20:05:38,271   global_step = 13399
2022-08-29 20:05:38,271   loss = 9.63880177644583
2022-08-29 20:05:38,271   rep_loss = 2.1824814631388736
2022-08-29 20:05:38,271 ***** Save model *****
2022-08-29 20:05:39,523 ***** Running evaluation *****
2022-08-29 20:05:39,523   Epoch = 2 iter 35799 step
2022-08-29 20:05:39,523   Num examples = 9815
2022-08-29 20:05:39,523   Batch size = 32
2022-08-29 20:05:39,525 ***** Eval results *****
2022-08-29 20:05:39,525   att_loss = 4.16282477715177
2022-08-29 20:05:39,525   cls_loss = 0.0
2022-08-29 20:05:39,525   global_step = 35799
2022-08-29 20:05:39,525   loss = 5.994921075309732
2022-08-29 20:05:39,525   rep_loss = 1.8320962972790096
2022-08-29 20:05:39,525 ***** Save model *****
2022-08-29 20:06:07,012 ***** Running evaluation *****
2022-08-29 20:06:07,013   Epoch = 26 iter 28599 step
2022-08-29 20:06:07,013   Num examples = 1043
2022-08-29 20:06:07,013   Batch size = 32
2022-08-29 20:06:07,014 ***** Eval results *****
2022-08-29 20:06:07,014   att_loss = 0.7841224854388392
2022-08-29 20:06:07,014   cls_loss = 0.0
2022-08-29 20:06:07,014   global_step = 28599
2022-08-29 20:06:07,014   loss = 2.3214121290923027
2022-08-29 20:06:07,015   rep_loss = 1.5372896444065907
2022-08-29 20:06:07,015 ***** Save model *****
2022-08-29 20:06:10,473 ***** Running evaluation *****
2022-08-29 20:06:10,474   Epoch = 18 iter 13199 step
2022-08-29 20:06:10,474   Num examples = 1500
2022-08-29 20:06:10,474   Batch size = 32
2022-08-29 20:06:10,475 ***** Eval results *****
2022-08-29 20:06:10,476   att_loss = 2.3310399224541403
2022-08-29 20:06:10,476   cls_loss = 0.0
2022-08-29 20:06:10,476   global_step = 13199
2022-08-29 20:06:10,476   loss = 3.8616881162470036
2022-08-29 20:06:10,476   rep_loss = 1.5306481950933284
2022-08-29 20:06:10,476 ***** Save model *****
2022-08-29 20:06:24,848 ***** Running evaluation *****
2022-08-29 20:06:24,848   Epoch = 43 iter 13599 step
2022-08-29 20:06:24,848   Num examples = 277
2022-08-29 20:06:24,848   Batch size = 32
2022-08-29 20:06:24,850 ***** Eval results *****
2022-08-29 20:06:24,850   att_loss = 7.325235189589779
2022-08-29 20:06:24,850   cls_loss = 0.0
2022-08-29 20:06:24,850   global_step = 13599
2022-08-29 20:06:24,850   loss = 9.49820813246533
2022-08-29 20:06:24,850   rep_loss = 2.172972975578983
2022-08-29 20:06:24,850 ***** Save model *****
2022-08-29 20:06:45,129 ***** Running evaluation *****
2022-08-29 20:06:45,129   Epoch = 26 iter 28799 step
2022-08-29 20:06:45,129   Num examples = 1043
2022-08-29 20:06:45,129   Batch size = 32
2022-08-29 20:06:45,131 ***** Eval results *****
2022-08-29 20:06:45,131   att_loss = 0.7891889859413661
2022-08-29 20:06:45,131   cls_loss = 0.0
2022-08-29 20:06:45,131   global_step = 28799
2022-08-29 20:06:45,131   loss = 2.3291966882755637
2022-08-29 20:06:45,131   rep_loss = 1.5400077032881032
2022-08-29 20:06:45,131 ***** Save model *****
2022-08-29 20:06:51,149 ***** Running evaluation *****
2022-08-29 20:06:51,150   Epoch = 18 iter 13399 step
2022-08-29 20:06:51,150   Num examples = 1500
2022-08-29 20:06:51,150   Batch size = 32
2022-08-29 20:06:51,151 ***** Eval results *****
2022-08-29 20:06:51,152   att_loss = 2.3101493514211553
2022-08-29 20:06:51,152   cls_loss = 0.0
2022-08-29 20:06:51,152   global_step = 13399
2022-08-29 20:06:51,152   loss = 3.839899654388428
2022-08-29 20:06:51,152   rep_loss = 1.5297503054769415
2022-08-29 20:06:51,152 ***** Save model *****
2022-08-29 20:06:57,003 ***** Running evaluation *****
2022-08-29 20:06:57,004   Epoch = 2 iter 35999 step
2022-08-29 20:06:57,004   Num examples = 9815
2022-08-29 20:06:57,004   Batch size = 32
2022-08-29 20:06:57,005 ***** Eval results *****
2022-08-29 20:06:57,005   att_loss = 4.161230458673865
2022-08-29 20:06:57,005   cls_loss = 0.0
2022-08-29 20:06:57,005   global_step = 35999
2022-08-29 20:06:57,005   loss = 5.992946206602509
2022-08-29 20:06:57,005   rep_loss = 1.831715746908961
2022-08-29 20:06:57,005 ***** Save model *****
2022-08-29 20:07:02,759 ***** Running evaluation *****
2022-08-29 20:07:02,759   Epoch = 44 iter 13799 step
2022-08-29 20:07:02,759   Num examples = 277
2022-08-29 20:07:02,759   Batch size = 32
2022-08-29 20:07:02,760 ***** Eval results *****
2022-08-29 20:07:02,760   att_loss = 7.250376792576002
2022-08-29 20:07:02,760   cls_loss = 0.0
2022-08-29 20:07:02,760   global_step = 13799
2022-08-29 20:07:02,761   loss = 9.411023334834887
2022-08-29 20:07:02,761   rep_loss = 2.1606465982354206
2022-08-29 20:07:02,761 ***** Save model *****
2022-08-29 20:07:23,266 ***** Running evaluation *****
2022-08-29 20:07:23,267   Epoch = 27 iter 28999 step
2022-08-29 20:07:23,267   Num examples = 1043
2022-08-29 20:07:23,267   Batch size = 32
2022-08-29 20:07:23,268 ***** Eval results *****
2022-08-29 20:07:23,268   att_loss = 0.7884341174839464
2022-08-29 20:07:23,268   cls_loss = 0.0
2022-08-29 20:07:23,268   global_step = 28999
2022-08-29 20:07:23,268   loss = 2.3242847553791446
2022-08-29 20:07:23,268   rep_loss = 1.5358506353354893
2022-08-29 20:07:23,268 ***** Save model *****
2022-08-29 20:07:31,788 ***** Running evaluation *****
2022-08-29 20:07:31,789   Epoch = 18 iter 13599 step
2022-08-29 20:07:31,789   Num examples = 1500
2022-08-29 20:07:31,789   Batch size = 32
2022-08-29 20:07:31,790 ***** Eval results *****
2022-08-29 20:07:31,790   att_loss = 2.30313847400524
2022-08-29 20:07:31,790   cls_loss = 0.0
2022-08-29 20:07:31,791   global_step = 13599
2022-08-29 20:07:31,791   loss = 3.828902444486265
2022-08-29 20:07:31,791   rep_loss = 1.5257639703044186
2022-08-29 20:07:31,791 ***** Save model *****
2022-08-29 20:07:40,628 ***** Running evaluation *****
2022-08-29 20:07:40,629   Epoch = 45 iter 13999 step
2022-08-29 20:07:40,629   Num examples = 277
2022-08-29 20:07:40,629   Batch size = 32
2022-08-29 20:07:40,630 ***** Eval results *****
2022-08-29 20:07:40,630   att_loss = 6.416344523429871
2022-08-29 20:07:40,630   cls_loss = 0.0
2022-08-29 20:07:40,630   global_step = 13999
2022-08-29 20:07:40,630   loss = 8.521404147148132
2022-08-29 20:07:40,630   rep_loss = 2.1050596833229065
2022-08-29 20:07:40,630 ***** Save model *****
2022-08-29 20:08:05,623 ***** Running evaluation *****
2022-08-29 20:08:05,624   Epoch = 27 iter 29199 step
2022-08-29 20:08:05,624   Num examples = 1043
2022-08-29 20:08:05,624   Batch size = 32
2022-08-29 20:08:05,625 ***** Eval results *****
2022-08-29 20:08:05,625   att_loss = 0.7858765558762983
2022-08-29 20:08:05,626   cls_loss = 0.0
2022-08-29 20:08:05,626   global_step = 29199
2022-08-29 20:08:05,626   loss = 2.3245200913799695
2022-08-29 20:08:05,626   rep_loss = 1.538643532548069
2022-08-29 20:08:05,626 ***** Save model *****
2022-08-29 20:08:07,513 ***** Running evaluation *****
2022-08-29 20:08:07,514   Epoch = 2 iter 36199 step
2022-08-29 20:08:07,514   Num examples = 9815
2022-08-29 20:08:07,514   Batch size = 32
2022-08-29 20:08:07,516 ***** Eval results *****
2022-08-29 20:08:07,516   att_loss = 4.159736949521906
2022-08-29 20:08:07,516   cls_loss = 0.0
2022-08-29 20:08:07,516   global_step = 36199
2022-08-29 20:08:07,516   loss = 5.991062371698284
2022-08-29 20:08:07,516   rep_loss = 1.831325421419624
2022-08-29 20:08:07,516 ***** Save model *****
2022-08-29 20:08:13,630 ***** Running evaluation *****
2022-08-29 20:08:13,631   Epoch = 19 iter 13799 step
2022-08-29 20:08:13,631   Num examples = 1500
2022-08-29 20:08:13,631   Batch size = 32
2022-08-29 20:08:13,632 ***** Eval results *****
2022-08-29 20:08:13,632   att_loss = 2.405630163326385
2022-08-29 20:08:13,632   cls_loss = 0.0
2022-08-29 20:08:13,632   global_step = 13799
2022-08-29 20:08:13,632   loss = 3.9409213430562597
2022-08-29 20:08:13,633   rep_loss = 1.535291183526349
2022-08-29 20:08:13,633 ***** Save model *****
2022-08-29 20:08:18,505 ***** Running evaluation *****
2022-08-29 20:08:18,506   Epoch = 45 iter 14199 step
2022-08-29 20:08:18,506   Num examples = 277
2022-08-29 20:08:18,506   Batch size = 32
2022-08-29 20:08:18,507 ***** Eval results *****
2022-08-29 20:08:18,507   att_loss = 7.2478669320835785
2022-08-29 20:08:18,507   cls_loss = 0.0
2022-08-29 20:08:18,507   global_step = 14199
2022-08-29 20:08:18,507   loss = 9.401079009560977
2022-08-29 20:08:18,507   rep_loss = 2.1532120494281544
2022-08-29 20:08:18,507 ***** Save model *****
2022-08-29 20:08:44,247 ***** Running evaluation *****
2022-08-29 20:08:44,248   Epoch = 27 iter 29399 step
2022-08-29 20:08:44,248   Num examples = 1043
2022-08-29 20:08:44,248   Batch size = 32
2022-08-29 20:08:44,249 ***** Eval results *****
2022-08-29 20:08:44,249   att_loss = 0.7838356967082456
2022-08-29 20:08:44,249   cls_loss = 0.0
2022-08-29 20:08:44,249   global_step = 29399
2022-08-29 20:08:44,249   loss = 2.321765166616355
2022-08-29 20:08:44,249   rep_loss = 1.537929469061152
2022-08-29 20:08:44,250 ***** Save model *****
2022-08-29 20:08:54,113 ***** Running evaluation *****
2022-08-29 20:08:54,113   Epoch = 19 iter 13999 step
2022-08-29 20:08:54,113   Num examples = 1500
2022-08-29 20:08:54,113   Batch size = 32
2022-08-29 20:08:54,115 ***** Eval results *****
2022-08-29 20:08:54,115   att_loss = 2.33574994939382
2022-08-29 20:08:54,115   cls_loss = 0.0
2022-08-29 20:08:54,115   global_step = 13999
2022-08-29 20:08:54,115   loss = 3.861378424641799
2022-08-29 20:08:54,115   rep_loss = 1.5256284819263704
2022-08-29 20:08:54,115 ***** Save model *****
2022-08-29 20:08:56,393 ***** Running evaluation *****
2022-08-29 20:08:56,394   Epoch = 46 iter 14399 step
2022-08-29 20:08:56,394   Num examples = 277
2022-08-29 20:08:56,394   Batch size = 32
2022-08-29 20:08:56,395 ***** Eval results *****
2022-08-29 20:08:56,395   att_loss = 7.041431009128529
2022-08-29 20:08:56,395   cls_loss = 0.0
2022-08-29 20:08:56,395   global_step = 14399
2022-08-29 20:08:56,395   loss = 9.178754529645365
2022-08-29 20:08:56,395   rep_loss = 2.137323542307782
2022-08-29 20:08:56,396 ***** Save model *****
2022-08-29 20:09:21,734 ***** Running evaluation *****
2022-08-29 20:09:21,735   Epoch = 2 iter 36399 step
2022-08-29 20:09:21,735   Num examples = 9815
2022-08-29 20:09:21,735   Batch size = 32
2022-08-29 20:09:21,736 ***** Eval results *****
2022-08-29 20:09:21,736   att_loss = 4.158378238731063
2022-08-29 20:09:21,736   cls_loss = 0.0
2022-08-29 20:09:21,736   global_step = 36399
2022-08-29 20:09:21,736   loss = 5.989280873691391
2022-08-29 20:09:21,736   rep_loss = 1.8309026344475783
2022-08-29 20:09:21,736 ***** Save model *****
2022-08-29 20:09:22,366 ***** Running evaluation *****
2022-08-29 20:09:22,367   Epoch = 27 iter 29599 step
2022-08-29 20:09:22,367   Num examples = 1043
2022-08-29 20:09:22,367   Batch size = 32
2022-08-29 20:09:22,369 ***** Eval results *****
2022-08-29 20:09:22,369   att_loss = 0.7853958778581331
2022-08-29 20:09:22,369   cls_loss = 0.0
2022-08-29 20:09:22,369   global_step = 29599
2022-08-29 20:09:22,369   loss = 2.320669355429953
2022-08-29 20:09:22,369   rep_loss = 1.5352734802278591
2022-08-29 20:09:22,369 ***** Save model *****
2022-08-29 20:09:33,057 ***** Running evaluation *****
2022-08-29 20:09:33,058   Epoch = 19 iter 14199 step
2022-08-29 20:09:33,058   Num examples = 1500
2022-08-29 20:09:33,058   Batch size = 32
2022-08-29 20:09:33,059 ***** Eval results *****
2022-08-29 20:09:33,059   att_loss = 2.312848713710449
2022-08-29 20:09:33,059   cls_loss = 0.0
2022-08-29 20:09:33,059   global_step = 14199
2022-08-29 20:09:33,059   loss = 3.832017475227479
2022-08-29 20:09:33,059   rep_loss = 1.5191687670815568
2022-08-29 20:09:33,059 ***** Save model *****
2022-08-29 20:09:35,401 ***** Running evaluation *****
2022-08-29 20:09:35,402   Epoch = 46 iter 14599 step
2022-08-29 20:09:35,402   Num examples = 277
2022-08-29 20:09:35,402   Batch size = 32
2022-08-29 20:09:35,403 ***** Eval results *****
2022-08-29 20:09:35,403   att_loss = 7.218348704100469
2022-08-29 20:09:35,403   cls_loss = 0.0
2022-08-29 20:09:35,403   global_step = 14599
2022-08-29 20:09:35,403   loss = 9.364727887684168
2022-08-29 20:09:35,404   rep_loss = 2.14637917992198
2022-08-29 20:09:35,404 ***** Save model *****
2022-08-29 20:10:03,039 ***** Running evaluation *****
2022-08-29 20:10:03,039   Epoch = 27 iter 29799 step
2022-08-29 20:10:03,039   Num examples = 1043
2022-08-29 20:10:03,039   Batch size = 32
2022-08-29 20:10:03,040 ***** Eval results *****
2022-08-29 20:10:03,041   att_loss = 0.782993676815944
2022-08-29 20:10:03,041   cls_loss = 0.0
2022-08-29 20:10:03,041   global_step = 29799
2022-08-29 20:10:03,041   loss = 2.316869751934198
2022-08-29 20:10:03,041   rep_loss = 1.5338760749325697
2022-08-29 20:10:03,041 ***** Save model *****
2022-08-29 20:10:11,903 ***** Running evaluation *****
2022-08-29 20:10:11,904   Epoch = 20 iter 14399 step
2022-08-29 20:10:11,904   Num examples = 1500
2022-08-29 20:10:11,904   Batch size = 32
2022-08-29 20:10:11,905 ***** Eval results *****
2022-08-29 20:10:11,905   att_loss = 2.2632901118351865
2022-08-29 20:10:11,905   cls_loss = 0.0
2022-08-29 20:10:11,905   global_step = 14399
2022-08-29 20:10:11,905   loss = 3.7729225280957346
2022-08-29 20:10:11,905   rep_loss = 1.5096324407137358
2022-08-29 20:10:11,905 ***** Save model *****
2022-08-29 20:10:14,317 ***** Running evaluation *****
2022-08-29 20:10:14,318   Epoch = 47 iter 14799 step
2022-08-29 20:10:14,318   Num examples = 277
2022-08-29 20:10:14,318   Batch size = 32
2022-08-29 20:10:14,319 ***** Eval results *****
2022-08-29 20:10:14,319   att_loss = 7.263522349871122
2022-08-29 20:10:14,319   cls_loss = 0.0
2022-08-29 20:10:14,319   global_step = 14799
2022-08-29 20:10:14,319   loss = 9.409060871208107
2022-08-29 20:10:14,319   rep_loss = 2.145538541641864
2022-08-29 20:10:14,319 ***** Save model *****
2022-08-29 20:10:34,159 ***** Running evaluation *****
2022-08-29 20:10:34,160   Epoch = 2 iter 36599 step
2022-08-29 20:10:34,160   Num examples = 9815
2022-08-29 20:10:34,160   Batch size = 32
2022-08-29 20:10:34,161 ***** Eval results *****
2022-08-29 20:10:34,161   att_loss = 4.157994205781434
2022-08-29 20:10:34,161   cls_loss = 0.0
2022-08-29 20:10:34,161   global_step = 36599
2022-08-29 20:10:34,161   loss = 5.988531377250859
2022-08-29 20:10:34,161   rep_loss = 1.8305371710541651
2022-08-29 20:10:34,161 ***** Save model *****
2022-08-29 20:10:41,207 ***** Running evaluation *****
2022-08-29 20:10:41,207   Epoch = 28 iter 29999 step
2022-08-29 20:10:41,207   Num examples = 1043
2022-08-29 20:10:41,207   Batch size = 32
2022-08-29 20:10:41,208 ***** Eval results *****
2022-08-29 20:10:41,208   att_loss = 0.7784649886583027
2022-08-29 20:10:41,209   cls_loss = 0.0
2022-08-29 20:10:41,209   global_step = 29999
2022-08-29 20:10:41,209   loss = 2.3068200073744123
2022-08-29 20:10:41,209   rep_loss = 1.52835501620644
2022-08-29 20:10:41,209 ***** Save model *****
2022-08-29 20:10:50,903 ***** Running evaluation *****
2022-08-29 20:10:50,904   Epoch = 20 iter 14599 step
2022-08-29 20:10:50,904   Num examples = 1500
2022-08-29 20:10:50,904   Batch size = 32
2022-08-29 20:10:50,905 ***** Eval results *****
2022-08-29 20:10:50,905   att_loss = 2.30866047178851
2022-08-29 20:10:50,905   cls_loss = 0.0
2022-08-29 20:10:50,905   global_step = 14599
2022-08-29 20:10:50,905   loss = 3.8230312219723497
2022-08-29 20:10:50,905   rep_loss = 1.5143707486874887
2022-08-29 20:10:50,906 ***** Save model *****
2022-08-29 20:10:53,182 ***** Running evaluation *****
2022-08-29 20:10:53,183   Epoch = 48 iter 14999 step
2022-08-29 20:10:53,183   Num examples = 277
2022-08-29 20:10:53,183   Batch size = 32
2022-08-29 20:10:53,184 ***** Eval results *****
2022-08-29 20:10:53,184   att_loss = 7.107489861233134
2022-08-29 20:10:53,184   cls_loss = 0.0
2022-08-29 20:10:53,184   global_step = 14999
2022-08-29 20:10:53,185   loss = 9.232682906406026
2022-08-29 20:10:53,185   rep_loss = 2.1251930401358807
2022-08-29 20:10:53,185 ***** Save model *****
2022-08-29 20:11:21,254 ***** Running evaluation *****
2022-08-29 20:11:21,255   Epoch = 28 iter 30199 step
2022-08-29 20:11:21,255   Num examples = 1043
2022-08-29 20:11:21,255   Batch size = 32
2022-08-29 20:11:21,256 ***** Eval results *****
2022-08-29 20:11:21,257   att_loss = 0.7873263284311456
2022-08-29 20:11:21,257   cls_loss = 0.0
2022-08-29 20:11:21,257   global_step = 30199
2022-08-29 20:11:21,257   loss = 2.3187438661769284
2022-08-29 20:11:21,257   rep_loss = 1.5314175343109389
2022-08-29 20:11:21,257 ***** Save model *****
2022-08-29 20:11:38,040 ***** Running evaluation *****
2022-08-29 20:11:38,041   Epoch = 48 iter 15199 step
2022-08-29 20:11:38,041   Num examples = 277
2022-08-29 20:11:38,041   Batch size = 32
2022-08-29 20:11:38,042 ***** Eval results *****
2022-08-29 20:11:38,042   att_loss = 7.19954638551522
2022-08-29 20:11:38,042   cls_loss = 0.0
2022-08-29 20:11:38,042   global_step = 15199
2022-08-29 20:11:38,042   loss = 9.335641220486911
2022-08-29 20:11:38,042   rep_loss = 2.1360948556463657
2022-08-29 20:11:38,042 ***** Save model *****
2022-08-29 20:11:40,036 ***** Running evaluation *****
2022-08-29 20:11:40,037   Epoch = 20 iter 14799 step
2022-08-29 20:11:40,037   Num examples = 1500
2022-08-29 20:11:40,037   Batch size = 32
2022-08-29 20:11:40,038 ***** Eval results *****
2022-08-29 20:11:40,038   att_loss = 2.2999633015695626
2022-08-29 20:11:40,038   cls_loss = 0.0
2022-08-29 20:11:40,038   global_step = 14799
2022-08-29 20:11:40,038   loss = 3.812001324460284
2022-08-29 20:11:40,038   rep_loss = 1.5120380250630998
2022-08-29 20:11:40,038 ***** Save model *****
2022-08-29 20:11:45,285 ***** Running evaluation *****
2022-08-29 20:11:45,285   Epoch = 2 iter 36799 step
2022-08-29 20:11:45,285   Num examples = 9815
2022-08-29 20:11:45,285   Batch size = 32
2022-08-29 20:11:45,286 ***** Eval results *****
2022-08-29 20:11:45,286   att_loss = 4.156279491816083
2022-08-29 20:11:45,286   cls_loss = 0.0
2022-08-29 20:11:45,287   global_step = 36799
2022-08-29 20:11:45,287   loss = 5.986437314951547
2022-08-29 20:11:45,287   rep_loss = 1.8301578229020448
2022-08-29 20:11:45,287 ***** Save model *****
2022-08-29 20:11:59,412 ***** Running evaluation *****
2022-08-29 20:11:59,413   Epoch = 28 iter 30399 step
2022-08-29 20:11:59,413   Num examples = 1043
2022-08-29 20:11:59,413   Batch size = 32
2022-08-29 20:11:59,414 ***** Eval results *****
2022-08-29 20:11:59,414   att_loss = 0.7857522510518932
2022-08-29 20:11:59,414   cls_loss = 0.0
2022-08-29 20:11:59,414   global_step = 30399
2022-08-29 20:11:59,414   loss = 2.316244340424586
2022-08-29 20:11:59,414   rep_loss = 1.530492086362357
2022-08-29 20:11:59,414 ***** Save model *****
2022-08-29 20:12:16,112 ***** Running evaluation *****
2022-08-29 20:12:16,112   Epoch = 49 iter 15399 step
2022-08-29 20:12:16,112   Num examples = 277
2022-08-29 20:12:16,112   Batch size = 32
2022-08-29 20:12:16,114 ***** Eval results *****
2022-08-29 20:12:16,114   att_loss = 7.277891528606415
2022-08-29 20:12:16,114   cls_loss = 0.0
2022-08-29 20:12:16,114   global_step = 15399
2022-08-29 20:12:16,114   loss = 9.417852625250816
2022-08-29 20:12:16,114   rep_loss = 2.139961099624634
2022-08-29 20:12:16,114 ***** Save model *****
2022-08-29 20:12:20,159 ***** Running evaluation *****
2022-08-29 20:12:20,160   Epoch = 20 iter 14999 step
2022-08-29 20:12:20,160   Num examples = 1500
2022-08-29 20:12:20,160   Batch size = 32
2022-08-29 20:12:20,161 ***** Eval results *****
2022-08-29 20:12:20,161   att_loss = 2.2646028592552936
2022-08-29 20:12:20,161   cls_loss = 0.0
2022-08-29 20:12:20,162   global_step = 14999
2022-08-29 20:12:20,162   loss = 3.7718366151311216
2022-08-29 20:12:20,162   rep_loss = 1.5072337568086078
2022-08-29 20:12:20,162 ***** Save model *****
2022-08-29 20:12:39,224 ***** Running evaluation *****
2022-08-29 20:12:39,225   Epoch = 28 iter 30599 step
2022-08-29 20:12:39,225   Num examples = 1043
2022-08-29 20:12:39,225   Batch size = 32
2022-08-29 20:12:39,226 ***** Eval results *****
2022-08-29 20:12:39,226   att_loss = 0.7820400762043411
2022-08-29 20:12:39,227   cls_loss = 0.0
2022-08-29 20:12:39,227   global_step = 30599
2022-08-29 20:12:39,227   loss = 2.3106380160763966
2022-08-29 20:12:39,227   rep_loss = 1.528597940129342
2022-08-29 20:12:39,227 ***** Save model *****
2022-08-29 20:12:47,735 The args: Namespace(aug_train=False, cache_dir='', data_dir='glue_data/RTE', data_url='', do_eval=False, do_lower_case=True, eval_batch_size=32, eval_step=100, gradient_accumulation_steps=1, hidden_act='quad', learning_rate=1e-05, log_path='tmp/distill/RTE/quad_2quad/bert-base-uncased/log.txt', max_seq_length=128, no_cuda=False, num_train_epochs=5.0, output_dir='tmp/distill/RTE/quad_2quad/bert-base-uncased/5e-05_1e-05_8_stage2', pred_distill=True, seed=42, softmax_act='2quad', student_model='tmp/distill/RTE/quad_2quad/bert-base-uncased/5e-05_1e-05_8', task_name='RTE', teacher_model='/home/ubuntu/checkpoints/exp/RTE', temperature=1.0, train_batch_size=8, warmup_proportion=0.1, weight_decay=0.0001)
2022-08-29 20:12:47,736 device: cuda n_gpu: 1
2022-08-29 20:12:47,824 Writing example 0 of 2490
2022-08-29 20:12:47,824 *** Example ***
2022-08-29 20:12:47,825 guid: train-0
2022-08-29 20:12:47,825 tokens: [CLS] no weapons of mass destruction found in iraq yet . [SEP] weapons of mass destruction found in iraq . [SEP]
2022-08-29 20:12:47,825 input_ids: 101 2053 4255 1997 3742 6215 2179 1999 5712 2664 1012 102 4255 1997 3742 6215 2179 1999 5712 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-08-29 20:12:47,825 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-08-29 20:12:47,825 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-08-29 20:12:47,825 label: not_entailment
2022-08-29 20:12:47,825 label_id: 1
2022-08-29 20:12:50,535 Writing example 0 of 277
2022-08-29 20:12:50,536 *** Example ***
2022-08-29 20:12:50,536 guid: dev-0
2022-08-29 20:12:50,536 tokens: [CLS] dana reeve , the widow of the actor christopher reeve , has died of lung cancer at age 44 , according to the christopher reeve foundation . [SEP] christopher reeve had an accident . [SEP]
2022-08-29 20:12:50,536 input_ids: 101 11271 20726 1010 1996 7794 1997 1996 3364 5696 20726 1010 2038 2351 1997 11192 4456 2012 2287 4008 1010 2429 2000 1996 5696 20726 3192 1012 102 5696 20726 2018 2019 4926 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-08-29 20:12:50,536 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-08-29 20:12:50,536 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-08-29 20:12:50,536 label: not_entailment
2022-08-29 20:12:50,536 label_id: 1
2022-08-29 20:12:50,826 Model config {
  "_name_or_path": "bert-base-uncased",
  "architectures": [
    "BertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "crypten": false,
  "finetuning_task": "rte",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "entailment",
    "1": "not_entailment"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "entailment": 0,
    "not_entailment": 1
  },
  "layer_norm_eps": 1e-12,
  "log_path": "tmp/distill/RTE/quad_2quad/bert-base-uncased/log.txt",
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "pre_trained": "",
  "problem_type": "single_label_classification",
  "softmax_act": "softmax",
  "torch_dtype": "float32",
  "training": "",
  "transformers_version": "4.20.0.dev0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

2022-08-29 20:12:53,524 Loading model /home/ubuntu/checkpoints/exp/RTE/pytorch_model.bin
2022-08-29 20:12:53,832 loading model...
2022-08-29 20:12:53,889 done!
2022-08-29 20:12:53,889 Weights from pretrained model not used in TinyBertForSequenceClassification: ['bert.embeddings.position_ids']
2022-08-29 20:12:56,685 ***** Running evaluation *****
2022-08-29 20:12:56,685   Epoch = 3 iter 36999 step
2022-08-29 20:12:56,685   Num examples = 9815
2022-08-29 20:12:56,686   Batch size = 32
2022-08-29 20:12:56,687 ***** Eval results *****
2022-08-29 20:12:56,687   att_loss = 4.0458386816004275
2022-08-29 20:12:56,687   cls_loss = 0.0
2022-08-29 20:12:56,687   global_step = 36999
2022-08-29 20:12:56,687   loss = 5.8497970411854405
2022-08-29 20:12:56,687   rep_loss = 1.8039583704804862
2022-08-29 20:12:56,687 ***** Save model *****
2022-08-29 20:12:58,395 ***** Teacher evaluation *****
2022-08-29 20:12:58,395 {'acc': 0.6967509025270758, 'eval_loss': 0.7074194153149923}
2022-08-29 20:12:58,396 Model config {
  "_name_or_path": "bert-base-uncased",
  "architectures": [
    "BertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "crypten": false,
  "finetuning_task": "rte",
  "gradient_checkpointing": false,
  "hidden_act": "quad",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "entailment",
    "1": "not_entailment"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "entailment": 0,
    "not_entailment": 1
  },
  "layer_norm_eps": 1e-12,
  "log_path": "tmp/distill/RTE/quad_2quad/bert-base-uncased/log.txt",
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "pre_trained": "",
  "problem_type": "single_label_classification",
  "softmax_act": "2quad",
  "torch_dtype": "float32",
  "training": "",
  "transformers_version": "4.20.0.dev0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

2022-08-29 20:12:59,115 ***** Running evaluation *****
2022-08-29 20:12:59,116   Epoch = 21 iter 15199 step
2022-08-29 20:12:59,116   Num examples = 1500
2022-08-29 20:12:59,116   Batch size = 32
2022-08-29 20:12:59,117 ***** Eval results *****
2022-08-29 20:12:59,117   att_loss = 2.325672593983737
2022-08-29 20:12:59,117   cls_loss = 0.0
2022-08-29 20:12:59,117   global_step = 15199
2022-08-29 20:12:59,117   loss = 3.8338287645135045
2022-08-29 20:12:59,117   rep_loss = 1.5081561912189831
2022-08-29 20:12:59,118 ***** Save model *****
2022-08-29 20:13:01,034 Loading model tmp/distill/RTE/quad_2quad/bert-base-uncased/5e-05_1e-05_8/pytorch_model.bin
2022-08-29 20:13:01,329 loading model...
2022-08-29 20:13:01,376 done!
2022-08-29 20:13:01,458 ***** Running training *****
2022-08-29 20:13:01,458   Num examples = 2490
2022-08-29 20:13:01,459   Batch size = 8
2022-08-29 20:13:01,459   Num steps = 1555
2022-08-29 20:13:01,460 n: bert.embeddings.word_embeddings.weight
2022-08-29 20:13:01,460 n: bert.embeddings.position_embeddings.weight
2022-08-29 20:13:01,460 n: bert.embeddings.token_type_embeddings.weight
2022-08-29 20:13:01,460 n: bert.embeddings.LayerNorm.weight
2022-08-29 20:13:01,460 n: bert.embeddings.LayerNorm.bias
2022-08-29 20:13:01,460 n: bert.encoder.layer.0.attention.self.query.weight
2022-08-29 20:13:01,460 n: bert.encoder.layer.0.attention.self.query.bias
2022-08-29 20:13:01,460 n: bert.encoder.layer.0.attention.self.key.weight
2022-08-29 20:13:01,460 n: bert.encoder.layer.0.attention.self.key.bias
2022-08-29 20:13:01,460 n: bert.encoder.layer.0.attention.self.value.weight
2022-08-29 20:13:01,460 n: bert.encoder.layer.0.attention.self.value.bias
2022-08-29 20:13:01,460 n: bert.encoder.layer.0.attention.output.dense.weight
2022-08-29 20:13:01,460 n: bert.encoder.layer.0.attention.output.dense.bias
2022-08-29 20:13:01,460 n: bert.encoder.layer.0.attention.output.LayerNorm.weight
2022-08-29 20:13:01,461 n: bert.encoder.layer.0.attention.output.LayerNorm.bias
2022-08-29 20:13:01,461 n: bert.encoder.layer.0.intermediate.dense.weight
2022-08-29 20:13:01,461 n: bert.encoder.layer.0.intermediate.dense.bias
2022-08-29 20:13:01,461 n: bert.encoder.layer.0.output.dense.weight
2022-08-29 20:13:01,461 n: bert.encoder.layer.0.output.dense.bias
2022-08-29 20:13:01,461 n: bert.encoder.layer.0.output.LayerNorm.weight
2022-08-29 20:13:01,461 n: bert.encoder.layer.0.output.LayerNorm.bias
2022-08-29 20:13:01,461 n: bert.encoder.layer.1.attention.self.query.weight
2022-08-29 20:13:01,461 n: bert.encoder.layer.1.attention.self.query.bias
2022-08-29 20:13:01,461 n: bert.encoder.layer.1.attention.self.key.weight
2022-08-29 20:13:01,461 n: bert.encoder.layer.1.attention.self.key.bias
2022-08-29 20:13:01,461 n: bert.encoder.layer.1.attention.self.value.weight
2022-08-29 20:13:01,461 n: bert.encoder.layer.1.attention.self.value.bias
2022-08-29 20:13:01,461 n: bert.encoder.layer.1.attention.output.dense.weight
2022-08-29 20:13:01,461 n: bert.encoder.layer.1.attention.output.dense.bias
2022-08-29 20:13:01,461 n: bert.encoder.layer.1.attention.output.LayerNorm.weight
2022-08-29 20:13:01,461 n: bert.encoder.layer.1.attention.output.LayerNorm.bias
2022-08-29 20:13:01,462 n: bert.encoder.layer.1.intermediate.dense.weight
2022-08-29 20:13:01,462 n: bert.encoder.layer.1.intermediate.dense.bias
2022-08-29 20:13:01,462 n: bert.encoder.layer.1.output.dense.weight
2022-08-29 20:13:01,462 n: bert.encoder.layer.1.output.dense.bias
2022-08-29 20:13:01,462 n: bert.encoder.layer.1.output.LayerNorm.weight
2022-08-29 20:13:01,462 n: bert.encoder.layer.1.output.LayerNorm.bias
2022-08-29 20:13:01,462 n: bert.encoder.layer.2.attention.self.query.weight
2022-08-29 20:13:01,462 n: bert.encoder.layer.2.attention.self.query.bias
2022-08-29 20:13:01,462 n: bert.encoder.layer.2.attention.self.key.weight
2022-08-29 20:13:01,462 n: bert.encoder.layer.2.attention.self.key.bias
2022-08-29 20:13:01,462 n: bert.encoder.layer.2.attention.self.value.weight
2022-08-29 20:13:01,462 n: bert.encoder.layer.2.attention.self.value.bias
2022-08-29 20:13:01,462 n: bert.encoder.layer.2.attention.output.dense.weight
2022-08-29 20:13:01,462 n: bert.encoder.layer.2.attention.output.dense.bias
2022-08-29 20:13:01,462 n: bert.encoder.layer.2.attention.output.LayerNorm.weight
2022-08-29 20:13:01,462 n: bert.encoder.layer.2.attention.output.LayerNorm.bias
2022-08-29 20:13:01,463 n: bert.encoder.layer.2.intermediate.dense.weight
2022-08-29 20:13:01,463 n: bert.encoder.layer.2.intermediate.dense.bias
2022-08-29 20:13:01,463 n: bert.encoder.layer.2.output.dense.weight
2022-08-29 20:13:01,463 n: bert.encoder.layer.2.output.dense.bias
2022-08-29 20:13:01,463 n: bert.encoder.layer.2.output.LayerNorm.weight
2022-08-29 20:13:01,463 n: bert.encoder.layer.2.output.LayerNorm.bias
2022-08-29 20:13:01,463 n: bert.encoder.layer.3.attention.self.query.weight
2022-08-29 20:13:01,463 n: bert.encoder.layer.3.attention.self.query.bias
2022-08-29 20:13:01,463 n: bert.encoder.layer.3.attention.self.key.weight
2022-08-29 20:13:01,463 n: bert.encoder.layer.3.attention.self.key.bias
2022-08-29 20:13:01,463 n: bert.encoder.layer.3.attention.self.value.weight
2022-08-29 20:13:01,463 n: bert.encoder.layer.3.attention.self.value.bias
2022-08-29 20:13:01,463 n: bert.encoder.layer.3.attention.output.dense.weight
2022-08-29 20:13:01,463 n: bert.encoder.layer.3.attention.output.dense.bias
2022-08-29 20:13:01,463 n: bert.encoder.layer.3.attention.output.LayerNorm.weight
2022-08-29 20:13:01,463 n: bert.encoder.layer.3.attention.output.LayerNorm.bias
2022-08-29 20:13:01,463 n: bert.encoder.layer.3.intermediate.dense.weight
2022-08-29 20:13:01,464 n: bert.encoder.layer.3.intermediate.dense.bias
2022-08-29 20:13:01,464 n: bert.encoder.layer.3.output.dense.weight
2022-08-29 20:13:01,464 n: bert.encoder.layer.3.output.dense.bias
2022-08-29 20:13:01,464 n: bert.encoder.layer.3.output.LayerNorm.weight
2022-08-29 20:13:01,464 n: bert.encoder.layer.3.output.LayerNorm.bias
2022-08-29 20:13:01,464 n: bert.encoder.layer.4.attention.self.query.weight
2022-08-29 20:13:01,464 n: bert.encoder.layer.4.attention.self.query.bias
2022-08-29 20:13:01,464 n: bert.encoder.layer.4.attention.self.key.weight
2022-08-29 20:13:01,464 n: bert.encoder.layer.4.attention.self.key.bias
2022-08-29 20:13:01,464 n: bert.encoder.layer.4.attention.self.value.weight
2022-08-29 20:13:01,464 n: bert.encoder.layer.4.attention.self.value.bias
2022-08-29 20:13:01,464 n: bert.encoder.layer.4.attention.output.dense.weight
2022-08-29 20:13:01,464 n: bert.encoder.layer.4.attention.output.dense.bias
2022-08-29 20:13:01,464 n: bert.encoder.layer.4.attention.output.LayerNorm.weight
2022-08-29 20:13:01,464 n: bert.encoder.layer.4.attention.output.LayerNorm.bias
2022-08-29 20:13:01,464 n: bert.encoder.layer.4.intermediate.dense.weight
2022-08-29 20:13:01,464 n: bert.encoder.layer.4.intermediate.dense.bias
2022-08-29 20:13:01,465 n: bert.encoder.layer.4.output.dense.weight
2022-08-29 20:13:01,465 n: bert.encoder.layer.4.output.dense.bias
2022-08-29 20:13:01,465 n: bert.encoder.layer.4.output.LayerNorm.weight
2022-08-29 20:13:01,465 n: bert.encoder.layer.4.output.LayerNorm.bias
2022-08-29 20:13:01,465 n: bert.encoder.layer.5.attention.self.query.weight
2022-08-29 20:13:01,465 n: bert.encoder.layer.5.attention.self.query.bias
2022-08-29 20:13:01,465 n: bert.encoder.layer.5.attention.self.key.weight
2022-08-29 20:13:01,465 n: bert.encoder.layer.5.attention.self.key.bias
2022-08-29 20:13:01,465 n: bert.encoder.layer.5.attention.self.value.weight
2022-08-29 20:13:01,465 n: bert.encoder.layer.5.attention.self.value.bias
2022-08-29 20:13:01,465 n: bert.encoder.layer.5.attention.output.dense.weight
2022-08-29 20:13:01,465 n: bert.encoder.layer.5.attention.output.dense.bias
2022-08-29 20:13:01,465 n: bert.encoder.layer.5.attention.output.LayerNorm.weight
2022-08-29 20:13:01,465 n: bert.encoder.layer.5.attention.output.LayerNorm.bias
2022-08-29 20:13:01,465 n: bert.encoder.layer.5.intermediate.dense.weight
2022-08-29 20:13:01,465 n: bert.encoder.layer.5.intermediate.dense.bias
2022-08-29 20:13:01,465 n: bert.encoder.layer.5.output.dense.weight
2022-08-29 20:13:01,466 n: bert.encoder.layer.5.output.dense.bias
2022-08-29 20:13:01,466 n: bert.encoder.layer.5.output.LayerNorm.weight
2022-08-29 20:13:01,466 n: bert.encoder.layer.5.output.LayerNorm.bias
2022-08-29 20:13:01,466 n: bert.encoder.layer.6.attention.self.query.weight
2022-08-29 20:13:01,466 n: bert.encoder.layer.6.attention.self.query.bias
2022-08-29 20:13:01,466 n: bert.encoder.layer.6.attention.self.key.weight
2022-08-29 20:13:01,466 n: bert.encoder.layer.6.attention.self.key.bias
2022-08-29 20:13:01,466 n: bert.encoder.layer.6.attention.self.value.weight
2022-08-29 20:13:01,466 n: bert.encoder.layer.6.attention.self.value.bias
2022-08-29 20:13:01,466 n: bert.encoder.layer.6.attention.output.dense.weight
2022-08-29 20:13:01,466 n: bert.encoder.layer.6.attention.output.dense.bias
2022-08-29 20:13:01,466 n: bert.encoder.layer.6.attention.output.LayerNorm.weight
2022-08-29 20:13:01,466 n: bert.encoder.layer.6.attention.output.LayerNorm.bias
2022-08-29 20:13:01,466 n: bert.encoder.layer.6.intermediate.dense.weight
2022-08-29 20:13:01,466 n: bert.encoder.layer.6.intermediate.dense.bias
2022-08-29 20:13:01,466 n: bert.encoder.layer.6.output.dense.weight
2022-08-29 20:13:01,466 n: bert.encoder.layer.6.output.dense.bias
2022-08-29 20:13:01,467 n: bert.encoder.layer.6.output.LayerNorm.weight
2022-08-29 20:13:01,467 n: bert.encoder.layer.6.output.LayerNorm.bias
2022-08-29 20:13:01,467 n: bert.encoder.layer.7.attention.self.query.weight
2022-08-29 20:13:01,467 n: bert.encoder.layer.7.attention.self.query.bias
2022-08-29 20:13:01,467 n: bert.encoder.layer.7.attention.self.key.weight
2022-08-29 20:13:01,467 n: bert.encoder.layer.7.attention.self.key.bias
2022-08-29 20:13:01,467 n: bert.encoder.layer.7.attention.self.value.weight
2022-08-29 20:13:01,467 n: bert.encoder.layer.7.attention.self.value.bias
2022-08-29 20:13:01,467 n: bert.encoder.layer.7.attention.output.dense.weight
2022-08-29 20:13:01,467 n: bert.encoder.layer.7.attention.output.dense.bias
2022-08-29 20:13:01,467 n: bert.encoder.layer.7.attention.output.LayerNorm.weight
2022-08-29 20:13:01,467 n: bert.encoder.layer.7.attention.output.LayerNorm.bias
2022-08-29 20:13:01,467 n: bert.encoder.layer.7.intermediate.dense.weight
2022-08-29 20:13:01,467 n: bert.encoder.layer.7.intermediate.dense.bias
2022-08-29 20:13:01,467 n: bert.encoder.layer.7.output.dense.weight
2022-08-29 20:13:01,467 n: bert.encoder.layer.7.output.dense.bias
2022-08-29 20:13:01,467 n: bert.encoder.layer.7.output.LayerNorm.weight
2022-08-29 20:13:01,468 n: bert.encoder.layer.7.output.LayerNorm.bias
2022-08-29 20:13:01,468 n: bert.encoder.layer.8.attention.self.query.weight
2022-08-29 20:13:01,468 n: bert.encoder.layer.8.attention.self.query.bias
2022-08-29 20:13:01,468 n: bert.encoder.layer.8.attention.self.key.weight
2022-08-29 20:13:01,468 n: bert.encoder.layer.8.attention.self.key.bias
2022-08-29 20:13:01,468 n: bert.encoder.layer.8.attention.self.value.weight
2022-08-29 20:13:01,468 n: bert.encoder.layer.8.attention.self.value.bias
2022-08-29 20:13:01,468 n: bert.encoder.layer.8.attention.output.dense.weight
2022-08-29 20:13:01,468 n: bert.encoder.layer.8.attention.output.dense.bias
2022-08-29 20:13:01,468 n: bert.encoder.layer.8.attention.output.LayerNorm.weight
2022-08-29 20:13:01,468 n: bert.encoder.layer.8.attention.output.LayerNorm.bias
2022-08-29 20:13:01,468 n: bert.encoder.layer.8.intermediate.dense.weight
2022-08-29 20:13:01,468 n: bert.encoder.layer.8.intermediate.dense.bias
2022-08-29 20:13:01,468 n: bert.encoder.layer.8.output.dense.weight
2022-08-29 20:13:01,468 n: bert.encoder.layer.8.output.dense.bias
2022-08-29 20:13:01,468 n: bert.encoder.layer.8.output.LayerNorm.weight
2022-08-29 20:13:01,468 n: bert.encoder.layer.8.output.LayerNorm.bias
2022-08-29 20:13:01,469 n: bert.encoder.layer.9.attention.self.query.weight
2022-08-29 20:13:01,469 n: bert.encoder.layer.9.attention.self.query.bias
2022-08-29 20:13:01,469 n: bert.encoder.layer.9.attention.self.key.weight
2022-08-29 20:13:01,469 n: bert.encoder.layer.9.attention.self.key.bias
2022-08-29 20:13:01,469 n: bert.encoder.layer.9.attention.self.value.weight
2022-08-29 20:13:01,469 n: bert.encoder.layer.9.attention.self.value.bias
2022-08-29 20:13:01,469 n: bert.encoder.layer.9.attention.output.dense.weight
2022-08-29 20:13:01,469 n: bert.encoder.layer.9.attention.output.dense.bias
2022-08-29 20:13:01,469 n: bert.encoder.layer.9.attention.output.LayerNorm.weight
2022-08-29 20:13:01,469 n: bert.encoder.layer.9.attention.output.LayerNorm.bias
2022-08-29 20:13:01,469 n: bert.encoder.layer.9.intermediate.dense.weight
2022-08-29 20:13:01,469 n: bert.encoder.layer.9.intermediate.dense.bias
2022-08-29 20:13:01,469 n: bert.encoder.layer.9.output.dense.weight
2022-08-29 20:13:01,469 n: bert.encoder.layer.9.output.dense.bias
2022-08-29 20:13:01,469 n: bert.encoder.layer.9.output.LayerNorm.weight
2022-08-29 20:13:01,469 n: bert.encoder.layer.9.output.LayerNorm.bias
2022-08-29 20:13:01,469 n: bert.encoder.layer.10.attention.self.query.weight
2022-08-29 20:13:01,470 n: bert.encoder.layer.10.attention.self.query.bias
2022-08-29 20:13:01,470 n: bert.encoder.layer.10.attention.self.key.weight
2022-08-29 20:13:01,470 n: bert.encoder.layer.10.attention.self.key.bias
2022-08-29 20:13:01,470 n: bert.encoder.layer.10.attention.self.value.weight
2022-08-29 20:13:01,470 n: bert.encoder.layer.10.attention.self.value.bias
2022-08-29 20:13:01,470 n: bert.encoder.layer.10.attention.output.dense.weight
2022-08-29 20:13:01,470 n: bert.encoder.layer.10.attention.output.dense.bias
2022-08-29 20:13:01,470 n: bert.encoder.layer.10.attention.output.LayerNorm.weight
2022-08-29 20:13:01,470 n: bert.encoder.layer.10.attention.output.LayerNorm.bias
2022-08-29 20:13:01,470 n: bert.encoder.layer.10.intermediate.dense.weight
2022-08-29 20:13:01,470 n: bert.encoder.layer.10.intermediate.dense.bias
2022-08-29 20:13:01,470 n: bert.encoder.layer.10.output.dense.weight
2022-08-29 20:13:01,470 n: bert.encoder.layer.10.output.dense.bias
2022-08-29 20:13:01,470 n: bert.encoder.layer.10.output.LayerNorm.weight
2022-08-29 20:13:01,470 n: bert.encoder.layer.10.output.LayerNorm.bias
2022-08-29 20:13:01,470 n: bert.encoder.layer.11.attention.self.query.weight
2022-08-29 20:13:01,470 n: bert.encoder.layer.11.attention.self.query.bias
2022-08-29 20:13:01,471 n: bert.encoder.layer.11.attention.self.key.weight
2022-08-29 20:13:01,471 n: bert.encoder.layer.11.attention.self.key.bias
2022-08-29 20:13:01,471 n: bert.encoder.layer.11.attention.self.value.weight
2022-08-29 20:13:01,471 n: bert.encoder.layer.11.attention.self.value.bias
2022-08-29 20:13:01,471 n: bert.encoder.layer.11.attention.output.dense.weight
2022-08-29 20:13:01,471 n: bert.encoder.layer.11.attention.output.dense.bias
2022-08-29 20:13:01,471 n: bert.encoder.layer.11.attention.output.LayerNorm.weight
2022-08-29 20:13:01,471 n: bert.encoder.layer.11.attention.output.LayerNorm.bias
2022-08-29 20:13:01,471 n: bert.encoder.layer.11.intermediate.dense.weight
2022-08-29 20:13:01,471 n: bert.encoder.layer.11.intermediate.dense.bias
2022-08-29 20:13:01,471 n: bert.encoder.layer.11.output.dense.weight
2022-08-29 20:13:01,471 n: bert.encoder.layer.11.output.dense.bias
2022-08-29 20:13:01,471 n: bert.encoder.layer.11.output.LayerNorm.weight
2022-08-29 20:13:01,471 n: bert.encoder.layer.11.output.LayerNorm.bias
2022-08-29 20:13:01,471 n: bert.pooler.dense.weight
2022-08-29 20:13:01,471 n: bert.pooler.dense.bias
2022-08-29 20:13:01,471 n: classifier.weight
2022-08-29 20:13:01,472 n: classifier.bias
2022-08-29 20:13:01,472 Total parameters: 109483778
2022-08-29 20:13:19,147 ***** Running evaluation *****
2022-08-29 20:13:19,147   Epoch = 0 iter 99 step
2022-08-29 20:13:19,147   Num examples = 277
2022-08-29 20:13:19,147   Batch size = 32
2022-08-29 20:13:19,565 ***** Running evaluation *****
2022-08-29 20:13:19,565   Epoch = 28 iter 30799 step
2022-08-29 20:13:19,565   Num examples = 1043
2022-08-29 20:13:19,565   Batch size = 32
2022-08-29 20:13:19,567 ***** Eval results *****
2022-08-29 20:13:19,567   att_loss = 0.7807616501547104
2022-08-29 20:13:19,567   cls_loss = 0.0
2022-08-29 20:13:19,567   global_step = 30799
2022-08-29 20:13:19,567   loss = 2.3104279619355443
2022-08-29 20:13:19,567   rep_loss = 1.52966631364556
2022-08-29 20:13:19,567 ***** Save model *****
2022-08-29 20:13:19,847 ***** Eval results *****
2022-08-29 20:13:19,847   acc = 0.592057761732852
2022-08-29 20:13:19,847   att_loss = 0.0
2022-08-29 20:13:19,847   cls_loss = 0.2286366130215953
2022-08-29 20:13:19,847   eval_loss = 0.689516113864051
2022-08-29 20:13:19,847   global_step = 99
2022-08-29 20:13:19,847   loss = 0.2286366130215953
2022-08-29 20:13:19,847   rep_loss = 0.0
2022-08-29 20:13:19,847 ***** Save model *****
2022-08-29 20:13:38,332 ***** Running evaluation *****
2022-08-29 20:13:38,332   Epoch = 0 iter 199 step
2022-08-29 20:13:38,332   Num examples = 277
2022-08-29 20:13:38,332   Batch size = 32
2022-08-29 20:13:38,624 ***** Running evaluation *****
2022-08-29 20:13:38,625   Epoch = 21 iter 15399 step
2022-08-29 20:13:38,625   Num examples = 1500
2022-08-29 20:13:38,625   Batch size = 32
2022-08-29 20:13:38,626 ***** Eval results *****
2022-08-29 20:13:38,626   att_loss = 2.2855169635695463
2022-08-29 20:13:38,627   cls_loss = 0.0
2022-08-29 20:13:38,627   global_step = 15399
2022-08-29 20:13:38,627   loss = 3.7892616352188253
2022-08-29 20:13:38,627   rep_loss = 1.5037446809334918
2022-08-29 20:13:38,627 ***** Save model *****
2022-08-29 20:13:39,032 ***** Eval results *****
2022-08-29 20:13:39,032   acc = 0.631768953068592
2022-08-29 20:13:39,032   att_loss = 0.0
2022-08-29 20:13:39,032   cls_loss = 0.25475415351552577
2022-08-29 20:13:39,032   eval_loss = 0.677458233303494
2022-08-29 20:13:39,032   global_step = 199
2022-08-29 20:13:39,032   loss = 0.25475415351552577
2022-08-29 20:13:39,032   rep_loss = 0.0
2022-08-29 20:13:39,032 ***** Save model *****
2022-08-29 20:13:59,393 ***** Running evaluation *****
2022-08-29 20:13:59,394   Epoch = 29 iter 30999 step
2022-08-29 20:13:59,394   Num examples = 1043
2022-08-29 20:13:59,394   Batch size = 32
2022-08-29 20:13:59,395 ***** Eval results *****
2022-08-29 20:13:59,395   att_loss = 0.8186671579325641
2022-08-29 20:13:59,395   cls_loss = 0.0
2022-08-29 20:13:59,395   global_step = 30999
2022-08-29 20:13:59,395   loss = 2.358618073993259
2022-08-29 20:13:59,395   rep_loss = 1.5399509094379566
2022-08-29 20:13:59,396 ***** Save model *****
2022-08-29 20:14:02,871 ***** Running evaluation *****
2022-08-29 20:14:02,871   Epoch = 0 iter 299 step
2022-08-29 20:14:02,871   Num examples = 277
2022-08-29 20:14:02,871   Batch size = 32
2022-08-29 20:14:03,569 ***** Eval results *****
2022-08-29 20:14:03,569   acc = 0.628158844765343
2022-08-29 20:14:03,569   att_loss = 0.0
2022-08-29 20:14:03,569   cls_loss = 0.25033032492949414
2022-08-29 20:14:03,570   eval_loss = 0.7121575011147393
2022-08-29 20:14:03,570   global_step = 299
2022-08-29 20:14:03,570   loss = 0.25033032492949414
2022-08-29 20:14:03,570   rep_loss = 0.0
2022-08-29 20:14:05,908 ***** Running evaluation *****
2022-08-29 20:14:05,908   Epoch = 3 iter 37199 step
2022-08-29 20:14:05,908   Num examples = 9815
2022-08-29 20:14:05,908   Batch size = 32
2022-08-29 20:14:05,909 ***** Eval results *****
2022-08-29 20:14:05,910   att_loss = 4.047901134416847
2022-08-29 20:14:05,910   cls_loss = 0.0
2022-08-29 20:14:05,910   global_step = 37199
2022-08-29 20:14:05,910   loss = 5.851321924535721
2022-08-29 20:14:05,910   rep_loss = 1.8034207913542042
2022-08-29 20:14:05,910 ***** Save model *****
2022-08-29 20:14:19,157 ***** Running evaluation *****
2022-08-29 20:14:19,157   Epoch = 21 iter 15599 step
2022-08-29 20:14:19,157   Num examples = 1500
2022-08-29 20:14:19,158   Batch size = 32
2022-08-29 20:14:19,159 ***** Eval results *****
2022-08-29 20:14:19,159   att_loss = 2.253655299115318
2022-08-29 20:14:19,159   cls_loss = 0.0
2022-08-29 20:14:19,159   global_step = 15599
2022-08-29 20:14:19,159   loss = 3.753313144200594
2022-08-29 20:14:19,159   rep_loss = 1.499657849203831
2022-08-29 20:14:19,159 ***** Save model *****
2022-08-29 20:14:21,375 ***** Running evaluation *****
2022-08-29 20:14:21,375   Epoch = 1 iter 399 step
2022-08-29 20:14:21,375   Num examples = 277
2022-08-29 20:14:21,376   Batch size = 32
2022-08-29 20:14:22,075 ***** Eval results *****
2022-08-29 20:14:22,075   acc = 0.631768953068592
2022-08-29 20:14:22,076   att_loss = 0.0
2022-08-29 20:14:22,076   cls_loss = 0.22528790631754833
2022-08-29 20:14:22,076   eval_loss = 0.6820380687713623
2022-08-29 20:14:22,076   global_step = 399
2022-08-29 20:14:22,076   loss = 0.22528790631754833
2022-08-29 20:14:22,076   rep_loss = 0.0
2022-08-29 20:14:39,223 ***** Running evaluation *****
2022-08-29 20:14:39,224   Epoch = 29 iter 31199 step
2022-08-29 20:14:39,224   Num examples = 1043
2022-08-29 20:14:39,224   Batch size = 32
2022-08-29 20:14:39,225 ***** Eval results *****
2022-08-29 20:14:39,226   att_loss = 0.7973665545165276
2022-08-29 20:14:39,226   cls_loss = 0.0
2022-08-29 20:14:39,226   global_step = 31199
2022-08-29 20:14:39,226   loss = 2.3335468632534213
2022-08-29 20:14:39,226   rep_loss = 1.5361803040105342
2022-08-29 20:14:39,226 ***** Save model *****
2022-08-29 20:14:39,879 ***** Running evaluation *****
2022-08-29 20:14:39,880   Epoch = 1 iter 499 step
2022-08-29 20:14:39,880   Num examples = 277
2022-08-29 20:14:39,880   Batch size = 32
2022-08-29 20:14:40,582 ***** Eval results *****
2022-08-29 20:14:40,582   acc = 0.6209386281588448
2022-08-29 20:14:40,582   att_loss = 0.0
2022-08-29 20:14:40,582   cls_loss = 0.22114746799652882
2022-08-29 20:14:40,582   eval_loss = 0.6983675824271308
2022-08-29 20:14:40,582   global_step = 499
2022-08-29 20:14:40,582   loss = 0.22114746799652882
2022-08-29 20:14:40,582   rep_loss = 0.0
2022-08-29 20:14:57,922 ***** Running evaluation *****
2022-08-29 20:14:57,923   Epoch = 22 iter 15799 step
2022-08-29 20:14:57,923   Num examples = 1500
2022-08-29 20:14:57,923   Batch size = 32
2022-08-29 20:14:57,924 ***** Eval results *****
2022-08-29 20:14:57,924   att_loss = 1.942682941754659
2022-08-29 20:14:57,924   cls_loss = 0.0
2022-08-29 20:14:57,924   global_step = 15799
2022-08-29 20:14:57,924   loss = 3.417151848475138
2022-08-29 20:14:57,924   rep_loss = 1.4744689067204793
2022-08-29 20:14:57,925 ***** Save model *****
2022-08-29 20:14:58,374 ***** Running evaluation *****
2022-08-29 20:14:58,375   Epoch = 1 iter 599 step
2022-08-29 20:14:58,375   Num examples = 277
2022-08-29 20:14:58,375   Batch size = 32
2022-08-29 20:14:59,078 ***** Eval results *****
2022-08-29 20:14:59,078   acc = 0.6064981949458483
2022-08-29 20:14:59,078   att_loss = 0.0
2022-08-29 20:14:59,078   cls_loss = 0.22335628769360483
2022-08-29 20:14:59,078   eval_loss = 0.6679661538865831
2022-08-29 20:14:59,078   global_step = 599
2022-08-29 20:14:59,078   loss = 0.22335628769360483
2022-08-29 20:14:59,078   rep_loss = 0.0
2022-08-29 20:15:16,885 ***** Running evaluation *****
2022-08-29 20:15:16,885   Epoch = 2 iter 699 step
2022-08-29 20:15:16,885   Num examples = 277
2022-08-29 20:15:16,885   Batch size = 32
2022-08-29 20:15:17,585 ***** Eval results *****
2022-08-29 20:15:17,586   acc = 0.6642599277978339
2022-08-29 20:15:17,586   att_loss = 0.0
2022-08-29 20:15:17,586   cls_loss = 0.19171145587385474
2022-08-29 20:15:17,586   eval_loss = 0.7253023717138503
2022-08-29 20:15:17,586   global_step = 699
2022-08-29 20:15:17,586   loss = 0.19171145587385474
2022-08-29 20:15:17,586   rep_loss = 0.0
2022-08-29 20:15:17,586 ***** Save model *****
2022-08-29 20:15:19,074 ***** Running evaluation *****
2022-08-29 20:15:19,075   Epoch = 3 iter 37399 step
2022-08-29 20:15:19,075   Num examples = 9815
2022-08-29 20:15:19,075   Batch size = 32
2022-08-29 20:15:19,076 ***** Eval results *****
2022-08-29 20:15:19,076   att_loss = 4.064504305250408
2022-08-29 20:15:19,076   cls_loss = 0.0
2022-08-29 20:15:19,077   global_step = 37399
2022-08-29 20:15:19,077   loss = 5.867009434683738
2022-08-29 20:15:19,077   rep_loss = 1.8025051337053344
2022-08-29 20:15:19,077 ***** Save model *****
2022-08-29 20:15:19,132 ***** Running evaluation *****
2022-08-29 20:15:19,132   Epoch = 29 iter 31399 step
2022-08-29 20:15:19,133   Num examples = 1043
2022-08-29 20:15:19,133   Batch size = 32
2022-08-29 20:15:19,134 ***** Eval results *****
2022-08-29 20:15:19,134   att_loss = 0.7904733473858174
2022-08-29 20:15:19,134   cls_loss = 0.0
2022-08-29 20:15:19,134   global_step = 31399
2022-08-29 20:15:19,134   loss = 2.321045643272668
2022-08-29 20:15:19,134   rep_loss = 1.530572292676296
2022-08-29 20:15:19,134 ***** Save model *****
2022-08-29 20:15:36,736 ***** Running evaluation *****
2022-08-29 20:15:36,737   Epoch = 22 iter 15999 step
2022-08-29 20:15:36,737   Num examples = 1500
2022-08-29 20:15:36,737   Batch size = 32
2022-08-29 20:15:36,738 ***** Eval results *****
2022-08-29 20:15:36,738   att_loss = 2.2739961153180728
2022-08-29 20:15:36,738   cls_loss = 0.0
2022-08-29 20:15:36,738   global_step = 15999
2022-08-29 20:15:36,738   loss = 3.7671891351051516
2022-08-29 20:15:36,738   rep_loss = 1.4931930209615547
2022-08-29 20:15:36,738 ***** Save model *****
2022-08-29 20:15:40,000 ***** Running evaluation *****
2022-08-29 20:15:40,001   Epoch = 2 iter 799 step
2022-08-29 20:15:40,001   Num examples = 277
2022-08-29 20:15:40,001   Batch size = 32
2022-08-29 20:15:40,703 ***** Eval results *****
2022-08-29 20:15:40,703   acc = 0.6678700361010831
2022-08-29 20:15:40,703   att_loss = 0.0
2022-08-29 20:15:40,703   cls_loss = 0.1966274379000152
2022-08-29 20:15:40,703   eval_loss = 0.6756699846850501
2022-08-29 20:15:40,703   global_step = 799
2022-08-29 20:15:40,703   loss = 0.1966274379000152
2022-08-29 20:15:40,703   rep_loss = 0.0
2022-08-29 20:15:40,703 ***** Save model *****
2022-08-29 20:16:01,577 ***** Running evaluation *****
2022-08-29 20:16:01,578   Epoch = 2 iter 899 step
2022-08-29 20:16:01,578   Num examples = 277
2022-08-29 20:16:01,578   Batch size = 32
2022-08-29 20:16:02,279 ***** Eval results *****
2022-08-29 20:16:02,279   acc = 0.631768953068592
2022-08-29 20:16:02,279   att_loss = 0.0
2022-08-29 20:16:02,279   cls_loss = 0.19211757521982228
2022-08-29 20:16:02,280   eval_loss = 0.692312197552787
2022-08-29 20:16:02,280   global_step = 899
2022-08-29 20:16:02,280   loss = 0.19211757521982228
2022-08-29 20:16:02,280   rep_loss = 0.0
2022-08-29 20:16:04,005 ***** Running evaluation *****
2022-08-29 20:16:04,005   Epoch = 29 iter 31599 step
2022-08-29 20:16:04,006   Num examples = 1043
2022-08-29 20:16:04,006   Batch size = 32
2022-08-29 20:16:04,007 ***** Eval results *****
2022-08-29 20:16:04,007   att_loss = 0.7833977835980709
2022-08-29 20:16:04,007   cls_loss = 0.0
2022-08-29 20:16:04,007   global_step = 31599
2022-08-29 20:16:04,007   loss = 2.3131005965541616
2022-08-29 20:16:04,007   rep_loss = 1.529702812290648
2022-08-29 20:16:04,007 ***** Save model *****
2022-08-29 20:16:15,656 ***** Running evaluation *****
2022-08-29 20:16:15,657   Epoch = 22 iter 16199 step
2022-08-29 20:16:15,657   Num examples = 1500
2022-08-29 20:16:15,657   Batch size = 32
2022-08-29 20:16:15,658 ***** Eval results *****
2022-08-29 20:16:15,658   att_loss = 2.2808840168321103
2022-08-29 20:16:15,658   cls_loss = 0.0
2022-08-29 20:16:15,658   global_step = 16199
2022-08-29 20:16:15,658   loss = 3.7757966950276947
2022-08-29 20:16:15,658   rep_loss = 1.4949126832242638
2022-08-29 20:16:15,658 ***** Save model *****
2022-08-29 20:16:20,100 ***** Running evaluation *****
2022-08-29 20:16:20,100   Epoch = 3 iter 999 step
2022-08-29 20:16:20,100   Num examples = 277
2022-08-29 20:16:20,100   Batch size = 32
2022-08-29 20:16:20,799 ***** Eval results *****
2022-08-29 20:16:20,799   acc = 0.6895306859205776
2022-08-29 20:16:20,799   att_loss = 0.0
2022-08-29 20:16:20,799   cls_loss = 0.17411492335976977
2022-08-29 20:16:20,799   eval_loss = 0.7012190024058024
2022-08-29 20:16:20,799   global_step = 999
2022-08-29 20:16:20,799   loss = 0.17411492335976977
2022-08-29 20:16:20,799   rep_loss = 0.0
2022-08-29 20:16:20,799 ***** Save model *****
2022-08-29 20:16:35,240 ***** Running evaluation *****
2022-08-29 20:16:35,240   Epoch = 3 iter 37599 step
2022-08-29 20:16:35,241   Num examples = 9815
2022-08-29 20:16:35,241   Batch size = 32
2022-08-29 20:16:35,242 ***** Eval results *****
2022-08-29 20:16:35,242   att_loss = 4.06186759198895
2022-08-29 20:16:35,242   cls_loss = 0.0
2022-08-29 20:16:35,242   global_step = 37599
2022-08-29 20:16:35,242   loss = 5.8641047332123035
2022-08-29 20:16:35,242   rep_loss = 1.8022371434983406
2022-08-29 20:16:35,242 ***** Save model *****
2022-08-29 20:16:41,630 ***** Running evaluation *****
2022-08-29 20:16:41,630   Epoch = 3 iter 1099 step
2022-08-29 20:16:41,630   Num examples = 277
2022-08-29 20:16:41,630   Batch size = 32
2022-08-29 20:16:42,331 ***** Eval results *****
2022-08-29 20:16:42,331   acc = 0.6570397111913358
2022-08-29 20:16:42,331   att_loss = 0.0
2022-08-29 20:16:42,331   cls_loss = 0.17515907337866635
2022-08-29 20:16:42,331   eval_loss = 0.6907051437430911
2022-08-29 20:16:42,331   global_step = 1099
2022-08-29 20:16:42,331   loss = 0.17515907337866635
2022-08-29 20:16:42,331   rep_loss = 0.0
2022-08-29 20:16:43,933 ***** Running evaluation *****
2022-08-29 20:16:43,934   Epoch = 29 iter 31799 step
2022-08-29 20:16:43,934   Num examples = 1043
2022-08-29 20:16:43,934   Batch size = 32
2022-08-29 20:16:43,935 ***** Eval results *****
2022-08-29 20:16:43,935   att_loss = 0.7842644082231418
2022-08-29 20:16:43,936   cls_loss = 0.0
2022-08-29 20:16:43,936   global_step = 31799
2022-08-29 20:16:43,936   loss = 2.314761815365165
2022-08-29 20:16:43,936   rep_loss = 1.5304974071059867
2022-08-29 20:16:43,936 ***** Save model *****
2022-08-29 20:16:54,600 ***** Running evaluation *****
2022-08-29 20:16:54,601   Epoch = 22 iter 16399 step
2022-08-29 20:16:54,601   Num examples = 1500
2022-08-29 20:16:54,601   Batch size = 32
2022-08-29 20:16:54,602 ***** Eval results *****
2022-08-29 20:16:54,602   att_loss = 2.255627513524905
2022-08-29 20:16:54,602   cls_loss = 0.0
2022-08-29 20:16:54,602   global_step = 16399
2022-08-29 20:16:54,603   loss = 3.7468503980494257
2022-08-29 20:16:54,603   rep_loss = 1.4912228906530252
2022-08-29 20:16:54,603 ***** Save model *****
2022-08-29 20:17:00,132 ***** Running evaluation *****
2022-08-29 20:17:00,132   Epoch = 3 iter 1199 step
2022-08-29 20:17:00,132   Num examples = 277
2022-08-29 20:17:00,132   Batch size = 32
2022-08-29 20:17:00,832 ***** Eval results *****
2022-08-29 20:17:00,833   acc = 0.6642599277978339
2022-08-29 20:17:00,833   att_loss = 0.0
2022-08-29 20:17:00,833   cls_loss = 0.17910781757611977
2022-08-29 20:17:00,833   eval_loss = 0.7101784348487854
2022-08-29 20:17:00,833   global_step = 1199
2022-08-29 20:17:00,833   loss = 0.17910781757611977
2022-08-29 20:17:00,833   rep_loss = 0.0
2022-08-29 20:17:18,620 ***** Running evaluation *****
2022-08-29 20:17:18,621   Epoch = 4 iter 1299 step
2022-08-29 20:17:18,621   Num examples = 277
2022-08-29 20:17:18,621   Batch size = 32
2022-08-29 20:17:19,321 ***** Eval results *****
2022-08-29 20:17:19,321   acc = 0.6570397111913358
2022-08-29 20:17:19,321   att_loss = 0.0
2022-08-29 20:17:19,321   cls_loss = 0.16806886954741043
2022-08-29 20:17:19,321   eval_loss = 0.7160463399357266
2022-08-29 20:17:19,321   global_step = 1299
2022-08-29 20:17:19,321   loss = 0.16806886954741043
2022-08-29 20:17:19,321   rep_loss = 0.0
2022-08-29 20:17:23,848 ***** Running evaluation *****
2022-08-29 20:17:23,848   Epoch = 29 iter 31999 step
2022-08-29 20:17:23,849   Num examples = 1043
2022-08-29 20:17:23,849   Batch size = 32
2022-08-29 20:17:23,850 ***** Eval results *****
2022-08-29 20:17:23,850   att_loss = 0.7821062899320709
2022-08-29 20:17:23,850   cls_loss = 0.0
2022-08-29 20:17:23,850   global_step = 31999
2022-08-29 20:17:23,850   loss = 2.311418562924386
2022-08-29 20:17:23,850   rep_loss = 1.5293122721507695
2022-08-29 20:17:23,850 ***** Save model *****
2022-08-29 20:17:33,409 ***** Running evaluation *****
2022-08-29 20:17:33,409   Epoch = 23 iter 16599 step
2022-08-29 20:17:33,409   Num examples = 1500
2022-08-29 20:17:33,409   Batch size = 32
2022-08-29 20:17:33,411 ***** Eval results *****
2022-08-29 20:17:33,411   att_loss = 2.163786196708679
2022-08-29 20:17:33,411   cls_loss = 0.0
2022-08-29 20:17:33,411   global_step = 16599
2022-08-29 20:17:33,411   loss = 3.6290698219748103
2022-08-29 20:17:33,411   rep_loss = 1.46528361825382
2022-08-29 20:17:33,411 ***** Save model *****
2022-08-29 20:17:37,121 ***** Running evaluation *****
2022-08-29 20:17:37,121   Epoch = 4 iter 1399 step
2022-08-29 20:17:37,121   Num examples = 277
2022-08-29 20:17:37,121   Batch size = 32
2022-08-29 20:17:37,823 ***** Eval results *****
2022-08-29 20:17:37,824   acc = 0.6498194945848376
2022-08-29 20:17:37,824   att_loss = 0.0
2022-08-29 20:17:37,824   cls_loss = 0.16837704744069806
2022-08-29 20:17:37,824   eval_loss = 0.7084462377760146
2022-08-29 20:17:37,824   global_step = 1399
2022-08-29 20:17:37,824   loss = 0.16837704744069806
2022-08-29 20:17:37,824   rep_loss = 0.0
2022-08-29 20:17:44,385 ***** Running evaluation *****
2022-08-29 20:17:44,386   Epoch = 3 iter 37799 step
2022-08-29 20:17:44,386   Num examples = 9815
2022-08-29 20:17:44,386   Batch size = 32
2022-08-29 20:17:44,387 ***** Eval results *****
2022-08-29 20:17:44,387   att_loss = 4.063979508910653
2022-08-29 20:17:44,387   cls_loss = 0.0
2022-08-29 20:17:44,387   global_step = 37799
2022-08-29 20:17:44,387   loss = 5.86589131248892
2022-08-29 20:17:44,387   rep_loss = 1.801911804303678
2022-08-29 20:17:44,387 ***** Save model *****
2022-08-29 20:17:55,617 ***** Running evaluation *****
2022-08-29 20:17:55,618   Epoch = 4 iter 1499 step
2022-08-29 20:17:55,618   Num examples = 277
2022-08-29 20:17:55,618   Batch size = 32
2022-08-29 20:17:56,319 ***** Eval results *****
2022-08-29 20:17:56,319   acc = 0.6498194945848376
2022-08-29 20:17:56,319   att_loss = 0.0
2022-08-29 20:17:56,319   cls_loss = 0.17012144820947273
2022-08-29 20:17:56,319   eval_loss = 0.7017534673213959
2022-08-29 20:17:56,319   global_step = 1499
2022-08-29 20:17:56,319   loss = 0.17012144820947273
2022-08-29 20:17:56,319   rep_loss = 0.0
2022-08-29 20:18:04,350 ***** Running evaluation *****
2022-08-29 20:18:04,351   Epoch = 30 iter 32199 step
2022-08-29 20:18:04,351   Num examples = 1043
2022-08-29 20:18:04,351   Batch size = 32
2022-08-29 20:18:04,352 ***** Eval results *****
2022-08-29 20:18:04,353   att_loss = 0.7784008646161301
2022-08-29 20:18:04,353   cls_loss = 0.0
2022-08-29 20:18:04,353   global_step = 32199
2022-08-29 20:18:04,353   loss = 2.3010873599622235
2022-08-29 20:18:04,353   rep_loss = 1.5226864972204532
2022-08-29 20:18:04,353 ***** Save model *****
2022-08-29 20:18:15,291 ***** Running evaluation *****
2022-08-29 20:18:15,291   Epoch = 23 iter 16799 step
2022-08-29 20:18:15,292   Num examples = 1500
2022-08-29 20:18:15,292   Batch size = 32
2022-08-29 20:18:15,293 ***** Eval results *****
2022-08-29 20:18:15,293   att_loss = 2.215226010690656
2022-08-29 20:18:15,293   cls_loss = 0.0
2022-08-29 20:18:15,293   global_step = 16799
2022-08-29 20:18:15,293   loss = 3.6938031071110777
2022-08-29 20:18:15,293   rep_loss = 1.478577088473136
2022-08-29 20:18:15,293 ***** Save model *****
2022-08-29 20:18:42,525 ***** Running evaluation *****
2022-08-29 20:18:42,526   Epoch = 30 iter 32399 step
2022-08-29 20:18:42,526   Num examples = 1043
2022-08-29 20:18:42,526   Batch size = 32
2022-08-29 20:18:42,528 ***** Eval results *****
2022-08-29 20:18:42,528   att_loss = 0.783052639542848
2022-08-29 20:18:42,528   cls_loss = 0.0
2022-08-29 20:18:42,528   global_step = 32399
2022-08-29 20:18:42,528   loss = 2.307387598709808
2022-08-29 20:18:42,528   rep_loss = 1.5243349629856418
2022-08-29 20:18:42,528 ***** Save model *****
2022-08-29 20:18:54,092 ***** Running evaluation *****
2022-08-29 20:18:54,092   Epoch = 23 iter 16999 step
2022-08-29 20:18:54,092   Num examples = 1500
2022-08-29 20:18:54,092   Batch size = 32
2022-08-29 20:18:54,094 ***** Eval results *****
2022-08-29 20:18:54,094   att_loss = 2.211163182356923
2022-08-29 20:18:54,094   cls_loss = 0.0
2022-08-29 20:18:54,094   global_step = 16999
2022-08-29 20:18:54,094   loss = 3.6909383881952347
2022-08-29 20:18:54,094   rep_loss = 1.4797752009224645
2022-08-29 20:18:54,094 ***** Save model *****
2022-08-29 20:18:55,554 ***** Running evaluation *****
2022-08-29 20:18:55,554   Epoch = 3 iter 37999 step
2022-08-29 20:18:55,554   Num examples = 9815
2022-08-29 20:18:55,555   Batch size = 32
2022-08-29 20:18:55,556 ***** Eval results *****
2022-08-29 20:18:55,556   att_loss = 4.059455580695283
2022-08-29 20:18:55,556   cls_loss = 0.0
2022-08-29 20:18:55,556   global_step = 37999
2022-08-29 20:18:55,556   loss = 5.861181962349362
2022-08-29 20:18:55,556   rep_loss = 1.801726383965896
2022-08-29 20:18:55,556 ***** Save model *****
2022-08-29 20:19:21,236 ***** Running evaluation *****
2022-08-29 20:19:21,236   Epoch = 30 iter 32599 step
2022-08-29 20:19:21,236   Num examples = 1043
2022-08-29 20:19:21,236   Batch size = 32
2022-08-29 20:19:21,237 ***** Eval results *****
2022-08-29 20:19:21,237   att_loss = 0.779380763472726
2022-08-29 20:19:21,237   cls_loss = 0.0
2022-08-29 20:19:21,237   global_step = 32599
2022-08-29 20:19:21,238   loss = 2.3030537848822332
2022-08-29 20:19:21,238   rep_loss = 1.5236730247149528
2022-08-29 20:19:21,238 ***** Save model *****
2022-08-29 20:19:36,252 ***** Running evaluation *****
2022-08-29 20:19:36,252   Epoch = 23 iter 17199 step
2022-08-29 20:19:36,252   Num examples = 1500
2022-08-29 20:19:36,252   Batch size = 32
2022-08-29 20:19:36,254 ***** Eval results *****
2022-08-29 20:19:36,254   att_loss = 2.2186752228841295
2022-08-29 20:19:36,254   cls_loss = 0.0
2022-08-29 20:19:36,254   global_step = 17199
2022-08-29 20:19:36,254   loss = 3.6987549607771157
2022-08-29 20:19:36,254   rep_loss = 1.480079732324085
2022-08-29 20:19:36,254 ***** Save model *****
2022-08-29 20:19:59,355 ***** Running evaluation *****
2022-08-29 20:19:59,356   Epoch = 30 iter 32799 step
2022-08-29 20:19:59,356   Num examples = 1043
2022-08-29 20:19:59,356   Batch size = 32
2022-08-29 20:19:59,357 ***** Eval results *****
2022-08-29 20:19:59,357   att_loss = 0.7773487175249142
2022-08-29 20:19:59,357   cls_loss = 0.0
2022-08-29 20:19:59,357   global_step = 32799
2022-08-29 20:19:59,358   loss = 2.3007912609102856
2022-08-29 20:19:59,358   rep_loss = 1.523442545270103
2022-08-29 20:19:59,358 ***** Save model *****
2022-08-29 20:20:09,547 ***** Running evaluation *****
2022-08-29 20:20:09,548   Epoch = 3 iter 38199 step
2022-08-29 20:20:09,548   Num examples = 9815
2022-08-29 20:20:09,548   Batch size = 32
2022-08-29 20:20:09,549 ***** Eval results *****
2022-08-29 20:20:09,549   att_loss = 4.0555039608117305
2022-08-29 20:20:09,549   cls_loss = 0.0
2022-08-29 20:20:09,549   global_step = 38199
2022-08-29 20:20:09,549   loss = 5.856661208608278
2022-08-29 20:20:09,549   rep_loss = 1.8011572481405855
2022-08-29 20:20:09,549 ***** Save model *****
2022-08-29 20:20:15,187 ***** Running evaluation *****
2022-08-29 20:20:15,188   Epoch = 24 iter 17399 step
2022-08-29 20:20:15,188   Num examples = 1500
2022-08-29 20:20:15,188   Batch size = 32
2022-08-29 20:20:15,189 ***** Eval results *****
2022-08-29 20:20:15,189   att_loss = 2.1830253643903905
2022-08-29 20:20:15,189   cls_loss = 0.0
2022-08-29 20:20:15,189   global_step = 17399
2022-08-29 20:20:15,189   loss = 3.6538749983210765
2022-08-29 20:20:15,189   rep_loss = 1.4708496310753736
2022-08-29 20:20:15,189 ***** Save model *****
2022-08-29 20:20:39,433 ***** Running evaluation *****
2022-08-29 20:20:39,433   Epoch = 30 iter 32999 step
2022-08-29 20:20:39,433   Num examples = 1043
2022-08-29 20:20:39,433   Batch size = 32
2022-08-29 20:20:39,434 ***** Eval results *****
2022-08-29 20:20:39,435   att_loss = 0.7781282803810923
2022-08-29 20:20:39,435   cls_loss = 0.0
2022-08-29 20:20:39,435   global_step = 32999
2022-08-29 20:20:39,435   loss = 2.30094202442388
2022-08-29 20:20:39,435   rep_loss = 1.5228137444157048
2022-08-29 20:20:39,435 ***** Save model *****
2022-08-29 20:20:54,071 ***** Running evaluation *****
2022-08-29 20:20:54,072   Epoch = 24 iter 17599 step
2022-08-29 20:20:54,072   Num examples = 1500
2022-08-29 20:20:54,072   Batch size = 32
2022-08-29 20:20:54,073 ***** Eval results *****
2022-08-29 20:20:54,073   att_loss = 2.1970494495100805
2022-08-29 20:20:54,073   cls_loss = 0.0
2022-08-29 20:20:54,073   global_step = 17599
2022-08-29 20:20:54,073   loss = 3.670727336114369
2022-08-29 20:20:54,073   rep_loss = 1.473677889202856
2022-08-29 20:20:54,073 ***** Save model *****
2022-08-29 20:21:18,576 ***** Running evaluation *****
2022-08-29 20:21:18,576   Epoch = 3 iter 38399 step
2022-08-29 20:21:18,576   Num examples = 9815
2022-08-29 20:21:18,576   Batch size = 32
2022-08-29 20:21:18,577 ***** Eval results *****
2022-08-29 20:21:18,578   att_loss = 4.064341403346645
2022-08-29 20:21:18,578   cls_loss = 0.0
2022-08-29 20:21:18,578   global_step = 38399
2022-08-29 20:21:18,578   loss = 5.8658308691214796
2022-08-29 20:21:18,578   rep_loss = 1.8014894642715646
2022-08-29 20:21:18,578 ***** Save model *****
2022-08-29 20:21:19,597 ***** Running evaluation *****
2022-08-29 20:21:19,598   Epoch = 31 iter 33199 step
2022-08-29 20:21:19,598   Num examples = 1043
2022-08-29 20:21:19,598   Batch size = 32
2022-08-29 20:21:19,599 ***** Eval results *****
2022-08-29 20:21:19,599   att_loss = 0.7492225399384131
2022-08-29 20:21:19,599   cls_loss = 0.0
2022-08-29 20:21:19,599   global_step = 33199
2022-08-29 20:21:19,599   loss = 2.263545038935902
2022-08-29 20:21:19,599   rep_loss = 1.5143225075124385
2022-08-29 20:21:19,599 ***** Save model *****
2022-08-29 20:21:35,424 ***** Running evaluation *****
2022-08-29 20:21:35,425   Epoch = 24 iter 17799 step
2022-08-29 20:21:35,425   Num examples = 1500
2022-08-29 20:21:35,425   Batch size = 32
2022-08-29 20:21:35,426 ***** Eval results *****
2022-08-29 20:21:35,427   att_loss = 2.208951873754067
2022-08-29 20:21:35,427   cls_loss = 0.0
2022-08-29 20:21:35,427   global_step = 17799
2022-08-29 20:21:35,427   loss = 3.6822511893311085
2022-08-29 20:21:35,427   rep_loss = 1.4732993151565499
2022-08-29 20:21:35,427 ***** Save model *****
2022-08-29 20:22:01,715 ***** Running evaluation *****
2022-08-29 20:22:01,716   Epoch = 31 iter 33399 step
2022-08-29 20:22:01,716   Num examples = 1043
2022-08-29 20:22:01,716   Batch size = 32
2022-08-29 20:22:01,717 ***** Eval results *****
2022-08-29 20:22:01,718   att_loss = 0.7663626996512266
2022-08-29 20:22:01,718   cls_loss = 0.0
2022-08-29 20:22:01,718   global_step = 33399
2022-08-29 20:22:01,718   loss = 2.2810841233459946
2022-08-29 20:22:01,718   rep_loss = 1.514721428405788
2022-08-29 20:22:01,718 ***** Save model *****
2022-08-29 20:22:14,295 ***** Running evaluation *****
2022-08-29 20:22:14,295   Epoch = 25 iter 17999 step
2022-08-29 20:22:14,296   Num examples = 1500
2022-08-29 20:22:14,296   Batch size = 32
2022-08-29 20:22:14,297 ***** Eval results *****
2022-08-29 20:22:14,297   att_loss = 2.2810091996679502
2022-08-29 20:22:14,297   cls_loss = 0.0
2022-08-29 20:22:14,297   global_step = 17999
2022-08-29 20:22:14,297   loss = 3.7564162624125577
2022-08-29 20:22:14,297   rep_loss = 1.4754070724759782
2022-08-29 20:22:14,297 ***** Save model *****
2022-08-29 20:22:30,658 ***** Running evaluation *****
2022-08-29 20:22:30,659   Epoch = 3 iter 38599 step
2022-08-29 20:22:30,659   Num examples = 9815
2022-08-29 20:22:30,659   Batch size = 32
2022-08-29 20:22:30,660 ***** Eval results *****
2022-08-29 20:22:30,660   att_loss = 4.06583853024251
2022-08-29 20:22:30,660   cls_loss = 0.0
2022-08-29 20:22:30,660   global_step = 38599
2022-08-29 20:22:30,660   loss = 5.867247132411329
2022-08-29 20:22:30,660   rep_loss = 1.8014086023690585
2022-08-29 20:22:30,660 ***** Save model *****
2022-08-29 20:22:40,062 ***** Running evaluation *****
2022-08-29 20:22:40,062   Epoch = 31 iter 33599 step
2022-08-29 20:22:40,062   Num examples = 1043
2022-08-29 20:22:40,062   Batch size = 32
2022-08-29 20:22:40,063 ***** Eval results *****
2022-08-29 20:22:40,063   att_loss = 0.7663473457765676
2022-08-29 20:22:40,064   cls_loss = 0.0
2022-08-29 20:22:40,064   global_step = 33599
2022-08-29 20:22:40,064   loss = 2.2827370441129884
2022-08-29 20:22:40,064   rep_loss = 1.5163897013712804
2022-08-29 20:22:40,064 ***** Save model *****
2022-08-29 20:22:53,256 ***** Running evaluation *****
2022-08-29 20:22:53,256   Epoch = 25 iter 18199 step
2022-08-29 20:22:53,256   Num examples = 1500
2022-08-29 20:22:53,256   Batch size = 32
2022-08-29 20:22:53,257 ***** Eval results *****
2022-08-29 20:22:53,257   att_loss = 2.2397085811239648
2022-08-29 20:22:53,257   cls_loss = 0.0
2022-08-29 20:22:53,257   global_step = 18199
2022-08-29 20:22:53,257   loss = 3.7140760412177887
2022-08-29 20:22:53,257   rep_loss = 1.4743674538700455
2022-08-29 20:22:53,258 ***** Save model *****
2022-08-29 20:23:20,459 ***** Running evaluation *****
2022-08-29 20:23:20,460   Epoch = 31 iter 33799 step
2022-08-29 20:23:20,460   Num examples = 1043
2022-08-29 20:23:20,460   Batch size = 32
2022-08-29 20:23:20,461 ***** Eval results *****
2022-08-29 20:23:20,461   att_loss = 0.7669229069252953
2022-08-29 20:23:20,461   cls_loss = 0.0
2022-08-29 20:23:20,462   global_step = 33799
2022-08-29 20:23:20,462   loss = 2.2821613149946574
2022-08-29 20:23:20,462   rep_loss = 1.5152384097945328
2022-08-29 20:23:20,462 ***** Save model *****
2022-08-29 20:23:32,145 ***** Running evaluation *****
2022-08-29 20:23:32,146   Epoch = 25 iter 18399 step
2022-08-29 20:23:32,146   Num examples = 1500
2022-08-29 20:23:32,146   Batch size = 32
2022-08-29 20:23:32,147 ***** Eval results *****
2022-08-29 20:23:32,148   att_loss = 2.2020706663152954
2022-08-29 20:23:32,148   cls_loss = 0.0
2022-08-29 20:23:32,148   global_step = 18399
2022-08-29 20:23:32,148   loss = 3.67123809317438
2022-08-29 20:23:32,148   rep_loss = 1.4691674247350883
2022-08-29 20:23:32,148 ***** Save model *****
2022-08-29 20:23:43,956 ***** Running evaluation *****
2022-08-29 20:23:43,956   Epoch = 3 iter 38799 step
2022-08-29 20:23:43,956   Num examples = 9815
2022-08-29 20:23:43,956   Batch size = 32
2022-08-29 20:23:43,958 ***** Eval results *****
2022-08-29 20:23:43,958   att_loss = 4.0607686581808515
2022-08-29 20:23:43,958   cls_loss = 0.0
2022-08-29 20:23:43,958   global_step = 38799
2022-08-29 20:23:43,958   loss = 5.861742863842368
2022-08-29 20:23:43,958   rep_loss = 1.8009742068019878
2022-08-29 20:23:43,958 ***** Save model *****
2022-08-29 20:23:58,698 ***** Running evaluation *****
2022-08-29 20:23:58,698   Epoch = 31 iter 33999 step
2022-08-29 20:23:58,699   Num examples = 1043
2022-08-29 20:23:58,699   Batch size = 32
2022-08-29 20:23:58,700 ***** Eval results *****
2022-08-29 20:23:58,700   att_loss = 0.7735315965348489
2022-08-29 20:23:58,700   cls_loss = 0.0
2022-08-29 20:23:58,700   global_step = 33999
2022-08-29 20:23:58,700   loss = 2.288843629068531
2022-08-29 20:23:58,700   rep_loss = 1.5153120322660967
2022-08-29 20:23:58,700 ***** Save model *****
2022-08-29 20:24:11,010 ***** Running evaluation *****
2022-08-29 20:24:11,011   Epoch = 25 iter 18599 step
2022-08-29 20:24:11,011   Num examples = 1500
2022-08-29 20:24:11,011   Batch size = 32
2022-08-29 20:24:11,012 ***** Eval results *****
2022-08-29 20:24:11,012   att_loss = 2.1845257041634323
2022-08-29 20:24:11,012   cls_loss = 0.0
2022-08-29 20:24:11,012   global_step = 18599
2022-08-29 20:24:11,012   loss = 3.649555359856557
2022-08-29 20:24:11,012   rep_loss = 1.4650296540399912
2022-08-29 20:24:11,013 ***** Save model *****
2022-08-29 20:24:36,945 ***** Running evaluation *****
2022-08-29 20:24:36,946   Epoch = 32 iter 34199 step
2022-08-29 20:24:36,946   Num examples = 1043
2022-08-29 20:24:36,946   Batch size = 32
2022-08-29 20:24:36,947 ***** Eval results *****
2022-08-29 20:24:36,947   att_loss = 0.7634622765624005
2022-08-29 20:24:36,947   cls_loss = 0.0
2022-08-29 20:24:36,947   global_step = 34199
2022-08-29 20:24:36,947   loss = 2.300224615179974
2022-08-29 20:24:36,947   rep_loss = 1.536762351575105
2022-08-29 20:24:36,948 ***** Save model *****
2022-08-29 20:24:54,973 ***** Running evaluation *****
2022-08-29 20:24:54,974   Epoch = 26 iter 18799 step
2022-08-29 20:24:54,974   Num examples = 1500
2022-08-29 20:24:54,974   Batch size = 32
2022-08-29 20:24:54,975 ***** Eval results *****
2022-08-29 20:24:54,975   att_loss = 2.15181612604447
2022-08-29 20:24:54,975   cls_loss = 0.0
2022-08-29 20:24:54,976   global_step = 18799
2022-08-29 20:24:54,976   loss = 3.6091382175911475
2022-08-29 20:24:54,976   rep_loss = 1.4573220824467317
2022-08-29 20:24:54,976 ***** Save model *****
2022-08-29 20:24:56,374 ***** Running evaluation *****
2022-08-29 20:24:56,375   Epoch = 3 iter 38999 step
2022-08-29 20:24:56,375   Num examples = 9815
2022-08-29 20:24:56,375   Batch size = 32
2022-08-29 20:24:56,376 ***** Eval results *****
2022-08-29 20:24:56,376   att_loss = 4.0591823887846825
2022-08-29 20:24:56,376   cls_loss = 0.0
2022-08-29 20:24:56,376   global_step = 38999
2022-08-29 20:24:56,376   loss = 5.859583261573631
2022-08-29 20:24:56,377   rep_loss = 1.8004008742068083
2022-08-29 20:24:56,377 ***** Save model *****
2022-08-29 20:25:15,214 ***** Running evaluation *****
2022-08-29 20:25:15,215   Epoch = 32 iter 34399 step
2022-08-29 20:25:15,215   Num examples = 1043
2022-08-29 20:25:15,215   Batch size = 32
2022-08-29 20:25:15,216 ***** Eval results *****
2022-08-29 20:25:15,216   att_loss = 0.7613790021883533
2022-08-29 20:25:15,216   cls_loss = 0.0
2022-08-29 20:25:15,216   global_step = 34399
2022-08-29 20:25:15,216   loss = 2.2780127958331944
2022-08-29 20:25:15,216   rep_loss = 1.5166337944466972
2022-08-29 20:25:15,217 ***** Save model *****
2022-08-29 20:25:37,868 ***** Running evaluation *****
2022-08-29 20:25:37,868   Epoch = 26 iter 18999 step
2022-08-29 20:25:37,868   Num examples = 1500
2022-08-29 20:25:37,868   Batch size = 32
2022-08-29 20:25:37,869 ***** Eval results *****
2022-08-29 20:25:37,869   att_loss = 2.185882049744943
2022-08-29 20:25:37,870   cls_loss = 0.0
2022-08-29 20:25:37,870   global_step = 18999
2022-08-29 20:25:37,870   loss = 3.6459543049515553
2022-08-29 20:25:37,870   rep_loss = 1.4600722498043786
2022-08-29 20:25:37,870 ***** Save model *****
2022-08-29 20:25:53,459 ***** Running evaluation *****
2022-08-29 20:25:53,460   Epoch = 32 iter 34599 step
2022-08-29 20:25:53,460   Num examples = 1043
2022-08-29 20:25:53,460   Batch size = 32
2022-08-29 20:25:53,461 ***** Eval results *****
2022-08-29 20:25:53,461   att_loss = 0.7635608557831874
2022-08-29 20:25:53,461   cls_loss = 0.0
2022-08-29 20:25:53,461   global_step = 34599
2022-08-29 20:25:53,461   loss = 2.2780524509172912
2022-08-29 20:25:53,461   rep_loss = 1.5144915938659198
2022-08-29 20:25:53,461 ***** Save model *****
2022-08-29 20:26:08,594 ***** Running evaluation *****
2022-08-29 20:26:08,595   Epoch = 3 iter 39199 step
2022-08-29 20:26:08,595   Num examples = 9815
2022-08-29 20:26:08,595   Batch size = 32
2022-08-29 20:26:08,596 ***** Eval results *****
2022-08-29 20:26:08,596   att_loss = 4.059928212861303
2022-08-29 20:26:08,596   cls_loss = 0.0
2022-08-29 20:26:08,596   global_step = 39199
2022-08-29 20:26:08,596   loss = 5.860377892064928
2022-08-29 20:26:08,596   rep_loss = 1.8004496810522184
2022-08-29 20:26:08,596 ***** Save model *****
2022-08-29 20:26:16,678 ***** Running evaluation *****
2022-08-29 20:26:16,679   Epoch = 26 iter 19199 step
2022-08-29 20:26:16,679   Num examples = 1500
2022-08-29 20:26:16,679   Batch size = 32
2022-08-29 20:26:16,680 ***** Eval results *****
2022-08-29 20:26:16,680   att_loss = 2.1709237078488886
2022-08-29 20:26:16,681   cls_loss = 0.0
2022-08-29 20:26:16,681   global_step = 19199
2022-08-29 20:26:16,681   loss = 3.6325880785221885
2022-08-29 20:26:16,681   rep_loss = 1.4616643646118106
2022-08-29 20:26:16,681 ***** Save model *****
2022-08-29 20:26:34,004 ***** Running evaluation *****
2022-08-29 20:26:34,005   Epoch = 32 iter 34799 step
2022-08-29 20:26:34,005   Num examples = 1043
2022-08-29 20:26:34,005   Batch size = 32
2022-08-29 20:26:34,006 ***** Eval results *****
2022-08-29 20:26:34,006   att_loss = 0.7709983362910453
2022-08-29 20:26:34,006   cls_loss = 0.0
2022-08-29 20:26:34,006   global_step = 34799
2022-08-29 20:26:34,006   loss = 2.285123013187182
2022-08-29 20:26:34,006   rep_loss = 1.5141246772309942
2022-08-29 20:26:34,007 ***** Save model *****
2022-08-29 20:26:55,368 ***** Running evaluation *****
2022-08-29 20:26:55,369   Epoch = 27 iter 19399 step
2022-08-29 20:26:55,369   Num examples = 1500
2022-08-29 20:26:55,369   Batch size = 32
2022-08-29 20:26:55,370 ***** Eval results *****
2022-08-29 20:26:55,370   att_loss = 2.0674445629119873
2022-08-29 20:26:55,370   cls_loss = 0.0
2022-08-29 20:26:55,370   global_step = 19399
2022-08-29 20:26:55,370   loss = 3.504471632150503
2022-08-29 20:26:55,370   rep_loss = 1.4370270875784068
2022-08-29 20:26:55,370 ***** Save model *****
2022-08-29 20:27:12,381 ***** Running evaluation *****
2022-08-29 20:27:12,381   Epoch = 32 iter 34999 step
2022-08-29 20:27:12,381   Num examples = 1043
2022-08-29 20:27:12,381   Batch size = 32
2022-08-29 20:27:12,382 ***** Eval results *****
2022-08-29 20:27:12,383   att_loss = 0.7737718946226891
2022-08-29 20:27:12,383   cls_loss = 0.0
2022-08-29 20:27:12,383   global_step = 34999
2022-08-29 20:27:12,383   loss = 2.292139977904745
2022-08-29 20:27:12,383   rep_loss = 1.5183680844046221
2022-08-29 20:27:12,383 ***** Save model *****
2022-08-29 20:27:18,934 ***** Running evaluation *****
2022-08-29 20:27:18,934   Epoch = 3 iter 39399 step
2022-08-29 20:27:18,935   Num examples = 9815
2022-08-29 20:27:18,935   Batch size = 32
2022-08-29 20:27:18,936 ***** Eval results *****
2022-08-29 20:27:18,936   att_loss = 4.054850821059891
2022-08-29 20:27:18,936   cls_loss = 0.0
2022-08-29 20:27:18,936   global_step = 39399
2022-08-29 20:27:18,936   loss = 5.854869115841269
2022-08-29 20:27:18,936   rep_loss = 1.8000182962565117
2022-08-29 20:27:18,936 ***** Save model *****
2022-08-29 20:27:34,091 ***** Running evaluation *****
2022-08-29 20:27:34,091   Epoch = 27 iter 19599 step
2022-08-29 20:27:34,092   Num examples = 1500
2022-08-29 20:27:34,092   Batch size = 32
2022-08-29 20:27:34,093 ***** Eval results *****
2022-08-29 20:27:34,093   att_loss = 2.1565164271654655
2022-08-29 20:27:34,093   cls_loss = 0.0
2022-08-29 20:27:34,093   global_step = 19599
2022-08-29 20:27:34,093   loss = 3.609105665359139
2022-08-29 20:27:34,093   rep_loss = 1.4525892409920134
2022-08-29 20:27:34,093 ***** Save model *****
2022-08-29 20:27:50,773 ***** Running evaluation *****
2022-08-29 20:27:50,773   Epoch = 32 iter 35199 step
2022-08-29 20:27:50,773   Num examples = 1043
2022-08-29 20:27:50,773   Batch size = 32
2022-08-29 20:27:50,775 ***** Eval results *****
2022-08-29 20:27:50,775   att_loss = 0.7730624251293413
2022-08-29 20:27:50,775   cls_loss = 0.0
2022-08-29 20:27:50,775   global_step = 35199
2022-08-29 20:27:50,775   loss = 2.29129371533529
2022-08-29 20:27:50,775   rep_loss = 1.5182312909925206
2022-08-29 20:27:50,775 ***** Save model *****
2022-08-29 20:28:17,300 ***** Running evaluation *****
2022-08-29 20:28:17,300   Epoch = 27 iter 19799 step
2022-08-29 20:28:17,300   Num examples = 1500
2022-08-29 20:28:17,300   Batch size = 32
2022-08-29 20:28:17,302 ***** Eval results *****
2022-08-29 20:28:17,302   att_loss = 2.16670964618572
2022-08-29 20:28:17,302   cls_loss = 0.0
2022-08-29 20:28:17,302   global_step = 19799
2022-08-29 20:28:17,302   loss = 3.619743083926148
2022-08-29 20:28:17,302   rep_loss = 1.45303343802907
2022-08-29 20:28:17,302 ***** Save model *****
2022-08-29 20:28:29,398 ***** Running evaluation *****
2022-08-29 20:28:29,399   Epoch = 33 iter 35399 step
2022-08-29 20:28:29,399   Num examples = 1043
2022-08-29 20:28:29,399   Batch size = 32
2022-08-29 20:28:29,400 ***** Eval results *****
2022-08-29 20:28:29,400   att_loss = 0.7759015410177169
2022-08-29 20:28:29,400   cls_loss = 0.0
2022-08-29 20:28:29,400   global_step = 35399
2022-08-29 20:28:29,400   loss = 2.286982234062687
2022-08-29 20:28:29,401   rep_loss = 1.5110806957367928
2022-08-29 20:28:29,401 ***** Save model *****
2022-08-29 20:28:30,123 ***** Running evaluation *****
2022-08-29 20:28:30,123   Epoch = 3 iter 39599 step
2022-08-29 20:28:30,123   Num examples = 9815
2022-08-29 20:28:30,124   Batch size = 32
2022-08-29 20:28:30,125 ***** Eval results *****
2022-08-29 20:28:30,125   att_loss = 4.056710625584829
2022-08-29 20:28:30,125   cls_loss = 0.0
2022-08-29 20:28:30,125   global_step = 39599
2022-08-29 20:28:30,125   loss = 5.856590787198846
2022-08-29 20:28:30,125   rep_loss = 1.7998801616568063
2022-08-29 20:28:30,125 ***** Save model *****
2022-08-29 20:28:56,156 ***** Running evaluation *****
2022-08-29 20:28:56,157   Epoch = 27 iter 19999 step
2022-08-29 20:28:56,157   Num examples = 1500
2022-08-29 20:28:56,157   Batch size = 32
2022-08-29 20:28:56,158 ***** Eval results *****
2022-08-29 20:28:56,158   att_loss = 2.163225415285804
2022-08-29 20:28:56,159   cls_loss = 0.0
2022-08-29 20:28:56,159   global_step = 19999
2022-08-29 20:28:56,159   loss = 3.6161984747813536
2022-08-29 20:28:56,159   rep_loss = 1.452973065524078
2022-08-29 20:28:56,159 ***** Save model *****
2022-08-29 20:29:09,222 ***** Running evaluation *****
2022-08-29 20:29:09,222   Epoch = 33 iter 35599 step
2022-08-29 20:29:09,222   Num examples = 1043
2022-08-29 20:29:09,222   Batch size = 32
2022-08-29 20:29:09,224 ***** Eval results *****
2022-08-29 20:29:09,224   att_loss = 0.7733934484737021
2022-08-29 20:29:09,224   cls_loss = 0.0
2022-08-29 20:29:09,224   global_step = 35599
2022-08-29 20:29:09,224   loss = 2.2850722188680943
2022-08-29 20:29:09,224   rep_loss = 1.511678768547488
2022-08-29 20:29:09,224 ***** Save model *****
2022-08-29 20:29:34,858 ***** Running evaluation *****
2022-08-29 20:29:34,858   Epoch = 28 iter 20199 step
2022-08-29 20:29:34,858   Num examples = 1500
2022-08-29 20:29:34,858   Batch size = 32
2022-08-29 20:29:34,860 ***** Eval results *****
2022-08-29 20:29:34,860   att_loss = 2.116313106135318
2022-08-29 20:29:34,860   cls_loss = 0.0
2022-08-29 20:29:34,860   global_step = 20199
2022-08-29 20:29:34,860   loss = 3.5586914915787546
2022-08-29 20:29:34,860   rep_loss = 1.4423783967369481
2022-08-29 20:29:34,860 ***** Save model *****
2022-08-29 20:29:42,005 ***** Running evaluation *****
2022-08-29 20:29:42,005   Epoch = 3 iter 39799 step
2022-08-29 20:29:42,005   Num examples = 9815
2022-08-29 20:29:42,005   Batch size = 32
2022-08-29 20:29:42,006 ***** Eval results *****
2022-08-29 20:29:42,006   att_loss = 4.058246438166318
2022-08-29 20:29:42,006   cls_loss = 0.0
2022-08-29 20:29:42,007   global_step = 39799
2022-08-29 20:29:42,007   loss = 5.857992562685569
2022-08-29 20:29:42,007   rep_loss = 1.7997461236010281
2022-08-29 20:29:42,007 ***** Save model *****
2022-08-29 20:29:47,779 ***** Running evaluation *****
2022-08-29 20:29:47,780   Epoch = 33 iter 35799 step
2022-08-29 20:29:47,780   Num examples = 1043
2022-08-29 20:29:47,780   Batch size = 32
2022-08-29 20:29:47,781 ***** Eval results *****
2022-08-29 20:29:47,781   att_loss = 0.7740581268662805
2022-08-29 20:29:47,781   cls_loss = 0.0
2022-08-29 20:29:47,781   global_step = 35799
2022-08-29 20:29:47,781   loss = 2.2866458126016567
2022-08-29 20:29:47,781   rep_loss = 1.5125876834800651
2022-08-29 20:29:47,781 ***** Save model *****
2022-08-29 20:30:13,592 ***** Running evaluation *****
2022-08-29 20:30:13,593   Epoch = 28 iter 20399 step
2022-08-29 20:30:13,593   Num examples = 1500
2022-08-29 20:30:13,593   Batch size = 32
2022-08-29 20:30:13,594 ***** Eval results *****
2022-08-29 20:30:13,594   att_loss = 2.1356928930444234
2022-08-29 20:30:13,594   cls_loss = 0.0
2022-08-29 20:30:13,594   global_step = 20399
2022-08-29 20:30:13,594   loss = 3.583418087231911
2022-08-29 20:30:13,594   rep_loss = 1.4477251978243812
2022-08-29 20:30:13,594 ***** Save model *****
2022-08-29 20:30:26,067 ***** Running evaluation *****
2022-08-29 20:30:26,067   Epoch = 33 iter 35999 step
2022-08-29 20:30:26,067   Num examples = 1043
2022-08-29 20:30:26,067   Batch size = 32
2022-08-29 20:30:26,068 ***** Eval results *****
2022-08-29 20:30:26,068   att_loss = 0.7731972917815707
2022-08-29 20:30:26,069   cls_loss = 0.0
2022-08-29 20:30:26,069   global_step = 35999
2022-08-29 20:30:26,069   loss = 2.2853636501640673
2022-08-29 20:30:26,069   rep_loss = 1.5121663557772604
2022-08-29 20:30:26,069 ***** Save model *****
2022-08-29 20:30:52,300 ***** Running evaluation *****
2022-08-29 20:30:52,300   Epoch = 28 iter 20599 step
2022-08-29 20:30:52,301   Num examples = 1500
2022-08-29 20:30:52,301   Batch size = 32
2022-08-29 20:30:52,302 ***** Eval results *****
2022-08-29 20:30:52,302   att_loss = 2.144958766060646
2022-08-29 20:30:52,302   cls_loss = 0.0
2022-08-29 20:30:52,302   global_step = 20599
2022-08-29 20:30:52,302   loss = 3.5924104521972966
2022-08-29 20:30:52,302   rep_loss = 1.4474516887857456
2022-08-29 20:30:52,302 ***** Save model *****
2022-08-29 20:30:54,214 ***** Running evaluation *****
2022-08-29 20:30:54,215   Epoch = 3 iter 39999 step
2022-08-29 20:30:54,215   Num examples = 9815
2022-08-29 20:30:54,215   Batch size = 32
2022-08-29 20:30:54,216 ***** Eval results *****
2022-08-29 20:30:54,216   att_loss = 4.060790482213942
2022-08-29 20:30:54,216   cls_loss = 0.0
2022-08-29 20:30:54,216   global_step = 39999
2022-08-29 20:30:54,216   loss = 5.860461680303934
2022-08-29 20:30:54,217   rep_loss = 1.799671197940326
2022-08-29 20:30:54,217 ***** Save model *****
2022-08-29 20:31:04,385 ***** Running evaluation *****
2022-08-29 20:31:04,386   Epoch = 33 iter 36199 step
2022-08-29 20:31:04,386   Num examples = 1043
2022-08-29 20:31:04,386   Batch size = 32
2022-08-29 20:31:04,387 ***** Eval results *****
2022-08-29 20:31:04,387   att_loss = 0.7724757781827637
2022-08-29 20:31:04,387   cls_loss = 0.0
2022-08-29 20:31:04,387   global_step = 36199
2022-08-29 20:31:04,387   loss = 2.28534506965058
2022-08-29 20:31:04,387   rep_loss = 1.512869289533006
2022-08-29 20:31:04,387 ***** Save model *****
2022-08-29 20:31:31,324 ***** Running evaluation *****
2022-08-29 20:31:31,325   Epoch = 28 iter 20799 step
2022-08-29 20:31:31,325   Num examples = 1500
2022-08-29 20:31:31,325   Batch size = 32
2022-08-29 20:31:31,326 ***** Eval results *****
2022-08-29 20:31:31,326   att_loss = 2.1569408734067737
2022-08-29 20:31:31,326   cls_loss = 0.0
2022-08-29 20:31:31,326   global_step = 20799
2022-08-29 20:31:31,327   loss = 3.606129736866025
2022-08-29 20:31:31,327   rep_loss = 1.449188863630775
2022-08-29 20:31:31,327 ***** Save model *****
2022-08-29 20:31:46,732 ***** Running evaluation *****
2022-08-29 20:31:46,732   Epoch = 34 iter 36399 step
2022-08-29 20:31:46,732   Num examples = 1043
2022-08-29 20:31:46,732   Batch size = 32
2022-08-29 20:31:46,734 ***** Eval results *****
2022-08-29 20:31:46,734   att_loss = 0.7569105474428198
2022-08-29 20:31:46,734   cls_loss = 0.0
2022-08-29 20:31:46,734   global_step = 36399
2022-08-29 20:31:46,734   loss = 2.259018911712471
2022-08-29 20:31:46,734   rep_loss = 1.502108350567434
2022-08-29 20:31:46,734 ***** Save model *****
2022-08-29 20:32:07,436 ***** Running evaluation *****
2022-08-29 20:32:07,436   Epoch = 3 iter 40199 step
2022-08-29 20:32:07,436   Num examples = 9815
2022-08-29 20:32:07,436   Batch size = 32
2022-08-29 20:32:07,438 ***** Eval results *****
2022-08-29 20:32:07,438   att_loss = 4.062461395976252
2022-08-29 20:32:07,438   cls_loss = 0.0
2022-08-29 20:32:07,438   global_step = 40199
2022-08-29 20:32:07,438   loss = 5.861849950321736
2022-08-29 20:32:07,438   rep_loss = 1.7993885543806907
2022-08-29 20:32:07,438 ***** Save model *****
2022-08-29 20:32:10,106 ***** Running evaluation *****
2022-08-29 20:32:10,106   Epoch = 29 iter 20999 step
2022-08-29 20:32:10,106   Num examples = 1500
2022-08-29 20:32:10,106   Batch size = 32
2022-08-29 20:32:10,107 ***** Eval results *****
2022-08-29 20:32:10,108   att_loss = 2.1318296025701837
2022-08-29 20:32:10,108   cls_loss = 0.0
2022-08-29 20:32:10,108   global_step = 20999
2022-08-29 20:32:10,108   loss = 3.5751889641002075
2022-08-29 20:32:10,108   rep_loss = 1.4433593682650119
2022-08-29 20:32:10,108 ***** Save model *****
2022-08-29 20:32:25,213 ***** Running evaluation *****
2022-08-29 20:32:25,214   Epoch = 34 iter 36599 step
2022-08-29 20:32:25,214   Num examples = 1043
2022-08-29 20:32:25,214   Batch size = 32
2022-08-29 20:32:25,215 ***** Eval results *****
2022-08-29 20:32:25,215   att_loss = 0.7670398670206502
2022-08-29 20:32:25,215   cls_loss = 0.0
2022-08-29 20:32:25,215   global_step = 36599
2022-08-29 20:32:25,215   loss = 2.2743440422031522
2022-08-29 20:32:25,215   rep_loss = 1.507304169159733
2022-08-29 20:32:25,215 ***** Save model *****
2022-08-29 20:32:49,406 ***** Running evaluation *****
2022-08-29 20:32:49,406   Epoch = 29 iter 21199 step
2022-08-29 20:32:49,406   Num examples = 1500
2022-08-29 20:32:49,406   Batch size = 32
2022-08-29 20:32:49,408 ***** Eval results *****
2022-08-29 20:32:49,408   att_loss = 2.1316750862237948
2022-08-29 20:32:49,408   cls_loss = 0.0
2022-08-29 20:32:49,408   global_step = 21199
2022-08-29 20:32:49,408   loss = 3.571336605820795
2022-08-29 20:32:49,408   rep_loss = 1.4396615214942305
2022-08-29 20:32:49,408 ***** Save model *****
2022-08-29 20:33:03,395 ***** Running evaluation *****
2022-08-29 20:33:03,396   Epoch = 34 iter 36799 step
2022-08-29 20:33:03,396   Num examples = 1043
2022-08-29 20:33:03,396   Batch size = 32
2022-08-29 20:33:03,397 ***** Eval results *****
2022-08-29 20:33:03,397   att_loss = 0.7676534808759082
2022-08-29 20:33:03,398   cls_loss = 0.0
2022-08-29 20:33:03,398   global_step = 36799
2022-08-29 20:33:03,398   loss = 2.2756157729170408
2022-08-29 20:33:03,398   rep_loss = 1.5079622878186267
2022-08-29 20:33:03,398 ***** Save model *****
2022-08-29 20:33:16,585 ***** Running evaluation *****
2022-08-29 20:33:16,586   Epoch = 3 iter 40399 step
2022-08-29 20:33:16,586   Num examples = 9815
2022-08-29 20:33:16,586   Batch size = 32
2022-08-29 20:33:16,587 ***** Eval results *****
2022-08-29 20:33:16,587   att_loss = 4.064227350052016
2022-08-29 20:33:16,587   cls_loss = 0.0
2022-08-29 20:33:16,587   global_step = 40399
2022-08-29 20:33:16,587   loss = 5.863744054635809
2022-08-29 20:33:16,587   rep_loss = 1.799516705614325
2022-08-29 20:33:16,587 ***** Save model *****
2022-08-29 20:33:28,062 ***** Running evaluation *****
2022-08-29 20:33:28,063   Epoch = 29 iter 21399 step
2022-08-29 20:33:28,063   Num examples = 1500
2022-08-29 20:33:28,063   Batch size = 32
2022-08-29 20:33:28,064 ***** Eval results *****
2022-08-29 20:33:28,064   att_loss = 2.1247213454965497
2022-08-29 20:33:28,064   cls_loss = 0.0
2022-08-29 20:33:28,065   global_step = 21399
2022-08-29 20:33:28,065   loss = 3.562705723417074
2022-08-29 20:33:28,065   rep_loss = 1.437984376887515
2022-08-29 20:33:28,065 ***** Save model *****
2022-08-29 20:33:41,819 ***** Running evaluation *****
2022-08-29 20:33:41,819   Epoch = 34 iter 36999 step
2022-08-29 20:33:41,819   Num examples = 1043
2022-08-29 20:33:41,819   Batch size = 32
2022-08-29 20:33:41,820 ***** Eval results *****
2022-08-29 20:33:41,821   att_loss = 0.766403037127449
2022-08-29 20:33:41,821   cls_loss = 0.0
2022-08-29 20:33:41,821   global_step = 36999
2022-08-29 20:33:41,821   loss = 2.2732291169562195
2022-08-29 20:33:41,821   rep_loss = 1.5068260771391868
2022-08-29 20:33:41,821 ***** Save model *****
2022-08-29 20:34:06,730 ***** Running evaluation *****
2022-08-29 20:34:06,730   Epoch = 30 iter 21599 step
2022-08-29 20:34:06,731   Num examples = 1500
2022-08-29 20:34:06,731   Batch size = 32
2022-08-29 20:34:06,732 ***** Eval results *****
2022-08-29 20:34:06,732   att_loss = 2.112287523382801
2022-08-29 20:34:06,732   cls_loss = 0.0
2022-08-29 20:34:06,732   global_step = 21599
2022-08-29 20:34:06,732   loss = 3.5691967333777477
2022-08-29 20:34:06,732   rep_loss = 1.4569092059539537
2022-08-29 20:34:06,732 ***** Save model *****
2022-08-29 20:34:20,060 ***** Running evaluation *****
2022-08-29 20:34:20,061   Epoch = 34 iter 37199 step
2022-08-29 20:34:20,061   Num examples = 1043
2022-08-29 20:34:20,061   Batch size = 32
2022-08-29 20:34:20,062 ***** Eval results *****
2022-08-29 20:34:20,062   att_loss = 0.7696476355951023
2022-08-29 20:34:20,062   cls_loss = 0.0
2022-08-29 20:34:20,062   global_step = 37199
2022-08-29 20:34:20,063   loss = 2.277195453778195
2022-08-29 20:34:20,063   rep_loss = 1.5075478153271766
2022-08-29 20:34:20,063 ***** Save model *****
2022-08-29 20:34:25,622 ***** Running evaluation *****
2022-08-29 20:34:25,622   Epoch = 3 iter 40599 step
2022-08-29 20:34:25,622   Num examples = 9815
2022-08-29 20:34:25,622   Batch size = 32
2022-08-29 20:34:25,624 ***** Eval results *****
2022-08-29 20:34:25,624   att_loss = 4.065550020681886
2022-08-29 20:34:25,624   cls_loss = 0.0
2022-08-29 20:34:25,624   global_step = 40599
2022-08-29 20:34:25,624   loss = 5.8648377468645165
2022-08-29 20:34:25,624   rep_loss = 1.7992877275995394
2022-08-29 20:34:25,624 ***** Save model *****
2022-08-29 20:34:45,584 ***** Running evaluation *****
2022-08-29 20:34:45,585   Epoch = 30 iter 21799 step
2022-08-29 20:34:45,585   Num examples = 1500
2022-08-29 20:34:45,585   Batch size = 32
2022-08-29 20:34:45,586 ***** Eval results *****
2022-08-29 20:34:45,587   att_loss = 2.1093475629924345
2022-08-29 20:34:45,587   cls_loss = 0.0
2022-08-29 20:34:45,587   global_step = 21799
2022-08-29 20:34:45,587   loss = 3.5462974783996817
2022-08-29 20:34:45,587   rep_loss = 1.4369499085032342
2022-08-29 20:34:45,587 ***** Save model *****
2022-08-29 20:35:03,130 ***** Running evaluation *****
2022-08-29 20:35:03,130   Epoch = 35 iter 37399 step
2022-08-29 20:35:03,130   Num examples = 1043
2022-08-29 20:35:03,130   Batch size = 32
2022-08-29 20:35:03,131 ***** Eval results *****
2022-08-29 20:35:03,132   att_loss = 0.7429432523877997
2022-08-29 20:35:03,132   cls_loss = 0.0
2022-08-29 20:35:03,132   global_step = 37399
2022-08-29 20:35:03,132   loss = 2.251258712065847
2022-08-29 20:35:03,132   rep_loss = 1.5083154690893072
2022-08-29 20:35:03,132 ***** Save model *****
2022-08-29 20:35:25,607 ***** Running evaluation *****
2022-08-29 20:35:25,608   Epoch = 30 iter 21999 step
2022-08-29 20:35:25,608   Num examples = 1500
2022-08-29 20:35:25,608   Batch size = 32
2022-08-29 20:35:25,609 ***** Eval results *****
2022-08-29 20:35:25,609   att_loss = 2.1254351248148997
2022-08-29 20:35:25,609   cls_loss = 0.0
2022-08-29 20:35:25,609   global_step = 21999
2022-08-29 20:35:25,609   loss = 3.5618016434131365
2022-08-29 20:35:25,609   rep_loss = 1.4363665141830777
2022-08-29 20:35:25,610 ***** Save model *****
2022-08-29 20:35:41,336 ***** Running evaluation *****
2022-08-29 20:35:41,336   Epoch = 35 iter 37599 step
2022-08-29 20:35:41,336   Num examples = 1043
2022-08-29 20:35:41,336   Batch size = 32
2022-08-29 20:35:41,338 ***** Eval results *****
2022-08-29 20:35:41,338   att_loss = 0.7663538817945681
2022-08-29 20:35:41,338   cls_loss = 0.0
2022-08-29 20:35:41,338   global_step = 37599
2022-08-29 20:35:41,338   loss = 2.2763639051620275
2022-08-29 20:35:41,338   rep_loss = 1.5100100160189416
2022-08-29 20:35:41,338 ***** Save model *****
2022-08-29 20:35:41,974 ***** Running evaluation *****
2022-08-29 20:35:41,975   Epoch = 3 iter 40799 step
2022-08-29 20:35:41,975   Num examples = 9815
2022-08-29 20:35:41,975   Batch size = 32
2022-08-29 20:35:41,977 ***** Eval results *****
2022-08-29 20:35:41,977   att_loss = 4.065974238824437
2022-08-29 20:35:41,977   cls_loss = 0.0
2022-08-29 20:35:41,977   global_step = 40799
2022-08-29 20:35:41,977   loss = 5.865065538039832
2022-08-29 20:35:41,977   rep_loss = 1.7990912997836273
2022-08-29 20:35:41,977 ***** Save model *****
2022-08-29 20:36:05,896 ***** Running evaluation *****
2022-08-29 20:36:05,897   Epoch = 30 iter 22199 step
2022-08-29 20:36:05,897   Num examples = 1500
2022-08-29 20:36:05,897   Batch size = 32
2022-08-29 20:36:05,898 ***** Eval results *****
2022-08-29 20:36:05,898   att_loss = 2.1382132638387144
2022-08-29 20:36:05,899   cls_loss = 0.0
2022-08-29 20:36:05,899   global_step = 22199
2022-08-29 20:36:05,899   loss = 3.5748972748045857
2022-08-29 20:36:05,899   rep_loss = 1.4366840026447372
2022-08-29 20:36:05,899 ***** Save model *****
2022-08-29 20:36:21,104 ***** Running evaluation *****
2022-08-29 20:36:21,105   Epoch = 35 iter 37799 step
2022-08-29 20:36:21,105   Num examples = 1043
2022-08-29 20:36:21,105   Batch size = 32
2022-08-29 20:36:21,106 ***** Eval results *****
2022-08-29 20:36:21,106   att_loss = 0.7671250479033568
2022-08-29 20:36:21,106   cls_loss = 0.0
2022-08-29 20:36:21,106   global_step = 37799
2022-08-29 20:36:21,107   loss = 2.2735470943063993
2022-08-29 20:36:21,107   rep_loss = 1.5064220408551163
2022-08-29 20:36:21,107 ***** Save model *****
2022-08-29 20:36:50,719 ***** Running evaluation *****
2022-08-29 20:36:50,720   Epoch = 31 iter 22399 step
2022-08-29 20:36:50,720   Num examples = 1500
2022-08-29 20:36:50,720   Batch size = 32
2022-08-29 20:36:50,721 ***** Eval results *****
2022-08-29 20:36:50,721   att_loss = 2.137112553238023
2022-08-29 20:36:50,721   cls_loss = 0.0
2022-08-29 20:36:50,721   global_step = 22399
2022-08-29 20:36:50,721   loss = 3.5693960680183787
2022-08-29 20:36:50,721   rep_loss = 1.4322835173167237
2022-08-29 20:36:50,721 ***** Save model *****
2022-08-29 20:36:53,835 ***** Running evaluation *****
2022-08-29 20:36:53,835   Epoch = 3 iter 40999 step
2022-08-29 20:36:53,835   Num examples = 9815
2022-08-29 20:36:53,835   Batch size = 32
2022-08-29 20:36:53,837 ***** Eval results *****
2022-08-29 20:36:53,837   att_loss = 4.065982460292172
2022-08-29 20:36:53,837   cls_loss = 0.0
2022-08-29 20:36:53,837   global_step = 40999
2022-08-29 20:36:53,837   loss = 5.864903200316304
2022-08-29 20:36:53,837   rep_loss = 1.7989207411347765
2022-08-29 20:36:53,837 ***** Save model *****
2022-08-29 20:36:59,308 ***** Running evaluation *****
2022-08-29 20:36:59,308   Epoch = 35 iter 37999 step
2022-08-29 20:36:59,308   Num examples = 1043
2022-08-29 20:36:59,308   Batch size = 32
2022-08-29 20:36:59,309 ***** Eval results *****
2022-08-29 20:36:59,309   att_loss = 0.761050965847607
2022-08-29 20:36:59,309   cls_loss = 0.0
2022-08-29 20:36:59,309   global_step = 37999
2022-08-29 20:36:59,310   loss = 2.263598421278601
2022-08-29 20:36:59,310   rep_loss = 1.5025474520607796
2022-08-29 20:36:59,310 ***** Save model *****
2022-08-29 20:37:30,954 ***** Running evaluation *****
2022-08-29 20:37:30,954   Epoch = 31 iter 22599 step
2022-08-29 20:37:30,955   Num examples = 1500
2022-08-29 20:37:30,955   Batch size = 32
2022-08-29 20:37:30,956 ***** Eval results *****
2022-08-29 20:37:30,956   att_loss = 2.1442575098132806
2022-08-29 20:37:30,956   cls_loss = 0.0
2022-08-29 20:37:30,956   global_step = 22599
2022-08-29 20:37:30,956   loss = 3.5772014405370807
2022-08-29 20:37:30,956   rep_loss = 1.4329439286262757
2022-08-29 20:37:30,956 ***** Save model *****
2022-08-29 20:37:37,531 ***** Running evaluation *****
2022-08-29 20:37:37,532   Epoch = 35 iter 38199 step
2022-08-29 20:37:37,532   Num examples = 1043
2022-08-29 20:37:37,532   Batch size = 32
2022-08-29 20:37:37,533 ***** Eval results *****
2022-08-29 20:37:37,533   att_loss = 0.7626173080323817
2022-08-29 20:37:37,533   cls_loss = 0.0
2022-08-29 20:37:37,533   global_step = 38199
2022-08-29 20:37:37,534   loss = 2.2634073035912055
2022-08-29 20:37:37,534   rep_loss = 1.500789993848556
2022-08-29 20:37:37,534 ***** Save model *****
2022-08-29 20:38:05,336 ***** Running evaluation *****
2022-08-29 20:38:05,336   Epoch = 3 iter 41199 step
2022-08-29 20:38:05,336   Num examples = 9815
2022-08-29 20:38:05,337   Batch size = 32
2022-08-29 20:38:05,338 ***** Eval results *****
2022-08-29 20:38:05,338   att_loss = 4.065054242653331
2022-08-29 20:38:05,338   cls_loss = 0.0
2022-08-29 20:38:05,338   global_step = 41199
2022-08-29 20:38:05,338   loss = 5.863708870953422
2022-08-29 20:38:05,338   rep_loss = 1.7986546293600911
2022-08-29 20:38:05,338 ***** Save model *****
2022-08-29 20:38:15,597 ***** Running evaluation *****
2022-08-29 20:38:15,597   Epoch = 31 iter 22799 step
2022-08-29 20:38:15,597   Num examples = 1500
2022-08-29 20:38:15,597   Batch size = 32
2022-08-29 20:38:15,599 ***** Eval results *****
2022-08-29 20:38:15,599   att_loss = 2.140030752488734
2022-08-29 20:38:15,599   cls_loss = 0.0
2022-08-29 20:38:15,599   global_step = 22799
2022-08-29 20:38:15,599   loss = 3.5728086993346153
2022-08-29 20:38:15,599   rep_loss = 1.4327779461848318
2022-08-29 20:38:15,599 ***** Save model *****
2022-08-29 20:38:20,012 ***** Running evaluation *****
2022-08-29 20:38:20,013   Epoch = 35 iter 38399 step
2022-08-29 20:38:20,013   Num examples = 1043
2022-08-29 20:38:20,013   Batch size = 32
2022-08-29 20:38:20,014 ***** Eval results *****
2022-08-29 20:38:20,014   att_loss = 0.7649933961645076
2022-08-29 20:38:20,014   cls_loss = 0.0
2022-08-29 20:38:20,014   global_step = 38399
2022-08-29 20:38:20,014   loss = 2.2656323955384274
2022-08-29 20:38:20,014   rep_loss = 1.500638998642754
2022-08-29 20:38:20,014 ***** Save model *****
2022-08-29 20:38:55,757 ***** Running evaluation *****
2022-08-29 20:38:55,757   Epoch = 32 iter 22999 step
2022-08-29 20:38:55,757   Num examples = 1500
2022-08-29 20:38:55,757   Batch size = 32
2022-08-29 20:38:55,759 ***** Eval results *****
2022-08-29 20:38:55,759   att_loss = 1.9936136318289714
2022-08-29 20:38:55,759   cls_loss = 0.0
2022-08-29 20:38:55,759   global_step = 22999
2022-08-29 20:38:55,759   loss = 3.389467581458714
2022-08-29 20:38:55,759   rep_loss = 1.3958539237146792
2022-08-29 20:38:55,759 ***** Save model *****
2022-08-29 20:38:58,201 ***** Running evaluation *****
2022-08-29 20:38:58,201   Epoch = 36 iter 38599 step
2022-08-29 20:38:58,201   Num examples = 1043
2022-08-29 20:38:58,201   Batch size = 32
2022-08-29 20:38:58,203 ***** Eval results *****
2022-08-29 20:38:58,203   att_loss = 0.7466866867431742
2022-08-29 20:38:58,203   cls_loss = 0.0
2022-08-29 20:38:58,203   global_step = 38599
2022-08-29 20:38:58,203   loss = 2.2449326420461895
2022-08-29 20:38:58,203   rep_loss = 1.4982459450399639
2022-08-29 20:38:58,203 ***** Save model *****
2022-08-29 20:39:16,720 ***** Running evaluation *****
2022-08-29 20:39:16,720   Epoch = 3 iter 41399 step
2022-08-29 20:39:16,720   Num examples = 9815
2022-08-29 20:39:16,721   Batch size = 32
2022-08-29 20:39:16,722 ***** Eval results *****
2022-08-29 20:39:16,722   att_loss = 4.06613248719225
2022-08-29 20:39:16,722   cls_loss = 0.0
2022-08-29 20:39:16,722   global_step = 41399
2022-08-29 20:39:16,722   loss = 5.86467012452393
2022-08-29 20:39:16,722   rep_loss = 1.798537638059517
2022-08-29 20:39:16,722 ***** Save model *****
2022-08-29 20:39:35,960 ***** Running evaluation *****
2022-08-29 20:39:35,960   Epoch = 32 iter 23199 step
2022-08-29 20:39:35,961   Num examples = 1500
2022-08-29 20:39:35,961   Batch size = 32
2022-08-29 20:39:35,962 ***** Eval results *****
2022-08-29 20:39:35,962   att_loss = 2.059871616919479
2022-08-29 20:39:35,962   cls_loss = 0.0
2022-08-29 20:39:35,962   global_step = 23199
2022-08-29 20:39:35,962   loss = 3.480041451518311
2022-08-29 20:39:35,962   rep_loss = 1.4201698383408277
2022-08-29 20:39:35,962 ***** Save model *****
2022-08-29 20:39:37,266 ***** Running evaluation *****
2022-08-29 20:39:37,266   Epoch = 36 iter 38799 step
2022-08-29 20:39:37,267   Num examples = 1043
2022-08-29 20:39:37,267   Batch size = 32
2022-08-29 20:39:37,268 ***** Eval results *****
2022-08-29 20:39:37,268   att_loss = 0.7601379704611254
2022-08-29 20:39:37,268   cls_loss = 0.0
2022-08-29 20:39:37,268   global_step = 38799
2022-08-29 20:39:37,268   loss = 2.2638905938194687
2022-08-29 20:39:37,268   rep_loss = 1.5037526201318812
2022-08-29 20:39:37,268 ***** Save model *****
2022-08-29 20:40:17,240 ***** Running evaluation *****
2022-08-29 20:40:17,241   Epoch = 32 iter 23399 step
2022-08-29 20:40:17,241   Num examples = 1500
2022-08-29 20:40:17,241   Batch size = 32
2022-08-29 20:40:17,242 ***** Eval results *****
2022-08-29 20:40:17,242   att_loss = 2.096487948235045
2022-08-29 20:40:17,242   cls_loss = 0.0
2022-08-29 20:40:17,242   global_step = 23399
2022-08-29 20:40:17,243   loss = 3.5194117588917977
2022-08-29 20:40:17,243   rep_loss = 1.4229238053021984
2022-08-29 20:40:17,243 ***** Save model *****
2022-08-29 20:40:17,440 ***** Running evaluation *****
2022-08-29 20:40:17,441   Epoch = 36 iter 38999 step
2022-08-29 20:40:17,441   Num examples = 1043
2022-08-29 20:40:17,441   Batch size = 32
2022-08-29 20:40:17,442 ***** Eval results *****
2022-08-29 20:40:17,442   att_loss = 0.7632999751182736
2022-08-29 20:40:17,442   cls_loss = 0.0
2022-08-29 20:40:17,442   global_step = 38999
2022-08-29 20:40:17,442   loss = 2.2654945313822337
2022-08-29 20:40:17,442   rep_loss = 1.5021945528023473
2022-08-29 20:40:17,442 ***** Save model *****
2022-08-29 20:40:25,806 ***** Running evaluation *****
2022-08-29 20:40:25,807   Epoch = 3 iter 41599 step
2022-08-29 20:40:25,807   Num examples = 9815
2022-08-29 20:40:25,807   Batch size = 32
2022-08-29 20:40:25,808 ***** Eval results *****
2022-08-29 20:40:25,808   att_loss = 4.064078967839268
2022-08-29 20:40:25,808   cls_loss = 0.0
2022-08-29 20:40:25,808   global_step = 41599
2022-08-29 20:40:25,808   loss = 5.862298968741946
2022-08-29 20:40:25,808   rep_loss = 1.7982200006785076
2022-08-29 20:40:25,808 ***** Save model *****
2022-08-29 20:40:59,739 ***** Running evaluation *****
2022-08-29 20:40:59,739   Epoch = 32 iter 23599 step
2022-08-29 20:40:59,740   Num examples = 1500
2022-08-29 20:40:59,740   Batch size = 32
2022-08-29 20:40:59,741 ***** Eval results *****
2022-08-29 20:40:59,741   att_loss = 2.108915037749093
2022-08-29 20:40:59,741   cls_loss = 0.0
2022-08-29 20:40:59,741   global_step = 23599
2022-08-29 20:40:59,741   loss = 3.535114393571024
2022-08-29 20:40:59,741   rep_loss = 1.4261993546738478
2022-08-29 20:40:59,741 ***** Save model *****
2022-08-29 20:41:00,163 ***** Running evaluation *****
2022-08-29 20:41:00,164   Epoch = 36 iter 39199 step
2022-08-29 20:41:00,164   Num examples = 1043
2022-08-29 20:41:00,164   Batch size = 32
2022-08-29 20:41:00,166 ***** Eval results *****
2022-08-29 20:41:00,166   att_loss = 0.7647763785922257
2022-08-29 20:41:00,166   cls_loss = 0.0
2022-08-29 20:41:00,166   global_step = 39199
2022-08-29 20:41:00,166   loss = 2.268300705838616
2022-08-29 20:41:00,166   rep_loss = 1.5035243248653793
2022-08-29 20:41:00,166 ***** Save model *****
2022-08-29 20:41:34,957 ***** Running evaluation *****
2022-08-29 20:41:34,957   Epoch = 3 iter 41799 step
2022-08-29 20:41:34,957   Num examples = 9815
2022-08-29 20:41:34,957   Batch size = 32
2022-08-29 20:41:34,958 ***** Eval results *****
2022-08-29 20:41:34,959   att_loss = 4.062665052649203
2022-08-29 20:41:34,959   cls_loss = 0.0
2022-08-29 20:41:34,959   global_step = 41799
2022-08-29 20:41:34,959   loss = 5.86060992149861
2022-08-29 20:41:34,959   rep_loss = 1.7979448683951402
2022-08-29 20:41:34,959 ***** Save model *****
2022-08-29 20:41:46,673 ***** Running evaluation *****
2022-08-29 20:41:46,673   Epoch = 36 iter 39399 step
2022-08-29 20:41:46,673   Num examples = 1043
2022-08-29 20:41:46,673   Batch size = 32
2022-08-29 20:41:46,674 ***** Eval results *****
2022-08-29 20:41:46,675   att_loss = 0.766115823974118
2022-08-29 20:41:46,675   cls_loss = 0.0
2022-08-29 20:41:46,675   global_step = 39399
2022-08-29 20:41:46,675   loss = 2.269756524094021
2022-08-29 20:41:46,675   rep_loss = 1.5036406969547773
2022-08-29 20:41:46,675 ***** Save model *****
2022-08-29 20:41:47,536 ***** Running evaluation *****
2022-08-29 20:41:47,537   Epoch = 33 iter 23799 step
2022-08-29 20:41:47,537   Num examples = 1500
2022-08-29 20:41:47,537   Batch size = 32
2022-08-29 20:41:47,539 ***** Eval results *****
2022-08-29 20:41:47,539   att_loss = 2.1528506710415796
2022-08-29 20:41:47,539   cls_loss = 0.0
2022-08-29 20:41:47,539   global_step = 23799
2022-08-29 20:41:47,539   loss = 3.5779511542547318
2022-08-29 20:41:47,539   rep_loss = 1.425100480942499
2022-08-29 20:41:47,539 ***** Save model *****
2022-08-29 20:42:27,831 ***** Running evaluation *****
2022-08-29 20:42:27,832   Epoch = 37 iter 39599 step
2022-08-29 20:42:27,832   Num examples = 1043
2022-08-29 20:42:27,832   Batch size = 32
2022-08-29 20:42:27,833 ***** Eval results *****
2022-08-29 20:42:27,834   att_loss = 0.7509086239768799
2022-08-29 20:42:27,834   cls_loss = 0.0
2022-08-29 20:42:27,834   global_step = 39599
2022-08-29 20:42:27,834   loss = 2.250450108424727
2022-08-29 20:42:27,834   rep_loss = 1.4995414736759232
2022-08-29 20:42:27,834 ***** Save model *****
2022-08-29 20:42:30,097 ***** Running evaluation *****
2022-08-29 20:42:30,098   Epoch = 33 iter 23999 step
2022-08-29 20:42:30,098   Num examples = 1500
2022-08-29 20:42:30,098   Batch size = 32
2022-08-29 20:42:30,099 ***** Eval results *****
2022-08-29 20:42:30,100   att_loss = 2.1135586550978362
2022-08-29 20:42:30,100   cls_loss = 0.0
2022-08-29 20:42:30,100   global_step = 23999
2022-08-29 20:42:30,100   loss = 3.540457663770582
2022-08-29 20:42:30,100   rep_loss = 1.4268990117995466
2022-08-29 20:42:30,100 ***** Save model *****
2022-08-29 20:42:46,222 ***** Running evaluation *****
2022-08-29 20:42:46,223   Epoch = 3 iter 41999 step
2022-08-29 20:42:46,223   Num examples = 9815
2022-08-29 20:42:46,223   Batch size = 32
2022-08-29 20:42:46,224 ***** Eval results *****
2022-08-29 20:42:46,224   att_loss = 4.063022128964684
2022-08-29 20:42:46,224   cls_loss = 0.0
2022-08-29 20:42:46,224   global_step = 41999
2022-08-29 20:42:46,224   loss = 5.860778714440019
2022-08-29 20:42:46,224   rep_loss = 1.797756585360401
2022-08-29 20:42:46,224 ***** Save model *****
2022-08-29 20:43:11,087 ***** Running evaluation *****
2022-08-29 20:43:11,088   Epoch = 37 iter 39799 step
2022-08-29 20:43:11,088   Num examples = 1043
2022-08-29 20:43:11,088   Batch size = 32
2022-08-29 20:43:11,089 ***** Eval results *****
2022-08-29 20:43:11,089   att_loss = 0.7520697358008408
2022-08-29 20:43:11,089   cls_loss = 0.0
2022-08-29 20:43:11,090   global_step = 39799
2022-08-29 20:43:11,090   loss = 2.2465184059244163
2022-08-29 20:43:11,090   rep_loss = 1.4944486622254334
2022-08-29 20:43:11,090 ***** Save model *****
2022-08-29 20:43:13,431 ***** Running evaluation *****
2022-08-29 20:43:13,432   Epoch = 33 iter 24199 step
2022-08-29 20:43:13,432   Num examples = 1500
2022-08-29 20:43:13,432   Batch size = 32
2022-08-29 20:43:13,433 ***** Eval results *****
2022-08-29 20:43:13,433   att_loss = 2.1065184798571144
2022-08-29 20:43:13,433   cls_loss = 0.0
2022-08-29 20:43:13,433   global_step = 24199
2022-08-29 20:43:13,433   loss = 3.529756785855435
2022-08-29 20:43:13,433   rep_loss = 1.4232383052901467
2022-08-29 20:43:13,433 ***** Save model *****
2022-08-29 20:43:50,317 ***** Running evaluation *****
2022-08-29 20:43:50,317   Epoch = 37 iter 39999 step
2022-08-29 20:43:50,317   Num examples = 1043
2022-08-29 20:43:50,318   Batch size = 32
2022-08-29 20:43:50,319 ***** Eval results *****
2022-08-29 20:43:50,319   att_loss = 0.7608990610633084
2022-08-29 20:43:50,319   cls_loss = 0.0
2022-08-29 20:43:50,319   global_step = 39999
2022-08-29 20:43:50,319   loss = 2.2578440092612
2022-08-29 20:43:50,319   rep_loss = 1.4969449448042529
2022-08-29 20:43:50,319 ***** Save model *****
2022-08-29 20:43:54,499 ***** Running evaluation *****
2022-08-29 20:43:54,499   Epoch = 33 iter 24399 step
2022-08-29 20:43:54,499   Num examples = 1500
2022-08-29 20:43:54,499   Batch size = 32
2022-08-29 20:43:54,501 ***** Eval results *****
2022-08-29 20:43:54,501   att_loss = 2.115531898559408
2022-08-29 20:43:54,501   cls_loss = 0.0
2022-08-29 20:43:54,501   global_step = 24399
2022-08-29 20:43:54,501   loss = 3.53899434542825
2022-08-29 20:43:54,501   rep_loss = 1.4234624463615688
2022-08-29 20:43:54,501 ***** Save model *****
2022-08-29 20:43:55,456 ***** Running evaluation *****
2022-08-29 20:43:55,456   Epoch = 3 iter 42199 step
2022-08-29 20:43:55,456   Num examples = 9815
2022-08-29 20:43:55,456   Batch size = 32
2022-08-29 20:43:55,458 ***** Eval results *****
2022-08-29 20:43:55,458   att_loss = 4.0628798363866565
2022-08-29 20:43:55,458   cls_loss = 0.0
2022-08-29 20:43:55,458   global_step = 42199
2022-08-29 20:43:55,458   loss = 5.860533769715908
2022-08-29 20:43:55,458   rep_loss = 1.7976539332628516
2022-08-29 20:43:55,458 ***** Save model *****
2022-08-29 20:44:29,839 ***** Running evaluation *****
2022-08-29 20:44:29,840   Epoch = 37 iter 40199 step
2022-08-29 20:44:29,840   Num examples = 1043
2022-08-29 20:44:29,840   Batch size = 32
2022-08-29 20:44:29,841 ***** Eval results *****
2022-08-29 20:44:29,842   att_loss = 0.7583190764288253
2022-08-29 20:44:29,842   cls_loss = 0.0
2022-08-29 20:44:29,842   global_step = 40199
2022-08-29 20:44:29,842   loss = 2.2544803425647855
2022-08-29 20:44:29,842   rep_loss = 1.496161262776108
2022-08-29 20:44:29,842 ***** Save model *****
2022-08-29 20:44:36,457 ***** Running evaluation *****
2022-08-29 20:44:36,458   Epoch = 34 iter 24599 step
2022-08-29 20:44:36,458   Num examples = 1500
2022-08-29 20:44:36,458   Batch size = 32
2022-08-29 20:44:36,459 ***** Eval results *****
2022-08-29 20:44:36,459   att_loss = 2.1159751122010584
2022-08-29 20:44:36,459   cls_loss = 0.0
2022-08-29 20:44:36,459   global_step = 24599
2022-08-29 20:44:36,460   loss = 3.539311658889852
2022-08-29 20:44:36,460   rep_loss = 1.4233365568885192
2022-08-29 20:44:36,460 ***** Save model *****
2022-08-29 20:45:09,366 ***** Running evaluation *****
2022-08-29 20:45:09,367   Epoch = 3 iter 42399 step
2022-08-29 20:45:09,367   Num examples = 9815
2022-08-29 20:45:09,367   Batch size = 32
2022-08-29 20:45:09,368 ***** Eval results *****
2022-08-29 20:45:09,368   att_loss = 4.060276871080942
2022-08-29 20:45:09,368   cls_loss = 0.0
2022-08-29 20:45:09,368   global_step = 42399
2022-08-29 20:45:09,368   loss = 5.857511750438757
2022-08-29 20:45:09,368   rep_loss = 1.79723487948586
2022-08-29 20:45:09,368 ***** Save model *****
2022-08-29 20:45:10,461 ***** Running evaluation *****
2022-08-29 20:45:10,461   Epoch = 37 iter 40399 step
2022-08-29 20:45:10,461   Num examples = 1043
2022-08-29 20:45:10,461   Batch size = 32
2022-08-29 20:45:10,462 ***** Eval results *****
2022-08-29 20:45:10,463   att_loss = 0.7615968318042237
2022-08-29 20:45:10,463   cls_loss = 0.0
2022-08-29 20:45:10,463   global_step = 40399
2022-08-29 20:45:10,463   loss = 2.2587494523446936
2022-08-29 20:45:10,463   rep_loss = 1.4971526194266798
2022-08-29 20:45:10,463 ***** Save model *****
2022-08-29 20:45:16,677 ***** Running evaluation *****
2022-08-29 20:45:16,678   Epoch = 34 iter 24799 step
2022-08-29 20:45:16,678   Num examples = 1500
2022-08-29 20:45:16,678   Batch size = 32
2022-08-29 20:45:16,679 ***** Eval results *****
2022-08-29 20:45:16,679   att_loss = 2.1101037932304756
2022-08-29 20:45:16,679   cls_loss = 0.0
2022-08-29 20:45:16,680   global_step = 24799
2022-08-29 20:45:16,680   loss = 3.531624659087307
2022-08-29 20:45:16,680   rep_loss = 1.4215208633925565
2022-08-29 20:45:16,680 ***** Save model *****
2022-08-29 20:45:50,787 ***** Running evaluation *****
2022-08-29 20:45:50,787   Epoch = 38 iter 40599 step
2022-08-29 20:45:50,787   Num examples = 1043
2022-08-29 20:45:50,787   Batch size = 32
2022-08-29 20:45:50,788 ***** Eval results *****
2022-08-29 20:45:50,789   att_loss = 0.7743980566660563
2022-08-29 20:45:50,789   cls_loss = 0.0
2022-08-29 20:45:50,789   global_step = 40599
2022-08-29 20:45:50,789   loss = 2.2836836020151776
2022-08-29 20:45:50,789   rep_loss = 1.509285537401835
2022-08-29 20:45:50,789 ***** Save model *****
2022-08-29 20:45:55,499 ***** Running evaluation *****
2022-08-29 20:45:55,499   Epoch = 34 iter 24999 step
2022-08-29 20:45:55,499   Num examples = 1500
2022-08-29 20:45:55,499   Batch size = 32
2022-08-29 20:45:55,500 ***** Eval results *****
2022-08-29 20:45:55,501   att_loss = 2.1084255367974407
2022-08-29 20:45:55,501   cls_loss = 0.0
2022-08-29 20:45:55,501   global_step = 24999
2022-08-29 20:45:55,501   loss = 3.5288475278695155
2022-08-29 20:45:55,501   rep_loss = 1.420421990462828
2022-08-29 20:45:55,501 ***** Save model *****
2022-08-29 20:46:21,862 ***** Running evaluation *****
2022-08-29 20:46:21,863   Epoch = 3 iter 42599 step
2022-08-29 20:46:21,863   Num examples = 9815
2022-08-29 20:46:21,863   Batch size = 32
2022-08-29 20:46:21,864 ***** Eval results *****
2022-08-29 20:46:21,864   att_loss = 4.059277227746546
2022-08-29 20:46:21,864   cls_loss = 0.0
2022-08-29 20:46:21,864   global_step = 42599
2022-08-29 20:46:21,864   loss = 5.8561577028854055
2022-08-29 20:46:21,864   rep_loss = 1.796880475550921
2022-08-29 20:46:21,864 ***** Save model *****
2022-08-29 20:46:33,870 ***** Running evaluation *****
2022-08-29 20:46:33,871   Epoch = 38 iter 40799 step
2022-08-29 20:46:33,871   Num examples = 1043
2022-08-29 20:46:33,871   Batch size = 32
2022-08-29 20:46:33,872 ***** Eval results *****
2022-08-29 20:46:33,872   att_loss = 0.7617480675841487
2022-08-29 20:46:33,872   cls_loss = 0.0
2022-08-29 20:46:33,872   global_step = 40799
2022-08-29 20:46:33,872   loss = 2.26248277120812
2022-08-29 20:46:33,872   rep_loss = 1.500734694059505
2022-08-29 20:46:33,872 ***** Save model *****
2022-08-29 20:46:39,129 ***** Running evaluation *****
2022-08-29 20:46:39,130   Epoch = 35 iter 25199 step
2022-08-29 20:46:39,130   Num examples = 1500
2022-08-29 20:46:39,130   Batch size = 32
2022-08-29 20:46:39,131 ***** Eval results *****
2022-08-29 20:46:39,132   att_loss = 2.113909087319305
2022-08-29 20:46:39,132   cls_loss = 0.0
2022-08-29 20:46:39,132   global_step = 25199
2022-08-29 20:46:39,132   loss = 3.542100194571675
2022-08-29 20:46:39,132   rep_loss = 1.4281910761542942
2022-08-29 20:46:39,132 ***** Save model *****
2022-08-29 20:47:12,085 ***** Running evaluation *****
2022-08-29 20:47:12,085   Epoch = 38 iter 40999 step
2022-08-29 20:47:12,085   Num examples = 1043
2022-08-29 20:47:12,085   Batch size = 32
2022-08-29 20:47:12,086 ***** Eval results *****
2022-08-29 20:47:12,086   att_loss = 0.7653786929975073
2022-08-29 20:47:12,086   cls_loss = 0.0
2022-08-29 20:47:12,086   global_step = 40999
2022-08-29 20:47:12,086   loss = 2.266017367753638
2022-08-29 20:47:12,087   rep_loss = 1.5006386670721583
2022-08-29 20:47:12,087 ***** Save model *****
2022-08-29 20:47:17,818 ***** Running evaluation *****
2022-08-29 20:47:17,818   Epoch = 35 iter 25399 step
2022-08-29 20:47:17,818   Num examples = 1500
2022-08-29 20:47:17,819   Batch size = 32
2022-08-29 20:47:17,820 ***** Eval results *****
2022-08-29 20:47:17,820   att_loss = 2.1017889422554954
2022-08-29 20:47:17,820   cls_loss = 0.0
2022-08-29 20:47:17,820   global_step = 25399
2022-08-29 20:47:17,820   loss = 3.5174844752457064
2022-08-29 20:47:17,820   rep_loss = 1.415695528558639
2022-08-29 20:47:17,820 ***** Save model *****
2022-08-29 20:47:30,823 ***** Running evaluation *****
2022-08-29 20:47:30,823   Epoch = 3 iter 42799 step
2022-08-29 20:47:30,823   Num examples = 9815
2022-08-29 20:47:30,823   Batch size = 32
2022-08-29 20:47:30,825 ***** Eval results *****
2022-08-29 20:47:30,825   att_loss = 4.058223748382342
2022-08-29 20:47:30,825   cls_loss = 0.0
2022-08-29 20:47:30,825   global_step = 42799
2022-08-29 20:47:30,825   loss = 5.854848533739344
2022-08-29 20:47:30,825   rep_loss = 1.7966247855163202
2022-08-29 20:47:30,825 ***** Save model *****
2022-08-29 20:47:50,311 ***** Running evaluation *****
2022-08-29 20:47:50,312   Epoch = 38 iter 41199 step
2022-08-29 20:47:50,312   Num examples = 1043
2022-08-29 20:47:50,312   Batch size = 32
2022-08-29 20:47:50,314 ***** Eval results *****
2022-08-29 20:47:50,314   att_loss = 0.7654640551989641
2022-08-29 20:47:50,314   cls_loss = 0.0
2022-08-29 20:47:50,314   global_step = 41199
2022-08-29 20:47:50,314   loss = 2.266223344376417
2022-08-29 20:47:50,314   rep_loss = 1.50075928312007
2022-08-29 20:47:50,314 ***** Save model *****
2022-08-29 20:47:58,216 ***** Running evaluation *****
2022-08-29 20:47:58,216   Epoch = 35 iter 25599 step
2022-08-29 20:47:58,217   Num examples = 1500
2022-08-29 20:47:58,217   Batch size = 32
2022-08-29 20:47:58,218 ***** Eval results *****
2022-08-29 20:47:58,218   att_loss = 2.112856767070827
2022-08-29 20:47:58,218   cls_loss = 0.0
2022-08-29 20:47:58,218   global_step = 25599
2022-08-29 20:47:58,218   loss = 3.5330289203220846
2022-08-29 20:47:58,218   rep_loss = 1.4201721593515197
2022-08-29 20:47:58,218 ***** Save model *****
2022-08-29 20:48:32,530 ***** Running evaluation *****
2022-08-29 20:48:32,531   Epoch = 38 iter 41399 step
2022-08-29 20:48:32,531   Num examples = 1043
2022-08-29 20:48:32,531   Batch size = 32
2022-08-29 20:48:32,532 ***** Eval results *****
2022-08-29 20:48:32,532   att_loss = 0.7626265017898536
2022-08-29 20:48:32,532   cls_loss = 0.0
2022-08-29 20:48:32,532   global_step = 41399
2022-08-29 20:48:32,532   loss = 2.2609299289668265
2022-08-29 20:48:32,532   rep_loss = 1.498303421874719
2022-08-29 20:48:32,532 ***** Save model *****
2022-08-29 20:48:39,780 ***** Running evaluation *****
2022-08-29 20:48:39,781   Epoch = 3 iter 42999 step
2022-08-29 20:48:39,781   Num examples = 9815
2022-08-29 20:48:39,781   Batch size = 32
2022-08-29 20:48:39,782 ***** Eval results *****
2022-08-29 20:48:39,782   att_loss = 4.058257370792158
2022-08-29 20:48:39,782   cls_loss = 0.0
2022-08-29 20:48:39,782   global_step = 42999
2022-08-29 20:48:39,782   loss = 5.854674785837863
2022-08-29 20:48:39,782   rep_loss = 1.7964174155852881
2022-08-29 20:48:39,782 ***** Save model *****
2022-08-29 20:48:40,399 ***** Running evaluation *****
2022-08-29 20:48:40,400   Epoch = 35 iter 25799 step
2022-08-29 20:48:40,400   Num examples = 1500
2022-08-29 20:48:40,400   Batch size = 32
2022-08-29 20:48:40,401 ***** Eval results *****
2022-08-29 20:48:40,401   att_loss = 2.098453376681637
2022-08-29 20:48:40,401   cls_loss = 0.0
2022-08-29 20:48:40,401   global_step = 25799
2022-08-29 20:48:40,401   loss = 3.5139731081433716
2022-08-29 20:48:40,402   rep_loss = 1.415519737163823
2022-08-29 20:48:40,402 ***** Save model *****
2022-08-29 20:49:10,857 ***** Running evaluation *****
2022-08-29 20:49:10,858   Epoch = 38 iter 41599 step
2022-08-29 20:49:10,858   Num examples = 1043
2022-08-29 20:49:10,858   Batch size = 32
2022-08-29 20:49:10,859 ***** Eval results *****
2022-08-29 20:49:10,859   att_loss = 0.7609211359411625
2022-08-29 20:49:10,859   cls_loss = 0.0
2022-08-29 20:49:10,859   global_step = 41599
2022-08-29 20:49:10,859   loss = 2.2564164477616107
2022-08-29 20:49:10,859   rep_loss = 1.4954953075042499
2022-08-29 20:49:10,859 ***** Save model *****
2022-08-29 20:49:23,324 ***** Running evaluation *****
2022-08-29 20:49:23,325   Epoch = 36 iter 25999 step
2022-08-29 20:49:23,325   Num examples = 1500
2022-08-29 20:49:23,325   Batch size = 32
2022-08-29 20:49:23,326 ***** Eval results *****
2022-08-29 20:49:23,326   att_loss = 2.1167882854575355
2022-08-29 20:49:23,326   cls_loss = 0.0
2022-08-29 20:49:23,326   global_step = 25999
2022-08-29 20:49:23,326   loss = 3.5346946447890324
2022-08-29 20:49:23,327   rep_loss = 1.4179063585420317
2022-08-29 20:49:23,327 ***** Save model *****
2022-08-29 20:49:49,220 ***** Running evaluation *****
2022-08-29 20:49:49,220   Epoch = 39 iter 41799 step
2022-08-29 20:49:49,220   Num examples = 1043
2022-08-29 20:49:49,220   Batch size = 32
2022-08-29 20:49:49,222 ***** Eval results *****
2022-08-29 20:49:49,222   att_loss = 0.7611069334607546
2022-08-29 20:49:49,222   cls_loss = 0.0
2022-08-29 20:49:49,222   global_step = 41799
2022-08-29 20:49:49,222   loss = 2.2477396507652436
2022-08-29 20:49:49,222   rep_loss = 1.4866327152771204
2022-08-29 20:49:49,222 ***** Save model *****
2022-08-29 20:49:52,242 ***** Running evaluation *****
2022-08-29 20:49:52,242   Epoch = 3 iter 43199 step
2022-08-29 20:49:52,242   Num examples = 9815
2022-08-29 20:49:52,242   Batch size = 32
2022-08-29 20:49:52,244 ***** Eval results *****
2022-08-29 20:49:52,244   att_loss = 4.05621224572731
2022-08-29 20:49:52,244   cls_loss = 0.0
2022-08-29 20:49:52,244   global_step = 43199
2022-08-29 20:49:52,244   loss = 5.8522460365086335
2022-08-29 20:49:52,244   rep_loss = 1.7960337910053314
2022-08-29 20:49:52,244 ***** Save model *****
2022-08-29 20:50:02,034 ***** Running evaluation *****
2022-08-29 20:50:02,035   Epoch = 36 iter 26199 step
2022-08-29 20:50:02,035   Num examples = 1500
2022-08-29 20:50:02,035   Batch size = 32
2022-08-29 20:50:02,036 ***** Eval results *****
2022-08-29 20:50:02,037   att_loss = 2.0678747328597935
2022-08-29 20:50:02,037   cls_loss = 0.0
2022-08-29 20:50:02,037   global_step = 26199
2022-08-29 20:50:02,037   loss = 3.4765769090407934
2022-08-29 20:50:02,037   rep_loss = 1.408702176181
2022-08-29 20:50:02,037 ***** Save model *****
2022-08-29 20:50:27,646 ***** Running evaluation *****
2022-08-29 20:50:27,647   Epoch = 39 iter 41999 step
2022-08-29 20:50:27,647   Num examples = 1043
2022-08-29 20:50:27,647   Batch size = 32
2022-08-29 20:50:27,648 ***** Eval results *****
2022-08-29 20:50:27,648   att_loss = 0.7599799250834957
2022-08-29 20:50:27,648   cls_loss = 0.0
2022-08-29 20:50:27,648   global_step = 41999
2022-08-29 20:50:27,648   loss = 2.2499151628367837
2022-08-29 20:50:27,649   rep_loss = 1.489935236293232
2022-08-29 20:50:27,649 ***** Save model *****
2022-08-29 20:50:40,764 ***** Running evaluation *****
2022-08-29 20:50:40,765   Epoch = 36 iter 26399 step
2022-08-29 20:50:40,765   Num examples = 1500
2022-08-29 20:50:40,765   Batch size = 32
2022-08-29 20:50:40,766 ***** Eval results *****
2022-08-29 20:50:40,766   att_loss = 2.078585207570486
2022-08-29 20:50:40,766   cls_loss = 0.0
2022-08-29 20:50:40,766   global_step = 26399
2022-08-29 20:50:40,767   loss = 3.487657797531294
2022-08-29 20:50:40,767   rep_loss = 1.4090725873645984
2022-08-29 20:50:40,767 ***** Save model *****
2022-08-29 20:51:01,800 ***** Running evaluation *****
2022-08-29 20:51:01,801   Epoch = 3 iter 43399 step
2022-08-29 20:51:01,801   Num examples = 9815
2022-08-29 20:51:01,801   Batch size = 32
2022-08-29 20:51:01,802 ***** Eval results *****
2022-08-29 20:51:01,802   att_loss = 4.0562725661556085
2022-08-29 20:51:01,802   cls_loss = 0.0
2022-08-29 20:51:01,802   global_step = 43399
2022-08-29 20:51:01,802   loss = 5.852086049533709
2022-08-29 20:51:01,802   rep_loss = 1.7958134834867026
2022-08-29 20:51:01,802 ***** Save model *****
2022-08-29 20:51:06,068 ***** Running evaluation *****
2022-08-29 20:51:06,069   Epoch = 39 iter 42199 step
2022-08-29 20:51:06,069   Num examples = 1043
2022-08-29 20:51:06,069   Batch size = 32
2022-08-29 20:51:06,070 ***** Eval results *****
2022-08-29 20:51:06,070   att_loss = 0.7584125763537461
2022-08-29 20:51:06,070   cls_loss = 0.0
2022-08-29 20:51:06,070   global_step = 42199
2022-08-29 20:51:06,070   loss = 2.248292845389525
2022-08-29 20:51:06,070   rep_loss = 1.4898802663116175
2022-08-29 20:51:06,071 ***** Save model *****
2022-08-29 20:51:19,487 ***** Running evaluation *****
2022-08-29 20:51:19,488   Epoch = 37 iter 26599 step
2022-08-29 20:51:19,488   Num examples = 1500
2022-08-29 20:51:19,488   Batch size = 32
2022-08-29 20:51:19,489 ***** Eval results *****
2022-08-29 20:51:19,489   att_loss = 2.158728017951503
2022-08-29 20:51:19,489   cls_loss = 0.0
2022-08-29 20:51:19,489   global_step = 26599
2022-08-29 20:51:19,489   loss = 3.5720989415139863
2022-08-29 20:51:19,489   rep_loss = 1.4133709018880671
2022-08-29 20:51:19,490 ***** Save model *****
2022-08-29 20:51:45,141 ***** Running evaluation *****
2022-08-29 20:51:45,141   Epoch = 39 iter 42399 step
2022-08-29 20:51:45,142   Num examples = 1043
2022-08-29 20:51:45,142   Batch size = 32
2022-08-29 20:51:45,143 ***** Eval results *****
2022-08-29 20:51:45,143   att_loss = 0.7560847139103186
2022-08-29 20:51:45,143   cls_loss = 0.0
2022-08-29 20:51:45,143   global_step = 42399
2022-08-29 20:51:45,143   loss = 2.2473281597995376
2022-08-29 20:51:45,143   rep_loss = 1.4912434442933784
2022-08-29 20:51:45,143 ***** Save model *****
2022-08-29 20:51:58,107 ***** Running evaluation *****
2022-08-29 20:51:58,107   Epoch = 37 iter 26799 step
2022-08-29 20:51:58,107   Num examples = 1500
2022-08-29 20:51:58,107   Batch size = 32
2022-08-29 20:51:58,109 ***** Eval results *****
2022-08-29 20:51:58,109   att_loss = 2.093743217861192
2022-08-29 20:51:58,109   cls_loss = 0.0
2022-08-29 20:51:58,109   global_step = 26799
2022-08-29 20:51:58,109   loss = 3.504020344034285
2022-08-29 20:51:58,109   rep_loss = 1.4102771215684424
2022-08-29 20:51:58,109 ***** Save model *****
2022-08-29 20:52:13,093 ***** Running evaluation *****
2022-08-29 20:52:13,093   Epoch = 3 iter 43599 step
2022-08-29 20:52:13,093   Num examples = 9815
2022-08-29 20:52:13,093   Batch size = 32
2022-08-29 20:52:13,094 ***** Eval results *****
2022-08-29 20:52:13,095   att_loss = 4.055886268967061
2022-08-29 20:52:13,095   cls_loss = 0.0
2022-08-29 20:52:13,095   global_step = 43599
2022-08-29 20:52:13,095   loss = 5.851406616128865
2022-08-29 20:52:13,095   rep_loss = 1.7955203467753305
2022-08-29 20:52:13,095 ***** Save model *****
2022-08-29 20:52:23,553 ***** Running evaluation *****
2022-08-29 20:52:23,554   Epoch = 39 iter 42599 step
2022-08-29 20:52:23,554   Num examples = 1043
2022-08-29 20:52:23,554   Batch size = 32
2022-08-29 20:52:23,555 ***** Eval results *****
2022-08-29 20:52:23,555   att_loss = 0.7576272775907829
2022-08-29 20:52:23,555   cls_loss = 0.0
2022-08-29 20:52:23,555   global_step = 42599
2022-08-29 20:52:23,556   loss = 2.2493262517540358
2022-08-29 20:52:23,556   rep_loss = 1.491698973282086
2022-08-29 20:52:23,556 ***** Save model *****
2022-08-29 20:52:36,743 ***** Running evaluation *****
2022-08-29 20:52:36,743   Epoch = 37 iter 26999 step
2022-08-29 20:52:36,743   Num examples = 1500
2022-08-29 20:52:36,744   Batch size = 32
2022-08-29 20:52:36,745 ***** Eval results *****
2022-08-29 20:52:36,745   att_loss = 2.0773612423251464
2022-08-29 20:52:36,745   cls_loss = 0.0
2022-08-29 20:52:36,745   global_step = 26999
2022-08-29 20:52:36,745   loss = 3.482301940543547
2022-08-29 20:52:36,745   rep_loss = 1.4049406913356473
2022-08-29 20:52:36,745 ***** Save model *****
2022-08-29 20:53:01,980 ***** Running evaluation *****
2022-08-29 20:53:01,981   Epoch = 40 iter 42799 step
2022-08-29 20:53:01,981   Num examples = 1043
2022-08-29 20:53:01,981   Batch size = 32
2022-08-29 20:53:01,982 ***** Eval results *****
2022-08-29 20:53:01,982   att_loss = 0.7561633828320081
2022-08-29 20:53:01,982   cls_loss = 0.0
2022-08-29 20:53:01,982   global_step = 42799
2022-08-29 20:53:01,982   loss = 2.2420767711687692
2022-08-29 20:53:01,982   rep_loss = 1.485913383809826
2022-08-29 20:53:01,982 ***** Save model *****
2022-08-29 20:53:15,399 ***** Running evaluation *****
2022-08-29 20:53:15,400   Epoch = 37 iter 27199 step
2022-08-29 20:53:15,400   Num examples = 1500
2022-08-29 20:53:15,400   Batch size = 32
2022-08-29 20:53:15,401 ***** Eval results *****
2022-08-29 20:53:15,401   att_loss = 2.0861114540371286
2022-08-29 20:53:15,401   cls_loss = 0.0
2022-08-29 20:53:15,401   global_step = 27199
2022-08-29 20:53:15,402   loss = 3.493190093439714
2022-08-29 20:53:15,402   rep_loss = 1.4070786350711262
2022-08-29 20:53:15,402 ***** Save model *****
2022-08-29 20:53:25,196 ***** Running evaluation *****
2022-08-29 20:53:25,197   Epoch = 3 iter 43799 step
2022-08-29 20:53:25,197   Num examples = 9815
2022-08-29 20:53:25,197   Batch size = 32
2022-08-29 20:53:25,198 ***** Eval results *****
2022-08-29 20:53:25,198   att_loss = 4.056494655835741
2022-08-29 20:53:25,198   cls_loss = 0.0
2022-08-29 20:53:25,198   global_step = 43799
2022-08-29 20:53:25,198   loss = 5.851900642047187
2022-08-29 20:53:25,198   rep_loss = 1.795405985324116
2022-08-29 20:53:25,198 ***** Save model *****
2022-08-29 20:53:40,250 ***** Running evaluation *****
2022-08-29 20:53:40,250   Epoch = 40 iter 42999 step
2022-08-29 20:53:40,251   Num examples = 1043
2022-08-29 20:53:40,251   Batch size = 32
2022-08-29 20:53:40,252 ***** Eval results *****
2022-08-29 20:53:40,252   att_loss = 0.7629226230378646
2022-08-29 20:53:40,252   cls_loss = 0.0
2022-08-29 20:53:40,252   global_step = 42999
2022-08-29 20:53:40,252   loss = 2.2540742628890555
2022-08-29 20:53:40,252   rep_loss = 1.4911516349375462
2022-08-29 20:53:40,252 ***** Save model *****
2022-08-29 20:53:54,078 ***** Running evaluation *****
2022-08-29 20:53:54,078   Epoch = 38 iter 27399 step
2022-08-29 20:53:54,078   Num examples = 1500
2022-08-29 20:53:54,078   Batch size = 32
2022-08-29 20:53:54,079 ***** Eval results *****
2022-08-29 20:53:54,079   att_loss = 2.073320533918298
2022-08-29 20:53:54,079   cls_loss = 0.0
2022-08-29 20:53:54,080   global_step = 27399
2022-08-29 20:53:54,080   loss = 3.474855669685032
2022-08-29 20:53:54,080   rep_loss = 1.4015351461327594
2022-08-29 20:53:54,080 ***** Save model *****
2022-08-29 20:54:18,521 ***** Running evaluation *****
2022-08-29 20:54:18,521   Epoch = 40 iter 43199 step
2022-08-29 20:54:18,521   Num examples = 1043
2022-08-29 20:54:18,521   Batch size = 32
2022-08-29 20:54:18,523 ***** Eval results *****
2022-08-29 20:54:18,523   att_loss = 0.7626992263724262
2022-08-29 20:54:18,523   cls_loss = 0.0
2022-08-29 20:54:18,523   global_step = 43199
2022-08-29 20:54:18,523   loss = 2.254632277130334
2022-08-29 20:54:18,523   rep_loss = 1.4919330454071777
2022-08-29 20:54:18,523 ***** Save model *****
2022-08-29 20:54:33,745 ***** Running evaluation *****
2022-08-29 20:54:33,745   Epoch = 38 iter 27599 step
2022-08-29 20:54:33,745   Num examples = 1500
2022-08-29 20:54:33,745   Batch size = 32
2022-08-29 20:54:33,747 ***** Eval results *****
2022-08-29 20:54:33,747   att_loss = 2.0825475617060585
2022-08-29 20:54:33,747   cls_loss = 0.0
2022-08-29 20:54:33,747   global_step = 27599
2022-08-29 20:54:33,747   loss = 3.4887781937917075
2022-08-29 20:54:33,747   rep_loss = 1.4062306430604723
2022-08-29 20:54:33,747 ***** Save model *****
2022-08-29 20:54:35,571 ***** Running evaluation *****
2022-08-29 20:54:35,571   Epoch = 3 iter 43999 step
2022-08-29 20:54:35,571   Num examples = 9815
2022-08-29 20:54:35,572   Batch size = 32
2022-08-29 20:54:35,573 ***** Eval results *****
2022-08-29 20:54:35,573   att_loss = 4.0575086764230255
2022-08-29 20:54:35,573   cls_loss = 0.0
2022-08-29 20:54:35,573   global_step = 43999
2022-08-29 20:54:35,573   loss = 5.852811850845863
2022-08-29 20:54:35,573   rep_loss = 1.7953031737924519
2022-08-29 20:54:35,573 ***** Save model *****
2022-08-29 20:54:56,719 ***** Running evaluation *****
2022-08-29 20:54:56,719   Epoch = 40 iter 43399 step
2022-08-29 20:54:56,719   Num examples = 1043
2022-08-29 20:54:56,720   Batch size = 32
2022-08-29 20:54:56,721 ***** Eval results *****
2022-08-29 20:54:56,721   att_loss = 0.7596334559576852
2022-08-29 20:54:56,721   cls_loss = 0.0
2022-08-29 20:54:56,721   global_step = 43399
2022-08-29 20:54:56,721   loss = 2.2514806009708344
2022-08-29 20:54:56,721   rep_loss = 1.491847140624
2022-08-29 20:54:56,721 ***** Save model *****
2022-08-29 20:55:18,643 ***** Running evaluation *****
2022-08-29 20:55:18,644   Epoch = 38 iter 27799 step
2022-08-29 20:55:18,644   Num examples = 1500
2022-08-29 20:55:18,644   Batch size = 32
2022-08-29 20:55:18,645 ***** Eval results *****
2022-08-29 20:55:18,645   att_loss = 2.0728264975316315
2022-08-29 20:55:18,646   cls_loss = 0.0
2022-08-29 20:55:18,646   global_step = 27799
2022-08-29 20:55:18,646   loss = 3.477957034805446
2022-08-29 20:55:18,646   rep_loss = 1.405130546301314
2022-08-29 20:55:18,646 ***** Save model *****
2022-08-29 20:55:34,937 ***** Running evaluation *****
2022-08-29 20:55:34,938   Epoch = 40 iter 43599 step
2022-08-29 20:55:34,938   Num examples = 1043
2022-08-29 20:55:34,938   Batch size = 32
2022-08-29 20:55:34,939 ***** Eval results *****
2022-08-29 20:55:34,939   att_loss = 0.7620659887587251
2022-08-29 20:55:34,939   cls_loss = 0.0
2022-08-29 20:55:34,939   global_step = 43599
2022-08-29 20:55:34,939   loss = 2.254525313198363
2022-08-29 20:55:34,939   rep_loss = 1.4924593228122074
2022-08-29 20:55:34,939 ***** Save model *****
2022-08-29 20:55:54,522 ***** Running evaluation *****
2022-08-29 20:55:54,522   Epoch = 3 iter 44199 step
2022-08-29 20:55:54,522   Num examples = 9815
2022-08-29 20:55:54,522   Batch size = 32
2022-08-29 20:55:54,523 ***** Eval results *****
2022-08-29 20:55:54,523   att_loss = 4.058835888599723
2022-08-29 20:55:54,523   cls_loss = 0.0
2022-08-29 20:55:54,523   global_step = 44199
2022-08-29 20:55:54,523   loss = 5.854107205408749
2022-08-29 20:55:54,523   rep_loss = 1.7952713165992067
2022-08-29 20:55:54,524 ***** Save model *****
2022-08-29 20:55:57,350 ***** Running evaluation *****
2022-08-29 20:55:57,351   Epoch = 38 iter 27999 step
2022-08-29 20:55:57,351   Num examples = 1500
2022-08-29 20:55:57,351   Batch size = 32
2022-08-29 20:55:57,352 ***** Eval results *****
2022-08-29 20:55:57,352   att_loss = 2.0750841157419697
2022-08-29 20:55:57,352   cls_loss = 0.0
2022-08-29 20:55:57,352   global_step = 27999
2022-08-29 20:55:57,352   loss = 3.480023851594725
2022-08-29 20:55:57,352   rep_loss = 1.4049397415214486
2022-08-29 20:55:57,353 ***** Save model *****
2022-08-29 20:56:13,185 ***** Running evaluation *****
2022-08-29 20:56:13,185   Epoch = 41 iter 43799 step
2022-08-29 20:56:13,185   Num examples = 1043
2022-08-29 20:56:13,185   Batch size = 32
2022-08-29 20:56:13,186 ***** Eval results *****
2022-08-29 20:56:13,186   att_loss = 0.7186129309914329
2022-08-29 20:56:13,186   cls_loss = 0.0
2022-08-29 20:56:13,187   global_step = 43799
2022-08-29 20:56:13,187   loss = 2.204499439759688
2022-08-29 20:56:13,187   rep_loss = 1.4858865521170876
2022-08-29 20:56:13,187 ***** Save model *****
2022-08-29 20:56:36,537 ***** Running evaluation *****
2022-08-29 20:56:36,538   Epoch = 39 iter 28199 step
2022-08-29 20:56:36,538   Num examples = 1500
2022-08-29 20:56:36,538   Batch size = 32
2022-08-29 20:56:36,539 ***** Eval results *****
2022-08-29 20:56:36,539   att_loss = 2.0843542662973937
2022-08-29 20:56:36,539   cls_loss = 0.0
2022-08-29 20:56:36,539   global_step = 28199
2022-08-29 20:56:36,539   loss = 3.4872614417584415
2022-08-29 20:56:36,539   rep_loss = 1.4029071790917875
2022-08-29 20:56:36,539 ***** Save model *****
2022-08-29 20:56:53,292 ***** Running evaluation *****
2022-08-29 20:56:53,292   Epoch = 41 iter 43999 step
2022-08-29 20:56:53,292   Num examples = 1043
2022-08-29 20:56:53,293   Batch size = 32
2022-08-29 20:56:53,294 ***** Eval results *****
2022-08-29 20:56:53,294   att_loss = 0.753855722492905
2022-08-29 20:56:53,294   cls_loss = 0.0
2022-08-29 20:56:53,294   global_step = 43999
2022-08-29 20:56:53,294   loss = 2.241095497144907
2022-08-29 20:56:53,294   rep_loss = 1.487239769284759
2022-08-29 20:56:53,294 ***** Save model *****
2022-08-29 20:57:05,236 ***** Running evaluation *****
2022-08-29 20:57:05,236   Epoch = 3 iter 44399 step
2022-08-29 20:57:05,236   Num examples = 9815
2022-08-29 20:57:05,236   Batch size = 32
2022-08-29 20:57:05,237 ***** Eval results *****
2022-08-29 20:57:05,238   att_loss = 4.060576870074978
2022-08-29 20:57:05,238   cls_loss = 0.0
2022-08-29 20:57:05,238   global_step = 44399
2022-08-29 20:57:05,238   loss = 5.855776020319582
2022-08-29 20:57:05,238   rep_loss = 1.7951991502760332
2022-08-29 20:57:05,238 ***** Save model *****
2022-08-29 20:57:15,357 ***** Running evaluation *****
2022-08-29 20:57:15,357   Epoch = 39 iter 28399 step
2022-08-29 20:57:15,357   Num examples = 1500
2022-08-29 20:57:15,357   Batch size = 32
2022-08-29 20:57:15,359 ***** Eval results *****
2022-08-29 20:57:15,359   att_loss = 2.058516428212375
2022-08-29 20:57:15,359   cls_loss = 0.0
2022-08-29 20:57:15,359   global_step = 28399
2022-08-29 20:57:15,359   loss = 3.4579052570785023
2022-08-29 20:57:15,359   rep_loss = 1.3993888267642005
2022-08-29 20:57:15,359 ***** Save model *****
2022-08-29 20:57:31,763 ***** Running evaluation *****
2022-08-29 20:57:31,763   Epoch = 41 iter 44199 step
2022-08-29 20:57:31,763   Num examples = 1043
2022-08-29 20:57:31,763   Batch size = 32
2022-08-29 20:57:31,764 ***** Eval results *****
2022-08-29 20:57:31,765   att_loss = 0.7582318589925185
2022-08-29 20:57:31,765   cls_loss = 0.0
2022-08-29 20:57:31,765   global_step = 44199
2022-08-29 20:57:31,765   loss = 2.2459689880519598
2022-08-29 20:57:31,765   rep_loss = 1.4877371286243708
2022-08-29 20:57:31,765 ***** Save model *****
2022-08-29 20:57:54,059 ***** Running evaluation *****
2022-08-29 20:57:54,060   Epoch = 39 iter 28599 step
2022-08-29 20:57:54,060   Num examples = 1500
2022-08-29 20:57:54,060   Batch size = 32
2022-08-29 20:57:54,061 ***** Eval results *****
2022-08-29 20:57:54,061   att_loss = 2.0805246436615885
2022-08-29 20:57:54,061   cls_loss = 0.0
2022-08-29 20:57:54,062   global_step = 28599
2022-08-29 20:57:54,062   loss = 3.485782725128097
2022-08-29 20:57:54,062   rep_loss = 1.4052580804681059
2022-08-29 20:57:54,062 ***** Save model *****
2022-08-29 20:58:09,959 ***** Running evaluation *****
2022-08-29 20:58:09,960   Epoch = 41 iter 44399 step
2022-08-29 20:58:09,960   Num examples = 1043
2022-08-29 20:58:09,960   Batch size = 32
2022-08-29 20:58:09,961 ***** Eval results *****
2022-08-29 20:58:09,961   att_loss = 0.7550451359460084
2022-08-29 20:58:09,961   cls_loss = 0.0
2022-08-29 20:58:09,961   global_step = 44399
2022-08-29 20:58:09,961   loss = 2.240902574870083
2022-08-29 20:58:09,961   rep_loss = 1.4858574379485485
2022-08-29 20:58:09,961 ***** Save model *****
2022-08-29 20:58:16,364 ***** Running evaluation *****
2022-08-29 20:58:16,364   Epoch = 3 iter 44599 step
2022-08-29 20:58:16,364   Num examples = 9815
2022-08-29 20:58:16,364   Batch size = 32
2022-08-29 20:58:16,365 ***** Eval results *****
2022-08-29 20:58:16,366   att_loss = 4.059319369994427
2022-08-29 20:58:16,366   cls_loss = 0.0
2022-08-29 20:58:16,366   global_step = 44599
2022-08-29 20:58:16,366   loss = 5.854352768301014
2022-08-29 20:58:16,366   rep_loss = 1.7950333983984514
2022-08-29 20:58:16,366 ***** Save model *****
2022-08-29 20:58:37,275 ***** Running evaluation *****
2022-08-29 20:58:37,275   Epoch = 40 iter 28799 step
2022-08-29 20:58:37,275   Num examples = 1500
2022-08-29 20:58:37,275   Batch size = 32
2022-08-29 20:58:37,277 ***** Eval results *****
2022-08-29 20:58:37,277   att_loss = 2.157910155344613
2022-08-29 20:58:37,277   cls_loss = 0.0
2022-08-29 20:58:37,277   global_step = 28799
2022-08-29 20:58:37,277   loss = 3.569247161285787
2022-08-29 20:58:37,277   rep_loss = 1.4113369848154769
2022-08-29 20:58:37,277 ***** Save model *****
2022-08-29 20:58:48,119 ***** Running evaluation *****
2022-08-29 20:58:48,120   Epoch = 41 iter 44599 step
2022-08-29 20:58:48,120   Num examples = 1043
2022-08-29 20:58:48,120   Batch size = 32
2022-08-29 20:58:48,121 ***** Eval results *****
2022-08-29 20:58:48,121   att_loss = 0.7518611179180121
2022-08-29 20:58:48,121   cls_loss = 0.0
2022-08-29 20:58:48,121   global_step = 44599
2022-08-29 20:58:48,121   loss = 2.237457966392931
2022-08-29 20:58:48,121   rep_loss = 1.4855968483279283
2022-08-29 20:58:48,122 ***** Save model *****
2022-08-29 20:59:15,865 ***** Running evaluation *****
2022-08-29 20:59:15,865   Epoch = 40 iter 28999 step
2022-08-29 20:59:15,865   Num examples = 1500
2022-08-29 20:59:15,865   Batch size = 32
2022-08-29 20:59:15,867 ***** Eval results *****
2022-08-29 20:59:15,867   att_loss = 2.0397157049520898
2022-08-29 20:59:15,867   cls_loss = 0.0
2022-08-29 20:59:15,867   global_step = 28999
2022-08-29 20:59:15,867   loss = 3.4319093842660227
2022-08-29 20:59:15,867   rep_loss = 1.3921936823048473
2022-08-29 20:59:15,867 ***** Save model *****
2022-08-29 20:59:25,359 ***** Running evaluation *****
2022-08-29 20:59:25,359   Epoch = 3 iter 44799 step
2022-08-29 20:59:25,360   Num examples = 9815
2022-08-29 20:59:25,360   Batch size = 32
2022-08-29 20:59:25,361 ***** Eval results *****
2022-08-29 20:59:25,361   att_loss = 4.058112281980354
2022-08-29 20:59:25,361   cls_loss = 0.0
2022-08-29 20:59:25,361   global_step = 44799
2022-08-29 20:59:25,361   loss = 5.85289836037347
2022-08-29 20:59:25,361   rep_loss = 1.794786078393116
2022-08-29 20:59:25,361 ***** Save model *****
2022-08-29 20:59:26,287 ***** Running evaluation *****
2022-08-29 20:59:26,288   Epoch = 41 iter 44799 step
2022-08-29 20:59:26,288   Num examples = 1043
2022-08-29 20:59:26,288   Batch size = 32
2022-08-29 20:59:26,289 ***** Eval results *****
2022-08-29 20:59:26,289   att_loss = 0.7558308658213101
2022-08-29 20:59:26,289   cls_loss = 0.0
2022-08-29 20:59:26,289   global_step = 44799
2022-08-29 20:59:26,289   loss = 2.2435770018282324
2022-08-29 20:59:26,289   rep_loss = 1.4877461342382383
2022-08-29 20:59:26,290 ***** Save model *****
2022-08-29 20:59:54,512 ***** Running evaluation *****
2022-08-29 20:59:54,512   Epoch = 40 iter 29199 step
2022-08-29 20:59:54,512   Num examples = 1500
2022-08-29 20:59:54,513   Batch size = 32
2022-08-29 20:59:54,514 ***** Eval results *****
2022-08-29 20:59:54,514   att_loss = 2.0526995574258313
2022-08-29 20:59:54,514   cls_loss = 0.0
2022-08-29 20:59:54,514   global_step = 29199
2022-08-29 20:59:54,514   loss = 3.4456491868530783
2022-08-29 20:59:54,514   rep_loss = 1.3929496299249893
2022-08-29 20:59:54,514 ***** Save model *****
2022-08-29 21:00:06,712 ***** Running evaluation *****
2022-08-29 21:00:06,712   Epoch = 42 iter 44999 step
2022-08-29 21:00:06,712   Num examples = 1043
2022-08-29 21:00:06,712   Batch size = 32
2022-08-29 21:00:06,713 ***** Eval results *****
2022-08-29 21:00:06,714   att_loss = 0.768737559968775
2022-08-29 21:00:06,714   cls_loss = 0.0
2022-08-29 21:00:06,714   global_step = 44999
2022-08-29 21:00:06,714   loss = 2.257145632397045
2022-08-29 21:00:06,714   rep_loss = 1.4884080711778227
2022-08-29 21:00:06,714 ***** Save model *****
2022-08-29 21:00:33,113 ***** Running evaluation *****
2022-08-29 21:00:33,114   Epoch = 40 iter 29399 step
2022-08-29 21:00:33,114   Num examples = 1500
2022-08-29 21:00:33,114   Batch size = 32
2022-08-29 21:00:33,115 ***** Eval results *****
2022-08-29 21:00:33,115   att_loss = 2.0626527615253636
2022-08-29 21:00:33,115   cls_loss = 0.0
2022-08-29 21:00:33,115   global_step = 29399
2022-08-29 21:00:33,115   loss = 3.459338974706905
2022-08-29 21:00:33,115   rep_loss = 1.396686212654844
2022-08-29 21:00:33,116 ***** Save model *****
2022-08-29 21:00:35,533 ***** Running evaluation *****
2022-08-29 21:00:35,533   Epoch = 3 iter 44999 step
2022-08-29 21:00:35,533   Num examples = 9815
2022-08-29 21:00:35,534   Batch size = 32
2022-08-29 21:00:35,535 ***** Eval results *****
2022-08-29 21:00:35,535   att_loss = 4.056490168809134
2022-08-29 21:00:35,535   cls_loss = 0.0
2022-08-29 21:00:35,535   global_step = 44999
2022-08-29 21:00:35,535   loss = 5.851032273273669
2022-08-29 21:00:35,535   rep_loss = 1.794542104289784
2022-08-29 21:00:35,535 ***** Save model *****
2022-08-29 21:00:46,818 ***** Running evaluation *****
2022-08-29 21:00:46,819   Epoch = 42 iter 45199 step
2022-08-29 21:00:46,819   Num examples = 1043
2022-08-29 21:00:46,819   Batch size = 32
2022-08-29 21:00:46,820 ***** Eval results *****
2022-08-29 21:00:46,820   att_loss = 0.7587810395931711
2022-08-29 21:00:46,820   cls_loss = 0.0
2022-08-29 21:00:46,820   global_step = 45199
2022-08-29 21:00:46,820   loss = 2.2479840068705923
2022-08-29 21:00:46,820   rep_loss = 1.4892029651052403
2022-08-29 21:00:46,820 ***** Save model *****
2022-08-29 21:01:11,716 ***** Running evaluation *****
2022-08-29 21:01:11,716   Epoch = 41 iter 29599 step
2022-08-29 21:01:11,716   Num examples = 1500
2022-08-29 21:01:11,717   Batch size = 32
2022-08-29 21:01:11,718 ***** Eval results *****
2022-08-29 21:01:11,718   att_loss = 2.009823703617783
2022-08-29 21:01:11,718   cls_loss = 0.0
2022-08-29 21:01:11,718   global_step = 29599
2022-08-29 21:01:11,718   loss = 3.395446533001728
2022-08-29 21:01:11,718   rep_loss = 1.3856228301243751
2022-08-29 21:01:11,718 ***** Save model *****
2022-08-29 21:01:28,530 ***** Running evaluation *****
2022-08-29 21:01:28,531   Epoch = 42 iter 45399 step
2022-08-29 21:01:28,531   Num examples = 1043
2022-08-29 21:01:28,531   Batch size = 32
2022-08-29 21:01:28,532 ***** Eval results *****
2022-08-29 21:01:28,532   att_loss = 0.7549644450127091
2022-08-29 21:01:28,532   cls_loss = 0.0
2022-08-29 21:01:28,532   global_step = 45399
2022-08-29 21:01:28,532   loss = 2.241992695775041
2022-08-29 21:01:28,532   rep_loss = 1.4870282499390632
2022-08-29 21:01:28,532 ***** Save model *****
2022-08-29 21:01:50,301 ***** Running evaluation *****
2022-08-29 21:01:50,301   Epoch = 41 iter 29799 step
2022-08-29 21:01:50,301   Num examples = 1500
2022-08-29 21:01:50,301   Batch size = 32
2022-08-29 21:01:50,303 ***** Eval results *****
2022-08-29 21:01:50,303   att_loss = 2.0436056380126617
2022-08-29 21:01:50,303   cls_loss = 0.0
2022-08-29 21:01:50,303   global_step = 29799
2022-08-29 21:01:50,303   loss = 3.4348993393853102
2022-08-29 21:01:50,303   rep_loss = 1.39129370005177
2022-08-29 21:01:50,303 ***** Save model *****
2022-08-29 21:01:50,921 ***** Running evaluation *****
2022-08-29 21:01:50,921   Epoch = 3 iter 45199 step
2022-08-29 21:01:50,921   Num examples = 9815
2022-08-29 21:01:50,921   Batch size = 32
2022-08-29 21:01:50,923 ***** Eval results *****
2022-08-29 21:01:50,923   att_loss = 4.056750631122012
2022-08-29 21:01:50,923   cls_loss = 0.0
2022-08-29 21:01:50,923   global_step = 45199
2022-08-29 21:01:50,923   loss = 5.851154679362768
2022-08-29 21:01:50,923   rep_loss = 1.7944040481838952
2022-08-29 21:01:50,923 ***** Save model *****
2022-08-29 21:02:10,001 ***** Running evaluation *****
2022-08-29 21:02:10,002   Epoch = 42 iter 45599 step
2022-08-29 21:02:10,002   Num examples = 1043
2022-08-29 21:02:10,002   Batch size = 32
2022-08-29 21:02:10,003 ***** Eval results *****
2022-08-29 21:02:10,003   att_loss = 0.7548954041215966
2022-08-29 21:02:10,003   cls_loss = 0.0
2022-08-29 21:02:10,003   global_step = 45599
2022-08-29 21:02:10,003   loss = 2.2418314544220794
2022-08-29 21:02:10,003   rep_loss = 1.4869360476130593
2022-08-29 21:02:10,003 ***** Save model *****
2022-08-29 21:02:30,701 ***** Running evaluation *****
2022-08-29 21:02:30,702   Epoch = 41 iter 29999 step
2022-08-29 21:02:30,702   Num examples = 1500
2022-08-29 21:02:30,702   Batch size = 32
2022-08-29 21:02:30,703 ***** Eval results *****
2022-08-29 21:02:30,703   att_loss = 2.0397428905262665
2022-08-29 21:02:30,703   cls_loss = 0.0
2022-08-29 21:02:30,703   global_step = 29999
2022-08-29 21:02:30,703   loss = 3.429971886191139
2022-08-29 21:02:30,703   rep_loss = 1.3902289967273433
2022-08-29 21:02:30,704 ***** Save model *****
2022-08-29 21:02:50,614 ***** Running evaluation *****
2022-08-29 21:02:50,615   Epoch = 42 iter 45799 step
2022-08-29 21:02:50,615   Num examples = 1043
2022-08-29 21:02:50,615   Batch size = 32
2022-08-29 21:02:50,616 ***** Eval results *****
2022-08-29 21:02:50,616   att_loss = 0.7533603181325744
2022-08-29 21:02:50,616   cls_loss = 0.0
2022-08-29 21:02:50,616   global_step = 45799
2022-08-29 21:02:50,616   loss = 2.238472515806034
2022-08-29 21:02:50,616   rep_loss = 1.4851121954295947
2022-08-29 21:02:50,616 ***** Save model *****
2022-08-29 21:03:04,479 ***** Running evaluation *****
2022-08-29 21:03:04,479   Epoch = 3 iter 45399 step
2022-08-29 21:03:04,479   Num examples = 9815
2022-08-29 21:03:04,479   Batch size = 32
2022-08-29 21:03:04,480 ***** Eval results *****
2022-08-29 21:03:04,480   att_loss = 4.05736885688923
2022-08-29 21:03:04,481   cls_loss = 0.0
2022-08-29 21:03:04,481   global_step = 45399
2022-08-29 21:03:04,481   loss = 5.851534589809219
2022-08-29 21:03:04,481   rep_loss = 1.7941657330449468
2022-08-29 21:03:04,481 ***** Save model *****
2022-08-29 21:03:09,318 ***** Running evaluation *****
2022-08-29 21:03:09,319   Epoch = 42 iter 30199 step
2022-08-29 21:03:09,319   Num examples = 1500
2022-08-29 21:03:09,319   Batch size = 32
2022-08-29 21:03:09,320 ***** Eval results *****
2022-08-29 21:03:09,320   att_loss = 2.0538420483123425
2022-08-29 21:03:09,320   cls_loss = 0.0
2022-08-29 21:03:09,320   global_step = 30199
2022-08-29 21:03:09,320   loss = 3.4395095581232114
2022-08-29 21:03:09,320   rep_loss = 1.385667509810869
2022-08-29 21:03:09,320 ***** Save model *****
2022-08-29 21:03:29,186 ***** Running evaluation *****
2022-08-29 21:03:29,186   Epoch = 43 iter 45999 step
2022-08-29 21:03:29,186   Num examples = 1043
2022-08-29 21:03:29,186   Batch size = 32
2022-08-29 21:03:29,188 ***** Eval results *****
2022-08-29 21:03:29,188   att_loss = 0.7386013436317443
2022-08-29 21:03:29,188   cls_loss = 0.0
2022-08-29 21:03:29,188   global_step = 45999
2022-08-29 21:03:29,188   loss = 2.21145690759023
2022-08-29 21:03:29,188   rep_loss = 1.4728555615743002
2022-08-29 21:03:29,188 ***** Save model *****
2022-08-29 21:03:50,053 ***** Running evaluation *****
2022-08-29 21:03:50,054   Epoch = 42 iter 30399 step
2022-08-29 21:03:50,054   Num examples = 1500
2022-08-29 21:03:50,054   Batch size = 32
2022-08-29 21:03:50,055 ***** Eval results *****
2022-08-29 21:03:50,056   att_loss = 2.0161489488656628
2022-08-29 21:03:50,056   cls_loss = 0.0
2022-08-29 21:03:50,056   global_step = 30399
2022-08-29 21:03:50,056   loss = 3.3953364611653143
2022-08-29 21:03:50,056   rep_loss = 1.3791875132807978
2022-08-29 21:03:50,056 ***** Save model *****
2022-08-29 21:04:07,755 ***** Running evaluation *****
2022-08-29 21:04:07,755   Epoch = 43 iter 46199 step
2022-08-29 21:04:07,755   Num examples = 1043
2022-08-29 21:04:07,755   Batch size = 32
2022-08-29 21:04:07,757 ***** Eval results *****
2022-08-29 21:04:07,757   att_loss = 0.7493863970583136
2022-08-29 21:04:07,757   cls_loss = 0.0
2022-08-29 21:04:07,757   global_step = 46199
2022-08-29 21:04:07,757   loss = 2.229457178549333
2022-08-29 21:04:07,757   rep_loss = 1.4800707882100885
2022-08-29 21:04:07,757 ***** Save model *****
2022-08-29 21:04:20,405 ***** Running evaluation *****
2022-08-29 21:04:20,405   Epoch = 3 iter 45599 step
2022-08-29 21:04:20,405   Num examples = 9815
2022-08-29 21:04:20,405   Batch size = 32
2022-08-29 21:04:20,406 ***** Eval results *****
2022-08-29 21:04:20,406   att_loss = 4.057713794621416
2022-08-29 21:04:20,406   cls_loss = 0.0
2022-08-29 21:04:20,406   global_step = 45599
2022-08-29 21:04:20,407   loss = 5.851735782004586
2022-08-29 21:04:20,407   rep_loss = 1.794021987152512
2022-08-29 21:04:20,407 ***** Save model *****
2022-08-29 21:04:28,780 ***** Running evaluation *****
2022-08-29 21:04:28,780   Epoch = 42 iter 30599 step
2022-08-29 21:04:28,780   Num examples = 1500
2022-08-29 21:04:28,780   Batch size = 32
2022-08-29 21:04:28,782 ***** Eval results *****
2022-08-29 21:04:28,782   att_loss = 2.0176072909116205
2022-08-29 21:04:28,782   cls_loss = 0.0
2022-08-29 21:04:28,782   global_step = 30599
2022-08-29 21:04:28,782   loss = 3.4025977190676446
2022-08-29 21:04:28,782   rep_loss = 1.3849904265414512
2022-08-29 21:04:28,782 ***** Save model *****
2022-08-29 21:04:46,897 ***** Running evaluation *****
2022-08-29 21:04:46,898   Epoch = 43 iter 46399 step
2022-08-29 21:04:46,898   Num examples = 1043
2022-08-29 21:04:46,898   Batch size = 32
2022-08-29 21:04:46,899 ***** Eval results *****
2022-08-29 21:04:46,899   att_loss = 0.7527095570062337
2022-08-29 21:04:46,899   cls_loss = 0.0
2022-08-29 21:04:46,899   global_step = 46399
2022-08-29 21:04:46,899   loss = 2.229996543432537
2022-08-29 21:04:46,899   rep_loss = 1.4772869928259598
2022-08-29 21:04:46,899 ***** Save model *****
2022-08-29 21:05:07,661 ***** Running evaluation *****
2022-08-29 21:05:07,661   Epoch = 42 iter 30799 step
2022-08-29 21:05:07,661   Num examples = 1500
2022-08-29 21:05:07,661   Batch size = 32
2022-08-29 21:05:07,662 ***** Eval results *****
2022-08-29 21:05:07,662   att_loss = 2.038916683957143
2022-08-29 21:05:07,662   cls_loss = 0.0
2022-08-29 21:05:07,662   global_step = 30799
2022-08-29 21:05:07,662   loss = 3.4260444329648885
2022-08-29 21:05:07,663   rep_loss = 1.387127751325189
2022-08-29 21:05:07,663 ***** Save model *****
2022-08-29 21:05:25,927 ***** Running evaluation *****
2022-08-29 21:05:25,928   Epoch = 43 iter 46599 step
2022-08-29 21:05:25,928   Num examples = 1043
2022-08-29 21:05:25,928   Batch size = 32
2022-08-29 21:05:25,929 ***** Eval results *****
2022-08-29 21:05:25,930   att_loss = 0.7554818877467403
2022-08-29 21:05:25,930   cls_loss = 0.0
2022-08-29 21:05:25,930   global_step = 46599
2022-08-29 21:05:25,930   loss = 2.2348678892630116
2022-08-29 21:05:25,930   rep_loss = 1.4793860076974938
2022-08-29 21:05:25,930 ***** Save model *****
2022-08-29 21:05:31,880 ***** Running evaluation *****
2022-08-29 21:05:31,880   Epoch = 3 iter 45799 step
2022-08-29 21:05:31,880   Num examples = 9815
2022-08-29 21:05:31,880   Batch size = 32
2022-08-29 21:05:31,881 ***** Eval results *****
2022-08-29 21:05:31,882   att_loss = 4.056921125413473
2022-08-29 21:05:31,882   cls_loss = 0.0
2022-08-29 21:05:31,882   global_step = 45799
2022-08-29 21:05:31,882   loss = 5.85072280998196
2022-08-29 21:05:31,882   rep_loss = 1.793801684077641
2022-08-29 21:05:31,882 ***** Save model *****
2022-08-29 21:05:46,525 ***** Running evaluation *****
2022-08-29 21:05:46,525   Epoch = 43 iter 30999 step
2022-08-29 21:05:46,526   Num examples = 1500
2022-08-29 21:05:46,526   Batch size = 32
2022-08-29 21:05:46,527 ***** Eval results *****
2022-08-29 21:05:46,527   att_loss = 2.0305408477783202
2022-08-29 21:05:46,527   cls_loss = 0.0
2022-08-29 21:05:46,527   global_step = 30999
2022-08-29 21:05:46,527   loss = 3.419519166946411
2022-08-29 21:05:46,527   rep_loss = 1.3889783248901366
2022-08-29 21:05:46,527 ***** Save model *****
2022-08-29 21:06:05,683 ***** Running evaluation *****
2022-08-29 21:06:05,683   Epoch = 43 iter 46799 step
2022-08-29 21:06:05,683   Num examples = 1043
2022-08-29 21:06:05,683   Batch size = 32
2022-08-29 21:06:05,684 ***** Eval results *****
2022-08-29 21:06:05,684   att_loss = 0.7545789610317776
2022-08-29 21:06:05,684   cls_loss = 0.0
2022-08-29 21:06:05,685   global_step = 46799
2022-08-29 21:06:05,685   loss = 2.233400339807783
2022-08-29 21:06:05,685   rep_loss = 1.4788213833400181
2022-08-29 21:06:05,685 ***** Save model *****
2022-08-29 21:06:25,413 ***** Running evaluation *****
2022-08-29 21:06:25,414   Epoch = 43 iter 31199 step
2022-08-29 21:06:25,414   Num examples = 1500
2022-08-29 21:06:25,414   Batch size = 32
2022-08-29 21:06:25,415 ***** Eval results *****
2022-08-29 21:06:25,415   att_loss = 2.013942987368657
2022-08-29 21:06:25,415   cls_loss = 0.0
2022-08-29 21:06:25,415   global_step = 31199
2022-08-29 21:06:25,415   loss = 3.3946422144082877
2022-08-29 21:06:25,415   rep_loss = 1.3806992193368766
2022-08-29 21:06:25,415 ***** Save model *****
2022-08-29 21:06:45,684 ***** Running evaluation *****
2022-08-29 21:06:45,685   Epoch = 44 iter 46999 step
2022-08-29 21:06:45,685   Num examples = 1043
2022-08-29 21:06:45,685   Batch size = 32
2022-08-29 21:06:45,686 ***** Eval results *****
2022-08-29 21:06:45,686   att_loss = 0.7951959030968803
2022-08-29 21:06:45,686   cls_loss = 0.0
2022-08-29 21:06:45,686   global_step = 46999
2022-08-29 21:06:45,686   loss = 2.287260123661586
2022-08-29 21:06:45,686   rep_loss = 1.4920642375946045
2022-08-29 21:06:45,686 ***** Save model *****
2022-08-29 21:06:47,961 ***** Running evaluation *****
2022-08-29 21:06:47,961   Epoch = 3 iter 45999 step
2022-08-29 21:06:47,961   Num examples = 9815
2022-08-29 21:06:47,961   Batch size = 32
2022-08-29 21:06:47,963 ***** Eval results *****
2022-08-29 21:06:47,963   att_loss = 4.055688557763934
2022-08-29 21:06:47,963   cls_loss = 0.0
2022-08-29 21:06:47,963   global_step = 45999
2022-08-29 21:06:47,963   loss = 5.849200395569157
2022-08-29 21:06:47,963   rep_loss = 1.7935118371952912
2022-08-29 21:06:47,963 ***** Save model *****
2022-08-29 21:07:07,174 ***** Running evaluation *****
2022-08-29 21:07:07,175   Epoch = 43 iter 31399 step
2022-08-29 21:07:07,175   Num examples = 1500
2022-08-29 21:07:07,175   Batch size = 32
2022-08-29 21:07:07,177 ***** Eval results *****
2022-08-29 21:07:07,177   att_loss = 2.023762767655509
2022-08-29 21:07:07,177   cls_loss = 0.0
2022-08-29 21:07:07,177   global_step = 31399
2022-08-29 21:07:07,177   loss = 3.407357984724499
2022-08-29 21:07:07,177   rep_loss = 1.383595212073553
2022-08-29 21:07:07,177 ***** Save model *****
2022-08-29 21:07:25,702 ***** Running evaluation *****
2022-08-29 21:07:25,702   Epoch = 44 iter 47199 step
2022-08-29 21:07:25,702   Num examples = 1043
2022-08-29 21:07:25,702   Batch size = 32
2022-08-29 21:07:25,703 ***** Eval results *****
2022-08-29 21:07:25,703   att_loss = 0.7525506735135967
2022-08-29 21:07:25,703   cls_loss = 0.0
2022-08-29 21:07:25,704   global_step = 47199
2022-08-29 21:07:25,704   loss = 2.236776199893675
2022-08-29 21:07:25,704   rep_loss = 1.4842255282517216
2022-08-29 21:07:25,704 ***** Save model *****
2022-08-29 21:07:46,037 ***** Running evaluation *****
2022-08-29 21:07:46,037   Epoch = 44 iter 31599 step
2022-08-29 21:07:46,037   Num examples = 1500
2022-08-29 21:07:46,037   Batch size = 32
2022-08-29 21:07:46,039 ***** Eval results *****
2022-08-29 21:07:46,039   att_loss = 2.4426235471452986
2022-08-29 21:07:46,039   cls_loss = 0.0
2022-08-29 21:07:46,039   global_step = 31599
2022-08-29 21:07:46,039   loss = 3.892331600189209
2022-08-29 21:07:46,039   rep_loss = 1.4497080530439104
2022-08-29 21:07:46,039 ***** Save model *****
2022-08-29 21:08:05,095 ***** Running evaluation *****
2022-08-29 21:08:05,096   Epoch = 3 iter 46199 step
2022-08-29 21:08:05,096   Num examples = 9815
2022-08-29 21:08:05,096   Batch size = 32
2022-08-29 21:08:05,097 ***** Eval results *****
2022-08-29 21:08:05,097   att_loss = 4.056103167924042
2022-08-29 21:08:05,097   cls_loss = 0.0
2022-08-29 21:08:05,097   global_step = 46199
2022-08-29 21:08:05,097   loss = 5.849493088763888
2022-08-29 21:08:05,097   rep_loss = 1.7933899198872891
2022-08-29 21:08:05,097 ***** Save model *****
2022-08-29 21:08:05,727 ***** Running evaluation *****
2022-08-29 21:08:05,728   Epoch = 44 iter 47399 step
2022-08-29 21:08:05,728   Num examples = 1043
2022-08-29 21:08:05,728   Batch size = 32
2022-08-29 21:08:05,730 ***** Eval results *****
2022-08-29 21:08:05,730   att_loss = 0.7551730513133347
2022-08-29 21:08:05,730   cls_loss = 0.0
2022-08-29 21:08:05,730   global_step = 47399
2022-08-29 21:08:05,730   loss = 2.239411530389247
2022-08-29 21:08:05,730   rep_loss = 1.484238476952405
2022-08-29 21:08:05,730 ***** Save model *****
2022-08-29 21:08:28,318 ***** Running evaluation *****
2022-08-29 21:08:28,318   Epoch = 44 iter 31799 step
2022-08-29 21:08:28,318   Num examples = 1500
2022-08-29 21:08:28,318   Batch size = 32
2022-08-29 21:08:28,320 ***** Eval results *****
2022-08-29 21:08:28,320   att_loss = 2.067664521327917
2022-08-29 21:08:28,320   cls_loss = 0.0
2022-08-29 21:08:28,320   global_step = 31799
2022-08-29 21:08:28,320   loss = 3.4550936118416162
2022-08-29 21:08:28,320   rep_loss = 1.387429089937809
2022-08-29 21:08:28,320 ***** Save model *****
2022-08-29 21:08:48,333 ***** Running evaluation *****
2022-08-29 21:08:48,334   Epoch = 44 iter 47599 step
2022-08-29 21:08:48,334   Num examples = 1043
2022-08-29 21:08:48,334   Batch size = 32
2022-08-29 21:08:48,335 ***** Eval results *****
2022-08-29 21:08:48,335   att_loss = 0.7516019123881025
2022-08-29 21:08:48,335   cls_loss = 0.0
2022-08-29 21:08:48,335   global_step = 47599
2022-08-29 21:08:48,335   loss = 2.2345706687924105
2022-08-29 21:08:48,336   rep_loss = 1.4829687579263375
2022-08-29 21:08:48,336 ***** Save model *****
2022-08-29 21:09:07,151 ***** Running evaluation *****
2022-08-29 21:09:07,152   Epoch = 44 iter 31999 step
2022-08-29 21:09:07,152   Num examples = 1500
2022-08-29 21:09:07,152   Batch size = 32
2022-08-29 21:09:07,153 ***** Eval results *****
2022-08-29 21:09:07,153   att_loss = 2.056624433332345
2022-08-29 21:09:07,153   cls_loss = 0.0
2022-08-29 21:09:07,153   global_step = 31999
2022-08-29 21:09:07,153   loss = 3.442966112633595
2022-08-29 21:09:07,153   rep_loss = 1.38634167812966
2022-08-29 21:09:07,153 ***** Save model *****
2022-08-29 21:09:25,449 ***** Running evaluation *****
2022-08-29 21:09:25,450   Epoch = 3 iter 46399 step
2022-08-29 21:09:25,450   Num examples = 9815
2022-08-29 21:09:25,450   Batch size = 32
2022-08-29 21:09:25,452 ***** Eval results *****
2022-08-29 21:09:25,453   att_loss = 4.054966451080612
2022-08-29 21:09:25,453   cls_loss = 0.0
2022-08-29 21:09:25,453   global_step = 46399
2022-08-29 21:09:25,453   loss = 5.848122752834506
2022-08-29 21:09:25,453   rep_loss = 1.7931563006098028
2022-08-29 21:09:25,453 ***** Save model *****
2022-08-29 21:09:28,234 ***** Running evaluation *****
2022-08-29 21:09:28,234   Epoch = 44 iter 47799 step
2022-08-29 21:09:28,234   Num examples = 1043
2022-08-29 21:09:28,234   Batch size = 32
2022-08-29 21:09:28,236 ***** Eval results *****
2022-08-29 21:09:28,236   att_loss = 0.752131713699674
2022-08-29 21:09:28,236   cls_loss = 0.0
2022-08-29 21:09:28,236   global_step = 47799
2022-08-29 21:09:28,236   loss = 2.235123610998766
2022-08-29 21:09:28,236   rep_loss = 1.482991897926898
2022-08-29 21:09:28,236 ***** Save model *****
2022-08-29 21:09:46,013 ***** Running evaluation *****
2022-08-29 21:09:46,014   Epoch = 44 iter 32199 step
2022-08-29 21:09:46,014   Num examples = 1500
2022-08-29 21:09:46,014   Batch size = 32
2022-08-29 21:09:46,015 ***** Eval results *****
2022-08-29 21:09:46,015   att_loss = 2.0369090456546237
2022-08-29 21:09:46,015   cls_loss = 0.0
2022-08-29 21:09:46,015   global_step = 32199
2022-08-29 21:09:46,015   loss = 3.4198654459964306
2022-08-29 21:09:46,015   rep_loss = 1.3829563958248157
2022-08-29 21:09:46,015 ***** Save model *****
2022-08-29 21:10:07,065 ***** Running evaluation *****
2022-08-29 21:10:07,066   Epoch = 44 iter 47999 step
2022-08-29 21:10:07,066   Num examples = 1043
2022-08-29 21:10:07,066   Batch size = 32
2022-08-29 21:10:07,067 ***** Eval results *****
2022-08-29 21:10:07,067   att_loss = 0.750725494092839
2022-08-29 21:10:07,067   cls_loss = 0.0
2022-08-29 21:10:07,067   global_step = 47999
2022-08-29 21:10:07,067   loss = 2.232854622004641
2022-08-29 21:10:07,067   rep_loss = 1.4821291295395351
2022-08-29 21:10:07,068 ***** Save model *****
2022-08-29 21:10:29,454 ***** Running evaluation *****
2022-08-29 21:10:29,455   Epoch = 45 iter 32399 step
2022-08-29 21:10:29,455   Num examples = 1500
2022-08-29 21:10:29,455   Batch size = 32
2022-08-29 21:10:29,456 ***** Eval results *****
2022-08-29 21:10:29,456   att_loss = 1.9779604137613531
2022-08-29 21:10:29,456   cls_loss = 0.0
2022-08-29 21:10:29,456   global_step = 32399
2022-08-29 21:10:29,456   loss = 3.3487764396024553
2022-08-29 21:10:29,456   rep_loss = 1.3708160365565438
2022-08-29 21:10:29,456 ***** Save model *****
2022-08-29 21:10:34,802 ***** Running evaluation *****
2022-08-29 21:10:34,802   Epoch = 3 iter 46599 step
2022-08-29 21:10:34,802   Num examples = 9815
2022-08-29 21:10:34,802   Batch size = 32
2022-08-29 21:10:34,804 ***** Eval results *****
2022-08-29 21:10:34,804   att_loss = 4.0534252558118995
2022-08-29 21:10:34,804   cls_loss = 0.0
2022-08-29 21:10:34,804   global_step = 46599
2022-08-29 21:10:34,804   loss = 5.846318545819986
2022-08-29 21:10:34,804   rep_loss = 1.7928932888264697
2022-08-29 21:10:34,804 ***** Save model *****
2022-08-29 21:10:47,645 ***** Running evaluation *****
2022-08-29 21:10:47,646   Epoch = 45 iter 48199 step
2022-08-29 21:10:47,646   Num examples = 1043
2022-08-29 21:10:47,646   Batch size = 32
2022-08-29 21:10:47,647 ***** Eval results *****
2022-08-29 21:10:47,647   att_loss = 0.743979200184774
2022-08-29 21:10:47,647   cls_loss = 0.0
2022-08-29 21:10:47,647   global_step = 48199
2022-08-29 21:10:47,647   loss = 2.222088527336395
2022-08-29 21:10:47,647   rep_loss = 1.4781093237211378
2022-08-29 21:10:47,648 ***** Save model *****
2022-08-29 21:11:08,301 ***** Running evaluation *****
2022-08-29 21:11:08,302   Epoch = 45 iter 32599 step
2022-08-29 21:11:08,302   Num examples = 1500
2022-08-29 21:11:08,302   Batch size = 32
2022-08-29 21:11:08,303 ***** Eval results *****
2022-08-29 21:11:08,304   att_loss = 1.9759204569159907
2022-08-29 21:11:08,304   cls_loss = 0.0
2022-08-29 21:11:08,304   global_step = 32599
2022-08-29 21:11:08,304   loss = 3.347775337192839
2022-08-29 21:11:08,304   rep_loss = 1.3718548910015596
2022-08-29 21:11:08,304 ***** Save model *****
2022-08-29 21:11:26,207 ***** Running evaluation *****
2022-08-29 21:11:26,207   Epoch = 45 iter 48399 step
2022-08-29 21:11:26,207   Num examples = 1043
2022-08-29 21:11:26,207   Batch size = 32
2022-08-29 21:11:26,209 ***** Eval results *****
2022-08-29 21:11:26,209   att_loss = 0.7455941851863467
2022-08-29 21:11:26,209   cls_loss = 0.0
2022-08-29 21:11:26,209   global_step = 48399
2022-08-29 21:11:26,209   loss = 2.217628562696564
2022-08-29 21:11:26,209   rep_loss = 1.4720343794442912
2022-08-29 21:11:26,209 ***** Save model *****
2022-08-29 21:11:46,178 ***** Running evaluation *****
2022-08-29 21:11:46,178   Epoch = 3 iter 46799 step
2022-08-29 21:11:46,178   Num examples = 9815
2022-08-29 21:11:46,178   Batch size = 32
2022-08-29 21:11:46,179 ***** Eval results *****
2022-08-29 21:11:46,179   att_loss = 4.054071435416459
2022-08-29 21:11:46,179   cls_loss = 0.0
2022-08-29 21:11:46,179   global_step = 46799
2022-08-29 21:11:46,180   loss = 5.846864776049782
2022-08-29 21:11:46,180   rep_loss = 1.7927933397976878
2022-08-29 21:11:46,180 ***** Save model *****
2022-08-29 21:11:47,164 ***** Running evaluation *****
2022-08-29 21:11:47,165   Epoch = 45 iter 32799 step
2022-08-29 21:11:47,165   Num examples = 1500
2022-08-29 21:11:47,165   Batch size = 32
2022-08-29 21:11:47,166 ***** Eval results *****
2022-08-29 21:11:47,167   att_loss = 2.0194842383905423
2022-08-29 21:11:47,167   cls_loss = 0.0
2022-08-29 21:11:47,167   global_step = 32799
2022-08-29 21:11:47,167   loss = 3.397015269792397
2022-08-29 21:11:47,167   rep_loss = 1.3775310372526173
2022-08-29 21:11:47,167 ***** Save model *****
2022-08-29 21:12:10,055 ***** Running evaluation *****
2022-08-29 21:12:10,055   Epoch = 45 iter 48599 step
2022-08-29 21:12:10,055   Num examples = 1043
2022-08-29 21:12:10,055   Batch size = 32
2022-08-29 21:12:10,057 ***** Eval results *****
2022-08-29 21:12:10,057   att_loss = 0.7462836652164778
2022-08-29 21:12:10,057   cls_loss = 0.0
2022-08-29 21:12:10,057   global_step = 48599
2022-08-29 21:12:10,057   loss = 2.2210653095829245
2022-08-29 21:12:10,057   rep_loss = 1.4747816452511167
2022-08-29 21:12:10,057 ***** Save model *****
2022-08-29 21:12:28,184 ***** Running evaluation *****
2022-08-29 21:12:28,185   Epoch = 45 iter 32999 step
2022-08-29 21:12:28,185   Num examples = 1500
2022-08-29 21:12:28,185   Batch size = 32
2022-08-29 21:12:28,186 ***** Eval results *****
2022-08-29 21:12:28,186   att_loss = 2.0274006067754913
2022-08-29 21:12:28,186   cls_loss = 0.0
2022-08-29 21:12:28,186   global_step = 32999
2022-08-29 21:12:28,186   loss = 3.406243249190044
2022-08-29 21:12:28,186   rep_loss = 1.378842646047927
2022-08-29 21:12:28,186 ***** Save model *****
2022-08-29 21:12:49,095 ***** Running evaluation *****
2022-08-29 21:12:49,096   Epoch = 45 iter 48799 step
2022-08-29 21:12:49,096   Num examples = 1043
2022-08-29 21:12:49,096   Batch size = 32
2022-08-29 21:12:49,098 ***** Eval results *****
2022-08-29 21:12:49,098   att_loss = 0.7495331211325603
2022-08-29 21:12:49,098   cls_loss = 0.0
2022-08-29 21:12:49,098   global_step = 48799
2022-08-29 21:12:49,098   loss = 2.2268112267144478
2022-08-29 21:12:49,098   rep_loss = 1.4772781073966432
2022-08-29 21:12:49,098 ***** Save model *****
2022-08-29 21:12:59,748 ***** Running evaluation *****
2022-08-29 21:12:59,749   Epoch = 3 iter 46999 step
2022-08-29 21:12:59,749   Num examples = 9815
2022-08-29 21:12:59,749   Batch size = 32
2022-08-29 21:12:59,750 ***** Eval results *****
2022-08-29 21:12:59,750   att_loss = 4.053004868087192
2022-08-29 21:12:59,750   cls_loss = 0.0
2022-08-29 21:12:59,750   global_step = 46999
2022-08-29 21:12:59,750   loss = 5.845558266543275
2022-08-29 21:12:59,750   rep_loss = 1.7925533972857581
2022-08-29 21:12:59,750 ***** Save model *****
2022-08-29 21:13:07,088 ***** Running evaluation *****
2022-08-29 21:13:07,088   Epoch = 46 iter 33199 step
2022-08-29 21:13:07,088   Num examples = 1500
2022-08-29 21:13:07,088   Batch size = 32
2022-08-29 21:13:07,089 ***** Eval results *****
2022-08-29 21:13:07,090   att_loss = 2.0729279308988335
2022-08-29 21:13:07,090   cls_loss = 0.0
2022-08-29 21:13:07,090   global_step = 33199
2022-08-29 21:13:07,090   loss = 3.455441204427976
2022-08-29 21:13:07,090   rep_loss = 1.3825132791061847
2022-08-29 21:13:07,090 ***** Save model *****
2022-08-29 21:13:27,874 ***** Running evaluation *****
2022-08-29 21:13:27,875   Epoch = 45 iter 48999 step
2022-08-29 21:13:27,875   Num examples = 1043
2022-08-29 21:13:27,875   Batch size = 32
2022-08-29 21:13:27,876 ***** Eval results *****
2022-08-29 21:13:27,876   att_loss = 0.7468682607522787
2022-08-29 21:13:27,876   cls_loss = 0.0
2022-08-29 21:13:27,876   global_step = 48999
2022-08-29 21:13:27,877   loss = 2.223052698456314
2022-08-29 21:13:27,877   rep_loss = 1.47618443897357
2022-08-29 21:13:27,877 ***** Save model *****
2022-08-29 21:13:47,022 ***** Running evaluation *****
2022-08-29 21:13:47,023   Epoch = 46 iter 33399 step
2022-08-29 21:13:47,023   Num examples = 1500
2022-08-29 21:13:47,023   Batch size = 32
2022-08-29 21:13:47,024 ***** Eval results *****
2022-08-29 21:13:47,024   att_loss = 2.0088973135318398
2022-08-29 21:13:47,024   cls_loss = 0.0
2022-08-29 21:13:47,024   global_step = 33399
2022-08-29 21:13:47,024   loss = 3.380800330735281
2022-08-29 21:13:47,024   rep_loss = 1.3719030146328908
2022-08-29 21:13:47,024 ***** Save model *****
2022-08-29 21:14:06,293 ***** Running evaluation *****
2022-08-29 21:14:06,294   Epoch = 46 iter 49199 step
2022-08-29 21:14:06,294   Num examples = 1043
2022-08-29 21:14:06,294   Batch size = 32
2022-08-29 21:14:06,295 ***** Eval results *****
2022-08-29 21:14:06,295   att_loss = 0.773205600154232
2022-08-29 21:14:06,295   cls_loss = 0.0
2022-08-29 21:14:06,295   global_step = 49199
2022-08-29 21:14:06,295   loss = 2.2520223520171476
2022-08-29 21:14:06,295   rep_loss = 1.4788167510234134
2022-08-29 21:14:06,296 ***** Save model *****
2022-08-29 21:14:09,805 ***** Running evaluation *****
2022-08-29 21:14:09,805   Epoch = 3 iter 47199 step
2022-08-29 21:14:09,805   Num examples = 9815
2022-08-29 21:14:09,805   Batch size = 32
2022-08-29 21:14:09,807 ***** Eval results *****
2022-08-29 21:14:09,807   att_loss = 4.053051006947779
2022-08-29 21:14:09,807   cls_loss = 0.0
2022-08-29 21:14:09,807   global_step = 47199
2022-08-29 21:14:09,807   loss = 5.845458770289532
2022-08-29 21:14:09,807   rep_loss = 1.7924077621939645
2022-08-29 21:14:09,807 ***** Save model *****
2022-08-29 21:14:26,025 ***** Running evaluation *****
2022-08-29 21:14:26,026   Epoch = 46 iter 33599 step
2022-08-29 21:14:26,026   Num examples = 1500
2022-08-29 21:14:26,026   Batch size = 32
2022-08-29 21:14:26,027 ***** Eval results *****
2022-08-29 21:14:26,027   att_loss = 2.0166171751590203
2022-08-29 21:14:26,027   cls_loss = 0.0
2022-08-29 21:14:26,027   global_step = 33599
2022-08-29 21:14:26,027   loss = 3.3908281079941998
2022-08-29 21:14:26,027   rep_loss = 1.3742109282421773
2022-08-29 21:14:26,027 ***** Save model *****
2022-08-29 21:14:45,037 ***** Running evaluation *****
2022-08-29 21:14:45,038   Epoch = 46 iter 49399 step
2022-08-29 21:14:45,038   Num examples = 1043
2022-08-29 21:14:45,038   Batch size = 32
2022-08-29 21:14:45,039 ***** Eval results *****
2022-08-29 21:14:45,039   att_loss = 0.7562655753755042
2022-08-29 21:14:45,039   cls_loss = 0.0
2022-08-29 21:14:45,039   global_step = 49399
2022-08-29 21:14:45,039   loss = 2.233490771473114
2022-08-29 21:14:45,040   rep_loss = 1.4772252020360799
2022-08-29 21:14:45,040 ***** Save model *****
2022-08-29 21:15:04,897 ***** Running evaluation *****
2022-08-29 21:15:04,897   Epoch = 47 iter 33799 step
2022-08-29 21:15:04,898   Num examples = 1500
2022-08-29 21:15:04,898   Batch size = 32
2022-08-29 21:15:04,899 ***** Eval results *****
2022-08-29 21:15:04,899   att_loss = 2.0424891642804415
2022-08-29 21:15:04,899   cls_loss = 0.0
2022-08-29 21:15:04,899   global_step = 33799
2022-08-29 21:15:04,899   loss = 3.416555148250652
2022-08-29 21:15:04,899   rep_loss = 1.3740659749732826
2022-08-29 21:15:04,899 ***** Save model *****
2022-08-29 21:15:19,848 ***** Running evaluation *****
2022-08-29 21:15:19,849   Epoch = 3 iter 47399 step
2022-08-29 21:15:19,849   Num examples = 9815
2022-08-29 21:15:19,849   Batch size = 32
2022-08-29 21:15:19,850 ***** Eval results *****
2022-08-29 21:15:19,850   att_loss = 4.053050476888076
2022-08-29 21:15:19,850   cls_loss = 0.0
2022-08-29 21:15:19,850   global_step = 47399
2022-08-29 21:15:19,850   loss = 5.8452709054937895
2022-08-29 21:15:19,850   rep_loss = 1.7922204273106943
2022-08-29 21:15:19,850 ***** Save model *****
2022-08-29 21:15:23,175 ***** Running evaluation *****
2022-08-29 21:15:23,175   Epoch = 46 iter 49599 step
2022-08-29 21:15:23,176   Num examples = 1043
2022-08-29 21:15:23,176   Batch size = 32
2022-08-29 21:15:23,177 ***** Eval results *****
2022-08-29 21:15:23,177   att_loss = 0.7526139732125965
2022-08-29 21:15:23,177   cls_loss = 0.0
2022-08-29 21:15:23,177   global_step = 49599
2022-08-29 21:15:23,177   loss = 2.2278409350956068
2022-08-29 21:15:23,177   rep_loss = 1.4752269656794845
2022-08-29 21:15:23,177 ***** Save model *****
2022-08-29 21:15:43,837 ***** Running evaluation *****
2022-08-29 21:15:43,838   Epoch = 47 iter 33999 step
2022-08-29 21:15:43,838   Num examples = 1500
2022-08-29 21:15:43,838   Batch size = 32
2022-08-29 21:15:43,839 ***** Eval results *****
2022-08-29 21:15:43,840   att_loss = 1.9946145559016895
2022-08-29 21:15:43,840   cls_loss = 0.0
2022-08-29 21:15:43,840   global_step = 33999
2022-08-29 21:15:43,840   loss = 3.367030407600252
2022-08-29 21:15:43,840   rep_loss = 1.3724158516985625
2022-08-29 21:15:43,840 ***** Save model *****
2022-08-29 21:16:01,323 ***** Running evaluation *****
2022-08-29 21:16:01,324   Epoch = 46 iter 49799 step
2022-08-29 21:16:01,324   Num examples = 1043
2022-08-29 21:16:01,324   Batch size = 32
2022-08-29 21:16:01,325 ***** Eval results *****
2022-08-29 21:16:01,325   att_loss = 0.7506876495484979
2022-08-29 21:16:01,325   cls_loss = 0.0
2022-08-29 21:16:01,325   global_step = 49799
2022-08-29 21:16:01,325   loss = 2.2256935291247646
2022-08-29 21:16:01,325   rep_loss = 1.4750058839289157
2022-08-29 21:16:01,326 ***** Save model *****
2022-08-29 21:16:22,759 ***** Running evaluation *****
2022-08-29 21:16:22,760   Epoch = 47 iter 34199 step
2022-08-29 21:16:22,760   Num examples = 1500
2022-08-29 21:16:22,760   Batch size = 32
2022-08-29 21:16:22,761 ***** Eval results *****
2022-08-29 21:16:22,761   att_loss = 2.006200819878652
2022-08-29 21:16:22,761   cls_loss = 0.0
2022-08-29 21:16:22,761   global_step = 34199
2022-08-29 21:16:22,761   loss = 3.3768905203863486
2022-08-29 21:16:22,761   rep_loss = 1.3706896965603692
2022-08-29 21:16:22,761 ***** Save model *****
2022-08-29 21:16:31,148 ***** Running evaluation *****
2022-08-29 21:16:31,149   Epoch = 3 iter 47599 step
2022-08-29 21:16:31,149   Num examples = 9815
2022-08-29 21:16:31,149   Batch size = 32
2022-08-29 21:16:31,150 ***** Eval results *****
2022-08-29 21:16:31,150   att_loss = 4.053287462933528
2022-08-29 21:16:31,150   cls_loss = 0.0
2022-08-29 21:16:31,150   global_step = 47599
2022-08-29 21:16:31,150   loss = 5.845339294048446
2022-08-29 21:16:31,150   rep_loss = 1.7920518295565542
2022-08-29 21:16:31,150 ***** Save model *****
2022-08-29 21:16:40,244 ***** Running evaluation *****
2022-08-29 21:16:40,245   Epoch = 46 iter 49999 step
2022-08-29 21:16:40,245   Num examples = 1043
2022-08-29 21:16:40,245   Batch size = 32
2022-08-29 21:16:40,246 ***** Eval results *****
2022-08-29 21:16:40,247   att_loss = 0.7522851971205833
2022-08-29 21:16:40,247   cls_loss = 0.0
2022-08-29 21:16:40,247   global_step = 49999
2022-08-29 21:16:40,247   loss = 2.2276806486460425
2022-08-29 21:16:40,247   rep_loss = 1.4753954536468645
2022-08-29 21:16:40,247 ***** Save model *****
2022-08-29 21:17:01,832 ***** Running evaluation *****
2022-08-29 21:17:01,833   Epoch = 47 iter 34399 step
2022-08-29 21:17:01,833   Num examples = 1500
2022-08-29 21:17:01,833   Batch size = 32
2022-08-29 21:17:01,834 ***** Eval results *****
2022-08-29 21:17:01,834   att_loss = 2.005728472246693
2022-08-29 21:17:01,835   cls_loss = 0.0
2022-08-29 21:17:01,835   global_step = 34399
2022-08-29 21:17:01,835   loss = 3.378431692605362
2022-08-29 21:17:01,835   rep_loss = 1.3727032179854366
2022-08-29 21:17:01,835 ***** Save model *****
2022-08-29 21:17:23,255 ***** Running evaluation *****
2022-08-29 21:17:23,255   Epoch = 47 iter 50199 step
2022-08-29 21:17:23,255   Num examples = 1043
2022-08-29 21:17:23,256   Batch size = 32
2022-08-29 21:17:23,257 ***** Eval results *****
2022-08-29 21:17:23,257   att_loss = 0.7030063072840372
2022-08-29 21:17:23,257   cls_loss = 0.0
2022-08-29 21:17:23,257   global_step = 50199
2022-08-29 21:17:23,257   loss = 2.1746431986490884
2022-08-29 21:17:23,257   rep_loss = 1.4716369311014812
2022-08-29 21:17:23,257 ***** Save model *****
2022-08-29 21:17:40,828 ***** Running evaluation *****
2022-08-29 21:17:40,828   Epoch = 48 iter 34599 step
2022-08-29 21:17:40,829   Num examples = 1500
2022-08-29 21:17:40,829   Batch size = 32
2022-08-29 21:17:40,831 ***** Eval results *****
2022-08-29 21:17:40,831   att_loss = 2.0990626547071667
2022-08-29 21:17:40,831   cls_loss = 0.0
2022-08-29 21:17:40,831   global_step = 34599
2022-08-29 21:17:40,831   loss = 3.4811438701770925
2022-08-29 21:17:40,831   rep_loss = 1.3820812101717348
2022-08-29 21:17:40,831 ***** Save model *****
2022-08-29 21:17:41,425 ***** Running evaluation *****
2022-08-29 21:17:41,425   Epoch = 3 iter 47799 step
2022-08-29 21:17:41,426   Num examples = 9815
2022-08-29 21:17:41,426   Batch size = 32
2022-08-29 21:17:41,427 ***** Eval results *****
2022-08-29 21:17:41,427   att_loss = 4.052161532023034
2022-08-29 21:17:41,427   cls_loss = 0.0
2022-08-29 21:17:41,427   global_step = 47799
2022-08-29 21:17:41,427   loss = 5.843979701608685
2022-08-29 21:17:41,427   rep_loss = 1.7918181680773595
2022-08-29 21:17:41,427 ***** Save model *****
2022-08-29 21:18:02,221 ***** Running evaluation *****
2022-08-29 21:18:02,222   Epoch = 47 iter 50399 step
2022-08-29 21:18:02,222   Num examples = 1043
2022-08-29 21:18:02,222   Batch size = 32
2022-08-29 21:18:02,223 ***** Eval results *****
2022-08-29 21:18:02,223   att_loss = 0.7376290468746806
2022-08-29 21:18:02,223   cls_loss = 0.0
2022-08-29 21:18:02,223   global_step = 50399
2022-08-29 21:18:02,223   loss = 2.205349838792397
2022-08-29 21:18:02,223   rep_loss = 1.4677207892751458
2022-08-29 21:18:02,223 ***** Save model *****
2022-08-29 21:18:21,559 ***** Running evaluation *****
2022-08-29 21:18:21,560   Epoch = 48 iter 34799 step
2022-08-29 21:18:21,560   Num examples = 1500
2022-08-29 21:18:21,560   Batch size = 32
2022-08-29 21:18:21,561 ***** Eval results *****
2022-08-29 21:18:21,561   att_loss = 2.056957224589675
2022-08-29 21:18:21,561   cls_loss = 0.0
2022-08-29 21:18:21,561   global_step = 34799
2022-08-29 21:18:21,561   loss = 3.43163217715363
2022-08-29 21:18:21,561   rep_loss = 1.3746749479379228
2022-08-29 21:18:21,562 ***** Save model *****
2022-08-29 21:18:41,488 ***** Running evaluation *****
2022-08-29 21:18:41,489   Epoch = 47 iter 50599 step
2022-08-29 21:18:41,489   Num examples = 1043
2022-08-29 21:18:41,489   Batch size = 32
2022-08-29 21:18:41,490 ***** Eval results *****
2022-08-29 21:18:41,490   att_loss = 0.741025890665078
2022-08-29 21:18:41,490   cls_loss = 0.0
2022-08-29 21:18:41,490   global_step = 50599
2022-08-29 21:18:41,490   loss = 2.213073123181724
2022-08-29 21:18:41,490   rep_loss = 1.4720472310376227
2022-08-29 21:18:41,490 ***** Save model *****
2022-08-29 21:18:55,431 ***** Running evaluation *****
2022-08-29 21:18:55,431   Epoch = 3 iter 47999 step
2022-08-29 21:18:55,431   Num examples = 9815
2022-08-29 21:18:55,431   Batch size = 32
2022-08-29 21:18:55,433 ***** Eval results *****
2022-08-29 21:18:55,433   att_loss = 4.0517872065737315
2022-08-29 21:18:55,433   cls_loss = 0.0
2022-08-29 21:18:55,433   global_step = 47999
2022-08-29 21:18:55,433   loss = 5.843417896688166
2022-08-29 21:18:55,433   rep_loss = 1.7916306882388005
2022-08-29 21:18:55,433 ***** Save model *****
2022-08-29 21:19:01,713 ***** Running evaluation *****
2022-08-29 21:19:01,714   Epoch = 48 iter 34999 step
2022-08-29 21:19:01,714   Num examples = 1500
2022-08-29 21:19:01,714   Batch size = 32
2022-08-29 21:19:01,716 ***** Eval results *****
2022-08-29 21:19:01,716   att_loss = 2.020983248559114
2022-08-29 21:19:01,716   cls_loss = 0.0
2022-08-29 21:19:01,716   global_step = 34999
2022-08-29 21:19:01,716   loss = 3.3914348241324737
2022-08-29 21:19:01,716   rep_loss = 1.370451576241823
2022-08-29 21:19:01,716 ***** Save model *****
2022-08-29 21:19:21,330 ***** Running evaluation *****
2022-08-29 21:19:21,331   Epoch = 47 iter 50799 step
2022-08-29 21:19:21,331   Num examples = 1043
2022-08-29 21:19:21,331   Batch size = 32
2022-08-29 21:19:21,332 ***** Eval results *****
2022-08-29 21:19:21,332   att_loss = 0.7509789865210678
2022-08-29 21:19:21,332   cls_loss = 0.0
2022-08-29 21:19:21,332   global_step = 50799
2022-08-29 21:19:21,332   loss = 2.2256831102703343
2022-08-29 21:19:21,332   rep_loss = 1.4747041254296627
2022-08-29 21:19:21,333 ***** Save model *****
2022-08-29 21:19:40,584 ***** Running evaluation *****
2022-08-29 21:19:40,584   Epoch = 49 iter 35199 step
2022-08-29 21:19:40,584   Num examples = 1500
2022-08-29 21:19:40,584   Batch size = 32
2022-08-29 21:19:40,586 ***** Eval results *****
2022-08-29 21:19:40,586   att_loss = 1.868707488564884
2022-08-29 21:19:40,586   cls_loss = 0.0
2022-08-29 21:19:40,586   global_step = 35199
2022-08-29 21:19:40,586   loss = 3.2256449110367718
2022-08-29 21:19:40,586   rep_loss = 1.356937422471888
2022-08-29 21:19:40,586 ***** Save model *****
2022-08-29 21:19:59,561 ***** Running evaluation *****
2022-08-29 21:19:59,561   Epoch = 47 iter 50999 step
2022-08-29 21:19:59,561   Num examples = 1043
2022-08-29 21:19:59,561   Batch size = 32
2022-08-29 21:19:59,563 ***** Eval results *****
2022-08-29 21:19:59,563   att_loss = 0.7467596886092669
2022-08-29 21:19:59,563   cls_loss = 0.0
2022-08-29 21:19:59,563   global_step = 50999
2022-08-29 21:19:59,563   loss = 2.219960420099023
2022-08-29 21:19:59,563   rep_loss = 1.4732007309330504
2022-08-29 21:19:59,563 ***** Save model *****
2022-08-29 21:20:06,938 ***** Running evaluation *****
2022-08-29 21:20:06,939   Epoch = 3 iter 48199 step
2022-08-29 21:20:06,939   Num examples = 9815
2022-08-29 21:20:06,939   Batch size = 32
2022-08-29 21:20:06,940 ***** Eval results *****
2022-08-29 21:20:06,940   att_loss = 4.050540511917611
2022-08-29 21:20:06,940   cls_loss = 0.0
2022-08-29 21:20:06,940   global_step = 48199
2022-08-29 21:20:06,940   loss = 5.841905672144391
2022-08-29 21:20:06,940   rep_loss = 1.7913651584573815
2022-08-29 21:20:06,940 ***** Save model *****
2022-08-29 21:20:19,474 ***** Running evaluation *****
2022-08-29 21:20:19,475   Epoch = 49 iter 35399 step
2022-08-29 21:20:19,475   Num examples = 1500
2022-08-29 21:20:19,475   Batch size = 32
2022-08-29 21:20:19,476 ***** Eval results *****
2022-08-29 21:20:19,477   att_loss = 1.9967565866109962
2022-08-29 21:20:19,477   cls_loss = 0.0
2022-08-29 21:20:19,477   global_step = 35399
2022-08-29 21:20:19,477   loss = 3.364304051421205
2022-08-29 21:20:19,477   rep_loss = 1.3675474757972401
2022-08-29 21:20:19,477 ***** Save model *****
2022-08-29 21:20:41,739 ***** Running evaluation *****
2022-08-29 21:20:41,740   Epoch = 47 iter 51199 step
2022-08-29 21:20:41,740   Num examples = 1043
2022-08-29 21:20:41,740   Batch size = 32
2022-08-29 21:20:41,741 ***** Eval results *****
2022-08-29 21:20:41,741   att_loss = 0.7479700938476285
2022-08-29 21:20:41,741   cls_loss = 0.0
2022-08-29 21:20:41,741   global_step = 51199
2022-08-29 21:20:41,741   loss = 2.222783300123091
2022-08-29 21:20:41,742   rep_loss = 1.4748132045818112
2022-08-29 21:20:41,742 ***** Save model *****
2022-08-29 21:20:58,187 ***** Running evaluation *****
2022-08-29 21:20:58,187   Epoch = 49 iter 35599 step
2022-08-29 21:20:58,187   Num examples = 1500
2022-08-29 21:20:58,187   Batch size = 32
2022-08-29 21:20:58,188 ***** Eval results *****
2022-08-29 21:20:58,188   att_loss = 2.0036771368923234
2022-08-29 21:20:58,188   cls_loss = 0.0
2022-08-29 21:20:58,188   global_step = 35599
2022-08-29 21:20:58,189   loss = 3.3720332312641097
2022-08-29 21:20:58,189   rep_loss = 1.3683561026621207
2022-08-29 21:20:58,189 ***** Save model *****
2022-08-29 21:21:16,824 ***** Running evaluation *****
2022-08-29 21:21:16,824   Epoch = 3 iter 48399 step
2022-08-29 21:21:16,824   Num examples = 9815
2022-08-29 21:21:16,824   Batch size = 32
2022-08-29 21:21:16,826 ***** Eval results *****
2022-08-29 21:21:16,826   att_loss = 4.051171552038102
2022-08-29 21:21:16,826   cls_loss = 0.0
2022-08-29 21:21:16,826   global_step = 48399
2022-08-29 21:21:16,826   loss = 5.842417950048238
2022-08-29 21:21:16,826   rep_loss = 1.79124639605521
2022-08-29 21:21:16,826 ***** Save model *****
2022-08-29 21:21:20,008 ***** Running evaluation *****
2022-08-29 21:21:20,008   Epoch = 48 iter 51399 step
2022-08-29 21:21:20,008   Num examples = 1043
2022-08-29 21:21:20,009   Batch size = 32
2022-08-29 21:21:20,010 ***** Eval results *****
2022-08-29 21:21:20,010   att_loss = 0.7367310868369208
2022-08-29 21:21:20,010   cls_loss = 0.0
2022-08-29 21:21:20,010   global_step = 51399
2022-08-29 21:21:20,010   loss = 2.20055544729586
2022-08-29 21:21:20,010   rep_loss = 1.463824361341971
2022-08-29 21:21:20,010 ***** Save model *****
2022-08-29 21:21:36,866 ***** Running evaluation *****
2022-08-29 21:21:36,866   Epoch = 49 iter 35799 step
2022-08-29 21:21:36,866   Num examples = 1500
2022-08-29 21:21:36,866   Batch size = 32
2022-08-29 21:21:36,868 ***** Eval results *****
2022-08-29 21:21:36,868   att_loss = 2.010210602751041
2022-08-29 21:21:36,868   cls_loss = 0.0
2022-08-29 21:21:36,868   global_step = 35799
2022-08-29 21:21:36,868   loss = 3.3783374369820667
2022-08-29 21:21:36,868   rep_loss = 1.3681268444710457
2022-08-29 21:21:36,868 ***** Save model *****
2022-08-29 21:21:58,332 ***** Running evaluation *****
2022-08-29 21:21:58,333   Epoch = 48 iter 51599 step
2022-08-29 21:21:58,333   Num examples = 1043
2022-08-29 21:21:58,333   Batch size = 32
2022-08-29 21:21:58,334 ***** Eval results *****
2022-08-29 21:21:58,334   att_loss = 0.7406885575002699
2022-08-29 21:21:58,334   cls_loss = 0.0
2022-08-29 21:21:58,334   global_step = 51599
2022-08-29 21:21:58,334   loss = 2.211976378355453
2022-08-29 21:21:58,334   rep_loss = 1.4712878170298107
2022-08-29 21:21:58,334 ***** Save model *****
2022-08-29 21:22:02,449 The args: Namespace(aug_train=False, cache_dir='', data_dir='glue_data/STSB', data_url='', do_eval=False, do_lower_case=True, eval_batch_size=32, eval_step=100, gradient_accumulation_steps=1, hidden_act='quad', learning_rate=1e-05, log_path='tmp/distill/STSB/quad_2quad/bert-base-uncased/log.txt', max_seq_length=128, no_cuda=False, num_train_epochs=5.0, output_dir='tmp/distill/STSB/quad_2quad/bert-base-uncased/5e-05_1e-05_8_stage2', pred_distill=True, seed=42, softmax_act='2quad', student_model='tmp/distill/STSB/quad_2quad/bert-base-uncased/5e-05_1e-05_8', task_name='STSB', teacher_model='/home/ubuntu/checkpoints/exp/STSB', temperature=1.0, train_batch_size=8, warmup_proportion=0.1, weight_decay=0.0001)
2022-08-29 21:22:02,449 device: cuda n_gpu: 1
2022-08-29 21:22:02,544 Writing example 0 of 5749
2022-08-29 21:22:02,544 *** Example ***
2022-08-29 21:22:02,544 guid: train-0
2022-08-29 21:22:02,544 tokens: [CLS] a plane is taking off . [SEP] an air plane is taking off . [SEP]
2022-08-29 21:22:02,544 input_ids: 101 1037 4946 2003 2635 2125 1012 102 2019 2250 4946 2003 2635 2125 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-08-29 21:22:02,544 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-08-29 21:22:02,544 segment_ids: 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-08-29 21:22:02,544 label: 5.000
2022-08-29 21:22:02,545 label_id: 5.0
2022-08-29 21:22:04,889 Writing example 0 of 1500
2022-08-29 21:22:04,889 *** Example ***
2022-08-29 21:22:04,889 guid: dev-0
2022-08-29 21:22:04,889 tokens: [CLS] a man with a hard hat is dancing . [SEP] a man wearing a hard hat is dancing . [SEP]
2022-08-29 21:22:04,890 input_ids: 101 1037 2158 2007 1037 2524 6045 2003 5613 1012 102 1037 2158 4147 1037 2524 6045 2003 5613 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-08-29 21:22:04,890 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-08-29 21:22:04,890 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-08-29 21:22:04,890 label: 5.000
2022-08-29 21:22:04,890 label_id: 5.0
2022-08-29 21:22:05,567 Model config {
  "_name_or_path": "bert-base-uncased",
  "architectures": [
    "BertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "crypten": false,
  "finetuning_task": "stsb",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0
  },
  "layer_norm_eps": 1e-12,
  "log_path": "tmp/distill/STSB/quad_2quad/bert-base-uncased/log.txt",
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "pre_trained": "",
  "problem_type": "regression",
  "softmax_act": "softmax",
  "torch_dtype": "float32",
  "training": "",
  "transformers_version": "4.20.0.dev0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

2022-08-29 21:22:08,287 Loading model /home/ubuntu/checkpoints/exp/STSB/pytorch_model.bin
2022-08-29 21:22:08,622 loading model...
2022-08-29 21:22:08,692 done!
2022-08-29 21:22:08,692 Weights from pretrained model not used in TinyBertForSequenceClassification: ['bert.embeddings.position_ids']
2022-08-29 21:22:16,108 ***** Teacher evaluation *****
2022-08-29 21:22:16,109 {'pearson': 0.8927170602068698, 'spearmanr': 0.8894144581995493, 'corr': 0.8910657592032096, 'eval_loss': 0.4806266178159004}
2022-08-29 21:22:16,110 Model config {
  "_name_or_path": "bert-base-uncased",
  "architectures": [
    "BertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "crypten": false,
  "finetuning_task": "stsb",
  "gradient_checkpointing": false,
  "hidden_act": "quad",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0
  },
  "layer_norm_eps": 1e-12,
  "log_path": "tmp/distill/STSB/quad_2quad/bert-base-uncased/log.txt",
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "pre_trained": "",
  "problem_type": "regression",
  "softmax_act": "2quad",
  "torch_dtype": "float32",
  "training": "",
  "transformers_version": "4.20.0.dev0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

2022-08-29 21:22:18,872 Loading model tmp/distill/STSB/quad_2quad/bert-base-uncased/5e-05_1e-05_8/pytorch_model.bin
2022-08-29 21:22:19,178 loading model...
2022-08-29 21:22:19,225 done!
2022-08-29 21:22:19,308 ***** Running training *****
2022-08-29 21:22:19,309   Num examples = 5749
2022-08-29 21:22:19,309   Batch size = 8
2022-08-29 21:22:19,309   Num steps = 3590
2022-08-29 21:22:19,310 n: bert.embeddings.word_embeddings.weight
2022-08-29 21:22:19,310 n: bert.embeddings.position_embeddings.weight
2022-08-29 21:22:19,310 n: bert.embeddings.token_type_embeddings.weight
2022-08-29 21:22:19,310 n: bert.embeddings.LayerNorm.weight
2022-08-29 21:22:19,310 n: bert.embeddings.LayerNorm.bias
2022-08-29 21:22:19,310 n: bert.encoder.layer.0.attention.self.query.weight
2022-08-29 21:22:19,310 n: bert.encoder.layer.0.attention.self.query.bias
2022-08-29 21:22:19,310 n: bert.encoder.layer.0.attention.self.key.weight
2022-08-29 21:22:19,310 n: bert.encoder.layer.0.attention.self.key.bias
2022-08-29 21:22:19,310 n: bert.encoder.layer.0.attention.self.value.weight
2022-08-29 21:22:19,310 n: bert.encoder.layer.0.attention.self.value.bias
2022-08-29 21:22:19,310 n: bert.encoder.layer.0.attention.output.dense.weight
2022-08-29 21:22:19,310 n: bert.encoder.layer.0.attention.output.dense.bias
2022-08-29 21:22:19,311 n: bert.encoder.layer.0.attention.output.LayerNorm.weight
2022-08-29 21:22:19,311 n: bert.encoder.layer.0.attention.output.LayerNorm.bias
2022-08-29 21:22:19,311 n: bert.encoder.layer.0.intermediate.dense.weight
2022-08-29 21:22:19,311 n: bert.encoder.layer.0.intermediate.dense.bias
2022-08-29 21:22:19,311 n: bert.encoder.layer.0.output.dense.weight
2022-08-29 21:22:19,311 n: bert.encoder.layer.0.output.dense.bias
2022-08-29 21:22:19,311 n: bert.encoder.layer.0.output.LayerNorm.weight
2022-08-29 21:22:19,311 n: bert.encoder.layer.0.output.LayerNorm.bias
2022-08-29 21:22:19,311 n: bert.encoder.layer.1.attention.self.query.weight
2022-08-29 21:22:19,311 n: bert.encoder.layer.1.attention.self.query.bias
2022-08-29 21:22:19,311 n: bert.encoder.layer.1.attention.self.key.weight
2022-08-29 21:22:19,311 n: bert.encoder.layer.1.attention.self.key.bias
2022-08-29 21:22:19,311 n: bert.encoder.layer.1.attention.self.value.weight
2022-08-29 21:22:19,311 n: bert.encoder.layer.1.attention.self.value.bias
2022-08-29 21:22:19,311 n: bert.encoder.layer.1.attention.output.dense.weight
2022-08-29 21:22:19,311 n: bert.encoder.layer.1.attention.output.dense.bias
2022-08-29 21:22:19,311 n: bert.encoder.layer.1.attention.output.LayerNorm.weight
2022-08-29 21:22:19,312 n: bert.encoder.layer.1.attention.output.LayerNorm.bias
2022-08-29 21:22:19,312 n: bert.encoder.layer.1.intermediate.dense.weight
2022-08-29 21:22:19,312 n: bert.encoder.layer.1.intermediate.dense.bias
2022-08-29 21:22:19,312 n: bert.encoder.layer.1.output.dense.weight
2022-08-29 21:22:19,312 n: bert.encoder.layer.1.output.dense.bias
2022-08-29 21:22:19,312 n: bert.encoder.layer.1.output.LayerNorm.weight
2022-08-29 21:22:19,312 n: bert.encoder.layer.1.output.LayerNorm.bias
2022-08-29 21:22:19,312 n: bert.encoder.layer.2.attention.self.query.weight
2022-08-29 21:22:19,312 n: bert.encoder.layer.2.attention.self.query.bias
2022-08-29 21:22:19,312 n: bert.encoder.layer.2.attention.self.key.weight
2022-08-29 21:22:19,312 n: bert.encoder.layer.2.attention.self.key.bias
2022-08-29 21:22:19,312 n: bert.encoder.layer.2.attention.self.value.weight
2022-08-29 21:22:19,312 n: bert.encoder.layer.2.attention.self.value.bias
2022-08-29 21:22:19,312 n: bert.encoder.layer.2.attention.output.dense.weight
2022-08-29 21:22:19,312 n: bert.encoder.layer.2.attention.output.dense.bias
2022-08-29 21:22:19,312 n: bert.encoder.layer.2.attention.output.LayerNorm.weight
2022-08-29 21:22:19,312 n: bert.encoder.layer.2.attention.output.LayerNorm.bias
2022-08-29 21:22:19,313 n: bert.encoder.layer.2.intermediate.dense.weight
2022-08-29 21:22:19,313 n: bert.encoder.layer.2.intermediate.dense.bias
2022-08-29 21:22:19,313 n: bert.encoder.layer.2.output.dense.weight
2022-08-29 21:22:19,313 n: bert.encoder.layer.2.output.dense.bias
2022-08-29 21:22:19,313 n: bert.encoder.layer.2.output.LayerNorm.weight
2022-08-29 21:22:19,313 n: bert.encoder.layer.2.output.LayerNorm.bias
2022-08-29 21:22:19,313 n: bert.encoder.layer.3.attention.self.query.weight
2022-08-29 21:22:19,313 n: bert.encoder.layer.3.attention.self.query.bias
2022-08-29 21:22:19,313 n: bert.encoder.layer.3.attention.self.key.weight
2022-08-29 21:22:19,313 n: bert.encoder.layer.3.attention.self.key.bias
2022-08-29 21:22:19,313 n: bert.encoder.layer.3.attention.self.value.weight
2022-08-29 21:22:19,313 n: bert.encoder.layer.3.attention.self.value.bias
2022-08-29 21:22:19,313 n: bert.encoder.layer.3.attention.output.dense.weight
2022-08-29 21:22:19,313 n: bert.encoder.layer.3.attention.output.dense.bias
2022-08-29 21:22:19,313 n: bert.encoder.layer.3.attention.output.LayerNorm.weight
2022-08-29 21:22:19,313 n: bert.encoder.layer.3.attention.output.LayerNorm.bias
2022-08-29 21:22:19,313 n: bert.encoder.layer.3.intermediate.dense.weight
2022-08-29 21:22:19,314 n: bert.encoder.layer.3.intermediate.dense.bias
2022-08-29 21:22:19,314 n: bert.encoder.layer.3.output.dense.weight
2022-08-29 21:22:19,314 n: bert.encoder.layer.3.output.dense.bias
2022-08-29 21:22:19,314 n: bert.encoder.layer.3.output.LayerNorm.weight
2022-08-29 21:22:19,314 n: bert.encoder.layer.3.output.LayerNorm.bias
2022-08-29 21:22:19,314 n: bert.encoder.layer.4.attention.self.query.weight
2022-08-29 21:22:19,314 n: bert.encoder.layer.4.attention.self.query.bias
2022-08-29 21:22:19,314 n: bert.encoder.layer.4.attention.self.key.weight
2022-08-29 21:22:19,314 n: bert.encoder.layer.4.attention.self.key.bias
2022-08-29 21:22:19,314 n: bert.encoder.layer.4.attention.self.value.weight
2022-08-29 21:22:19,314 n: bert.encoder.layer.4.attention.self.value.bias
2022-08-29 21:22:19,314 n: bert.encoder.layer.4.attention.output.dense.weight
2022-08-29 21:22:19,314 n: bert.encoder.layer.4.attention.output.dense.bias
2022-08-29 21:22:19,314 n: bert.encoder.layer.4.attention.output.LayerNorm.weight
2022-08-29 21:22:19,314 n: bert.encoder.layer.4.attention.output.LayerNorm.bias
2022-08-29 21:22:19,314 n: bert.encoder.layer.4.intermediate.dense.weight
2022-08-29 21:22:19,315 n: bert.encoder.layer.4.intermediate.dense.bias
2022-08-29 21:22:19,315 n: bert.encoder.layer.4.output.dense.weight
2022-08-29 21:22:19,315 n: bert.encoder.layer.4.output.dense.bias
2022-08-29 21:22:19,315 n: bert.encoder.layer.4.output.LayerNorm.weight
2022-08-29 21:22:19,315 n: bert.encoder.layer.4.output.LayerNorm.bias
2022-08-29 21:22:19,315 n: bert.encoder.layer.5.attention.self.query.weight
2022-08-29 21:22:19,315 n: bert.encoder.layer.5.attention.self.query.bias
2022-08-29 21:22:19,315 n: bert.encoder.layer.5.attention.self.key.weight
2022-08-29 21:22:19,315 n: bert.encoder.layer.5.attention.self.key.bias
2022-08-29 21:22:19,315 n: bert.encoder.layer.5.attention.self.value.weight
2022-08-29 21:22:19,315 n: bert.encoder.layer.5.attention.self.value.bias
2022-08-29 21:22:19,315 n: bert.encoder.layer.5.attention.output.dense.weight
2022-08-29 21:22:19,315 n: bert.encoder.layer.5.attention.output.dense.bias
2022-08-29 21:22:19,315 n: bert.encoder.layer.5.attention.output.LayerNorm.weight
2022-08-29 21:22:19,315 n: bert.encoder.layer.5.attention.output.LayerNorm.bias
2022-08-29 21:22:19,315 n: bert.encoder.layer.5.intermediate.dense.weight
2022-08-29 21:22:19,315 n: bert.encoder.layer.5.intermediate.dense.bias
2022-08-29 21:22:19,316 n: bert.encoder.layer.5.output.dense.weight
2022-08-29 21:22:19,316 n: bert.encoder.layer.5.output.dense.bias
2022-08-29 21:22:19,316 n: bert.encoder.layer.5.output.LayerNorm.weight
2022-08-29 21:22:19,316 n: bert.encoder.layer.5.output.LayerNorm.bias
2022-08-29 21:22:19,316 n: bert.encoder.layer.6.attention.self.query.weight
2022-08-29 21:22:19,316 n: bert.encoder.layer.6.attention.self.query.bias
2022-08-29 21:22:19,316 n: bert.encoder.layer.6.attention.self.key.weight
2022-08-29 21:22:19,316 n: bert.encoder.layer.6.attention.self.key.bias
2022-08-29 21:22:19,316 n: bert.encoder.layer.6.attention.self.value.weight
2022-08-29 21:22:19,316 n: bert.encoder.layer.6.attention.self.value.bias
2022-08-29 21:22:19,316 n: bert.encoder.layer.6.attention.output.dense.weight
2022-08-29 21:22:19,316 n: bert.encoder.layer.6.attention.output.dense.bias
2022-08-29 21:22:19,316 n: bert.encoder.layer.6.attention.output.LayerNorm.weight
2022-08-29 21:22:19,316 n: bert.encoder.layer.6.attention.output.LayerNorm.bias
2022-08-29 21:22:19,316 n: bert.encoder.layer.6.intermediate.dense.weight
2022-08-29 21:22:19,316 n: bert.encoder.layer.6.intermediate.dense.bias
2022-08-29 21:22:19,316 n: bert.encoder.layer.6.output.dense.weight
2022-08-29 21:22:19,317 n: bert.encoder.layer.6.output.dense.bias
2022-08-29 21:22:19,317 n: bert.encoder.layer.6.output.LayerNorm.weight
2022-08-29 21:22:19,317 n: bert.encoder.layer.6.output.LayerNorm.bias
2022-08-29 21:22:19,317 n: bert.encoder.layer.7.attention.self.query.weight
2022-08-29 21:22:19,317 n: bert.encoder.layer.7.attention.self.query.bias
2022-08-29 21:22:19,317 n: bert.encoder.layer.7.attention.self.key.weight
2022-08-29 21:22:19,317 n: bert.encoder.layer.7.attention.self.key.bias
2022-08-29 21:22:19,317 n: bert.encoder.layer.7.attention.self.value.weight
2022-08-29 21:22:19,317 n: bert.encoder.layer.7.attention.self.value.bias
2022-08-29 21:22:19,317 n: bert.encoder.layer.7.attention.output.dense.weight
2022-08-29 21:22:19,317 n: bert.encoder.layer.7.attention.output.dense.bias
2022-08-29 21:22:19,317 n: bert.encoder.layer.7.attention.output.LayerNorm.weight
2022-08-29 21:22:19,317 n: bert.encoder.layer.7.attention.output.LayerNorm.bias
2022-08-29 21:22:19,317 n: bert.encoder.layer.7.intermediate.dense.weight
2022-08-29 21:22:19,317 n: bert.encoder.layer.7.intermediate.dense.bias
2022-08-29 21:22:19,317 n: bert.encoder.layer.7.output.dense.weight
2022-08-29 21:22:19,317 n: bert.encoder.layer.7.output.dense.bias
2022-08-29 21:22:19,318 n: bert.encoder.layer.7.output.LayerNorm.weight
2022-08-29 21:22:19,318 n: bert.encoder.layer.7.output.LayerNorm.bias
2022-08-29 21:22:19,318 n: bert.encoder.layer.8.attention.self.query.weight
2022-08-29 21:22:19,318 n: bert.encoder.layer.8.attention.self.query.bias
2022-08-29 21:22:19,318 n: bert.encoder.layer.8.attention.self.key.weight
2022-08-29 21:22:19,318 n: bert.encoder.layer.8.attention.self.key.bias
2022-08-29 21:22:19,318 n: bert.encoder.layer.8.attention.self.value.weight
2022-08-29 21:22:19,318 n: bert.encoder.layer.8.attention.self.value.bias
2022-08-29 21:22:19,318 n: bert.encoder.layer.8.attention.output.dense.weight
2022-08-29 21:22:19,318 n: bert.encoder.layer.8.attention.output.dense.bias
2022-08-29 21:22:19,318 n: bert.encoder.layer.8.attention.output.LayerNorm.weight
2022-08-29 21:22:19,318 n: bert.encoder.layer.8.attention.output.LayerNorm.bias
2022-08-29 21:22:19,318 n: bert.encoder.layer.8.intermediate.dense.weight
2022-08-29 21:22:19,318 n: bert.encoder.layer.8.intermediate.dense.bias
2022-08-29 21:22:19,318 n: bert.encoder.layer.8.output.dense.weight
2022-08-29 21:22:19,318 n: bert.encoder.layer.8.output.dense.bias
2022-08-29 21:22:19,318 n: bert.encoder.layer.8.output.LayerNorm.weight
2022-08-29 21:22:19,319 n: bert.encoder.layer.8.output.LayerNorm.bias
2022-08-29 21:22:19,319 n: bert.encoder.layer.9.attention.self.query.weight
2022-08-29 21:22:19,319 n: bert.encoder.layer.9.attention.self.query.bias
2022-08-29 21:22:19,319 n: bert.encoder.layer.9.attention.self.key.weight
2022-08-29 21:22:19,319 n: bert.encoder.layer.9.attention.self.key.bias
2022-08-29 21:22:19,319 n: bert.encoder.layer.9.attention.self.value.weight
2022-08-29 21:22:19,319 n: bert.encoder.layer.9.attention.self.value.bias
2022-08-29 21:22:19,319 n: bert.encoder.layer.9.attention.output.dense.weight
2022-08-29 21:22:19,319 n: bert.encoder.layer.9.attention.output.dense.bias
2022-08-29 21:22:19,319 n: bert.encoder.layer.9.attention.output.LayerNorm.weight
2022-08-29 21:22:19,319 n: bert.encoder.layer.9.attention.output.LayerNorm.bias
2022-08-29 21:22:19,319 n: bert.encoder.layer.9.intermediate.dense.weight
2022-08-29 21:22:19,319 n: bert.encoder.layer.9.intermediate.dense.bias
2022-08-29 21:22:19,319 n: bert.encoder.layer.9.output.dense.weight
2022-08-29 21:22:19,319 n: bert.encoder.layer.9.output.dense.bias
2022-08-29 21:22:19,319 n: bert.encoder.layer.9.output.LayerNorm.weight
2022-08-29 21:22:19,319 n: bert.encoder.layer.9.output.LayerNorm.bias
2022-08-29 21:22:19,320 n: bert.encoder.layer.10.attention.self.query.weight
2022-08-29 21:22:19,320 n: bert.encoder.layer.10.attention.self.query.bias
2022-08-29 21:22:19,320 n: bert.encoder.layer.10.attention.self.key.weight
2022-08-29 21:22:19,320 n: bert.encoder.layer.10.attention.self.key.bias
2022-08-29 21:22:19,320 n: bert.encoder.layer.10.attention.self.value.weight
2022-08-29 21:22:19,320 n: bert.encoder.layer.10.attention.self.value.bias
2022-08-29 21:22:19,320 n: bert.encoder.layer.10.attention.output.dense.weight
2022-08-29 21:22:19,320 n: bert.encoder.layer.10.attention.output.dense.bias
2022-08-29 21:22:19,320 n: bert.encoder.layer.10.attention.output.LayerNorm.weight
2022-08-29 21:22:19,320 n: bert.encoder.layer.10.attention.output.LayerNorm.bias
2022-08-29 21:22:19,320 n: bert.encoder.layer.10.intermediate.dense.weight
2022-08-29 21:22:19,320 n: bert.encoder.layer.10.intermediate.dense.bias
2022-08-29 21:22:19,320 n: bert.encoder.layer.10.output.dense.weight
2022-08-29 21:22:19,320 n: bert.encoder.layer.10.output.dense.bias
2022-08-29 21:22:19,320 n: bert.encoder.layer.10.output.LayerNorm.weight
2022-08-29 21:22:19,320 n: bert.encoder.layer.10.output.LayerNorm.bias
2022-08-29 21:22:19,320 n: bert.encoder.layer.11.attention.self.query.weight
2022-08-29 21:22:19,321 n: bert.encoder.layer.11.attention.self.query.bias
2022-08-29 21:22:19,321 n: bert.encoder.layer.11.attention.self.key.weight
2022-08-29 21:22:19,321 n: bert.encoder.layer.11.attention.self.key.bias
2022-08-29 21:22:19,321 n: bert.encoder.layer.11.attention.self.value.weight
2022-08-29 21:22:19,321 n: bert.encoder.layer.11.attention.self.value.bias
2022-08-29 21:22:19,321 n: bert.encoder.layer.11.attention.output.dense.weight
2022-08-29 21:22:19,321 n: bert.encoder.layer.11.attention.output.dense.bias
2022-08-29 21:22:19,321 n: bert.encoder.layer.11.attention.output.LayerNorm.weight
2022-08-29 21:22:19,321 n: bert.encoder.layer.11.attention.output.LayerNorm.bias
2022-08-29 21:22:19,321 n: bert.encoder.layer.11.intermediate.dense.weight
2022-08-29 21:22:19,321 n: bert.encoder.layer.11.intermediate.dense.bias
2022-08-29 21:22:19,321 n: bert.encoder.layer.11.output.dense.weight
2022-08-29 21:22:19,321 n: bert.encoder.layer.11.output.dense.bias
2022-08-29 21:22:19,321 n: bert.encoder.layer.11.output.LayerNorm.weight
2022-08-29 21:22:19,321 n: bert.encoder.layer.11.output.LayerNorm.bias
2022-08-29 21:22:19,321 n: bert.pooler.dense.weight
2022-08-29 21:22:19,321 n: bert.pooler.dense.bias
2022-08-29 21:22:19,322 n: classifier.weight
2022-08-29 21:22:19,322 n: classifier.bias
2022-08-29 21:22:19,322 Total parameters: 109483009
2022-08-29 21:22:26,608 ***** Running evaluation *****
2022-08-29 21:22:26,609   Epoch = 3 iter 48599 step
2022-08-29 21:22:26,609   Num examples = 9815
2022-08-29 21:22:26,609   Batch size = 32
2022-08-29 21:22:26,610 ***** Eval results *****
2022-08-29 21:22:26,610   att_loss = 4.050127875758374
2022-08-29 21:22:26,610   cls_loss = 0.0
2022-08-29 21:22:26,610   global_step = 48599
2022-08-29 21:22:26,610   loss = 5.841146987058625
2022-08-29 21:22:26,610   rep_loss = 1.7910191097122774
2022-08-29 21:22:26,610 ***** Save model *****
2022-08-29 21:22:36,806 ***** Running evaluation *****
2022-08-29 21:22:36,807   Epoch = 48 iter 51799 step
2022-08-29 21:22:36,807   Num examples = 1043
2022-08-29 21:22:36,807   Batch size = 32
2022-08-29 21:22:36,808 ***** Eval results *****
2022-08-29 21:22:36,808   att_loss = 0.744062364714168
2022-08-29 21:22:36,808   cls_loss = 0.0
2022-08-29 21:22:36,808   global_step = 51799
2022-08-29 21:22:36,808   loss = 2.2148415641249897
2022-08-29 21:22:36,809   rep_loss = 1.4707791961242105
2022-08-29 21:22:36,809 ***** Save model *****
2022-08-29 21:22:37,206 ***** Running evaluation *****
2022-08-29 21:22:37,207   Epoch = 0 iter 99 step
2022-08-29 21:22:37,207   Num examples = 1500
2022-08-29 21:22:37,207   Batch size = 32
2022-08-29 21:22:40,991 ***** Eval results *****
2022-08-29 21:22:40,991   att_loss = 0.0
2022-08-29 21:22:40,991   cls_loss = 0.21351490544174054
2022-08-29 21:22:40,991   corr = 0.834496574236372
2022-08-29 21:22:40,991   eval_loss = 0.8289763036560505
2022-08-29 21:22:40,992   global_step = 99
2022-08-29 21:22:40,992   loss = 0.21351490544174054
2022-08-29 21:22:40,992   pearson = 0.8304116381197577
2022-08-29 21:22:40,992   rep_loss = 0.0
2022-08-29 21:22:40,992   spearmanr = 0.8385815103529863
2022-08-29 21:22:40,992 ***** Save model *****
2022-08-29 21:22:59,793 ***** Running evaluation *****
2022-08-29 21:22:59,793   Epoch = 0 iter 199 step
2022-08-29 21:22:59,793   Num examples = 1500
2022-08-29 21:22:59,793   Batch size = 32
2022-08-29 21:23:03,584 ***** Eval results *****
2022-08-29 21:23:03,584   att_loss = 0.0
2022-08-29 21:23:03,584   cls_loss = 0.23516344000122055
2022-08-29 21:23:03,584   corr = 0.8100418079304327
2022-08-29 21:23:03,584   eval_loss = 0.8357990980148315
2022-08-29 21:23:03,584   global_step = 199
2022-08-29 21:23:03,584   loss = 0.23516344000122055
2022-08-29 21:23:03,584   pearson = 0.8074015664188497
2022-08-29 21:23:03,584   rep_loss = 0.0
2022-08-29 21:23:03,584   spearmanr = 0.8126820494420156
2022-08-29 21:23:15,476 ***** Running evaluation *****
2022-08-29 21:23:15,476   Epoch = 48 iter 51999 step
2022-08-29 21:23:15,476   Num examples = 1043
2022-08-29 21:23:15,476   Batch size = 32
2022-08-29 21:23:15,478 ***** Eval results *****
2022-08-29 21:23:15,478   att_loss = 0.7455510629683125
2022-08-29 21:23:15,478   cls_loss = 0.0
2022-08-29 21:23:15,478   global_step = 51999
2022-08-29 21:23:15,478   loss = 2.215673929980012
2022-08-29 21:23:15,478   rep_loss = 1.4701228634840777
2022-08-29 21:23:15,478 ***** Save model *****
2022-08-29 21:23:21,568 ***** Running evaluation *****
2022-08-29 21:23:21,568   Epoch = 0 iter 299 step
2022-08-29 21:23:21,568   Num examples = 1500
2022-08-29 21:23:21,569   Batch size = 32
2022-08-29 21:23:25,350 ***** Eval results *****
2022-08-29 21:23:25,350   att_loss = 0.0
2022-08-29 21:23:25,350   cls_loss = 0.23163187605979849
2022-08-29 21:23:25,350   corr = 0.8318929795022331
2022-08-29 21:23:25,350   eval_loss = 0.8125797811340778
2022-08-29 21:23:25,350   global_step = 299
2022-08-29 21:23:25,350   loss = 0.23163187605979849
2022-08-29 21:23:25,350   pearson = 0.8308297105356502
2022-08-29 21:23:25,350   rep_loss = 0.0
2022-08-29 21:23:25,350   spearmanr = 0.8329562484688159
2022-08-29 21:23:39,417 ***** Running evaluation *****
2022-08-29 21:23:39,418   Epoch = 3 iter 48799 step
2022-08-29 21:23:39,418   Num examples = 9815
2022-08-29 21:23:39,418   Batch size = 32
2022-08-29 21:23:39,419 ***** Eval results *****
2022-08-29 21:23:39,419   att_loss = 4.050313436903461
2022-08-29 21:23:39,419   cls_loss = 0.0
2022-08-29 21:23:39,419   global_step = 48799
2022-08-29 21:23:39,419   loss = 5.841223514997042
2022-08-29 21:23:39,419   rep_loss = 1.7909100761243304
2022-08-29 21:23:39,420 ***** Save model *****
2022-08-29 21:23:43,340 ***** Running evaluation *****
2022-08-29 21:23:43,341   Epoch = 0 iter 399 step
2022-08-29 21:23:43,341   Num examples = 1500
2022-08-29 21:23:43,341   Batch size = 32
2022-08-29 21:23:47,127 ***** Eval results *****
2022-08-29 21:23:47,128   att_loss = 0.0
2022-08-29 21:23:47,128   cls_loss = 0.23861605301032537
2022-08-29 21:23:47,128   corr = 0.8310969755838205
2022-08-29 21:23:47,128   eval_loss = 0.7019948420372415
2022-08-29 21:23:47,128   global_step = 399
2022-08-29 21:23:47,128   loss = 0.23861605301032537
2022-08-29 21:23:47,128   pearson = 0.8327724702596189
2022-08-29 21:23:47,128   rep_loss = 0.0
2022-08-29 21:23:47,128   spearmanr = 0.8294214809080219
2022-08-29 21:23:57,890 ***** Running evaluation *****
2022-08-29 21:23:57,891   Epoch = 48 iter 52199 step
2022-08-29 21:23:57,891   Num examples = 1043
2022-08-29 21:23:57,891   Batch size = 32
2022-08-29 21:23:57,892 ***** Eval results *****
2022-08-29 21:23:57,892   att_loss = 0.7458783675961316
2022-08-29 21:23:57,893   cls_loss = 0.0
2022-08-29 21:23:57,893   global_step = 52199
2022-08-29 21:23:57,893   loss = 2.218015330105542
2022-08-29 21:23:57,893   rep_loss = 1.4721369598001082
2022-08-29 21:23:57,893 ***** Save model *****
2022-08-29 21:24:05,181 ***** Running evaluation *****
2022-08-29 21:24:05,181   Epoch = 0 iter 499 step
2022-08-29 21:24:05,182   Num examples = 1500
2022-08-29 21:24:05,182   Batch size = 32
2022-08-29 21:24:08,967 ***** Eval results *****
2022-08-29 21:24:08,968   att_loss = 0.0
2022-08-29 21:24:08,968   cls_loss = 0.2436969581886917
2022-08-29 21:24:08,968   corr = 0.8069999551746518
2022-08-29 21:24:08,968   eval_loss = 1.162744365156965
2022-08-29 21:24:08,968   global_step = 499
2022-08-29 21:24:08,968   loss = 0.2436969581886917
2022-08-29 21:24:08,968   pearson = 0.8047002333452544
2022-08-29 21:24:08,968   rep_loss = 0.0
2022-08-29 21:24:08,968   spearmanr = 0.8092996770040491
2022-08-29 21:24:27,006 ***** Running evaluation *****
2022-08-29 21:24:27,007   Epoch = 0 iter 599 step
2022-08-29 21:24:27,007   Num examples = 1500
2022-08-29 21:24:27,007   Batch size = 32
2022-08-29 21:24:30,790 ***** Eval results *****
2022-08-29 21:24:30,790   att_loss = 0.0
2022-08-29 21:24:30,790   cls_loss = 0.2493387578220071
2022-08-29 21:24:30,791   corr = 0.7730204503063826
2022-08-29 21:24:30,791   eval_loss = 1.356582874313314
2022-08-29 21:24:30,791   global_step = 599
2022-08-29 21:24:30,791   loss = 0.2493387578220071
2022-08-29 21:24:30,791   pearson = 0.7581549163034424
2022-08-29 21:24:30,791   rep_loss = 0.0
2022-08-29 21:24:30,791   spearmanr = 0.7878859843093227
2022-08-29 21:24:36,259 ***** Running evaluation *****
2022-08-29 21:24:36,259   Epoch = 49 iter 52399 step
2022-08-29 21:24:36,259   Num examples = 1043
2022-08-29 21:24:36,259   Batch size = 32
2022-08-29 21:24:36,261 ***** Eval results *****
2022-08-29 21:24:36,261   att_loss = 0.7653836972677885
2022-08-29 21:24:36,261   cls_loss = 0.0
2022-08-29 21:24:36,261   global_step = 52399
2022-08-29 21:24:36,261   loss = 2.2443287159079937
2022-08-29 21:24:36,261   rep_loss = 1.4789450310949068
2022-08-29 21:24:36,261 ***** Save model *****
2022-08-29 21:24:48,462 ***** Running evaluation *****
2022-08-29 21:24:48,462   Epoch = 3 iter 48999 step
2022-08-29 21:24:48,462   Num examples = 9815
2022-08-29 21:24:48,462   Batch size = 32
2022-08-29 21:24:48,463 ***** Eval results *****
2022-08-29 21:24:48,463   att_loss = 4.049973691799127
2022-08-29 21:24:48,463   cls_loss = 0.0
2022-08-29 21:24:48,463   global_step = 48999
2022-08-29 21:24:48,464   loss = 5.840690800029617
2022-08-29 21:24:48,464   rep_loss = 1.7907171062153
2022-08-29 21:24:48,464 ***** Save model *****
2022-08-29 21:24:48,843 ***** Running evaluation *****
2022-08-29 21:24:48,844   Epoch = 0 iter 699 step
2022-08-29 21:24:48,844   Num examples = 1500
2022-08-29 21:24:48,844   Batch size = 32
2022-08-29 21:24:52,636 ***** Eval results *****
2022-08-29 21:24:52,636   att_loss = 0.0
2022-08-29 21:24:52,636   cls_loss = 0.25079751948696777
2022-08-29 21:24:52,637   corr = 0.8456708795559688
2022-08-29 21:24:52,637   eval_loss = 1.0373317142750353
2022-08-29 21:24:52,637   global_step = 699
2022-08-29 21:24:52,637   loss = 0.25079751948696777
2022-08-29 21:24:52,637   pearson = 0.8410113705546068
2022-08-29 21:24:52,637   rep_loss = 0.0
2022-08-29 21:24:52,637   spearmanr = 0.8503303885573307
2022-08-29 21:24:52,637 ***** Save model *****
2022-08-29 21:25:13,865 ***** Running evaluation *****
2022-08-29 21:25:13,865   Epoch = 1 iter 799 step
2022-08-29 21:25:13,865   Num examples = 1500
2022-08-29 21:25:13,865   Batch size = 32
2022-08-29 21:25:14,657 ***** Running evaluation *****
2022-08-29 21:25:14,658   Epoch = 49 iter 52599 step
2022-08-29 21:25:14,658   Num examples = 1043
2022-08-29 21:25:14,658   Batch size = 32
2022-08-29 21:25:14,659 ***** Eval results *****
2022-08-29 21:25:14,659   att_loss = 0.7513982498467192
2022-08-29 21:25:14,659   cls_loss = 0.0
2022-08-29 21:25:14,659   global_step = 52599
2022-08-29 21:25:14,659   loss = 2.2250377770220298
2022-08-29 21:25:14,659   rep_loss = 1.473639525724261
2022-08-29 21:25:14,659 ***** Save model *****
2022-08-29 21:25:17,655 ***** Eval results *****
2022-08-29 21:25:17,655   att_loss = 0.0
2022-08-29 21:25:17,655   cls_loss = 0.19174839850561118
2022-08-29 21:25:17,655   corr = 0.8255328747560806
2022-08-29 21:25:17,656   eval_loss = 1.1865472222896332
2022-08-29 21:25:17,656   global_step = 799
2022-08-29 21:25:17,656   loss = 0.19174839850561118
2022-08-29 21:25:17,656   pearson = 0.8197407477192373
2022-08-29 21:25:17,656   rep_loss = 0.0
2022-08-29 21:25:17,656   spearmanr = 0.8313250017929238
2022-08-29 21:25:35,697 ***** Running evaluation *****
2022-08-29 21:25:35,698   Epoch = 1 iter 899 step
2022-08-29 21:25:35,698   Num examples = 1500
2022-08-29 21:25:35,698   Batch size = 32
2022-08-29 21:25:39,482 ***** Eval results *****
2022-08-29 21:25:39,482   att_loss = 0.0
2022-08-29 21:25:39,482   cls_loss = 0.1753216129599026
2022-08-29 21:25:39,482   corr = 0.8270237692432807
2022-08-29 21:25:39,482   eval_loss = 0.9122987628934232
2022-08-29 21:25:39,482   global_step = 899
2022-08-29 21:25:39,483   loss = 0.1753216129599026
2022-08-29 21:25:39,483   pearson = 0.8278953623229187
2022-08-29 21:25:39,483   rep_loss = 0.0
2022-08-29 21:25:39,483   spearmanr = 0.8261521761636427
2022-08-29 21:25:52,846 ***** Running evaluation *****
2022-08-29 21:25:52,847   Epoch = 49 iter 52799 step
2022-08-29 21:25:52,847   Num examples = 1043
2022-08-29 21:25:52,847   Batch size = 32
2022-08-29 21:25:52,848 ***** Eval results *****
2022-08-29 21:25:52,848   att_loss = 0.7482527451673463
2022-08-29 21:25:52,848   cls_loss = 0.0
2022-08-29 21:25:52,848   global_step = 52799
2022-08-29 21:25:52,848   loss = 2.2192348369439103
2022-08-29 21:25:52,848   rep_loss = 1.470982089032453
2022-08-29 21:25:52,848 ***** Save model *****
2022-08-29 21:25:57,533 ***** Running evaluation *****
2022-08-29 21:25:57,533   Epoch = 1 iter 999 step
2022-08-29 21:25:57,533   Num examples = 1500
2022-08-29 21:25:57,533   Batch size = 32
2022-08-29 21:25:59,791 ***** Running evaluation *****
2022-08-29 21:25:59,791   Epoch = 4 iter 49199 step
2022-08-29 21:25:59,791   Num examples = 9815
2022-08-29 21:25:59,791   Batch size = 32
2022-08-29 21:25:59,792 ***** Eval results *****
2022-08-29 21:25:59,792   att_loss = 3.944886120505955
2022-08-29 21:25:59,792   cls_loss = 0.0
2022-08-29 21:25:59,792   global_step = 49199
2022-08-29 21:25:59,792   loss = 5.717007064819336
2022-08-29 21:25:59,792   rep_loss = 1.7721209339473558
2022-08-29 21:25:59,793 ***** Save model *****
2022-08-29 21:26:01,323 ***** Eval results *****
2022-08-29 21:26:01,324   att_loss = 0.0
2022-08-29 21:26:01,324   cls_loss = 0.17670786742691044
2022-08-29 21:26:01,324   corr = 0.8247678427977563
2022-08-29 21:26:01,324   eval_loss = 0.8542664447363387
2022-08-29 21:26:01,324   global_step = 999
2022-08-29 21:26:01,324   loss = 0.17670786742691044
2022-08-29 21:26:01,324   pearson = 0.8214902910451721
2022-08-29 21:26:01,324   rep_loss = 0.0
2022-08-29 21:26:01,324   spearmanr = 0.8280453945503405
2022-08-29 21:26:19,381 ***** Running evaluation *****
2022-08-29 21:26:19,381   Epoch = 1 iter 1099 step
2022-08-29 21:26:19,381   Num examples = 1500
2022-08-29 21:26:19,382   Batch size = 32
2022-08-29 21:26:23,168 ***** Eval results *****
2022-08-29 21:26:23,169   att_loss = 0.0
2022-08-29 21:26:23,169   cls_loss = 0.17813428272334297
2022-08-29 21:26:23,169   corr = 0.790708222212001
2022-08-29 21:26:23,169   eval_loss = 1.2811654222772477
2022-08-29 21:26:23,169   global_step = 1099
2022-08-29 21:26:23,169   loss = 0.17813428272334297
2022-08-29 21:26:23,169   pearson = 0.7830725223018588
2022-08-29 21:26:23,169   rep_loss = 0.0
2022-08-29 21:26:23,169   spearmanr = 0.7983439221221433
2022-08-29 21:26:31,030 ***** Running evaluation *****
2022-08-29 21:26:31,031   Epoch = 49 iter 52999 step
2022-08-29 21:26:31,031   Num examples = 1043
2022-08-29 21:26:31,031   Batch size = 32
2022-08-29 21:26:31,032 ***** Eval results *****
2022-08-29 21:26:31,032   att_loss = 0.7441197862957549
2022-08-29 21:26:31,033   cls_loss = 0.0
2022-08-29 21:26:31,033   global_step = 52999
2022-08-29 21:26:31,033   loss = 2.212765640225904
2022-08-29 21:26:31,033   rep_loss = 1.4686458529024824
2022-08-29 21:26:31,033 ***** Save model *****
2022-08-29 21:26:41,220 ***** Running evaluation *****
2022-08-29 21:26:41,221   Epoch = 1 iter 1199 step
2022-08-29 21:26:41,221   Num examples = 1500
2022-08-29 21:26:41,221   Batch size = 32
2022-08-29 21:26:45,005 ***** Eval results *****
2022-08-29 21:26:45,005   att_loss = 0.0
2022-08-29 21:26:45,005   cls_loss = 0.1865071468287595
2022-08-29 21:26:45,006   corr = 0.8175519918838066
2022-08-29 21:26:45,006   eval_loss = 0.8728515204597027
2022-08-29 21:26:45,006   global_step = 1199
2022-08-29 21:26:45,006   loss = 0.1865071468287595
2022-08-29 21:26:45,006   pearson = 0.8161159620078537
2022-08-29 21:26:45,006   rep_loss = 0.0
2022-08-29 21:26:45,006   spearmanr = 0.8189880217597593
2022-08-29 21:27:03,068 ***** Running evaluation *****
2022-08-29 21:27:03,069   Epoch = 1 iter 1299 step
2022-08-29 21:27:03,069   Num examples = 1500
2022-08-29 21:27:03,069   Batch size = 32
2022-08-29 21:27:06,854 ***** Eval results *****
2022-08-29 21:27:06,854   att_loss = 0.0
2022-08-29 21:27:06,854   cls_loss = 0.18468487137694573
2022-08-29 21:27:06,854   corr = 0.8254278602473878
2022-08-29 21:27:06,854   eval_loss = 1.1015765058233382
2022-08-29 21:27:06,854   global_step = 1299
2022-08-29 21:27:06,854   loss = 0.18468487137694573
2022-08-29 21:27:06,854   pearson = 0.8197737277661361
2022-08-29 21:27:06,855   rep_loss = 0.0
2022-08-29 21:27:06,855   spearmanr = 0.8310819927286395
2022-08-29 21:27:09,320 ***** Running evaluation *****
2022-08-29 21:27:09,320   Epoch = 49 iter 53199 step
2022-08-29 21:27:09,320   Num examples = 1043
2022-08-29 21:27:09,320   Batch size = 32
2022-08-29 21:27:09,321 ***** Eval results *****
2022-08-29 21:27:09,321   att_loss = 0.7462274986292389
2022-08-29 21:27:09,322   cls_loss = 0.0
2022-08-29 21:27:09,322   global_step = 53199
2022-08-29 21:27:09,322   loss = 2.2143217242712794
2022-08-29 21:27:09,322   rep_loss = 1.4680942239233368
2022-08-29 21:27:09,322 ***** Save model *****
2022-08-29 21:27:10,875 ***** Running evaluation *****
2022-08-29 21:27:10,876   Epoch = 4 iter 49399 step
2022-08-29 21:27:10,876   Num examples = 9815
2022-08-29 21:27:10,876   Batch size = 32
2022-08-29 21:27:10,877 ***** Eval results *****
2022-08-29 21:27:10,877   att_loss = 3.992655878975278
2022-08-29 21:27:10,877   cls_loss = 0.0
2022-08-29 21:27:10,877   global_step = 49399
2022-08-29 21:27:10,877   loss = 5.764435589502728
2022-08-29 21:27:10,877   rep_loss = 1.7717797003095113
2022-08-29 21:27:10,877 ***** Save model *****
2022-08-29 21:27:24,947 ***** Running evaluation *****
2022-08-29 21:27:24,948   Epoch = 1 iter 1399 step
2022-08-29 21:27:24,948   Num examples = 1500
2022-08-29 21:27:24,948   Batch size = 32
2022-08-29 21:27:28,738 ***** Eval results *****
2022-08-29 21:27:28,738   att_loss = 0.0
2022-08-29 21:27:28,738   cls_loss = 0.17880140542546383
2022-08-29 21:27:28,738   corr = 0.8200680223018104
2022-08-29 21:27:28,738   eval_loss = 1.0709564660457855
2022-08-29 21:27:28,738   global_step = 1399
2022-08-29 21:27:28,738   loss = 0.17880140542546383
2022-08-29 21:27:28,738   pearson = 0.8179136932125558
2022-08-29 21:27:28,738   rep_loss = 0.0
2022-08-29 21:27:28,738   spearmanr = 0.8222223513910649
2022-08-29 21:27:46,782 ***** Running evaluation *****
2022-08-29 21:27:46,782   Epoch = 2 iter 1499 step
2022-08-29 21:27:46,782   Num examples = 1500
2022-08-29 21:27:46,782   Batch size = 32
2022-08-29 21:27:48,361 ***** Running evaluation *****
2022-08-29 21:27:48,362   Epoch = 49 iter 53399 step
2022-08-29 21:27:48,362   Num examples = 1043
2022-08-29 21:27:48,362   Batch size = 32
2022-08-29 21:27:48,363 ***** Eval results *****
2022-08-29 21:27:48,363   att_loss = 0.7436140298619937
2022-08-29 21:27:48,363   cls_loss = 0.0
2022-08-29 21:27:48,363   global_step = 53399
2022-08-29 21:27:48,363   loss = 2.211768607950255
2022-08-29 21:27:48,363   rep_loss = 1.4681545760772334
2022-08-29 21:27:48,363 ***** Save model *****
2022-08-29 21:27:50,576 ***** Eval results *****
2022-08-29 21:27:50,576   att_loss = 0.0
2022-08-29 21:27:50,576   cls_loss = 0.1296929336847767
2022-08-29 21:27:50,576   corr = 0.7954315253076493
2022-08-29 21:27:50,576   eval_loss = 1.104064940138066
2022-08-29 21:27:50,576   global_step = 1499
2022-08-29 21:27:50,576   loss = 0.1296929336847767
2022-08-29 21:27:50,576   pearson = 0.7911358001376
2022-08-29 21:27:50,576   rep_loss = 0.0
2022-08-29 21:27:50,576   spearmanr = 0.7997272504776985
2022-08-29 21:27:53,882 The args: Namespace(aug_train=False, cache_dir='', data_dir='glue_data/CoLA', data_url='', do_eval=False, do_lower_case=True, eval_batch_size=32, eval_step=100, gradient_accumulation_steps=1, hidden_act='quad', learning_rate=1e-05, log_path='tmp/distill/CoLA/quad_2quad/bert-base-uncased/log.txt', max_seq_length=128, no_cuda=False, num_train_epochs=5.0, output_dir='tmp/distill/CoLA/quad_2quad/bert-base-uncased/5e-05_1e-05_8_stage2', pred_distill=True, seed=42, softmax_act='2quad', student_model='tmp/distill/CoLA/quad_2quad/bert-base-uncased/5e-05_1e-05_8', task_name='CoLA', teacher_model='/home/ubuntu/checkpoints/exp/CoLA', temperature=1.0, train_batch_size=8, warmup_proportion=0.1, weight_decay=0.0001)
2022-08-29 21:27:53,882 device: cuda n_gpu: 1
2022-08-29 21:27:53,971 Writing example 0 of 8551
2022-08-29 21:27:53,972 *** Example ***
2022-08-29 21:27:53,972 guid: train-0
2022-08-29 21:27:53,972 tokens: [CLS] our friends won ' t buy this analysis , let alone the next one we propose . [SEP]
2022-08-29 21:27:53,972 input_ids: 101 2256 2814 2180 1005 1056 4965 2023 4106 1010 2292 2894 1996 2279 2028 2057 16599 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-08-29 21:27:53,972 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-08-29 21:27:53,972 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-08-29 21:27:53,972 label: 1
2022-08-29 21:27:53,972 label_id: 1
2022-08-29 21:27:55,435 Writing example 0 of 1043
2022-08-29 21:27:55,436 *** Example ***
2022-08-29 21:27:55,436 guid: dev-0
2022-08-29 21:27:55,436 tokens: [CLS] the sailors rode the breeze clear of the rocks . [SEP]
2022-08-29 21:27:55,436 input_ids: 101 1996 11279 8469 1996 9478 3154 1997 1996 5749 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-08-29 21:27:55,436 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-08-29 21:27:55,436 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-08-29 21:27:55,436 label: 1
2022-08-29 21:27:55,436 label_id: 1
2022-08-29 21:27:55,606 Model config {
  "_name_or_path": "bert-base-uncased",
  "architectures": [
    "BertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "crypten": false,
  "finetuning_task": "cola",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "unacceptable",
    "1": "acceptable"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "acceptable": 1,
    "unacceptable": 0
  },
  "layer_norm_eps": 1e-12,
  "log_path": "tmp/distill/CoLA/quad_2quad/bert-base-uncased/log.txt",
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "pre_trained": "",
  "problem_type": "single_label_classification",
  "softmax_act": "softmax",
  "torch_dtype": "float32",
  "training": "",
  "transformers_version": "4.20.0.dev0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

2022-08-29 21:27:58,303 Loading model /home/ubuntu/checkpoints/exp/CoLA/pytorch_model.bin
2022-08-29 21:27:58,617 loading model...
2022-08-29 21:27:58,688 done!
2022-08-29 21:27:58,688 Weights from pretrained model not used in TinyBertForSequenceClassification: ['bert.embeddings.position_ids']
2022-08-29 21:28:04,831 ***** Teacher evaluation *****
2022-08-29 21:28:04,832 {'mcc': 0.5778590180299453, 'eval_loss': 0.5587321456070199}
2022-08-29 21:28:04,832 Model config {
  "_name_or_path": "bert-base-uncased",
  "architectures": [
    "BertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "crypten": false,
  "finetuning_task": "cola",
  "gradient_checkpointing": false,
  "hidden_act": "quad",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "unacceptable",
    "1": "acceptable"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "acceptable": 1,
    "unacceptable": 0
  },
  "layer_norm_eps": 1e-12,
  "log_path": "tmp/distill/CoLA/quad_2quad/bert-base-uncased/log.txt",
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "pre_trained": "",
  "problem_type": "single_label_classification",
  "softmax_act": "2quad",
  "torch_dtype": "float32",
  "training": "",
  "transformers_version": "4.20.0.dev0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

2022-08-29 21:28:07,457 Loading model tmp/distill/CoLA/quad_2quad/bert-base-uncased/5e-05_1e-05_8/pytorch_model.bin
2022-08-29 21:28:07,739 loading model...
2022-08-29 21:28:07,793 done!
2022-08-29 21:28:07,872 ***** Running training *****
2022-08-29 21:28:07,872   Num examples = 8551
2022-08-29 21:28:07,872   Batch size = 8
2022-08-29 21:28:07,872   Num steps = 5340
2022-08-29 21:28:07,873 n: bert.embeddings.word_embeddings.weight
2022-08-29 21:28:07,873 n: bert.embeddings.position_embeddings.weight
2022-08-29 21:28:07,873 n: bert.embeddings.token_type_embeddings.weight
2022-08-29 21:28:07,873 n: bert.embeddings.LayerNorm.weight
2022-08-29 21:28:07,873 n: bert.embeddings.LayerNorm.bias
2022-08-29 21:28:07,873 n: bert.encoder.layer.0.attention.self.query.weight
2022-08-29 21:28:07,873 n: bert.encoder.layer.0.attention.self.query.bias
2022-08-29 21:28:07,873 n: bert.encoder.layer.0.attention.self.key.weight
2022-08-29 21:28:07,873 n: bert.encoder.layer.0.attention.self.key.bias
2022-08-29 21:28:07,873 n: bert.encoder.layer.0.attention.self.value.weight
2022-08-29 21:28:07,873 n: bert.encoder.layer.0.attention.self.value.bias
2022-08-29 21:28:07,873 n: bert.encoder.layer.0.attention.output.dense.weight
2022-08-29 21:28:07,874 n: bert.encoder.layer.0.attention.output.dense.bias
2022-08-29 21:28:07,874 n: bert.encoder.layer.0.attention.output.LayerNorm.weight
2022-08-29 21:28:07,874 n: bert.encoder.layer.0.attention.output.LayerNorm.bias
2022-08-29 21:28:07,874 n: bert.encoder.layer.0.intermediate.dense.weight
2022-08-29 21:28:07,874 n: bert.encoder.layer.0.intermediate.dense.bias
2022-08-29 21:28:07,874 n: bert.encoder.layer.0.output.dense.weight
2022-08-29 21:28:07,874 n: bert.encoder.layer.0.output.dense.bias
2022-08-29 21:28:07,874 n: bert.encoder.layer.0.output.LayerNorm.weight
2022-08-29 21:28:07,874 n: bert.encoder.layer.0.output.LayerNorm.bias
2022-08-29 21:28:07,874 n: bert.encoder.layer.1.attention.self.query.weight
2022-08-29 21:28:07,874 n: bert.encoder.layer.1.attention.self.query.bias
2022-08-29 21:28:07,874 n: bert.encoder.layer.1.attention.self.key.weight
2022-08-29 21:28:07,874 n: bert.encoder.layer.1.attention.self.key.bias
2022-08-29 21:28:07,874 n: bert.encoder.layer.1.attention.self.value.weight
2022-08-29 21:28:07,874 n: bert.encoder.layer.1.attention.self.value.bias
2022-08-29 21:28:07,874 n: bert.encoder.layer.1.attention.output.dense.weight
2022-08-29 21:28:07,874 n: bert.encoder.layer.1.attention.output.dense.bias
2022-08-29 21:28:07,875 n: bert.encoder.layer.1.attention.output.LayerNorm.weight
2022-08-29 21:28:07,875 n: bert.encoder.layer.1.attention.output.LayerNorm.bias
2022-08-29 21:28:07,875 n: bert.encoder.layer.1.intermediate.dense.weight
2022-08-29 21:28:07,875 n: bert.encoder.layer.1.intermediate.dense.bias
2022-08-29 21:28:07,875 n: bert.encoder.layer.1.output.dense.weight
2022-08-29 21:28:07,875 n: bert.encoder.layer.1.output.dense.bias
2022-08-29 21:28:07,875 n: bert.encoder.layer.1.output.LayerNorm.weight
2022-08-29 21:28:07,875 n: bert.encoder.layer.1.output.LayerNorm.bias
2022-08-29 21:28:07,875 n: bert.encoder.layer.2.attention.self.query.weight
2022-08-29 21:28:07,875 n: bert.encoder.layer.2.attention.self.query.bias
2022-08-29 21:28:07,875 n: bert.encoder.layer.2.attention.self.key.weight
2022-08-29 21:28:07,875 n: bert.encoder.layer.2.attention.self.key.bias
2022-08-29 21:28:07,875 n: bert.encoder.layer.2.attention.self.value.weight
2022-08-29 21:28:07,875 n: bert.encoder.layer.2.attention.self.value.bias
2022-08-29 21:28:07,875 n: bert.encoder.layer.2.attention.output.dense.weight
2022-08-29 21:28:07,875 n: bert.encoder.layer.2.attention.output.dense.bias
2022-08-29 21:28:07,875 n: bert.encoder.layer.2.attention.output.LayerNorm.weight
2022-08-29 21:28:07,875 n: bert.encoder.layer.2.attention.output.LayerNorm.bias
2022-08-29 21:28:07,875 n: bert.encoder.layer.2.intermediate.dense.weight
2022-08-29 21:28:07,876 n: bert.encoder.layer.2.intermediate.dense.bias
2022-08-29 21:28:07,876 n: bert.encoder.layer.2.output.dense.weight
2022-08-29 21:28:07,876 n: bert.encoder.layer.2.output.dense.bias
2022-08-29 21:28:07,876 n: bert.encoder.layer.2.output.LayerNorm.weight
2022-08-29 21:28:07,876 n: bert.encoder.layer.2.output.LayerNorm.bias
2022-08-29 21:28:07,876 n: bert.encoder.layer.3.attention.self.query.weight
2022-08-29 21:28:07,876 n: bert.encoder.layer.3.attention.self.query.bias
2022-08-29 21:28:07,876 n: bert.encoder.layer.3.attention.self.key.weight
2022-08-29 21:28:07,876 n: bert.encoder.layer.3.attention.self.key.bias
2022-08-29 21:28:07,876 n: bert.encoder.layer.3.attention.self.value.weight
2022-08-29 21:28:07,876 n: bert.encoder.layer.3.attention.self.value.bias
2022-08-29 21:28:07,876 n: bert.encoder.layer.3.attention.output.dense.weight
2022-08-29 21:28:07,876 n: bert.encoder.layer.3.attention.output.dense.bias
2022-08-29 21:28:07,876 n: bert.encoder.layer.3.attention.output.LayerNorm.weight
2022-08-29 21:28:07,876 n: bert.encoder.layer.3.attention.output.LayerNorm.bias
2022-08-29 21:28:07,876 n: bert.encoder.layer.3.intermediate.dense.weight
2022-08-29 21:28:07,876 n: bert.encoder.layer.3.intermediate.dense.bias
2022-08-29 21:28:07,876 n: bert.encoder.layer.3.output.dense.weight
2022-08-29 21:28:07,877 n: bert.encoder.layer.3.output.dense.bias
2022-08-29 21:28:07,877 n: bert.encoder.layer.3.output.LayerNorm.weight
2022-08-29 21:28:07,877 n: bert.encoder.layer.3.output.LayerNorm.bias
2022-08-29 21:28:07,877 n: bert.encoder.layer.4.attention.self.query.weight
2022-08-29 21:28:07,877 n: bert.encoder.layer.4.attention.self.query.bias
2022-08-29 21:28:07,877 n: bert.encoder.layer.4.attention.self.key.weight
2022-08-29 21:28:07,877 n: bert.encoder.layer.4.attention.self.key.bias
2022-08-29 21:28:07,877 n: bert.encoder.layer.4.attention.self.value.weight
2022-08-29 21:28:07,877 n: bert.encoder.layer.4.attention.self.value.bias
2022-08-29 21:28:07,877 n: bert.encoder.layer.4.attention.output.dense.weight
2022-08-29 21:28:07,877 n: bert.encoder.layer.4.attention.output.dense.bias
2022-08-29 21:28:07,877 n: bert.encoder.layer.4.attention.output.LayerNorm.weight
2022-08-29 21:28:07,877 n: bert.encoder.layer.4.attention.output.LayerNorm.bias
2022-08-29 21:28:07,877 n: bert.encoder.layer.4.intermediate.dense.weight
2022-08-29 21:28:07,877 n: bert.encoder.layer.4.intermediate.dense.bias
2022-08-29 21:28:07,877 n: bert.encoder.layer.4.output.dense.weight
2022-08-29 21:28:07,877 n: bert.encoder.layer.4.output.dense.bias
2022-08-29 21:28:07,877 n: bert.encoder.layer.4.output.LayerNorm.weight
2022-08-29 21:28:07,878 n: bert.encoder.layer.4.output.LayerNorm.bias
2022-08-29 21:28:07,878 n: bert.encoder.layer.5.attention.self.query.weight
2022-08-29 21:28:07,878 n: bert.encoder.layer.5.attention.self.query.bias
2022-08-29 21:28:07,878 n: bert.encoder.layer.5.attention.self.key.weight
2022-08-29 21:28:07,878 n: bert.encoder.layer.5.attention.self.key.bias
2022-08-29 21:28:07,878 n: bert.encoder.layer.5.attention.self.value.weight
2022-08-29 21:28:07,878 n: bert.encoder.layer.5.attention.self.value.bias
2022-08-29 21:28:07,878 n: bert.encoder.layer.5.attention.output.dense.weight
2022-08-29 21:28:07,878 n: bert.encoder.layer.5.attention.output.dense.bias
2022-08-29 21:28:07,878 n: bert.encoder.layer.5.attention.output.LayerNorm.weight
2022-08-29 21:28:07,878 n: bert.encoder.layer.5.attention.output.LayerNorm.bias
2022-08-29 21:28:07,878 n: bert.encoder.layer.5.intermediate.dense.weight
2022-08-29 21:28:07,878 n: bert.encoder.layer.5.intermediate.dense.bias
2022-08-29 21:28:07,878 n: bert.encoder.layer.5.output.dense.weight
2022-08-29 21:28:07,878 n: bert.encoder.layer.5.output.dense.bias
2022-08-29 21:28:07,878 n: bert.encoder.layer.5.output.LayerNorm.weight
2022-08-29 21:28:07,878 n: bert.encoder.layer.5.output.LayerNorm.bias
2022-08-29 21:28:07,878 n: bert.encoder.layer.6.attention.self.query.weight
2022-08-29 21:28:07,879 n: bert.encoder.layer.6.attention.self.query.bias
2022-08-29 21:28:07,879 n: bert.encoder.layer.6.attention.self.key.weight
2022-08-29 21:28:07,879 n: bert.encoder.layer.6.attention.self.key.bias
2022-08-29 21:28:07,879 n: bert.encoder.layer.6.attention.self.value.weight
2022-08-29 21:28:07,879 n: bert.encoder.layer.6.attention.self.value.bias
2022-08-29 21:28:07,879 n: bert.encoder.layer.6.attention.output.dense.weight
2022-08-29 21:28:07,879 n: bert.encoder.layer.6.attention.output.dense.bias
2022-08-29 21:28:07,879 n: bert.encoder.layer.6.attention.output.LayerNorm.weight
2022-08-29 21:28:07,879 n: bert.encoder.layer.6.attention.output.LayerNorm.bias
2022-08-29 21:28:07,879 n: bert.encoder.layer.6.intermediate.dense.weight
2022-08-29 21:28:07,879 n: bert.encoder.layer.6.intermediate.dense.bias
2022-08-29 21:28:07,879 n: bert.encoder.layer.6.output.dense.weight
2022-08-29 21:28:07,879 n: bert.encoder.layer.6.output.dense.bias
2022-08-29 21:28:07,879 n: bert.encoder.layer.6.output.LayerNorm.weight
2022-08-29 21:28:07,879 n: bert.encoder.layer.6.output.LayerNorm.bias
2022-08-29 21:28:07,879 n: bert.encoder.layer.7.attention.self.query.weight
2022-08-29 21:28:07,879 n: bert.encoder.layer.7.attention.self.query.bias
2022-08-29 21:28:07,879 n: bert.encoder.layer.7.attention.self.key.weight
2022-08-29 21:28:07,880 n: bert.encoder.layer.7.attention.self.key.bias
2022-08-29 21:28:07,880 n: bert.encoder.layer.7.attention.self.value.weight
2022-08-29 21:28:07,880 n: bert.encoder.layer.7.attention.self.value.bias
2022-08-29 21:28:07,880 n: bert.encoder.layer.7.attention.output.dense.weight
2022-08-29 21:28:07,880 n: bert.encoder.layer.7.attention.output.dense.bias
2022-08-29 21:28:07,880 n: bert.encoder.layer.7.attention.output.LayerNorm.weight
2022-08-29 21:28:07,880 n: bert.encoder.layer.7.attention.output.LayerNorm.bias
2022-08-29 21:28:07,880 n: bert.encoder.layer.7.intermediate.dense.weight
2022-08-29 21:28:07,880 n: bert.encoder.layer.7.intermediate.dense.bias
2022-08-29 21:28:07,880 n: bert.encoder.layer.7.output.dense.weight
2022-08-29 21:28:07,880 n: bert.encoder.layer.7.output.dense.bias
2022-08-29 21:28:07,880 n: bert.encoder.layer.7.output.LayerNorm.weight
2022-08-29 21:28:07,880 n: bert.encoder.layer.7.output.LayerNorm.bias
2022-08-29 21:28:07,880 n: bert.encoder.layer.8.attention.self.query.weight
2022-08-29 21:28:07,880 n: bert.encoder.layer.8.attention.self.query.bias
2022-08-29 21:28:07,880 n: bert.encoder.layer.8.attention.self.key.weight
2022-08-29 21:28:07,880 n: bert.encoder.layer.8.attention.self.key.bias
2022-08-29 21:28:07,880 n: bert.encoder.layer.8.attention.self.value.weight
2022-08-29 21:28:07,881 n: bert.encoder.layer.8.attention.self.value.bias
2022-08-29 21:28:07,881 n: bert.encoder.layer.8.attention.output.dense.weight
2022-08-29 21:28:07,881 n: bert.encoder.layer.8.attention.output.dense.bias
2022-08-29 21:28:07,881 n: bert.encoder.layer.8.attention.output.LayerNorm.weight
2022-08-29 21:28:07,881 n: bert.encoder.layer.8.attention.output.LayerNorm.bias
2022-08-29 21:28:07,881 n: bert.encoder.layer.8.intermediate.dense.weight
2022-08-29 21:28:07,881 n: bert.encoder.layer.8.intermediate.dense.bias
2022-08-29 21:28:07,881 n: bert.encoder.layer.8.output.dense.weight
2022-08-29 21:28:07,881 n: bert.encoder.layer.8.output.dense.bias
2022-08-29 21:28:07,881 n: bert.encoder.layer.8.output.LayerNorm.weight
2022-08-29 21:28:07,881 n: bert.encoder.layer.8.output.LayerNorm.bias
2022-08-29 21:28:07,881 n: bert.encoder.layer.9.attention.self.query.weight
2022-08-29 21:28:07,881 n: bert.encoder.layer.9.attention.self.query.bias
2022-08-29 21:28:07,881 n: bert.encoder.layer.9.attention.self.key.weight
2022-08-29 21:28:07,881 n: bert.encoder.layer.9.attention.self.key.bias
2022-08-29 21:28:07,881 n: bert.encoder.layer.9.attention.self.value.weight
2022-08-29 21:28:07,881 n: bert.encoder.layer.9.attention.self.value.bias
2022-08-29 21:28:07,881 n: bert.encoder.layer.9.attention.output.dense.weight
2022-08-29 21:28:07,882 n: bert.encoder.layer.9.attention.output.dense.bias
2022-08-29 21:28:07,882 n: bert.encoder.layer.9.attention.output.LayerNorm.weight
2022-08-29 21:28:07,882 n: bert.encoder.layer.9.attention.output.LayerNorm.bias
2022-08-29 21:28:07,882 n: bert.encoder.layer.9.intermediate.dense.weight
2022-08-29 21:28:07,882 n: bert.encoder.layer.9.intermediate.dense.bias
2022-08-29 21:28:07,882 n: bert.encoder.layer.9.output.dense.weight
2022-08-29 21:28:07,882 n: bert.encoder.layer.9.output.dense.bias
2022-08-29 21:28:07,882 n: bert.encoder.layer.9.output.LayerNorm.weight
2022-08-29 21:28:07,882 n: bert.encoder.layer.9.output.LayerNorm.bias
2022-08-29 21:28:07,882 n: bert.encoder.layer.10.attention.self.query.weight
2022-08-29 21:28:07,882 n: bert.encoder.layer.10.attention.self.query.bias
2022-08-29 21:28:07,882 n: bert.encoder.layer.10.attention.self.key.weight
2022-08-29 21:28:07,882 n: bert.encoder.layer.10.attention.self.key.bias
2022-08-29 21:28:07,882 n: bert.encoder.layer.10.attention.self.value.weight
2022-08-29 21:28:07,882 n: bert.encoder.layer.10.attention.self.value.bias
2022-08-29 21:28:07,882 n: bert.encoder.layer.10.attention.output.dense.weight
2022-08-29 21:28:07,882 n: bert.encoder.layer.10.attention.output.dense.bias
2022-08-29 21:28:07,882 n: bert.encoder.layer.10.attention.output.LayerNorm.weight
2022-08-29 21:28:07,883 n: bert.encoder.layer.10.attention.output.LayerNorm.bias
2022-08-29 21:28:07,883 n: bert.encoder.layer.10.intermediate.dense.weight
2022-08-29 21:28:07,883 n: bert.encoder.layer.10.intermediate.dense.bias
2022-08-29 21:28:07,883 n: bert.encoder.layer.10.output.dense.weight
2022-08-29 21:28:07,883 n: bert.encoder.layer.10.output.dense.bias
2022-08-29 21:28:07,883 n: bert.encoder.layer.10.output.LayerNorm.weight
2022-08-29 21:28:07,883 n: bert.encoder.layer.10.output.LayerNorm.bias
2022-08-29 21:28:07,883 n: bert.encoder.layer.11.attention.self.query.weight
2022-08-29 21:28:07,883 n: bert.encoder.layer.11.attention.self.query.bias
2022-08-29 21:28:07,883 n: bert.encoder.layer.11.attention.self.key.weight
2022-08-29 21:28:07,883 n: bert.encoder.layer.11.attention.self.key.bias
2022-08-29 21:28:07,883 n: bert.encoder.layer.11.attention.self.value.weight
2022-08-29 21:28:07,883 n: bert.encoder.layer.11.attention.self.value.bias
2022-08-29 21:28:07,883 n: bert.encoder.layer.11.attention.output.dense.weight
2022-08-29 21:28:07,883 n: bert.encoder.layer.11.attention.output.dense.bias
2022-08-29 21:28:07,883 n: bert.encoder.layer.11.attention.output.LayerNorm.weight
2022-08-29 21:28:07,883 n: bert.encoder.layer.11.attention.output.LayerNorm.bias
2022-08-29 21:28:07,883 n: bert.encoder.layer.11.intermediate.dense.weight
2022-08-29 21:28:07,884 n: bert.encoder.layer.11.intermediate.dense.bias
2022-08-29 21:28:07,884 n: bert.encoder.layer.11.output.dense.weight
2022-08-29 21:28:07,884 n: bert.encoder.layer.11.output.dense.bias
2022-08-29 21:28:07,884 n: bert.encoder.layer.11.output.LayerNorm.weight
2022-08-29 21:28:07,884 n: bert.encoder.layer.11.output.LayerNorm.bias
2022-08-29 21:28:07,884 n: bert.pooler.dense.weight
2022-08-29 21:28:07,884 n: bert.pooler.dense.bias
2022-08-29 21:28:07,884 n: classifier.weight
2022-08-29 21:28:07,884 n: classifier.bias
2022-08-29 21:28:07,884 Total parameters: 109483778
2022-08-29 21:28:09,142 ***** Running evaluation *****
2022-08-29 21:28:09,143   Epoch = 2 iter 1599 step
2022-08-29 21:28:09,143   Num examples = 1500
2022-08-29 21:28:09,143   Batch size = 32
2022-08-29 21:28:12,931 ***** Eval results *****
2022-08-29 21:28:12,931   att_loss = 0.0
2022-08-29 21:28:12,931   cls_loss = 0.12838666540987653
2022-08-29 21:28:12,931   corr = 0.7989137128117366
2022-08-29 21:28:12,931   eval_loss = 1.2060611301914175
2022-08-29 21:28:12,931   global_step = 1599
2022-08-29 21:28:12,931   loss = 0.12838666540987653
2022-08-29 21:28:12,931   pearson = 0.7972835696007458
2022-08-29 21:28:12,932   rep_loss = 0.0
2022-08-29 21:28:12,932   spearmanr = 0.8005438560227275
2022-08-29 21:28:22,271 ***** Running evaluation *****
2022-08-29 21:28:22,272   Epoch = 4 iter 49599 step
2022-08-29 21:28:22,272   Num examples = 9815
2022-08-29 21:28:22,272   Batch size = 32
2022-08-29 21:28:22,273 ***** Eval results *****
2022-08-29 21:28:22,273   att_loss = 3.9986012051406417
2022-08-29 21:28:22,273   cls_loss = 0.0
2022-08-29 21:28:22,273   global_step = 49599
2022-08-29 21:28:22,273   loss = 5.770827967449299
2022-08-29 21:28:22,273   rep_loss = 1.772226760456863
2022-08-29 21:28:22,273 ***** Save model *****
2022-08-29 21:28:25,337 ***** Running evaluation *****
2022-08-29 21:28:25,337   Epoch = 0 iter 99 step
2022-08-29 21:28:25,337   Num examples = 1043
2022-08-29 21:28:25,337   Batch size = 32
2022-08-29 21:28:27,941 ***** Eval results *****
2022-08-29 21:28:27,941   att_loss = 0.0
2022-08-29 21:28:27,941   cls_loss = 0.09141080405073937
2022-08-29 21:28:27,941   eval_loss = 0.5911110957915132
2022-08-29 21:28:27,942   global_step = 99
2022-08-29 21:28:27,942   loss = 0.09141080405073937
2022-08-29 21:28:27,942   mcc = 0.5237621121274929
2022-08-29 21:28:27,942   rep_loss = 0.0
2022-08-29 21:28:27,942 ***** Save model *****
2022-08-29 21:28:30,978 ***** Running evaluation *****
2022-08-29 21:28:30,978   Epoch = 2 iter 1699 step
2022-08-29 21:28:30,979   Num examples = 1500
2022-08-29 21:28:30,979   Batch size = 32
2022-08-29 21:28:34,763 ***** Eval results *****
2022-08-29 21:28:34,763   att_loss = 0.0
2022-08-29 21:28:34,763   cls_loss = 0.1343046895267279
2022-08-29 21:28:34,763   corr = 0.804315843206193
2022-08-29 21:28:34,763   eval_loss = 1.0244262846226388
2022-08-29 21:28:34,763   global_step = 1699
2022-08-29 21:28:34,763   loss = 0.1343046895267279
2022-08-29 21:28:34,763   pearson = 0.8038480730191269
2022-08-29 21:28:34,763   rep_loss = 0.0
2022-08-29 21:28:34,763   spearmanr = 0.8047836133932591
2022-08-29 21:28:46,359 ***** Running evaluation *****
2022-08-29 21:28:46,359   Epoch = 0 iter 199 step
2022-08-29 21:28:46,360   Num examples = 1043
2022-08-29 21:28:46,360   Batch size = 32
2022-08-29 21:28:48,967 ***** Eval results *****
2022-08-29 21:28:48,967   att_loss = 0.0
2022-08-29 21:28:48,967   cls_loss = 0.0931732220453533
2022-08-29 21:28:48,967   eval_loss = 0.6061675847705567
2022-08-29 21:28:48,967   global_step = 199
2022-08-29 21:28:48,967   loss = 0.0931732220453533
2022-08-29 21:28:48,967   mcc = 0.4660859395741973
2022-08-29 21:28:48,967   rep_loss = 0.0
2022-08-29 21:28:52,799 ***** Running evaluation *****
2022-08-29 21:28:52,800   Epoch = 2 iter 1799 step
2022-08-29 21:28:52,800   Num examples = 1500
2022-08-29 21:28:52,800   Batch size = 32
2022-08-29 21:28:56,590 ***** Eval results *****
2022-08-29 21:28:56,590   att_loss = 0.0
2022-08-29 21:28:56,590   cls_loss = 0.13436773035049274
2022-08-29 21:28:56,590   corr = 0.8005107692235591
2022-08-29 21:28:56,590   eval_loss = 1.1645271508617605
2022-08-29 21:28:56,590   global_step = 1799
2022-08-29 21:28:56,590   loss = 0.13436773035049274
2022-08-29 21:28:56,590   pearson = 0.7959257077568219
2022-08-29 21:28:56,590   rep_loss = 0.0
2022-08-29 21:28:56,590   spearmanr = 0.8050958306902964
2022-08-29 21:29:06,556 ***** Running evaluation *****
2022-08-29 21:29:06,557   Epoch = 0 iter 299 step
2022-08-29 21:29:06,557   Num examples = 1043
2022-08-29 21:29:06,557   Batch size = 32
2022-08-29 21:29:09,163 ***** Eval results *****
2022-08-29 21:29:09,163   att_loss = 0.0
2022-08-29 21:29:09,163   cls_loss = 0.0934680263608395
2022-08-29 21:29:09,164   eval_loss = 0.5488073439760641
2022-08-29 21:29:09,164   global_step = 299
2022-08-29 21:29:09,164   loss = 0.0934680263608395
2022-08-29 21:29:09,164   mcc = 0.4451327853705901
2022-08-29 21:29:09,164   rep_loss = 0.0
2022-08-29 21:29:14,638 ***** Running evaluation *****
2022-08-29 21:29:14,639   Epoch = 2 iter 1899 step
2022-08-29 21:29:14,639   Num examples = 1500
2022-08-29 21:29:14,639   Batch size = 32
2022-08-29 21:29:18,424 ***** Eval results *****
2022-08-29 21:29:18,424   att_loss = 0.0
2022-08-29 21:29:18,424   cls_loss = 0.13408835743177785
2022-08-29 21:29:18,424   corr = 0.818589107821902
2022-08-29 21:29:18,425   eval_loss = 0.9860667031496129
2022-08-29 21:29:18,425   global_step = 1899
2022-08-29 21:29:18,425   loss = 0.13408835743177785
2022-08-29 21:29:18,425   pearson = 0.8182124892730767
2022-08-29 21:29:18,425   rep_loss = 0.0
2022-08-29 21:29:18,425   spearmanr = 0.8189657263707273
2022-08-29 21:29:26,753 ***** Running evaluation *****
2022-08-29 21:29:26,754   Epoch = 0 iter 399 step
2022-08-29 21:29:26,754   Num examples = 1043
2022-08-29 21:29:26,754   Batch size = 32
2022-08-29 21:29:29,366 ***** Eval results *****
2022-08-29 21:29:29,366   att_loss = 0.0
2022-08-29 21:29:29,366   cls_loss = 0.09558749990654469
2022-08-29 21:29:29,366   eval_loss = 0.553129490126263
2022-08-29 21:29:29,366   global_step = 399
2022-08-29 21:29:29,366   loss = 0.09558749990654469
2022-08-29 21:29:29,366   mcc = 0.5259770638545269
2022-08-29 21:29:29,366   rep_loss = 0.0
2022-08-29 21:29:29,366 ***** Save model *****
2022-08-29 21:29:35,598 ***** Running evaluation *****
2022-08-29 21:29:35,599   Epoch = 4 iter 49799 step
2022-08-29 21:29:35,599   Num examples = 9815
2022-08-29 21:29:35,599   Batch size = 32
2022-08-29 21:29:35,600 ***** Eval results *****
2022-08-29 21:29:35,600   att_loss = 4.01170907820855
2022-08-29 21:29:35,600   cls_loss = 0.0
2022-08-29 21:29:35,600   global_step = 49799
2022-08-29 21:29:35,600   loss = 5.785204627297142
2022-08-29 21:29:35,600   rep_loss = 1.773495551256033
2022-08-29 21:29:35,600 ***** Save model *****
2022-08-29 21:29:36,493 ***** Running evaluation *****
2022-08-29 21:29:36,494   Epoch = 2 iter 1999 step
2022-08-29 21:29:36,494   Num examples = 1500
2022-08-29 21:29:36,494   Batch size = 32
2022-08-29 21:29:40,280 ***** Eval results *****
2022-08-29 21:29:40,281   att_loss = 0.0
2022-08-29 21:29:40,281   cls_loss = 0.13086596109819984
2022-08-29 21:29:40,281   corr = 0.8224647991070195
2022-08-29 21:29:40,281   eval_loss = 0.9542611263533856
2022-08-29 21:29:40,281   global_step = 1999
2022-08-29 21:29:40,281   loss = 0.13086596109819984
2022-08-29 21:29:40,281   pearson = 0.8189076641330448
2022-08-29 21:29:40,281   rep_loss = 0.0
2022-08-29 21:29:40,281   spearmanr = 0.8260219340809943
2022-08-29 21:29:50,437 ***** Running evaluation *****
2022-08-29 21:29:50,438   Epoch = 0 iter 499 step
2022-08-29 21:29:50,438   Num examples = 1043
2022-08-29 21:29:50,438   Batch size = 32
2022-08-29 21:29:53,048 ***** Eval results *****
2022-08-29 21:29:53,048   att_loss = 0.0
2022-08-29 21:29:53,048   cls_loss = 0.09448061466874005
2022-08-29 21:29:53,048   eval_loss = 0.7886181031212662
2022-08-29 21:29:53,048   global_step = 499
2022-08-29 21:29:53,048   loss = 0.09448061466874005
2022-08-29 21:29:53,048   mcc = 0.3985483130094994
2022-08-29 21:29:53,048   rep_loss = 0.0
2022-08-29 21:29:58,344 ***** Running evaluation *****
2022-08-29 21:29:58,345   Epoch = 2 iter 2099 step
2022-08-29 21:29:58,345   Num examples = 1500
2022-08-29 21:29:58,345   Batch size = 32
2022-08-29 21:30:02,132 ***** Eval results *****
2022-08-29 21:30:02,132   att_loss = 0.0
2022-08-29 21:30:02,132   cls_loss = 0.12780034116456682
2022-08-29 21:30:02,132   corr = 0.788572829849295
2022-08-29 21:30:02,132   eval_loss = 1.2632308919379052
2022-08-29 21:30:02,132   global_step = 2099
2022-08-29 21:30:02,132   loss = 0.12780034116456682
2022-08-29 21:30:02,132   pearson = 0.7861548120393635
2022-08-29 21:30:02,132   rep_loss = 0.0
2022-08-29 21:30:02,132   spearmanr = 0.7909908476592264
2022-08-29 21:30:11,315 ***** Running evaluation *****
2022-08-29 21:30:11,315   Epoch = 0 iter 599 step
2022-08-29 21:30:11,316   Num examples = 1043
2022-08-29 21:30:11,316   Batch size = 32
2022-08-29 21:30:13,928 ***** Eval results *****
2022-08-29 21:30:13,928   att_loss = 0.0
2022-08-29 21:30:13,928   cls_loss = 0.0951663519384865
2022-08-29 21:30:13,928   eval_loss = 0.5922391762336096
2022-08-29 21:30:13,928   global_step = 599
2022-08-29 21:30:13,928   loss = 0.0951663519384865
2022-08-29 21:30:13,928   mcc = 0.521189146186809
2022-08-29 21:30:13,929   rep_loss = 0.0
2022-08-29 21:30:20,185 ***** Running evaluation *****
2022-08-29 21:30:20,186   Epoch = 3 iter 2199 step
2022-08-29 21:30:20,186   Num examples = 1500
2022-08-29 21:30:20,186   Batch size = 32
2022-08-29 21:30:23,974 ***** Eval results *****
2022-08-29 21:30:23,974   att_loss = 0.0
2022-08-29 21:30:23,974   cls_loss = 0.10563228722247812
2022-08-29 21:30:23,974   corr = 0.7900830352842915
2022-08-29 21:30:23,974   eval_loss = 1.1607406833704481
2022-08-29 21:30:23,974   global_step = 2199
2022-08-29 21:30:23,974   loss = 0.10563228722247812
2022-08-29 21:30:23,975   pearson = 0.7902357538894078
2022-08-29 21:30:23,975   rep_loss = 0.0
2022-08-29 21:30:23,975   spearmanr = 0.7899303166791751
2022-08-29 21:30:32,241 ***** Running evaluation *****
2022-08-29 21:30:32,241   Epoch = 0 iter 699 step
2022-08-29 21:30:32,241   Num examples = 1043
2022-08-29 21:30:32,241   Batch size = 32
2022-08-29 21:30:34,852 ***** Eval results *****
2022-08-29 21:30:34,852   att_loss = 0.0
2022-08-29 21:30:34,852   cls_loss = 0.0938640266859395
2022-08-29 21:30:34,852   eval_loss = 0.7123753455552188
2022-08-29 21:30:34,852   global_step = 699
2022-08-29 21:30:34,852   loss = 0.0938640266859395
2022-08-29 21:30:34,852   mcc = 0.4529794977975582
2022-08-29 21:30:34,852   rep_loss = 0.0
2022-08-29 21:30:42,018 ***** Running evaluation *****
2022-08-29 21:30:42,019   Epoch = 3 iter 2299 step
2022-08-29 21:30:42,019   Num examples = 1500
2022-08-29 21:30:42,019   Batch size = 32
2022-08-29 21:30:45,807 ***** Eval results *****
2022-08-29 21:30:45,807   att_loss = 0.0
2022-08-29 21:30:45,807   cls_loss = 0.08883874032261042
2022-08-29 21:30:45,807   corr = 0.7839031558698448
2022-08-29 21:30:45,807   eval_loss = 1.4197925075571587
2022-08-29 21:30:45,807   global_step = 2299
2022-08-29 21:30:45,808   loss = 0.08883874032261042
2022-08-29 21:30:45,808   pearson = 0.7801610411194039
2022-08-29 21:30:45,808   rep_loss = 0.0
2022-08-29 21:30:45,808   spearmanr = 0.7876452706202858
2022-08-29 21:30:50,003 ***** Running evaluation *****
2022-08-29 21:30:50,004   Epoch = 4 iter 49999 step
2022-08-29 21:30:50,004   Num examples = 9815
2022-08-29 21:30:50,004   Batch size = 32
2022-08-29 21:30:50,005 ***** Eval results *****
2022-08-29 21:30:50,006   att_loss = 4.007917293433935
2022-08-29 21:30:50,006   cls_loss = 0.0
2022-08-29 21:30:50,006   global_step = 49999
2022-08-29 21:30:50,006   loss = 5.781538704314518
2022-08-29 21:30:50,006   rep_loss = 1.7736214134862514
2022-08-29 21:30:50,006 ***** Save model *****
2022-08-29 21:30:53,070 ***** Running evaluation *****
2022-08-29 21:30:53,071   Epoch = 0 iter 799 step
2022-08-29 21:30:53,071   Num examples = 1043
2022-08-29 21:30:53,071   Batch size = 32
2022-08-29 21:30:55,681 ***** Eval results *****
2022-08-29 21:30:55,681   att_loss = 0.0
2022-08-29 21:30:55,681   cls_loss = 0.0943275313167011
2022-08-29 21:30:55,681   eval_loss = 0.5926852826819275
2022-08-29 21:30:55,681   global_step = 799
2022-08-29 21:30:55,681   loss = 0.0943275313167011
2022-08-29 21:30:55,682   mcc = 0.47143760362685355
2022-08-29 21:30:55,682   rep_loss = 0.0
2022-08-29 21:31:03,702 ***** Running evaluation *****
2022-08-29 21:31:03,703   Epoch = 3 iter 2399 step
2022-08-29 21:31:03,703   Num examples = 1500
2022-08-29 21:31:03,703   Batch size = 32
2022-08-29 21:31:07,486 ***** Eval results *****
2022-08-29 21:31:07,487   att_loss = 0.0
2022-08-29 21:31:07,487   cls_loss = 0.09297053111451013
2022-08-29 21:31:07,487   corr = 0.7914605447685995
2022-08-29 21:31:07,487   eval_loss = 1.2654842198529141
2022-08-29 21:31:07,487   global_step = 2399
2022-08-29 21:31:07,487   loss = 0.09297053111451013
2022-08-29 21:31:07,487   pearson = 0.7888100386598448
2022-08-29 21:31:07,487   rep_loss = 0.0
2022-08-29 21:31:07,487   spearmanr = 0.7941110508773542
2022-08-29 21:31:13,267 ***** Running evaluation *****
2022-08-29 21:31:13,267   Epoch = 0 iter 899 step
2022-08-29 21:31:13,267   Num examples = 1043
2022-08-29 21:31:13,267   Batch size = 32
2022-08-29 21:31:15,877 ***** Eval results *****
2022-08-29 21:31:15,877   att_loss = 0.0
2022-08-29 21:31:15,877   cls_loss = 0.09533367260826443
2022-08-29 21:31:15,877   eval_loss = 0.6228934833497712
2022-08-29 21:31:15,877   global_step = 899
2022-08-29 21:31:15,877   loss = 0.09533367260826443
2022-08-29 21:31:15,878   mcc = 0.48649404197979973
2022-08-29 21:31:15,878   rep_loss = 0.0
2022-08-29 21:31:24,834 ***** Running evaluation *****
2022-08-29 21:31:24,835   Epoch = 3 iter 2499 step
2022-08-29 21:31:24,835   Num examples = 1500
2022-08-29 21:31:24,835   Batch size = 32
2022-08-29 21:31:28,628 ***** Eval results *****
2022-08-29 21:31:28,629   att_loss = 0.0
2022-08-29 21:31:28,629   cls_loss = 0.09527827105012493
2022-08-29 21:31:28,629   corr = 0.8048230105191019
2022-08-29 21:31:28,629   eval_loss = 1.0788331447129553
2022-08-29 21:31:28,629   global_step = 2499
2022-08-29 21:31:28,629   loss = 0.09527827105012493
2022-08-29 21:31:28,629   pearson = 0.803770609739429
2022-08-29 21:31:28,629   rep_loss = 0.0
2022-08-29 21:31:28,629   spearmanr = 0.8058754112987748
2022-08-29 21:31:33,455 ***** Running evaluation *****
2022-08-29 21:31:33,455   Epoch = 0 iter 999 step
2022-08-29 21:31:33,455   Num examples = 1043
2022-08-29 21:31:33,455   Batch size = 32
2022-08-29 21:31:36,064 ***** Eval results *****
2022-08-29 21:31:36,065   att_loss = 0.0
2022-08-29 21:31:36,065   cls_loss = 0.09538600957384696
2022-08-29 21:31:36,065   eval_loss = 0.5138749103202964
2022-08-29 21:31:36,065   global_step = 999
2022-08-29 21:31:36,065   loss = 0.09538600957384696
2022-08-29 21:31:36,065   mcc = 0.4859602944352706
2022-08-29 21:31:36,065   rep_loss = 0.0
2022-08-29 21:31:45,976 ***** Running evaluation *****
2022-08-29 21:31:45,977   Epoch = 3 iter 2599 step
2022-08-29 21:31:45,977   Num examples = 1500
2022-08-29 21:31:45,977   Batch size = 32
2022-08-29 21:31:49,767 ***** Eval results *****
2022-08-29 21:31:49,767   att_loss = 0.0
2022-08-29 21:31:49,767   cls_loss = 0.09637723293448432
2022-08-29 21:31:49,767   corr = 0.782164962777814
2022-08-29 21:31:49,767   eval_loss = 1.193115382118428
2022-08-29 21:31:49,767   global_step = 2599
2022-08-29 21:31:49,767   loss = 0.09637723293448432
2022-08-29 21:31:49,767   pearson = 0.7786928840774646
2022-08-29 21:31:49,767   rep_loss = 0.0
2022-08-29 21:31:49,767   spearmanr = 0.7856370414781635
2022-08-29 21:31:53,644 ***** Running evaluation *****
2022-08-29 21:31:53,644   Epoch = 1 iter 1099 step
2022-08-29 21:31:53,644   Num examples = 1043
2022-08-29 21:31:53,644   Batch size = 32
2022-08-29 21:31:56,257 ***** Eval results *****
2022-08-29 21:31:56,257   att_loss = 0.0
2022-08-29 21:31:56,257   cls_loss = 0.0663400721405783
2022-08-29 21:31:56,257   eval_loss = 0.5648219759265581
2022-08-29 21:31:56,257   global_step = 1099
2022-08-29 21:31:56,258   loss = 0.0663400721405783
2022-08-29 21:31:56,258   mcc = 0.5158102563819117
2022-08-29 21:31:56,258   rep_loss = 0.0
2022-08-29 21:32:00,097 ***** Running evaluation *****
2022-08-29 21:32:00,097   Epoch = 4 iter 50199 step
2022-08-29 21:32:00,097   Num examples = 9815
2022-08-29 21:32:00,097   Batch size = 32
2022-08-29 21:32:00,098 ***** Eval results *****
2022-08-29 21:32:00,099   att_loss = 4.003028355799448
2022-08-29 21:32:00,099   cls_loss = 0.0
2022-08-29 21:32:00,099   global_step = 50199
2022-08-29 21:32:00,099   loss = 5.77591438806645
2022-08-29 21:32:00,099   rep_loss = 1.7728860333361434
2022-08-29 21:32:00,099 ***** Save model *****
2022-08-29 21:32:07,122 ***** Running evaluation *****
2022-08-29 21:32:07,123   Epoch = 3 iter 2699 step
2022-08-29 21:32:07,123   Num examples = 1500
2022-08-29 21:32:07,123   Batch size = 32
2022-08-29 21:32:10,911 ***** Eval results *****
2022-08-29 21:32:10,911   att_loss = 0.0
2022-08-29 21:32:10,911   cls_loss = 0.09660545113846797
2022-08-29 21:32:10,911   corr = 0.8171584968023631
2022-08-29 21:32:10,911   eval_loss = 1.028234636529963
2022-08-29 21:32:10,912   global_step = 2699
2022-08-29 21:32:10,912   loss = 0.09660545113846797
2022-08-29 21:32:10,912   pearson = 0.8151117119087496
2022-08-29 21:32:10,912   rep_loss = 0.0
2022-08-29 21:32:10,912   spearmanr = 0.8192052816959764
2022-08-29 21:32:13,839 ***** Running evaluation *****
2022-08-29 21:32:13,840   Epoch = 1 iter 1199 step
2022-08-29 21:32:13,840   Num examples = 1043
2022-08-29 21:32:13,840   Batch size = 32
2022-08-29 21:32:16,456 ***** Eval results *****
2022-08-29 21:32:16,456   att_loss = 0.0
2022-08-29 21:32:16,456   cls_loss = 0.07684953152450896
2022-08-29 21:32:16,456   eval_loss = 0.6006345306382035
2022-08-29 21:32:16,456   global_step = 1199
2022-08-29 21:32:16,456   loss = 0.07684953152450896
2022-08-29 21:32:16,456   mcc = 0.4952437657124396
2022-08-29 21:32:16,456   rep_loss = 0.0
2022-08-29 21:32:28,286 ***** Running evaluation *****
2022-08-29 21:32:28,287   Epoch = 3 iter 2799 step
2022-08-29 21:32:28,287   Num examples = 1500
2022-08-29 21:32:28,287   Batch size = 32
2022-08-29 21:32:32,081 ***** Eval results *****
2022-08-29 21:32:32,081   att_loss = 0.0
2022-08-29 21:32:32,081   cls_loss = 0.09784534385269002
2022-08-29 21:32:32,081   corr = 0.7855971863643283
2022-08-29 21:32:32,081   eval_loss = 1.213403582255891
2022-08-29 21:32:32,081   global_step = 2799
2022-08-29 21:32:32,081   loss = 0.09784534385269002
2022-08-29 21:32:32,081   pearson = 0.7820978413216533
2022-08-29 21:32:32,081   rep_loss = 0.0
2022-08-29 21:32:32,082   spearmanr = 0.7890965314070033
2022-08-29 21:32:34,045 ***** Running evaluation *****
2022-08-29 21:32:34,045   Epoch = 1 iter 1299 step
2022-08-29 21:32:34,045   Num examples = 1043
2022-08-29 21:32:34,045   Batch size = 32
2022-08-29 21:32:36,655 ***** Eval results *****
2022-08-29 21:32:36,655   att_loss = 0.0
2022-08-29 21:32:36,655   cls_loss = 0.08383352494923584
2022-08-29 21:32:36,655   eval_loss = 0.5541660232525883
2022-08-29 21:32:36,655   global_step = 1299
2022-08-29 21:32:36,655   loss = 0.08383352494923584
2022-08-29 21:32:36,655   mcc = 0.4886634303966225
2022-08-29 21:32:36,655   rep_loss = 0.0
2022-08-29 21:32:49,443 ***** Running evaluation *****
2022-08-29 21:32:49,443   Epoch = 4 iter 2899 step
2022-08-29 21:32:49,443   Num examples = 1500
2022-08-29 21:32:49,443   Batch size = 32
2022-08-29 21:32:53,229 ***** Eval results *****
2022-08-29 21:32:53,230   att_loss = 0.0
2022-08-29 21:32:53,230   cls_loss = 0.09078543329680408
2022-08-29 21:32:53,230   corr = 0.8111890311521237
2022-08-29 21:32:53,230   eval_loss = 1.1190103815591081
2022-08-29 21:32:53,230   global_step = 2899
2022-08-29 21:32:53,230   loss = 0.09078543329680408
2022-08-29 21:32:53,230   pearson = 0.8079431006220583
2022-08-29 21:32:53,230   rep_loss = 0.0
2022-08-29 21:32:53,230   spearmanr = 0.8144349616821892
2022-08-29 21:32:54,228 ***** Running evaluation *****
2022-08-29 21:32:54,229   Epoch = 1 iter 1399 step
2022-08-29 21:32:54,229   Num examples = 1043
2022-08-29 21:32:54,229   Batch size = 32
2022-08-29 21:32:56,840 ***** Eval results *****
2022-08-29 21:32:56,840   att_loss = 0.0
2022-08-29 21:32:56,840   cls_loss = 0.08406578205818496
2022-08-29 21:32:56,841   eval_loss = 0.6157675448692206
2022-08-29 21:32:56,841   global_step = 1399
2022-08-29 21:32:56,841   loss = 0.08406578205818496
2022-08-29 21:32:56,841   mcc = 0.49920513654867127
2022-08-29 21:32:56,841   rep_loss = 0.0
2022-08-29 21:33:10,586 ***** Running evaluation *****
2022-08-29 21:33:10,587   Epoch = 4 iter 2999 step
2022-08-29 21:33:10,587   Num examples = 1500
2022-08-29 21:33:10,587   Batch size = 32
2022-08-29 21:33:12,254 ***** Running evaluation *****
2022-08-29 21:33:12,254   Epoch = 4 iter 50399 step
2022-08-29 21:33:12,254   Num examples = 9815
2022-08-29 21:33:12,254   Batch size = 32
2022-08-29 21:33:12,255 ***** Eval results *****
2022-08-29 21:33:12,255   att_loss = 4.001380870550758
2022-08-29 21:33:12,255   cls_loss = 0.0
2022-08-29 21:33:12,256   global_step = 50399
2022-08-29 21:33:12,256   loss = 5.774475797863515
2022-08-29 21:33:12,256   rep_loss = 1.773094928581905
2022-08-29 21:33:12,256 ***** Save model *****
2022-08-29 21:33:14,384 ***** Eval results *****
2022-08-29 21:33:14,385   att_loss = 0.0
2022-08-29 21:33:14,385   cls_loss = 0.08493745373195316
2022-08-29 21:33:14,385   corr = 0.7933345625101008
2022-08-29 21:33:14,385   eval_loss = 1.1715463501341798
2022-08-29 21:33:14,385   global_step = 2999
2022-08-29 21:33:14,385   loss = 0.08493745373195316
2022-08-29 21:33:14,385   pearson = 0.7918235640332807
2022-08-29 21:33:14,385   rep_loss = 0.0
2022-08-29 21:33:14,385   spearmanr = 0.7948455609869209
2022-08-29 21:33:14,431 ***** Running evaluation *****
2022-08-29 21:33:14,431   Epoch = 1 iter 1499 step
2022-08-29 21:33:14,431   Num examples = 1043
2022-08-29 21:33:14,431   Batch size = 32
2022-08-29 21:33:17,043 ***** Eval results *****
2022-08-29 21:33:17,043   att_loss = 0.0
2022-08-29 21:33:17,043   cls_loss = 0.08690546425340347
2022-08-29 21:33:17,043   eval_loss = 0.6126617161613522
2022-08-29 21:33:17,043   global_step = 1499
2022-08-29 21:33:17,043   loss = 0.08690546425340347
2022-08-29 21:33:17,043   mcc = 0.47253285864835903
2022-08-29 21:33:17,043   rep_loss = 0.0
2022-08-29 21:33:31,747 ***** Running evaluation *****
2022-08-29 21:33:31,748   Epoch = 4 iter 3099 step
2022-08-29 21:33:31,748   Num examples = 1500
2022-08-29 21:33:31,748   Batch size = 32
2022-08-29 21:33:34,615 ***** Running evaluation *****
2022-08-29 21:33:34,615   Epoch = 1 iter 1599 step
2022-08-29 21:33:34,615   Num examples = 1043
2022-08-29 21:33:34,615   Batch size = 32
2022-08-29 21:33:35,539 ***** Eval results *****
2022-08-29 21:33:35,539   att_loss = 0.0
2022-08-29 21:33:35,539   cls_loss = 0.08237966379798982
2022-08-29 21:33:35,539   corr = 0.7925944250528407
2022-08-29 21:33:35,539   eval_loss = 1.2553650746320157
2022-08-29 21:33:35,539   global_step = 3099
2022-08-29 21:33:35,539   loss = 0.08237966379798982
2022-08-29 21:33:35,539   pearson = 0.7889874561506067
2022-08-29 21:33:35,539   rep_loss = 0.0
2022-08-29 21:33:35,539   spearmanr = 0.7962013939550748
2022-08-29 21:33:37,225 ***** Eval results *****
2022-08-29 21:33:37,225   att_loss = 0.0
2022-08-29 21:33:37,226   cls_loss = 0.08759478875304345
2022-08-29 21:33:37,226   eval_loss = 0.5932073317693941
2022-08-29 21:33:37,226   global_step = 1599
2022-08-29 21:33:37,226   loss = 0.08759478875304345
2022-08-29 21:33:37,226   mcc = 0.5005533832835016
2022-08-29 21:33:37,226   rep_loss = 0.0
2022-08-29 21:33:52,910 ***** Running evaluation *****
2022-08-29 21:33:52,910   Epoch = 4 iter 3199 step
2022-08-29 21:33:52,910   Num examples = 1500
2022-08-29 21:33:52,910   Batch size = 32
2022-08-29 21:33:54,813 ***** Running evaluation *****
2022-08-29 21:33:54,814   Epoch = 1 iter 1699 step
2022-08-29 21:33:54,814   Num examples = 1043
2022-08-29 21:33:54,814   Batch size = 32
2022-08-29 21:33:56,704 ***** Eval results *****
2022-08-29 21:33:56,704   att_loss = 0.0
2022-08-29 21:33:56,704   cls_loss = 0.082503505296071
2022-08-29 21:33:56,704   corr = 0.7913441162678669
2022-08-29 21:33:56,705   eval_loss = 1.1642524286153468
2022-08-29 21:33:56,705   global_step = 3199
2022-08-29 21:33:56,705   loss = 0.082503505296071
2022-08-29 21:33:56,705   pearson = 0.7905293591604468
2022-08-29 21:33:56,705   rep_loss = 0.0
2022-08-29 21:33:56,705   spearmanr = 0.7921588733752872
2022-08-29 21:33:57,429 ***** Eval results *****
2022-08-29 21:33:57,430   att_loss = 0.0
2022-08-29 21:33:57,430   cls_loss = 0.08681276234283028
2022-08-29 21:33:57,430   eval_loss = 0.5403171430031458
2022-08-29 21:33:57,430   global_step = 1699
2022-08-29 21:33:57,430   loss = 0.08681276234283028
2022-08-29 21:33:57,430   mcc = 0.5316714424849144
2022-08-29 21:33:57,430   rep_loss = 0.0
2022-08-29 21:33:57,430 ***** Save model *****
2022-08-29 21:34:14,083 ***** Running evaluation *****
2022-08-29 21:34:14,084   Epoch = 4 iter 3299 step
2022-08-29 21:34:14,084   Num examples = 1500
2022-08-29 21:34:14,084   Batch size = 32
2022-08-29 21:34:17,878 ***** Eval results *****
2022-08-29 21:34:17,878   att_loss = 0.0
2022-08-29 21:34:17,878   cls_loss = 0.0821826454313075
2022-08-29 21:34:17,878   corr = 0.7843496544123463
2022-08-29 21:34:17,878   eval_loss = 1.2866733923871467
2022-08-29 21:34:17,878   global_step = 3299
2022-08-29 21:34:17,878   loss = 0.0821826454313075
2022-08-29 21:34:17,878   pearson = 0.7826676763272374
2022-08-29 21:34:17,878   rep_loss = 0.0
2022-08-29 21:34:17,878   spearmanr = 0.7860316324974551
2022-08-29 21:34:18,164 ***** Running evaluation *****
2022-08-29 21:34:18,165   Epoch = 1 iter 1799 step
2022-08-29 21:34:18,165   Num examples = 1043
2022-08-29 21:34:18,165   Batch size = 32
2022-08-29 21:34:20,784 ***** Eval results *****
2022-08-29 21:34:20,784   att_loss = 0.0
2022-08-29 21:34:20,784   cls_loss = 0.08724618852699097
2022-08-29 21:34:20,784   eval_loss = 0.5278021957386624
2022-08-29 21:34:20,784   global_step = 1799
2022-08-29 21:34:20,784   loss = 0.08724618852699097
2022-08-29 21:34:20,784   mcc = 0.5108951669698317
2022-08-29 21:34:20,784   rep_loss = 0.0
2022-08-29 21:34:24,522 ***** Running evaluation *****
2022-08-29 21:34:24,522   Epoch = 4 iter 50599 step
2022-08-29 21:34:24,522   Num examples = 9815
2022-08-29 21:34:24,522   Batch size = 32
2022-08-29 21:34:24,523 ***** Eval results *****
2022-08-29 21:34:24,524   att_loss = 4.009705858891553
2022-08-29 21:34:24,524   cls_loss = 0.0
2022-08-29 21:34:24,524   global_step = 50599
2022-08-29 21:34:24,524   loss = 5.783428344789511
2022-08-29 21:34:24,524   rep_loss = 1.773722486920876
2022-08-29 21:34:24,524 ***** Save model *****
2022-08-29 21:34:35,253 ***** Running evaluation *****
2022-08-29 21:34:35,254   Epoch = 4 iter 3399 step
2022-08-29 21:34:35,254   Num examples = 1500
2022-08-29 21:34:35,254   Batch size = 32
2022-08-29 21:34:38,364 ***** Running evaluation *****
2022-08-29 21:34:38,365   Epoch = 1 iter 1899 step
2022-08-29 21:34:38,365   Num examples = 1043
2022-08-29 21:34:38,365   Batch size = 32
2022-08-29 21:34:39,043 ***** Eval results *****
2022-08-29 21:34:39,044   att_loss = 0.0
2022-08-29 21:34:39,044   cls_loss = 0.08144002049044481
2022-08-29 21:34:39,044   corr = 0.7912572456070204
2022-08-29 21:34:39,044   eval_loss = 1.2219984220697524
2022-08-29 21:34:39,044   global_step = 3399
2022-08-29 21:34:39,044   loss = 0.08144002049044481
2022-08-29 21:34:39,044   pearson = 0.7894832400132925
2022-08-29 21:34:39,044   rep_loss = 0.0
2022-08-29 21:34:39,044   spearmanr = 0.7930312512007484
2022-08-29 21:34:40,974 ***** Eval results *****
2022-08-29 21:34:40,974   att_loss = 0.0
2022-08-29 21:34:40,974   cls_loss = 0.08635467906380531
2022-08-29 21:34:40,974   eval_loss = 0.49299040007771866
2022-08-29 21:34:40,974   global_step = 1899
2022-08-29 21:34:40,974   loss = 0.08635467906380531
2022-08-29 21:34:40,974   mcc = 0.5093030018169853
2022-08-29 21:34:40,974   rep_loss = 0.0
2022-08-29 21:34:56,404 ***** Running evaluation *****
2022-08-29 21:34:56,404   Epoch = 4 iter 3499 step
2022-08-29 21:34:56,404   Num examples = 1500
2022-08-29 21:34:56,404   Batch size = 32
2022-08-29 21:34:58,545 ***** Running evaluation *****
2022-08-29 21:34:58,546   Epoch = 1 iter 1999 step
2022-08-29 21:34:58,546   Num examples = 1043
2022-08-29 21:34:58,546   Batch size = 32
2022-08-29 21:35:00,196 ***** Eval results *****
2022-08-29 21:35:00,196   att_loss = 0.0
2022-08-29 21:35:00,196   cls_loss = 0.08291460898076518
2022-08-29 21:35:00,197   corr = 0.7882711960241993
2022-08-29 21:35:00,197   eval_loss = 1.2203955932500514
2022-08-29 21:35:00,197   global_step = 3499
2022-08-29 21:35:00,197   loss = 0.08291460898076518
2022-08-29 21:35:00,197   pearson = 0.7869078211246593
2022-08-29 21:35:00,197   rep_loss = 0.0
2022-08-29 21:35:00,197   spearmanr = 0.7896345709237393
2022-08-29 21:35:01,160 ***** Eval results *****
2022-08-29 21:35:01,160   att_loss = 0.0
2022-08-29 21:35:01,160   cls_loss = 0.08650437019021119
2022-08-29 21:35:01,160   eval_loss = 0.6023815265207579
2022-08-29 21:35:01,160   global_step = 1999
2022-08-29 21:35:01,160   loss = 0.08650437019021119
2022-08-29 21:35:01,160   mcc = 0.5152882228132933
2022-08-29 21:35:01,160   rep_loss = 0.0
2022-08-29 21:35:18,755 ***** Running evaluation *****
2022-08-29 21:35:18,755   Epoch = 1 iter 2099 step
2022-08-29 21:35:18,755   Num examples = 1043
2022-08-29 21:35:18,755   Batch size = 32
2022-08-29 21:35:21,367 ***** Eval results *****
2022-08-29 21:35:21,367   att_loss = 0.0
2022-08-29 21:35:21,367   cls_loss = 0.08660532805660358
2022-08-29 21:35:21,367   eval_loss = 0.5356082263769526
2022-08-29 21:35:21,367   global_step = 2099
2022-08-29 21:35:21,367   loss = 0.08660532805660358
2022-08-29 21:35:21,367   mcc = 0.5047489222406755
2022-08-29 21:35:21,367   rep_loss = 0.0
2022-08-29 21:35:33,834 ***** Running evaluation *****
2022-08-29 21:35:33,835   Epoch = 4 iter 50799 step
2022-08-29 21:35:33,835   Num examples = 9815
2022-08-29 21:35:33,835   Batch size = 32
2022-08-29 21:35:33,836 ***** Eval results *****
2022-08-29 21:35:33,836   att_loss = 4.010327911237933
2022-08-29 21:35:33,836   cls_loss = 0.0
2022-08-29 21:35:33,836   global_step = 50799
2022-08-29 21:35:33,836   loss = 5.78411937024086
2022-08-29 21:35:33,836   rep_loss = 1.7737914601150824
2022-08-29 21:35:33,836 ***** Save model *****
2022-08-29 21:35:38,977 ***** Running evaluation *****
2022-08-29 21:35:38,978   Epoch = 2 iter 2199 step
2022-08-29 21:35:38,978   Num examples = 1043
2022-08-29 21:35:38,978   Batch size = 32
2022-08-29 21:35:41,589 ***** Eval results *****
2022-08-29 21:35:41,589   att_loss = 0.0
2022-08-29 21:35:41,589   cls_loss = 0.0951732045837811
2022-08-29 21:35:41,589   eval_loss = 0.5679723155317884
2022-08-29 21:35:41,589   global_step = 2199
2022-08-29 21:35:41,589   loss = 0.0951732045837811
2022-08-29 21:35:41,589   mcc = 0.4901439331630071
2022-08-29 21:35:41,589   rep_loss = 0.0
2022-08-29 21:35:59,142 ***** Running evaluation *****
2022-08-29 21:35:59,142   Epoch = 2 iter 2299 step
2022-08-29 21:35:59,142   Num examples = 1043
2022-08-29 21:35:59,142   Batch size = 32
2022-08-29 21:36:01,756 ***** Eval results *****
2022-08-29 21:36:01,756   att_loss = 0.0
2022-08-29 21:36:01,756   cls_loss = 0.0886182041118847
2022-08-29 21:36:01,756   eval_loss = 0.5446896528204282
2022-08-29 21:36:01,756   global_step = 2299
2022-08-29 21:36:01,756   loss = 0.0886182041118847
2022-08-29 21:36:01,756   mcc = 0.4905937147239641
2022-08-29 21:36:01,756   rep_loss = 0.0
2022-08-29 21:36:19,317 ***** Running evaluation *****
2022-08-29 21:36:19,318   Epoch = 2 iter 2399 step
2022-08-29 21:36:19,318   Num examples = 1043
2022-08-29 21:36:19,318   Batch size = 32
2022-08-29 21:36:21,931 ***** Eval results *****
2022-08-29 21:36:21,931   att_loss = 0.0
2022-08-29 21:36:21,931   cls_loss = 0.08712302877819131
2022-08-29 21:36:21,931   eval_loss = 0.6094202080910857
2022-08-29 21:36:21,931   global_step = 2399
2022-08-29 21:36:21,931   loss = 0.08712302877819131
2022-08-29 21:36:21,931   mcc = 0.510112473854056
2022-08-29 21:36:21,931   rep_loss = 0.0
2022-08-29 21:36:39,512 ***** Running evaluation *****
2022-08-29 21:36:39,513   Epoch = 2 iter 2499 step
2022-08-29 21:36:39,513   Num examples = 1043
2022-08-29 21:36:39,513   Batch size = 32
2022-08-29 21:36:42,127 ***** Eval results *****
2022-08-29 21:36:42,127   att_loss = 0.0
2022-08-29 21:36:42,127   cls_loss = 0.08536355459636728
2022-08-29 21:36:42,127   eval_loss = 0.5603664736404563
2022-08-29 21:36:42,127   global_step = 2499
2022-08-29 21:36:42,127   loss = 0.08536355459636728
2022-08-29 21:36:42,128   mcc = 0.4901764242082043
2022-08-29 21:36:42,128   rep_loss = 0.0
2022-08-29 21:36:45,042 ***** Running evaluation *****
2022-08-29 21:36:45,043   Epoch = 4 iter 50999 step
2022-08-29 21:36:45,043   Num examples = 9815
2022-08-29 21:36:45,043   Batch size = 32
2022-08-29 21:36:45,044 ***** Eval results *****
2022-08-29 21:36:45,044   att_loss = 4.005310501317754
2022-08-29 21:36:45,044   cls_loss = 0.0
2022-08-29 21:36:45,044   global_step = 50999
2022-08-29 21:36:45,044   loss = 5.7788341420126335
2022-08-29 21:36:45,044   rep_loss = 1.7735236400723768
2022-08-29 21:36:45,044 ***** Save model *****
2022-08-29 21:36:59,717 ***** Running evaluation *****
2022-08-29 21:36:59,717   Epoch = 2 iter 2599 step
2022-08-29 21:36:59,717   Num examples = 1043
2022-08-29 21:36:59,717   Batch size = 32
2022-08-29 21:37:02,332 ***** Eval results *****
2022-08-29 21:37:02,332   att_loss = 0.0
2022-08-29 21:37:02,332   cls_loss = 0.08587689026305151
2022-08-29 21:37:02,332   eval_loss = 0.5908783550063769
2022-08-29 21:37:02,332   global_step = 2599
2022-08-29 21:37:02,332   loss = 0.08587689026305151
2022-08-29 21:37:02,332   mcc = 0.5027162727680555
2022-08-29 21:37:02,332   rep_loss = 0.0
2022-08-29 21:37:19,889 ***** Running evaluation *****
2022-08-29 21:37:19,889   Epoch = 2 iter 2699 step
2022-08-29 21:37:19,889   Num examples = 1043
2022-08-29 21:37:19,889   Batch size = 32
2022-08-29 21:37:22,504 ***** Eval results *****
2022-08-29 21:37:22,504   att_loss = 0.0
2022-08-29 21:37:22,504   cls_loss = 0.08470124928992023
2022-08-29 21:37:22,504   eval_loss = 0.5735522090937152
2022-08-29 21:37:22,504   global_step = 2699
2022-08-29 21:37:22,504   loss = 0.08470124928992023
2022-08-29 21:37:22,504   mcc = 0.5073136963601746
2022-08-29 21:37:22,504   rep_loss = 0.0
2022-08-29 21:37:40,052 ***** Running evaluation *****
2022-08-29 21:37:40,052   Epoch = 2 iter 2799 step
2022-08-29 21:37:40,052   Num examples = 1043
2022-08-29 21:37:40,053   Batch size = 32
2022-08-29 21:37:42,665 ***** Eval results *****
2022-08-29 21:37:42,666   att_loss = 0.0
2022-08-29 21:37:42,666   cls_loss = 0.08242417260341425
2022-08-29 21:37:42,666   eval_loss = 0.6751248701052233
2022-08-29 21:37:42,666   global_step = 2799
2022-08-29 21:37:42,666   loss = 0.08242417260341425
2022-08-29 21:37:42,666   mcc = 0.4992878421902516
2022-08-29 21:37:42,666   rep_loss = 0.0
2022-08-29 21:37:54,023 ***** Running evaluation *****
2022-08-29 21:37:54,023   Epoch = 4 iter 51199 step
2022-08-29 21:37:54,023   Num examples = 9815
2022-08-29 21:37:54,023   Batch size = 32
2022-08-29 21:37:54,024 ***** Eval results *****
2022-08-29 21:37:54,024   att_loss = 4.00112371895612
2022-08-29 21:37:54,025   cls_loss = 0.0
2022-08-29 21:37:54,025   global_step = 51199
2022-08-29 21:37:54,025   loss = 5.774265364157674
2022-08-29 21:37:54,025   rep_loss = 1.7731416434542788
2022-08-29 21:37:54,025 ***** Save model *****
2022-08-29 21:38:00,261 ***** Running evaluation *****
2022-08-29 21:38:00,261   Epoch = 2 iter 2899 step
2022-08-29 21:38:00,261   Num examples = 1043
2022-08-29 21:38:00,261   Batch size = 32
2022-08-29 21:38:02,883 ***** Eval results *****
2022-08-29 21:38:02,883   att_loss = 0.0
2022-08-29 21:38:02,883   cls_loss = 0.08284287978583754
2022-08-29 21:38:02,883   eval_loss = 0.566735980862921
2022-08-29 21:38:02,883   global_step = 2899
2022-08-29 21:38:02,883   loss = 0.08284287978583754
2022-08-29 21:38:02,883   mcc = 0.5129121393126265
2022-08-29 21:38:02,883   rep_loss = 0.0
2022-08-29 21:38:20,439 ***** Running evaluation *****
2022-08-29 21:38:20,439   Epoch = 2 iter 2999 step
2022-08-29 21:38:20,439   Num examples = 1043
2022-08-29 21:38:20,439   Batch size = 32
2022-08-29 21:38:23,056 ***** Eval results *****
2022-08-29 21:38:23,056   att_loss = 0.0
2022-08-29 21:38:23,056   cls_loss = 0.08257075133037388
2022-08-29 21:38:23,056   eval_loss = 0.5360032015226104
2022-08-29 21:38:23,056   global_step = 2999
2022-08-29 21:38:23,056   loss = 0.08257075133037388
2022-08-29 21:38:23,056   mcc = 0.49419533040588925
2022-08-29 21:38:23,057   rep_loss = 0.0
2022-08-29 21:38:40,667 ***** Running evaluation *****
2022-08-29 21:38:40,667   Epoch = 2 iter 3099 step
2022-08-29 21:38:40,667   Num examples = 1043
2022-08-29 21:38:40,667   Batch size = 32
2022-08-29 21:38:43,282 ***** Eval results *****
2022-08-29 21:38:43,282   att_loss = 0.0
2022-08-29 21:38:43,282   cls_loss = 0.08251951308969335
2022-08-29 21:38:43,282   eval_loss = 0.5410620062188669
2022-08-29 21:38:43,283   global_step = 3099
2022-08-29 21:38:43,283   loss = 0.08251951308969335
2022-08-29 21:38:43,283   mcc = 0.5430177264495386
2022-08-29 21:38:43,283   rep_loss = 0.0
2022-08-29 21:38:43,283 ***** Save model *****
2022-08-29 21:39:04,132 ***** Running evaluation *****
2022-08-29 21:39:04,133   Epoch = 2 iter 3199 step
2022-08-29 21:39:04,133   Num examples = 1043
2022-08-29 21:39:04,133   Batch size = 32
2022-08-29 21:39:05,218 ***** Running evaluation *****
2022-08-29 21:39:05,219   Epoch = 4 iter 51399 step
2022-08-29 21:39:05,219   Num examples = 9815
2022-08-29 21:39:05,219   Batch size = 32
2022-08-29 21:39:05,220 ***** Eval results *****
2022-08-29 21:39:05,220   att_loss = 3.997986024914241
2022-08-29 21:39:05,220   cls_loss = 0.0
2022-08-29 21:39:05,220   global_step = 51399
2022-08-29 21:39:05,220   loss = 5.770827068468922
2022-08-29 21:39:05,220   rep_loss = 1.7728410413404263
2022-08-29 21:39:05,220 ***** Save model *****
2022-08-29 21:39:06,751 ***** Eval results *****
2022-08-29 21:39:06,751   att_loss = 0.0
2022-08-29 21:39:06,751   cls_loss = 0.08276919753186454
2022-08-29 21:39:06,751   eval_loss = 0.5567320391084208
2022-08-29 21:39:06,751   global_step = 3199
2022-08-29 21:39:06,751   loss = 0.08276919753186454
2022-08-29 21:39:06,751   mcc = 0.5144566045467207
2022-08-29 21:39:06,751   rep_loss = 0.0
2022-08-29 21:39:24,451 ***** Running evaluation *****
2022-08-29 21:39:24,451   Epoch = 3 iter 3299 step
2022-08-29 21:39:24,451   Num examples = 1043
2022-08-29 21:39:24,451   Batch size = 32
2022-08-29 21:39:27,063 ***** Eval results *****
2022-08-29 21:39:27,063   att_loss = 0.0
2022-08-29 21:39:27,063   cls_loss = 0.07694219731186566
2022-08-29 21:39:27,063   eval_loss = 0.582053040238944
2022-08-29 21:39:27,063   global_step = 3299
2022-08-29 21:39:27,063   loss = 0.07694219731186566
2022-08-29 21:39:27,063   mcc = 0.5102026507035501
2022-08-29 21:39:27,063   rep_loss = 0.0
2022-08-29 21:39:44,751 ***** Running evaluation *****
2022-08-29 21:39:44,752   Epoch = 3 iter 3399 step
2022-08-29 21:39:44,752   Num examples = 1043
2022-08-29 21:39:44,752   Batch size = 32
2022-08-29 21:39:47,369 ***** Eval results *****
2022-08-29 21:39:47,369   att_loss = 0.0
2022-08-29 21:39:47,369   cls_loss = 0.0835763461028154
2022-08-29 21:39:47,369   eval_loss = 0.5543064014478163
2022-08-29 21:39:47,369   global_step = 3399
2022-08-29 21:39:47,369   loss = 0.0835763461028154
2022-08-29 21:39:47,369   mcc = 0.4975221305697031
2022-08-29 21:39:47,369   rep_loss = 0.0
2022-08-29 21:40:05,081 ***** Running evaluation *****
2022-08-29 21:40:05,081   Epoch = 3 iter 3499 step
2022-08-29 21:40:05,082   Num examples = 1043
2022-08-29 21:40:05,082   Batch size = 32
2022-08-29 21:40:07,698 ***** Eval results *****
2022-08-29 21:40:07,698   att_loss = 0.0
2022-08-29 21:40:07,699   cls_loss = 0.0818648503682876
2022-08-29 21:40:07,699   eval_loss = 0.5644224834713069
2022-08-29 21:40:07,699   global_step = 3499
2022-08-29 21:40:07,699   loss = 0.0818648503682876
2022-08-29 21:40:07,699   mcc = 0.5025526056573781
2022-08-29 21:40:07,699   rep_loss = 0.0
2022-08-29 21:40:14,153 ***** Running evaluation *****
2022-08-29 21:40:14,154   Epoch = 4 iter 51599 step
2022-08-29 21:40:14,154   Num examples = 9815
2022-08-29 21:40:14,154   Batch size = 32
2022-08-29 21:40:14,155 ***** Eval results *****
2022-08-29 21:40:14,155   att_loss = 3.997726255003548
2022-08-29 21:40:14,155   cls_loss = 0.0
2022-08-29 21:40:14,155   global_step = 51599
2022-08-29 21:40:14,155   loss = 5.770455117538482
2022-08-29 21:40:14,155   rep_loss = 1.772728859216982
2022-08-29 21:40:14,155 ***** Save model *****
2022-08-29 21:40:25,440 ***** Running evaluation *****
2022-08-29 21:40:25,440   Epoch = 3 iter 3599 step
2022-08-29 21:40:25,440   Num examples = 1043
2022-08-29 21:40:25,440   Batch size = 32
2022-08-29 21:40:28,054 ***** Eval results *****
2022-08-29 21:40:28,054   att_loss = 0.0
2022-08-29 21:40:28,054   cls_loss = 0.08139089832105968
2022-08-29 21:40:28,055   eval_loss = 0.5576815783525958
2022-08-29 21:40:28,055   global_step = 3599
2022-08-29 21:40:28,055   loss = 0.08139089832105968
2022-08-29 21:40:28,055   mcc = 0.5176404953790767
2022-08-29 21:40:28,055   rep_loss = 0.0
2022-08-29 21:40:45,771 ***** Running evaluation *****
2022-08-29 21:40:45,771   Epoch = 3 iter 3699 step
2022-08-29 21:40:45,771   Num examples = 1043
2022-08-29 21:40:45,772   Batch size = 32
2022-08-29 21:40:48,384 ***** Eval results *****
2022-08-29 21:40:48,384   att_loss = 0.0
2022-08-29 21:40:48,385   cls_loss = 0.08353372414530527
2022-08-29 21:40:48,385   eval_loss = 0.5381596946355068
2022-08-29 21:40:48,385   global_step = 3699
2022-08-29 21:40:48,385   loss = 0.08353372414530527
2022-08-29 21:40:48,385   mcc = 0.5047443398055037
2022-08-29 21:40:48,385   rep_loss = 0.0
2022-08-29 21:41:06,077 ***** Running evaluation *****
2022-08-29 21:41:06,078   Epoch = 3 iter 3799 step
2022-08-29 21:41:06,078   Num examples = 1043
2022-08-29 21:41:06,078   Batch size = 32
2022-08-29 21:41:08,694 ***** Eval results *****
2022-08-29 21:41:08,694   att_loss = 0.0
2022-08-29 21:41:08,694   cls_loss = 0.08307974876663765
2022-08-29 21:41:08,694   eval_loss = 0.5835337896238674
2022-08-29 21:41:08,694   global_step = 3799
2022-08-29 21:41:08,694   loss = 0.08307974876663765
2022-08-29 21:41:08,694   mcc = 0.5046330570516372
2022-08-29 21:41:08,694   rep_loss = 0.0
2022-08-29 21:41:23,319 ***** Running evaluation *****
2022-08-29 21:41:23,320   Epoch = 4 iter 51799 step
2022-08-29 21:41:23,320   Num examples = 9815
2022-08-29 21:41:23,320   Batch size = 32
2022-08-29 21:41:23,321 ***** Eval results *****
2022-08-29 21:41:23,321   att_loss = 3.9990320603491853
2022-08-29 21:41:23,321   cls_loss = 0.0
2022-08-29 21:41:23,321   global_step = 51799
2022-08-29 21:41:23,321   loss = 5.771806504405884
2022-08-29 21:41:23,321   rep_loss = 1.772774440192825
2022-08-29 21:41:23,322 ***** Save model *****
2022-08-29 21:41:26,423 ***** Running evaluation *****
2022-08-29 21:41:26,424   Epoch = 3 iter 3899 step
2022-08-29 21:41:26,424   Num examples = 1043
2022-08-29 21:41:26,424   Batch size = 32
2022-08-29 21:41:29,148 ***** Eval results *****
2022-08-29 21:41:29,148   att_loss = 0.0
2022-08-29 21:41:29,148   cls_loss = 0.0833478253924375
2022-08-29 21:41:29,148   eval_loss = 0.5563273425355102
2022-08-29 21:41:29,148   global_step = 3899
2022-08-29 21:41:29,148   loss = 0.0833478253924375
2022-08-29 21:41:29,148   mcc = 0.5208528714430889
2022-08-29 21:41:29,148   rep_loss = 0.0
2022-08-29 21:41:46,873 ***** Running evaluation *****
2022-08-29 21:41:46,873   Epoch = 3 iter 3999 step
2022-08-29 21:41:46,873   Num examples = 1043
2022-08-29 21:41:46,873   Batch size = 32
2022-08-29 21:41:49,491 ***** Eval results *****
2022-08-29 21:41:49,492   att_loss = 0.0
2022-08-29 21:41:49,492   cls_loss = 0.08302339616717783
2022-08-29 21:41:49,492   eval_loss = 0.5814947081786214
2022-08-29 21:41:49,492   global_step = 3999
2022-08-29 21:41:49,492   loss = 0.08302339616717783
2022-08-29 21:41:49,492   mcc = 0.5024061937657525
2022-08-29 21:41:49,492   rep_loss = 0.0
2022-08-29 21:42:07,222 ***** Running evaluation *****
2022-08-29 21:42:07,222   Epoch = 3 iter 4099 step
2022-08-29 21:42:07,222   Num examples = 1043
2022-08-29 21:42:07,222   Batch size = 32
2022-08-29 21:42:09,837 ***** Eval results *****
2022-08-29 21:42:09,837   att_loss = 0.0
2022-08-29 21:42:09,838   cls_loss = 0.08302880938492674
2022-08-29 21:42:09,838   eval_loss = 0.5709826388593876
2022-08-29 21:42:09,838   global_step = 4099
2022-08-29 21:42:09,838   loss = 0.08302880938492674
2022-08-29 21:42:09,838   mcc = 0.500410139799897
2022-08-29 21:42:09,838   rep_loss = 0.0
2022-08-29 21:42:27,565 ***** Running evaluation *****
2022-08-29 21:42:27,566   Epoch = 3 iter 4199 step
2022-08-29 21:42:27,566   Num examples = 1043
2022-08-29 21:42:27,566   Batch size = 32
2022-08-29 21:42:30,182 ***** Eval results *****
2022-08-29 21:42:30,182   att_loss = 0.0
2022-08-29 21:42:30,182   cls_loss = 0.08241108225555575
2022-08-29 21:42:30,182   eval_loss = 0.5669087898550611
2022-08-29 21:42:30,183   global_step = 4199
2022-08-29 21:42:30,183   loss = 0.08241108225555575
2022-08-29 21:42:30,183   mcc = 0.5091281484717528
2022-08-29 21:42:30,183   rep_loss = 0.0
2022-08-29 21:42:35,387 ***** Running evaluation *****
2022-08-29 21:42:35,388   Epoch = 4 iter 51999 step
2022-08-29 21:42:35,388   Num examples = 9815
2022-08-29 21:42:35,388   Batch size = 32
2022-08-29 21:42:35,389 ***** Eval results *****
2022-08-29 21:42:35,389   att_loss = 3.998990721236371
2022-08-29 21:42:35,389   cls_loss = 0.0
2022-08-29 21:42:35,389   global_step = 51999
2022-08-29 21:42:35,389   loss = 5.771622772936551
2022-08-29 21:42:35,389   rep_loss = 1.7726320479787234
2022-08-29 21:42:35,389 ***** Save model *****
2022-08-29 21:42:47,937 ***** Running evaluation *****
2022-08-29 21:42:47,937   Epoch = 4 iter 4299 step
2022-08-29 21:42:47,937   Num examples = 1043
2022-08-29 21:42:47,937   Batch size = 32
2022-08-29 21:42:50,553 ***** Eval results *****
2022-08-29 21:42:50,554   att_loss = 0.0
2022-08-29 21:42:50,554   cls_loss = 0.06576358747703058
2022-08-29 21:42:50,554   eval_loss = 0.566398541132609
2022-08-29 21:42:50,554   global_step = 4299
2022-08-29 21:42:50,554   loss = 0.06576358747703058
2022-08-29 21:42:50,554   mcc = 0.5052340847713823
2022-08-29 21:42:50,554   rep_loss = 0.0
2022-08-29 21:43:08,233 ***** Running evaluation *****
2022-08-29 21:43:08,234   Epoch = 4 iter 4399 step
2022-08-29 21:43:08,234   Num examples = 1043
2022-08-29 21:43:08,234   Batch size = 32
2022-08-29 21:43:10,854 ***** Eval results *****
2022-08-29 21:43:10,855   att_loss = 0.0
2022-08-29 21:43:10,855   cls_loss = 0.07840703787120777
2022-08-29 21:43:10,855   eval_loss = 0.5212073021314361
2022-08-29 21:43:10,855   global_step = 4399
2022-08-29 21:43:10,855   loss = 0.07840703787120777
2022-08-29 21:43:10,855   mcc = 0.5140072587631689
2022-08-29 21:43:10,855   rep_loss = 0.0
2022-08-29 21:43:28,481 ***** Running evaluation *****
2022-08-29 21:43:28,481   Epoch = 4 iter 4499 step
2022-08-29 21:43:28,481   Num examples = 1043
2022-08-29 21:43:28,481   Batch size = 32
2022-08-29 21:43:31,097 ***** Eval results *****
2022-08-29 21:43:31,098   att_loss = 0.0
2022-08-29 21:43:31,098   cls_loss = 0.08291180420355125
2022-08-29 21:43:31,098   eval_loss = 0.5166979712067228
2022-08-29 21:43:31,098   global_step = 4499
2022-08-29 21:43:31,098   loss = 0.08291180420355125
2022-08-29 21:43:31,098   mcc = 0.539019545585709
2022-08-29 21:43:31,098   rep_loss = 0.0
2022-08-29 21:43:44,465 ***** Running evaluation *****
2022-08-29 21:43:44,465   Epoch = 4 iter 52199 step
2022-08-29 21:43:44,465   Num examples = 9815
2022-08-29 21:43:44,465   Batch size = 32
2022-08-29 21:43:44,467 ***** Eval results *****
2022-08-29 21:43:44,467   att_loss = 4.002236312158992
2022-08-29 21:43:44,467   cls_loss = 0.0
2022-08-29 21:43:44,467   global_step = 52199
2022-08-29 21:43:44,467   loss = 5.775146553872293
2022-08-29 21:43:44,467   rep_loss = 1.7729102383073223
2022-08-29 21:43:44,467 ***** Save model *****
2022-08-29 21:43:48,764 ***** Running evaluation *****
2022-08-29 21:43:48,764   Epoch = 4 iter 4599 step
2022-08-29 21:43:48,764   Num examples = 1043
2022-08-29 21:43:48,764   Batch size = 32
2022-08-29 21:43:51,382 ***** Eval results *****
2022-08-29 21:43:51,383   att_loss = 0.0
2022-08-29 21:43:51,383   cls_loss = 0.08246564435621649
2022-08-29 21:43:51,383   eval_loss = 0.5276453296343485
2022-08-29 21:43:51,383   global_step = 4599
2022-08-29 21:43:51,383   loss = 0.08246564435621649
2022-08-29 21:43:51,383   mcc = 0.5325167447166456
2022-08-29 21:43:51,383   rep_loss = 0.0
2022-08-29 21:44:09,039 ***** Running evaluation *****
2022-08-29 21:44:09,040   Epoch = 4 iter 4699 step
2022-08-29 21:44:09,040   Num examples = 1043
2022-08-29 21:44:09,040   Batch size = 32
2022-08-29 21:44:11,655 ***** Eval results *****
2022-08-29 21:44:11,655   att_loss = 0.0
2022-08-29 21:44:11,655   cls_loss = 0.0809738467244619
2022-08-29 21:44:11,656   eval_loss = 0.5430092511303497
2022-08-29 21:44:11,656   global_step = 4699
2022-08-29 21:44:11,656   loss = 0.0809738467244619
2022-08-29 21:44:11,656   mcc = 0.5373623427702773
2022-08-29 21:44:11,656   rep_loss = 0.0
2022-08-29 21:44:29,286 ***** Running evaluation *****
2022-08-29 21:44:29,287   Epoch = 4 iter 4799 step
2022-08-29 21:44:29,287   Num examples = 1043
2022-08-29 21:44:29,287   Batch size = 32
2022-08-29 21:44:31,900 ***** Eval results *****
2022-08-29 21:44:31,900   att_loss = 0.0
2022-08-29 21:44:31,900   cls_loss = 0.08039986102910716
2022-08-29 21:44:31,900   eval_loss = 0.5445756909973694
2022-08-29 21:44:31,900   global_step = 4799
2022-08-29 21:44:31,900   loss = 0.08039986102910716
2022-08-29 21:44:31,901   mcc = 0.5398085142164725
2022-08-29 21:44:31,901   rep_loss = 0.0
2022-08-29 21:44:49,528 ***** Running evaluation *****
2022-08-29 21:44:49,529   Epoch = 4 iter 4899 step
2022-08-29 21:44:49,529   Num examples = 1043
2022-08-29 21:44:49,529   Batch size = 32
2022-08-29 21:44:52,142 ***** Eval results *****
2022-08-29 21:44:52,142   att_loss = 0.0
2022-08-29 21:44:52,142   cls_loss = 0.07925427719903144
2022-08-29 21:44:52,143   eval_loss = 0.5584010786630891
2022-08-29 21:44:52,143   global_step = 4899
2022-08-29 21:44:52,143   loss = 0.07925427719903144
2022-08-29 21:44:52,143   mcc = 0.5160691863640823
2022-08-29 21:44:52,143   rep_loss = 0.0
2022-08-29 21:44:57,175 ***** Running evaluation *****
2022-08-29 21:44:57,175   Epoch = 4 iter 52399 step
2022-08-29 21:44:57,183   Num examples = 9815
2022-08-29 21:44:57,183   Batch size = 32
2022-08-29 21:44:57,184 ***** Eval results *****
2022-08-29 21:44:57,184   att_loss = 4.002479596087656
2022-08-29 21:44:57,184   cls_loss = 0.0
2022-08-29 21:44:57,184   global_step = 52399
2022-08-29 21:44:57,184   loss = 5.7753606817840994
2022-08-29 21:44:57,184   rep_loss = 1.7728810822442287
2022-08-29 21:44:57,184 ***** Save model *****
2022-08-29 21:45:09,831 ***** Running evaluation *****
2022-08-29 21:45:09,832   Epoch = 4 iter 4999 step
2022-08-29 21:45:09,832   Num examples = 1043
2022-08-29 21:45:09,832   Batch size = 32
2022-08-29 21:45:12,449 ***** Eval results *****
2022-08-29 21:45:12,449   att_loss = 0.0
2022-08-29 21:45:12,449   cls_loss = 0.07802892647468404
2022-08-29 21:45:12,449   eval_loss = 0.5636609657244249
2022-08-29 21:45:12,449   global_step = 4999
2022-08-29 21:45:12,449   loss = 0.07802892647468404
2022-08-29 21:45:12,449   mcc = 0.5185657415144317
2022-08-29 21:45:12,449   rep_loss = 0.0
2022-08-29 21:45:30,093 ***** Running evaluation *****
2022-08-29 21:45:30,094   Epoch = 4 iter 5099 step
2022-08-29 21:45:30,094   Num examples = 1043
2022-08-29 21:45:30,094   Batch size = 32
2022-08-29 21:45:32,710 ***** Eval results *****
2022-08-29 21:45:32,710   att_loss = 0.0
2022-08-29 21:45:32,710   cls_loss = 0.0785721475562529
2022-08-29 21:45:32,711   eval_loss = 0.5520047213543545
2022-08-29 21:45:32,711   global_step = 5099
2022-08-29 21:45:32,711   loss = 0.0785721475562529
2022-08-29 21:45:32,711   mcc = 0.5159092620622691
2022-08-29 21:45:32,711   rep_loss = 0.0
2022-08-29 21:45:50,345 ***** Running evaluation *****
2022-08-29 21:45:50,346   Epoch = 4 iter 5199 step
2022-08-29 21:45:50,346   Num examples = 1043
2022-08-29 21:45:50,346   Batch size = 32
2022-08-29 21:45:52,957 ***** Eval results *****
2022-08-29 21:45:52,957   att_loss = 0.0
2022-08-29 21:45:52,957   cls_loss = 0.0788310162276994
2022-08-29 21:45:52,957   eval_loss = 0.5333314246752046
2022-08-29 21:45:52,957   global_step = 5199
2022-08-29 21:45:52,957   loss = 0.0788310162276994
2022-08-29 21:45:52,957   mcc = 0.5215519772866539
2022-08-29 21:45:52,957   rep_loss = 0.0
2022-08-29 21:46:09,832 ***** Running evaluation *****
2022-08-29 21:46:09,832   Epoch = 4 iter 52599 step
2022-08-29 21:46:09,832   Num examples = 9815
2022-08-29 21:46:09,833   Batch size = 32
2022-08-29 21:46:09,834 ***** Eval results *****
2022-08-29 21:46:09,834   att_loss = 4.004639739230551
2022-08-29 21:46:09,834   cls_loss = 0.0
2022-08-29 21:46:09,834   global_step = 52599
2022-08-29 21:46:09,834   loss = 5.77753222385477
2022-08-29 21:46:09,834   rep_loss = 1.7728924822841226
2022-08-29 21:46:09,834 ***** Save model *****
2022-08-29 21:46:10,619 ***** Running evaluation *****
2022-08-29 21:46:10,619   Epoch = 4 iter 5299 step
2022-08-29 21:46:10,619   Num examples = 1043
2022-08-29 21:46:10,620   Batch size = 32
2022-08-29 21:46:13,235 ***** Eval results *****
2022-08-29 21:46:13,236   att_loss = 0.0
2022-08-29 21:46:13,236   cls_loss = 0.07932350199266963
2022-08-29 21:46:13,236   eval_loss = 0.539964970991467
2022-08-29 21:46:13,236   global_step = 5299
2022-08-29 21:46:13,236   loss = 0.07932350199266963
2022-08-29 21:46:13,236   mcc = 0.5187251192358523
2022-08-29 21:46:13,236   rep_loss = 0.0
2022-08-29 21:47:19,507 ***** Running evaluation *****
2022-08-29 21:47:19,507   Epoch = 4 iter 52799 step
2022-08-29 21:47:19,507   Num examples = 9815
2022-08-29 21:47:19,507   Batch size = 32
2022-08-29 21:47:19,508 ***** Eval results *****
2022-08-29 21:47:19,509   att_loss = 4.005694367137925
2022-08-29 21:47:19,509   cls_loss = 0.0
2022-08-29 21:47:19,509   global_step = 52799
2022-08-29 21:47:19,509   loss = 5.778561241437417
2022-08-29 21:47:19,509   rep_loss = 1.7728668720853762
2022-08-29 21:47:19,509 ***** Save model *****
2022-08-29 21:48:29,175 ***** Running evaluation *****
2022-08-29 21:48:29,175   Epoch = 4 iter 52999 step
2022-08-29 21:48:29,175   Num examples = 9815
2022-08-29 21:48:29,175   Batch size = 32
2022-08-29 21:48:29,177 ***** Eval results *****
2022-08-29 21:48:29,177   att_loss = 4.0070230073551265
2022-08-29 21:48:29,177   cls_loss = 0.0
2022-08-29 21:48:29,177   global_step = 52999
2022-08-29 21:48:29,177   loss = 5.779965286084367
2022-08-29 21:48:29,177   rep_loss = 1.7729422765673348
2022-08-29 21:48:29,177 ***** Save model *****
2022-08-29 21:49:41,004 ***** Running evaluation *****
2022-08-29 21:49:41,004   Epoch = 4 iter 53199 step
2022-08-29 21:49:41,004   Num examples = 9815
2022-08-29 21:49:41,004   Batch size = 32
2022-08-29 21:49:41,005 ***** Eval results *****
2022-08-29 21:49:41,006   att_loss = 4.0068613295132165
2022-08-29 21:49:41,006   cls_loss = 0.0
2022-08-29 21:49:41,006   global_step = 53199
2022-08-29 21:49:41,006   loss = 5.779688032922015
2022-08-29 21:49:41,006   rep_loss = 1.772826701438876
2022-08-29 21:49:41,006 ***** Save model *****
2022-08-29 21:50:50,814 ***** Running evaluation *****
2022-08-29 21:50:50,815   Epoch = 4 iter 53399 step
2022-08-29 21:50:50,815   Num examples = 9815
2022-08-29 21:50:50,815   Batch size = 32
2022-08-29 21:50:50,816 ***** Eval results *****
2022-08-29 21:50:50,816   att_loss = 4.004431543427574
2022-08-29 21:50:50,816   cls_loss = 0.0
2022-08-29 21:50:50,816   global_step = 53399
2022-08-29 21:50:50,816   loss = 5.776976204249977
2022-08-29 21:50:50,816   rep_loss = 1.7725446586122657
2022-08-29 21:50:50,816 ***** Save model *****
2022-08-29 21:52:02,527 ***** Running evaluation *****
2022-08-29 21:52:02,527   Epoch = 4 iter 53599 step
2022-08-29 21:52:02,527   Num examples = 9815
2022-08-29 21:52:02,527   Batch size = 32
2022-08-29 21:52:02,529 ***** Eval results *****
2022-08-29 21:52:02,529   att_loss = 4.002378738546953
2022-08-29 21:52:02,529   cls_loss = 0.0
2022-08-29 21:52:02,529   global_step = 53599
2022-08-29 21:52:02,529   loss = 5.774697086634166
2022-08-29 21:52:02,529   rep_loss = 1.772318345974978
2022-08-29 21:52:02,529 ***** Save model *****
2022-08-29 21:53:15,951 ***** Running evaluation *****
2022-08-29 21:53:15,951   Epoch = 4 iter 53799 step
2022-08-29 21:53:15,951   Num examples = 9815
2022-08-29 21:53:15,952   Batch size = 32
2022-08-29 21:53:15,953 ***** Eval results *****
2022-08-29 21:53:15,953   att_loss = 4.000802617563549
2022-08-29 21:53:15,953   cls_loss = 0.0
2022-08-29 21:53:15,953   global_step = 53799
2022-08-29 21:53:15,953   loss = 5.772842823182285
2022-08-29 21:53:15,953   rep_loss = 1.7720402041523227
2022-08-29 21:53:15,953 ***** Save model *****
2022-08-29 21:54:28,477 ***** Running evaluation *****
2022-08-29 21:54:28,478   Epoch = 4 iter 53999 step
2022-08-29 21:54:28,478   Num examples = 9815
2022-08-29 21:54:28,478   Batch size = 32
2022-08-29 21:54:28,479 ***** Eval results *****
2022-08-29 21:54:28,480   att_loss = 3.999727009950795
2022-08-29 21:54:28,480   cls_loss = 0.0
2022-08-29 21:54:28,480   global_step = 53999
2022-08-29 21:54:28,480   loss = 5.771588617811853
2022-08-29 21:54:28,480   rep_loss = 1.7718616065270782
2022-08-29 21:54:28,480 ***** Save model *****
2022-08-29 21:55:38,387 ***** Running evaluation *****
2022-08-29 21:55:38,388   Epoch = 4 iter 54199 step
2022-08-29 21:55:38,388   Num examples = 9815
2022-08-29 21:55:38,388   Batch size = 32
2022-08-29 21:55:38,389 ***** Eval results *****
2022-08-29 21:55:38,389   att_loss = 3.9990202395680594
2022-08-29 21:55:38,389   cls_loss = 0.0
2022-08-29 21:55:38,389   global_step = 54199
2022-08-29 21:55:38,389   loss = 5.770647571373545
2022-08-29 21:55:38,389   rep_loss = 1.7716273309664758
2022-08-29 21:55:38,389 ***** Save model *****
2022-08-29 21:56:50,355 ***** Running evaluation *****
2022-08-29 21:56:50,356   Epoch = 4 iter 54399 step
2022-08-29 21:56:50,356   Num examples = 9815
2022-08-29 21:56:50,356   Batch size = 32
2022-08-29 21:56:50,357 ***** Eval results *****
2022-08-29 21:56:50,357   att_loss = 4.000431896310198
2022-08-29 21:56:50,357   cls_loss = 0.0
2022-08-29 21:56:50,357   global_step = 54399
2022-08-29 21:56:50,357   loss = 5.772043648645324
2022-08-29 21:56:50,357   rep_loss = 1.7716117513931144
2022-08-29 21:56:50,357 ***** Save model *****
2022-08-29 21:57:59,967 ***** Running evaluation *****
2022-08-29 21:57:59,967   Epoch = 4 iter 54599 step
2022-08-29 21:57:59,968   Num examples = 9815
2022-08-29 21:57:59,968   Batch size = 32
2022-08-29 21:57:59,969 ***** Eval results *****
2022-08-29 21:57:59,969   att_loss = 3.998779176995631
2022-08-29 21:57:59,969   cls_loss = 0.0
2022-08-29 21:57:59,969   global_step = 54599
2022-08-29 21:57:59,969   loss = 5.770149406215221
2022-08-29 21:57:59,969   rep_loss = 1.7713702288088966
2022-08-29 21:57:59,969 ***** Save model *****
2022-08-29 21:59:11,812 ***** Running evaluation *****
2022-08-29 21:59:11,812   Epoch = 4 iter 54799 step
2022-08-29 21:59:11,812   Num examples = 9815
2022-08-29 21:59:11,812   Batch size = 32
2022-08-29 21:59:11,814 ***** Eval results *****
2022-08-29 21:59:11,814   att_loss = 3.9990156933808683
2022-08-29 21:59:11,814   cls_loss = 0.0
2022-08-29 21:59:11,814   global_step = 54799
2022-08-29 21:59:11,814   loss = 5.770247683270606
2022-08-29 21:59:11,814   rep_loss = 1.7712319899314553
2022-08-29 21:59:11,814 ***** Save model *****
2022-08-29 22:00:23,750 ***** Running evaluation *****
2022-08-29 22:00:23,750   Epoch = 4 iter 54999 step
2022-08-29 22:00:23,750   Num examples = 9815
2022-08-29 22:00:23,750   Batch size = 32
2022-08-29 22:00:23,752 ***** Eval results *****
2022-08-29 22:00:23,752   att_loss = 3.9998732052107333
2022-08-29 22:00:23,752   cls_loss = 0.0
2022-08-29 22:00:23,752   global_step = 54999
2022-08-29 22:00:23,752   loss = 5.771007898915952
2022-08-29 22:00:23,752   rep_loss = 1.7711346937656807
2022-08-29 22:00:23,752 ***** Save model *****
2022-08-29 22:01:35,000 ***** Running evaluation *****
2022-08-29 22:01:35,001   Epoch = 4 iter 55199 step
2022-08-29 22:01:35,001   Num examples = 9815
2022-08-29 22:01:35,001   Batch size = 32
2022-08-29 22:01:35,002 ***** Eval results *****
2022-08-29 22:01:35,002   att_loss = 4.000409648112361
2022-08-29 22:01:35,002   cls_loss = 0.0
2022-08-29 22:01:35,002   global_step = 55199
2022-08-29 22:01:35,002   loss = 5.771486526342659
2022-08-29 22:01:35,003   rep_loss = 1.7710768778014163
2022-08-29 22:01:35,003 ***** Save model *****
2022-08-29 22:02:44,618 ***** Running evaluation *****
2022-08-29 22:02:44,618   Epoch = 4 iter 55399 step
2022-08-29 22:02:44,618   Num examples = 9815
2022-08-29 22:02:44,618   Batch size = 32
2022-08-29 22:02:44,620 ***** Eval results *****
2022-08-29 22:02:44,620   att_loss = 3.999995039421136
2022-08-29 22:02:44,620   cls_loss = 0.0
2022-08-29 22:02:44,620   global_step = 55399
2022-08-29 22:02:44,620   loss = 5.770888657316931
2022-08-29 22:02:44,620   rep_loss = 1.7708936174049892
2022-08-29 22:02:44,620 ***** Save model *****
2022-08-29 22:03:57,913 ***** Running evaluation *****
2022-08-29 22:03:57,913   Epoch = 4 iter 55599 step
2022-08-29 22:03:57,913   Num examples = 9815
2022-08-29 22:03:57,913   Batch size = 32
2022-08-29 22:03:57,915 ***** Eval results *****
2022-08-29 22:03:57,915   att_loss = 4.000113660742115
2022-08-29 22:03:57,915   cls_loss = 0.0
2022-08-29 22:03:57,915   global_step = 55599
2022-08-29 22:03:57,915   loss = 5.77091216521362
2022-08-29 22:03:57,915   rep_loss = 1.7707985038676815
2022-08-29 22:03:57,915 ***** Save model *****
2022-08-29 22:05:07,549 ***** Running evaluation *****
2022-08-29 22:05:07,549   Epoch = 4 iter 55799 step
2022-08-29 22:05:07,549   Num examples = 9815
2022-08-29 22:05:07,550   Batch size = 32
2022-08-29 22:05:07,551 ***** Eval results *****
2022-08-29 22:05:07,551   att_loss = 3.9993111095229636
2022-08-29 22:05:07,551   cls_loss = 0.0
2022-08-29 22:05:07,551   global_step = 55799
2022-08-29 22:05:07,551   loss = 5.770001561057629
2022-08-29 22:05:07,551   rep_loss = 1.770690450576021
2022-08-29 22:05:07,551 ***** Save model *****
2022-08-29 22:06:19,376 ***** Running evaluation *****
2022-08-29 22:06:19,376   Epoch = 4 iter 55999 step
2022-08-29 22:06:19,377   Num examples = 9815
2022-08-29 22:06:19,377   Batch size = 32
2022-08-29 22:06:19,378 ***** Eval results *****
2022-08-29 22:06:19,378   att_loss = 3.998903962278056
2022-08-29 22:06:19,378   cls_loss = 0.0
2022-08-29 22:06:19,378   global_step = 55999
2022-08-29 22:06:19,378   loss = 5.769529874860939
2022-08-29 22:06:19,378   rep_loss = 1.7706259118760743
2022-08-29 22:06:19,378 ***** Save model *****
2022-08-29 22:07:31,056 ***** Running evaluation *****
2022-08-29 22:07:31,056   Epoch = 4 iter 56199 step
2022-08-29 22:07:31,056   Num examples = 9815
2022-08-29 22:07:31,056   Batch size = 32
2022-08-29 22:07:31,057 ***** Eval results *****
2022-08-29 22:07:31,057   att_loss = 3.997599261090191
2022-08-29 22:07:31,058   cls_loss = 0.0
2022-08-29 22:07:31,058   global_step = 56199
2022-08-29 22:07:31,058   loss = 5.768015968874987
2022-08-29 22:07:31,058   rep_loss = 1.7704167067460082
2022-08-29 22:07:31,058 ***** Save model *****
2022-08-29 22:08:40,505 ***** Running evaluation *****
2022-08-29 22:08:40,506   Epoch = 4 iter 56399 step
2022-08-29 22:08:40,506   Num examples = 9815
2022-08-29 22:08:40,506   Batch size = 32
2022-08-29 22:08:40,507 ***** Eval results *****
2022-08-29 22:08:40,507   att_loss = 3.9969931740672804
2022-08-29 22:08:40,508   cls_loss = 0.0
2022-08-29 22:08:40,508   global_step = 56399
2022-08-29 22:08:40,508   loss = 5.76729210705604
2022-08-29 22:08:40,508   rep_loss = 1.770298931685035
2022-08-29 22:08:40,508 ***** Save model *****
2022-08-29 22:09:52,195 ***** Running evaluation *****
2022-08-29 22:09:52,196   Epoch = 4 iter 56599 step
2022-08-29 22:09:52,196   Num examples = 9815
2022-08-29 22:09:52,196   Batch size = 32
2022-08-29 22:09:52,197 ***** Eval results *****
2022-08-29 22:09:52,197   att_loss = 3.9956318695705413
2022-08-29 22:09:52,197   cls_loss = 0.0
2022-08-29 22:09:52,197   global_step = 56599
2022-08-29 22:09:52,197   loss = 5.765735017023956
2022-08-29 22:09:52,197   rep_loss = 1.770103146184387
2022-08-29 22:09:52,197 ***** Save model *****
2022-08-29 22:11:01,720 ***** Running evaluation *****
2022-08-29 22:11:01,720   Epoch = 4 iter 56799 step
2022-08-29 22:11:01,720   Num examples = 9815
2022-08-29 22:11:01,721   Batch size = 32
2022-08-29 22:11:01,722 ***** Eval results *****
2022-08-29 22:11:01,722   att_loss = 3.9948497338335156
2022-08-29 22:11:01,722   cls_loss = 0.0
2022-08-29 22:11:01,722   global_step = 56799
2022-08-29 22:11:01,722   loss = 5.764823710478055
2022-08-29 22:11:01,722   rep_loss = 1.7699739761964424
2022-08-29 22:11:01,722 ***** Save model *****
2022-08-29 22:12:13,525 ***** Running evaluation *****
2022-08-29 22:12:13,525   Epoch = 4 iter 56999 step
2022-08-29 22:12:13,525   Num examples = 9815
2022-08-29 22:12:13,525   Batch size = 32
2022-08-29 22:12:13,527 ***** Eval results *****
2022-08-29 22:12:13,527   att_loss = 3.995149194478838
2022-08-29 22:12:13,527   cls_loss = 0.0
2022-08-29 22:12:13,527   global_step = 56999
2022-08-29 22:12:13,527   loss = 5.765022994262541
2022-08-29 22:12:13,527   rep_loss = 1.769873799286684
2022-08-29 22:12:13,527 ***** Save model *****
2022-08-29 22:13:23,136 ***** Running evaluation *****
2022-08-29 22:13:23,137   Epoch = 4 iter 57199 step
2022-08-29 22:13:23,137   Num examples = 9815
2022-08-29 22:13:23,137   Batch size = 32
2022-08-29 22:13:23,138 ***** Eval results *****
2022-08-29 22:13:23,138   att_loss = 3.9930796757790312
2022-08-29 22:13:23,139   cls_loss = 0.0
2022-08-29 22:13:23,139   global_step = 57199
2022-08-29 22:13:23,139   loss = 5.762689032939504
2022-08-29 22:13:23,139   rep_loss = 1.7696093564994233
2022-08-29 22:13:23,139 ***** Save model *****
2022-08-29 22:14:32,808 ***** Running evaluation *****
2022-08-29 22:14:32,809   Epoch = 4 iter 57399 step
2022-08-29 22:14:32,809   Num examples = 9815
2022-08-29 22:14:32,809   Batch size = 32
2022-08-29 22:14:32,811 ***** Eval results *****
2022-08-29 22:14:32,811   att_loss = 3.9925530547774564
2022-08-29 22:14:32,811   cls_loss = 0.0
2022-08-29 22:14:32,811   global_step = 57399
2022-08-29 22:14:32,811   loss = 5.762056806613649
2022-08-29 22:14:32,811   rep_loss = 1.7695037510906868
2022-08-29 22:14:32,811 ***** Save model *****
2022-08-29 22:15:44,792 ***** Running evaluation *****
2022-08-29 22:15:44,793   Epoch = 4 iter 57599 step
2022-08-29 22:15:44,793   Num examples = 9815
2022-08-29 22:15:44,793   Batch size = 32
2022-08-29 22:15:44,794 ***** Eval results *****
2022-08-29 22:15:44,794   att_loss = 3.9921403415330494
2022-08-29 22:15:44,794   cls_loss = 0.0
2022-08-29 22:15:44,794   global_step = 57599
2022-08-29 22:15:44,795   loss = 5.761537754206677
2022-08-29 22:15:44,795   rep_loss = 1.7693974120716311
2022-08-29 22:15:44,795 ***** Save model *****
2022-08-29 22:16:54,529 ***** Running evaluation *****
2022-08-29 22:16:54,530   Epoch = 4 iter 57799 step
2022-08-29 22:16:54,530   Num examples = 9815
2022-08-29 22:16:54,530   Batch size = 32
2022-08-29 22:16:54,531 ***** Eval results *****
2022-08-29 22:16:54,531   att_loss = 3.9927787553696406
2022-08-29 22:16:54,531   cls_loss = 0.0
2022-08-29 22:16:54,531   global_step = 57799
2022-08-29 22:16:54,531   loss = 5.76211938179572
2022-08-29 22:16:54,531   rep_loss = 1.7693406260977924
2022-08-29 22:16:54,532 ***** Save model *****
2022-08-29 22:18:06,480 ***** Running evaluation *****
2022-08-29 22:18:06,480   Epoch = 4 iter 57999 step
2022-08-29 22:18:06,480   Num examples = 9815
2022-08-29 22:18:06,480   Batch size = 32
2022-08-29 22:18:06,481 ***** Eval results *****
2022-08-29 22:18:06,482   att_loss = 3.9921542604385434
2022-08-29 22:18:06,482   cls_loss = 0.0
2022-08-29 22:18:06,482   global_step = 57999
2022-08-29 22:18:06,482   loss = 5.761385753511781
2022-08-29 22:18:06,482   rep_loss = 1.7692314927924302
2022-08-29 22:18:06,482 ***** Save model *****
2022-08-29 22:19:18,281 ***** Running evaluation *****
2022-08-29 22:19:18,281   Epoch = 4 iter 58199 step
2022-08-29 22:19:18,281   Num examples = 9815
2022-08-29 22:19:18,281   Batch size = 32
2022-08-29 22:19:18,282 ***** Eval results *****
2022-08-29 22:19:18,283   att_loss = 3.9923603999503547
2022-08-29 22:19:18,283   cls_loss = 0.0
2022-08-29 22:19:18,283   global_step = 58199
2022-08-29 22:19:18,283   loss = 5.761442362261682
2022-08-29 22:19:18,283   rep_loss = 1.7690819621151512
2022-08-29 22:19:18,283 ***** Save model *****
2022-08-29 22:20:27,654 ***** Running evaluation *****
2022-08-29 22:20:27,654   Epoch = 4 iter 58399 step
2022-08-29 22:20:27,654   Num examples = 9815
2022-08-29 22:20:27,654   Batch size = 32
2022-08-29 22:20:27,655 ***** Eval results *****
2022-08-29 22:20:27,656   att_loss = 3.991577725991471
2022-08-29 22:20:27,656   cls_loss = 0.0
2022-08-29 22:20:27,656   global_step = 58399
2022-08-29 22:20:27,656   loss = 5.760516662362181
2022-08-29 22:20:27,656   rep_loss = 1.7689389360251762
2022-08-29 22:20:27,656 ***** Save model *****
2022-08-29 22:21:37,524 ***** Running evaluation *****
2022-08-29 22:21:37,524   Epoch = 4 iter 58599 step
2022-08-29 22:21:37,524   Num examples = 9815
2022-08-29 22:21:37,524   Batch size = 32
2022-08-29 22:21:37,525 ***** Eval results *****
2022-08-29 22:21:37,526   att_loss = 3.990878266932397
2022-08-29 22:21:37,526   cls_loss = 0.0
2022-08-29 22:21:37,526   global_step = 58599
2022-08-29 22:21:37,526   loss = 5.759704530608698
2022-08-29 22:21:37,526   rep_loss = 1.7688262632002156
2022-08-29 22:21:37,526 ***** Save model *****
2022-08-29 22:22:50,328 ***** Running evaluation *****
2022-08-29 22:22:50,329   Epoch = 4 iter 58799 step
2022-08-29 22:22:50,329   Num examples = 9815
2022-08-29 22:22:50,329   Batch size = 32
2022-08-29 22:22:50,330 ***** Eval results *****
2022-08-29 22:22:50,330   att_loss = 3.989900754176644
2022-08-29 22:22:50,330   cls_loss = 0.0
2022-08-29 22:22:50,330   global_step = 58799
2022-08-29 22:22:50,330   loss = 5.758558777098918
2022-08-29 22:22:50,330   rep_loss = 1.7686580228486506
2022-08-29 22:22:50,330 ***** Save model *****
2022-08-29 22:24:00,882 ***** Running evaluation *****
2022-08-29 22:24:00,882   Epoch = 4 iter 58999 step
2022-08-29 22:24:00,882   Num examples = 9815
2022-08-29 22:24:00,883   Batch size = 32
2022-08-29 22:24:00,884 ***** Eval results *****
2022-08-29 22:24:00,884   att_loss = 3.9904768764882514
2022-08-29 22:24:00,884   cls_loss = 0.0
2022-08-29 22:24:00,884   global_step = 58999
2022-08-29 22:24:00,884   loss = 5.759011345275171
2022-08-29 22:24:00,884   rep_loss = 1.7685344689552434
2022-08-29 22:24:00,884 ***** Save model *****
2022-08-29 22:25:13,388 ***** Running evaluation *****
2022-08-29 22:25:13,389   Epoch = 4 iter 59199 step
2022-08-29 22:25:13,389   Num examples = 9815
2022-08-29 22:25:13,389   Batch size = 32
2022-08-29 22:25:13,390 ***** Eval results *****
2022-08-29 22:25:13,391   att_loss = 3.9897667760608577
2022-08-29 22:25:13,391   cls_loss = 0.0
2022-08-29 22:25:13,391   global_step = 59199
2022-08-29 22:25:13,391   loss = 5.758134084589341
2022-08-29 22:25:13,391   rep_loss = 1.7683673088938308
2022-08-29 22:25:13,391 ***** Save model *****
2022-08-29 22:26:24,442 ***** Running evaluation *****
2022-08-29 22:26:24,442   Epoch = 4 iter 59399 step
2022-08-29 22:26:24,442   Num examples = 9815
2022-08-29 22:26:24,442   Batch size = 32
2022-08-29 22:26:24,443 ***** Eval results *****
2022-08-29 22:26:24,443   att_loss = 3.988239360676191
2022-08-29 22:26:24,443   cls_loss = 0.0
2022-08-29 22:26:24,443   global_step = 59399
2022-08-29 22:26:24,443   loss = 5.756375878352999
2022-08-29 22:26:24,444   rep_loss = 1.7681365179310593
2022-08-29 22:26:24,444 ***** Save model *****
2022-08-29 22:27:33,392 ***** Running evaluation *****
2022-08-29 22:27:33,393   Epoch = 4 iter 59599 step
2022-08-29 22:27:33,393   Num examples = 9815
2022-08-29 22:27:33,393   Batch size = 32
2022-08-29 22:27:33,394 ***** Eval results *****
2022-08-29 22:27:33,395   att_loss = 3.9865775869961757
2022-08-29 22:27:33,395   cls_loss = 0.0
2022-08-29 22:27:33,395   global_step = 59599
2022-08-29 22:27:33,395   loss = 5.754536775560873
2022-08-29 22:27:33,395   rep_loss = 1.7679591887234167
2022-08-29 22:27:33,395 ***** Save model *****
2022-08-29 22:28:43,157 ***** Running evaluation *****
2022-08-29 22:28:43,158   Epoch = 4 iter 59799 step
2022-08-29 22:28:43,158   Num examples = 9815
2022-08-29 22:28:43,158   Batch size = 32
2022-08-29 22:28:43,159 ***** Eval results *****
2022-08-29 22:28:43,159   att_loss = 3.9865388607233414
2022-08-29 22:28:43,159   cls_loss = 0.0
2022-08-29 22:28:43,159   global_step = 59799
2022-08-29 22:28:43,159   loss = 5.754360038322987
2022-08-29 22:28:43,159   rep_loss = 1.7678211776107702
2022-08-29 22:28:43,159 ***** Save model *****
2022-08-29 22:29:55,742 ***** Running evaluation *****
2022-08-29 22:29:55,742   Epoch = 4 iter 59999 step
2022-08-29 22:29:55,742   Num examples = 9815
2022-08-29 22:29:55,742   Batch size = 32
2022-08-29 22:29:55,743 ***** Eval results *****
2022-08-29 22:29:55,743   att_loss = 3.9859656534007093
2022-08-29 22:29:55,744   cls_loss = 0.0
2022-08-29 22:29:55,744   global_step = 59999
2022-08-29 22:29:55,744   loss = 5.753605861506547
2022-08-29 22:29:55,744   rep_loss = 1.767640208127681
2022-08-29 22:29:55,744 ***** Save model *****
2022-08-29 22:31:06,179 ***** Running evaluation *****
2022-08-29 22:31:06,180   Epoch = 4 iter 60199 step
2022-08-29 22:31:06,180   Num examples = 9815
2022-08-29 22:31:06,180   Batch size = 32
2022-08-29 22:31:06,182 ***** Eval results *****
2022-08-29 22:31:06,182   att_loss = 3.9848805985684552
2022-08-29 22:31:06,182   cls_loss = 0.0
2022-08-29 22:31:06,182   global_step = 60199
2022-08-29 22:31:06,182   loss = 5.752364043957791
2022-08-29 22:31:06,182   rep_loss = 1.7674834455394874
2022-08-29 22:31:06,182 ***** Save model *****
2022-08-29 22:32:18,825 ***** Running evaluation *****
2022-08-29 22:32:18,825   Epoch = 4 iter 60399 step
2022-08-29 22:32:18,825   Num examples = 9815
2022-08-29 22:32:18,826   Batch size = 32
2022-08-29 22:32:18,827 ***** Eval results *****
2022-08-29 22:32:18,827   att_loss = 3.9846523161014518
2022-08-29 22:32:18,827   cls_loss = 0.0
2022-08-29 22:32:18,827   global_step = 60399
2022-08-29 22:32:18,827   loss = 5.752009586944445
2022-08-29 22:32:18,827   rep_loss = 1.7673572711063812
2022-08-29 22:32:18,827 ***** Save model *****
2022-08-29 22:33:31,503 ***** Running evaluation *****
2022-08-29 22:33:31,504   Epoch = 4 iter 60599 step
2022-08-29 22:33:31,504   Num examples = 9815
2022-08-29 22:33:31,504   Batch size = 32
2022-08-29 22:33:31,505 ***** Eval results *****
2022-08-29 22:33:31,505   att_loss = 3.9842297513434226
2022-08-29 22:33:31,505   cls_loss = 0.0
2022-08-29 22:33:31,505   global_step = 60599
2022-08-29 22:33:31,506   loss = 5.751442164935809
2022-08-29 22:33:31,506   rep_loss = 1.7672124137890841
2022-08-29 22:33:31,506 ***** Save model *****
2022-08-29 22:34:42,806 ***** Running evaluation *****
2022-08-29 22:34:42,806   Epoch = 4 iter 60799 step
2022-08-29 22:34:42,806   Num examples = 9815
2022-08-29 22:34:42,806   Batch size = 32
2022-08-29 22:34:42,807 ***** Eval results *****
2022-08-29 22:34:42,807   att_loss = 3.9840074749416234
2022-08-29 22:34:42,807   cls_loss = 0.0
2022-08-29 22:34:42,808   global_step = 60799
2022-08-29 22:34:42,808   loss = 5.7510624567260615
2022-08-29 22:34:42,808   rep_loss = 1.7670549819370747
2022-08-29 22:34:42,808 ***** Save model *****
2022-08-29 22:35:52,219 ***** Running evaluation *****
2022-08-29 22:35:52,219   Epoch = 4 iter 60999 step
2022-08-29 22:35:52,219   Num examples = 9815
2022-08-29 22:35:52,219   Batch size = 32
2022-08-29 22:35:52,221 ***** Eval results *****
2022-08-29 22:35:52,221   att_loss = 3.984445751488734
2022-08-29 22:35:52,221   cls_loss = 0.0
2022-08-29 22:35:52,221   global_step = 60999
2022-08-29 22:35:52,221   loss = 5.751423662748349
2022-08-29 22:35:52,221   rep_loss = 1.7669779114096895
2022-08-29 22:35:52,221 ***** Save model *****
2022-08-29 22:37:04,674 ***** Running evaluation *****
2022-08-29 22:37:04,674   Epoch = 4 iter 61199 step
2022-08-29 22:37:04,674   Num examples = 9815
2022-08-29 22:37:04,674   Batch size = 32
2022-08-29 22:37:04,676 ***** Eval results *****
2022-08-29 22:37:04,676   att_loss = 3.9837138243662245
2022-08-29 22:37:04,676   cls_loss = 0.0
2022-08-29 22:37:04,676   global_step = 61199
2022-08-29 22:37:04,676   loss = 5.750529074462248
2022-08-29 22:37:04,676   rep_loss = 1.7668152502042618
2022-08-29 22:37:04,676 ***** Save model *****
2022-08-29 22:38:03,388 The args: Namespace(aug_train=False, cache_dir='', data_dir='glue_data/MNLI', data_url='', do_eval=False, do_lower_case=True, eval_batch_size=32, eval_step=100, gradient_accumulation_steps=1, hidden_act='quad', learning_rate=1e-05, log_path='tmp/distill/MNLI/quad_2quad/bert-base-uncased/log.txt', max_seq_length=128, no_cuda=False, num_train_epochs=5.0, output_dir='tmp/distill/MNLI/quad_2quad/bert-base-uncased/5e-05_1e-05_32_stage2', pred_distill=True, seed=42, softmax_act='2quad', student_model='tmp/distill/MNLI/quad_2quad/bert-base-uncased/5e-05_1e-05_32', task_name='MNLI', teacher_model='/home/ubuntu/checkpoints/exp/MNLI', temperature=1.0, train_batch_size=32, warmup_proportion=0.1, weight_decay=0.0001)
2022-08-29 22:38:03,388 device: cuda n_gpu: 2
2022-08-29 22:38:10,410 Writing example 0 of 392702
2022-08-29 22:38:10,412 *** Example ***
2022-08-29 22:38:10,412 guid: train-0
2022-08-29 22:38:10,412 tokens: [CLS] conceptual ##ly cream ski ##mming has two basic dimensions - product and geography . [SEP] product and geography are what make cream ski ##mming work . [SEP]
2022-08-29 22:38:10,412 input_ids: 101 17158 2135 6949 8301 25057 2038 2048 3937 9646 1011 4031 1998 10505 1012 102 4031 1998 10505 2024 2054 2191 6949 8301 25057 2147 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-08-29 22:38:10,412 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-08-29 22:38:10,412 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-08-29 22:38:10,412 label: neutral
2022-08-29 22:38:10,412 label_id: 1
2022-08-29 22:38:16,338 Writing example 10000 of 392702
2022-08-29 22:38:22,070 Writing example 20000 of 392702
2022-08-29 22:38:27,805 Writing example 30000 of 392702
2022-08-29 22:38:33,860 Writing example 40000 of 392702
2022-08-29 22:38:39,676 Writing example 50000 of 392702
2022-08-29 22:38:45,425 Writing example 60000 of 392702
2022-08-29 22:38:51,231 Writing example 70000 of 392702
2022-08-29 22:38:57,516 Writing example 80000 of 392702
2022-08-29 22:39:03,308 Writing example 90000 of 392702
2022-08-29 22:39:09,067 Writing example 100000 of 392702
2022-08-29 22:39:14,847 Writing example 110000 of 392702
2022-08-29 22:39:20,669 Writing example 120000 of 392702
2022-08-29 22:39:27,171 Writing example 130000 of 392702
2022-08-29 22:39:32,963 Writing example 140000 of 392702
2022-08-29 22:39:38,743 Writing example 150000 of 392702
2022-08-29 22:39:44,577 Writing example 160000 of 392702
2022-08-29 22:39:50,370 Writing example 170000 of 392702
2022-08-29 22:39:56,187 Writing example 180000 of 392702
2022-08-29 22:40:02,950 Writing example 190000 of 392702
2022-08-29 22:40:08,770 Writing example 200000 of 392702
2022-08-29 22:40:14,572 Writing example 210000 of 392702
2022-08-29 22:40:20,362 Writing example 220000 of 392702
2022-08-29 22:40:26,196 Writing example 230000 of 392702
2022-08-29 22:40:31,940 Writing example 240000 of 392702
2022-08-29 22:40:37,724 Writing example 250000 of 392702
2022-08-29 22:40:44,905 Writing example 260000 of 392702
2022-08-29 22:40:50,732 Writing example 270000 of 392702
2022-08-29 22:40:56,449 Writing example 280000 of 392702
2022-08-29 22:41:02,217 Writing example 290000 of 392702
2022-08-29 22:41:07,973 Writing example 300000 of 392702
2022-08-29 22:41:13,815 Writing example 310000 of 392702
2022-08-29 22:41:19,622 Writing example 320000 of 392702
2022-08-29 22:41:25,409 Writing example 330000 of 392702
2022-08-29 22:41:31,259 Writing example 340000 of 392702
2022-08-29 22:41:38,793 Writing example 350000 of 392702
2022-08-29 22:41:44,603 Writing example 360000 of 392702
2022-08-29 22:41:50,404 Writing example 370000 of 392702
2022-08-29 22:41:56,251 Writing example 380000 of 392702
2022-08-29 22:42:02,057 Writing example 390000 of 392702
2022-08-29 22:42:07,765 Writing example 0 of 9815
2022-08-29 22:42:07,765 *** Example ***
2022-08-29 22:42:07,765 guid: dev_matched-0
2022-08-29 22:42:07,766 tokens: [CLS] the new rights are nice enough [SEP] everyone really likes the newest benefits [SEP]
2022-08-29 22:42:07,766 input_ids: 101 1996 2047 2916 2024 3835 2438 102 3071 2428 7777 1996 14751 6666 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-08-29 22:42:07,766 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-08-29 22:42:07,766 segment_ids: 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-08-29 22:42:07,766 label: neutral
2022-08-29 22:42:07,766 label_id: 1
2022-08-29 22:42:13,440 Model config {
  "_name_or_path": "bert-base-uncased",
  "architectures": [
    "BertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "crypten": false,
  "finetuning_task": "mnli",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "entailment",
    "1": "neutral",
    "2": "contradiction"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "contradiction": 2,
    "entailment": 0,
    "neutral": 1
  },
  "layer_norm_eps": 1e-12,
  "log_path": "tmp/distill/MNLI/quad_2quad/bert-base-uncased/log.txt",
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "pre_trained": "",
  "problem_type": "single_label_classification",
  "softmax_act": "softmax",
  "torch_dtype": "float32",
  "training": "",
  "transformers_version": "4.20.0.dev0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

2022-08-29 22:42:16,086 Loading model /home/ubuntu/checkpoints/exp/MNLI/pytorch_model.bin
2022-08-29 22:42:16,417 loading model...
2022-08-29 22:42:16,464 done!
2022-08-29 22:42:16,465 Weights from pretrained model not used in TinyBertForSequenceClassification: ['bert.embeddings.position_ids']
2022-08-29 22:42:43,164 ***** Teacher evaluation *****
2022-08-29 22:42:43,164 {'acc': 0.8473764645950076, 'eval_loss': 0.5279645242209543}
2022-08-29 22:42:43,165 Model config {
  "_name_or_path": "bert-base-uncased",
  "architectures": [
    "BertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "crypten": false,
  "finetuning_task": "mnli",
  "gradient_checkpointing": false,
  "hidden_act": "quad",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "entailment",
    "1": "neutral",
    "2": "contradiction"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "contradiction": 2,
    "entailment": 0,
    "neutral": 1
  },
  "layer_norm_eps": 1e-12,
  "log_path": "tmp/distill/MNLI/quad_2quad/bert-base-uncased/log.txt",
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "pre_trained": "",
  "problem_type": "single_label_classification",
  "softmax_act": "2quad",
  "torch_dtype": "float32",
  "training": "",
  "transformers_version": "4.20.0.dev0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

2022-08-29 22:42:45,778 Loading model tmp/distill/MNLI/quad_2quad/bert-base-uncased/5e-05_1e-05_32/pytorch_model.bin
2022-08-29 22:42:46,073 loading model...
2022-08-29 22:42:46,119 done!
2022-08-29 22:42:46,198 ***** Running training *****
2022-08-29 22:42:46,198   Num examples = 392702
2022-08-29 22:42:46,198   Batch size = 32
2022-08-29 22:42:46,198   Num steps = 61355
2022-08-29 22:42:46,200 n: module.bert.embeddings.word_embeddings.weight
2022-08-29 22:42:46,200 n: module.bert.embeddings.position_embeddings.weight
2022-08-29 22:42:46,200 n: module.bert.embeddings.token_type_embeddings.weight
2022-08-29 22:42:46,200 n: module.bert.embeddings.LayerNorm.weight
2022-08-29 22:42:46,200 n: module.bert.embeddings.LayerNorm.bias
2022-08-29 22:42:46,200 n: module.bert.encoder.layer.0.attention.self.query.weight
2022-08-29 22:42:46,200 n: module.bert.encoder.layer.0.attention.self.query.bias
2022-08-29 22:42:46,200 n: module.bert.encoder.layer.0.attention.self.key.weight
2022-08-29 22:42:46,200 n: module.bert.encoder.layer.0.attention.self.key.bias
2022-08-29 22:42:46,201 n: module.bert.encoder.layer.0.attention.self.value.weight
2022-08-29 22:42:46,201 n: module.bert.encoder.layer.0.attention.self.value.bias
2022-08-29 22:42:46,201 n: module.bert.encoder.layer.0.attention.output.dense.weight
2022-08-29 22:42:46,201 n: module.bert.encoder.layer.0.attention.output.dense.bias
2022-08-29 22:42:46,201 n: module.bert.encoder.layer.0.attention.output.LayerNorm.weight
2022-08-29 22:42:46,201 n: module.bert.encoder.layer.0.attention.output.LayerNorm.bias
2022-08-29 22:42:46,201 n: module.bert.encoder.layer.0.intermediate.dense.weight
2022-08-29 22:42:46,201 n: module.bert.encoder.layer.0.intermediate.dense.bias
2022-08-29 22:42:46,201 n: module.bert.encoder.layer.0.output.dense.weight
2022-08-29 22:42:46,201 n: module.bert.encoder.layer.0.output.dense.bias
2022-08-29 22:42:46,201 n: module.bert.encoder.layer.0.output.LayerNorm.weight
2022-08-29 22:42:46,201 n: module.bert.encoder.layer.0.output.LayerNorm.bias
2022-08-29 22:42:46,201 n: module.bert.encoder.layer.1.attention.self.query.weight
2022-08-29 22:42:46,201 n: module.bert.encoder.layer.1.attention.self.query.bias
2022-08-29 22:42:46,201 n: module.bert.encoder.layer.1.attention.self.key.weight
2022-08-29 22:42:46,201 n: module.bert.encoder.layer.1.attention.self.key.bias
2022-08-29 22:42:46,201 n: module.bert.encoder.layer.1.attention.self.value.weight
2022-08-29 22:42:46,202 n: module.bert.encoder.layer.1.attention.self.value.bias
2022-08-29 22:42:46,202 n: module.bert.encoder.layer.1.attention.output.dense.weight
2022-08-29 22:42:46,202 n: module.bert.encoder.layer.1.attention.output.dense.bias
2022-08-29 22:42:46,202 n: module.bert.encoder.layer.1.attention.output.LayerNorm.weight
2022-08-29 22:42:46,202 n: module.bert.encoder.layer.1.attention.output.LayerNorm.bias
2022-08-29 22:42:46,202 n: module.bert.encoder.layer.1.intermediate.dense.weight
2022-08-29 22:42:46,202 n: module.bert.encoder.layer.1.intermediate.dense.bias
2022-08-29 22:42:46,202 n: module.bert.encoder.layer.1.output.dense.weight
2022-08-29 22:42:46,202 n: module.bert.encoder.layer.1.output.dense.bias
2022-08-29 22:42:46,202 n: module.bert.encoder.layer.1.output.LayerNorm.weight
2022-08-29 22:42:46,202 n: module.bert.encoder.layer.1.output.LayerNorm.bias
2022-08-29 22:42:46,202 n: module.bert.encoder.layer.2.attention.self.query.weight
2022-08-29 22:42:46,202 n: module.bert.encoder.layer.2.attention.self.query.bias
2022-08-29 22:42:46,202 n: module.bert.encoder.layer.2.attention.self.key.weight
2022-08-29 22:42:46,202 n: module.bert.encoder.layer.2.attention.self.key.bias
2022-08-29 22:42:46,202 n: module.bert.encoder.layer.2.attention.self.value.weight
2022-08-29 22:42:46,202 n: module.bert.encoder.layer.2.attention.self.value.bias
2022-08-29 22:42:46,203 n: module.bert.encoder.layer.2.attention.output.dense.weight
2022-08-29 22:42:46,203 n: module.bert.encoder.layer.2.attention.output.dense.bias
2022-08-29 22:42:46,203 n: module.bert.encoder.layer.2.attention.output.LayerNorm.weight
2022-08-29 22:42:46,203 n: module.bert.encoder.layer.2.attention.output.LayerNorm.bias
2022-08-29 22:42:46,203 n: module.bert.encoder.layer.2.intermediate.dense.weight
2022-08-29 22:42:46,203 n: module.bert.encoder.layer.2.intermediate.dense.bias
2022-08-29 22:42:46,203 n: module.bert.encoder.layer.2.output.dense.weight
2022-08-29 22:42:46,203 n: module.bert.encoder.layer.2.output.dense.bias
2022-08-29 22:42:46,203 n: module.bert.encoder.layer.2.output.LayerNorm.weight
2022-08-29 22:42:46,203 n: module.bert.encoder.layer.2.output.LayerNorm.bias
2022-08-29 22:42:46,203 n: module.bert.encoder.layer.3.attention.self.query.weight
2022-08-29 22:42:46,203 n: module.bert.encoder.layer.3.attention.self.query.bias
2022-08-29 22:42:46,203 n: module.bert.encoder.layer.3.attention.self.key.weight
2022-08-29 22:42:46,203 n: module.bert.encoder.layer.3.attention.self.key.bias
2022-08-29 22:42:46,203 n: module.bert.encoder.layer.3.attention.self.value.weight
2022-08-29 22:42:46,203 n: module.bert.encoder.layer.3.attention.self.value.bias
2022-08-29 22:42:46,203 n: module.bert.encoder.layer.3.attention.output.dense.weight
2022-08-29 22:42:46,204 n: module.bert.encoder.layer.3.attention.output.dense.bias
2022-08-29 22:42:46,204 n: module.bert.encoder.layer.3.attention.output.LayerNorm.weight
2022-08-29 22:42:46,204 n: module.bert.encoder.layer.3.attention.output.LayerNorm.bias
2022-08-29 22:42:46,204 n: module.bert.encoder.layer.3.intermediate.dense.weight
2022-08-29 22:42:46,204 n: module.bert.encoder.layer.3.intermediate.dense.bias
2022-08-29 22:42:46,204 n: module.bert.encoder.layer.3.output.dense.weight
2022-08-29 22:42:46,204 n: module.bert.encoder.layer.3.output.dense.bias
2022-08-29 22:42:46,204 n: module.bert.encoder.layer.3.output.LayerNorm.weight
2022-08-29 22:42:46,204 n: module.bert.encoder.layer.3.output.LayerNorm.bias
2022-08-29 22:42:46,204 n: module.bert.encoder.layer.4.attention.self.query.weight
2022-08-29 22:42:46,204 n: module.bert.encoder.layer.4.attention.self.query.bias
2022-08-29 22:42:46,204 n: module.bert.encoder.layer.4.attention.self.key.weight
2022-08-29 22:42:46,204 n: module.bert.encoder.layer.4.attention.self.key.bias
2022-08-29 22:42:46,204 n: module.bert.encoder.layer.4.attention.self.value.weight
2022-08-29 22:42:46,204 n: module.bert.encoder.layer.4.attention.self.value.bias
2022-08-29 22:42:46,204 n: module.bert.encoder.layer.4.attention.output.dense.weight
2022-08-29 22:42:46,204 n: module.bert.encoder.layer.4.attention.output.dense.bias
2022-08-29 22:42:46,205 n: module.bert.encoder.layer.4.attention.output.LayerNorm.weight
2022-08-29 22:42:46,205 n: module.bert.encoder.layer.4.attention.output.LayerNorm.bias
2022-08-29 22:42:46,205 n: module.bert.encoder.layer.4.intermediate.dense.weight
2022-08-29 22:42:46,205 n: module.bert.encoder.layer.4.intermediate.dense.bias
2022-08-29 22:42:46,205 n: module.bert.encoder.layer.4.output.dense.weight
2022-08-29 22:42:46,205 n: module.bert.encoder.layer.4.output.dense.bias
2022-08-29 22:42:46,205 n: module.bert.encoder.layer.4.output.LayerNorm.weight
2022-08-29 22:42:46,205 n: module.bert.encoder.layer.4.output.LayerNorm.bias
2022-08-29 22:42:46,205 n: module.bert.encoder.layer.5.attention.self.query.weight
2022-08-29 22:42:46,205 n: module.bert.encoder.layer.5.attention.self.query.bias
2022-08-29 22:42:46,205 n: module.bert.encoder.layer.5.attention.self.key.weight
2022-08-29 22:42:46,205 n: module.bert.encoder.layer.5.attention.self.key.bias
2022-08-29 22:42:46,205 n: module.bert.encoder.layer.5.attention.self.value.weight
2022-08-29 22:42:46,205 n: module.bert.encoder.layer.5.attention.self.value.bias
2022-08-29 22:42:46,205 n: module.bert.encoder.layer.5.attention.output.dense.weight
2022-08-29 22:42:46,205 n: module.bert.encoder.layer.5.attention.output.dense.bias
2022-08-29 22:42:46,205 n: module.bert.encoder.layer.5.attention.output.LayerNorm.weight
2022-08-29 22:42:46,206 n: module.bert.encoder.layer.5.attention.output.LayerNorm.bias
2022-08-29 22:42:46,206 n: module.bert.encoder.layer.5.intermediate.dense.weight
2022-08-29 22:42:46,206 n: module.bert.encoder.layer.5.intermediate.dense.bias
2022-08-29 22:42:46,206 n: module.bert.encoder.layer.5.output.dense.weight
2022-08-29 22:42:46,206 n: module.bert.encoder.layer.5.output.dense.bias
2022-08-29 22:42:46,206 n: module.bert.encoder.layer.5.output.LayerNorm.weight
2022-08-29 22:42:46,206 n: module.bert.encoder.layer.5.output.LayerNorm.bias
2022-08-29 22:42:46,206 n: module.bert.encoder.layer.6.attention.self.query.weight
2022-08-29 22:42:46,206 n: module.bert.encoder.layer.6.attention.self.query.bias
2022-08-29 22:42:46,206 n: module.bert.encoder.layer.6.attention.self.key.weight
2022-08-29 22:42:46,206 n: module.bert.encoder.layer.6.attention.self.key.bias
2022-08-29 22:42:46,206 n: module.bert.encoder.layer.6.attention.self.value.weight
2022-08-29 22:42:46,206 n: module.bert.encoder.layer.6.attention.self.value.bias
2022-08-29 22:42:46,206 n: module.bert.encoder.layer.6.attention.output.dense.weight
2022-08-29 22:42:46,206 n: module.bert.encoder.layer.6.attention.output.dense.bias
2022-08-29 22:42:46,206 n: module.bert.encoder.layer.6.attention.output.LayerNorm.weight
2022-08-29 22:42:46,206 n: module.bert.encoder.layer.6.attention.output.LayerNorm.bias
2022-08-29 22:42:46,207 n: module.bert.encoder.layer.6.intermediate.dense.weight
2022-08-29 22:42:46,207 n: module.bert.encoder.layer.6.intermediate.dense.bias
2022-08-29 22:42:46,207 n: module.bert.encoder.layer.6.output.dense.weight
2022-08-29 22:42:46,207 n: module.bert.encoder.layer.6.output.dense.bias
2022-08-29 22:42:46,207 n: module.bert.encoder.layer.6.output.LayerNorm.weight
2022-08-29 22:42:46,207 n: module.bert.encoder.layer.6.output.LayerNorm.bias
2022-08-29 22:42:46,207 n: module.bert.encoder.layer.7.attention.self.query.weight
2022-08-29 22:42:46,207 n: module.bert.encoder.layer.7.attention.self.query.bias
2022-08-29 22:42:46,207 n: module.bert.encoder.layer.7.attention.self.key.weight
2022-08-29 22:42:46,207 n: module.bert.encoder.layer.7.attention.self.key.bias
2022-08-29 22:42:46,207 n: module.bert.encoder.layer.7.attention.self.value.weight
2022-08-29 22:42:46,207 n: module.bert.encoder.layer.7.attention.self.value.bias
2022-08-29 22:42:46,207 n: module.bert.encoder.layer.7.attention.output.dense.weight
2022-08-29 22:42:46,207 n: module.bert.encoder.layer.7.attention.output.dense.bias
2022-08-29 22:42:46,207 n: module.bert.encoder.layer.7.attention.output.LayerNorm.weight
2022-08-29 22:42:46,207 n: module.bert.encoder.layer.7.attention.output.LayerNorm.bias
2022-08-29 22:42:46,207 n: module.bert.encoder.layer.7.intermediate.dense.weight
2022-08-29 22:42:46,208 n: module.bert.encoder.layer.7.intermediate.dense.bias
2022-08-29 22:42:46,208 n: module.bert.encoder.layer.7.output.dense.weight
2022-08-29 22:42:46,208 n: module.bert.encoder.layer.7.output.dense.bias
2022-08-29 22:42:46,208 n: module.bert.encoder.layer.7.output.LayerNorm.weight
2022-08-29 22:42:46,208 n: module.bert.encoder.layer.7.output.LayerNorm.bias
2022-08-29 22:42:46,208 n: module.bert.encoder.layer.8.attention.self.query.weight
2022-08-29 22:42:46,208 n: module.bert.encoder.layer.8.attention.self.query.bias
2022-08-29 22:42:46,208 n: module.bert.encoder.layer.8.attention.self.key.weight
2022-08-29 22:42:46,208 n: module.bert.encoder.layer.8.attention.self.key.bias
2022-08-29 22:42:46,208 n: module.bert.encoder.layer.8.attention.self.value.weight
2022-08-29 22:42:46,208 n: module.bert.encoder.layer.8.attention.self.value.bias
2022-08-29 22:42:46,208 n: module.bert.encoder.layer.8.attention.output.dense.weight
2022-08-29 22:42:46,208 n: module.bert.encoder.layer.8.attention.output.dense.bias
2022-08-29 22:42:46,208 n: module.bert.encoder.layer.8.attention.output.LayerNorm.weight
2022-08-29 22:42:46,208 n: module.bert.encoder.layer.8.attention.output.LayerNorm.bias
2022-08-29 22:42:46,208 n: module.bert.encoder.layer.8.intermediate.dense.weight
2022-08-29 22:42:46,208 n: module.bert.encoder.layer.8.intermediate.dense.bias
2022-08-29 22:42:46,208 n: module.bert.encoder.layer.8.output.dense.weight
2022-08-29 22:42:46,209 n: module.bert.encoder.layer.8.output.dense.bias
2022-08-29 22:42:46,209 n: module.bert.encoder.layer.8.output.LayerNorm.weight
2022-08-29 22:42:46,209 n: module.bert.encoder.layer.8.output.LayerNorm.bias
2022-08-29 22:42:46,209 n: module.bert.encoder.layer.9.attention.self.query.weight
2022-08-29 22:42:46,209 n: module.bert.encoder.layer.9.attention.self.query.bias
2022-08-29 22:42:46,209 n: module.bert.encoder.layer.9.attention.self.key.weight
2022-08-29 22:42:46,209 n: module.bert.encoder.layer.9.attention.self.key.bias
2022-08-29 22:42:46,209 n: module.bert.encoder.layer.9.attention.self.value.weight
2022-08-29 22:42:46,209 n: module.bert.encoder.layer.9.attention.self.value.bias
2022-08-29 22:42:46,209 n: module.bert.encoder.layer.9.attention.output.dense.weight
2022-08-29 22:42:46,209 n: module.bert.encoder.layer.9.attention.output.dense.bias
2022-08-29 22:42:46,209 n: module.bert.encoder.layer.9.attention.output.LayerNorm.weight
2022-08-29 22:42:46,209 n: module.bert.encoder.layer.9.attention.output.LayerNorm.bias
2022-08-29 22:42:46,209 n: module.bert.encoder.layer.9.intermediate.dense.weight
2022-08-29 22:42:46,209 n: module.bert.encoder.layer.9.intermediate.dense.bias
2022-08-29 22:42:46,209 n: module.bert.encoder.layer.9.output.dense.weight
2022-08-29 22:42:46,209 n: module.bert.encoder.layer.9.output.dense.bias
2022-08-29 22:42:46,210 n: module.bert.encoder.layer.9.output.LayerNorm.weight
2022-08-29 22:42:46,210 n: module.bert.encoder.layer.9.output.LayerNorm.bias
2022-08-29 22:42:46,210 n: module.bert.encoder.layer.10.attention.self.query.weight
2022-08-29 22:42:46,210 n: module.bert.encoder.layer.10.attention.self.query.bias
2022-08-29 22:42:46,210 n: module.bert.encoder.layer.10.attention.self.key.weight
2022-08-29 22:42:46,210 n: module.bert.encoder.layer.10.attention.self.key.bias
2022-08-29 22:42:46,210 n: module.bert.encoder.layer.10.attention.self.value.weight
2022-08-29 22:42:46,210 n: module.bert.encoder.layer.10.attention.self.value.bias
2022-08-29 22:42:46,210 n: module.bert.encoder.layer.10.attention.output.dense.weight
2022-08-29 22:42:46,210 n: module.bert.encoder.layer.10.attention.output.dense.bias
2022-08-29 22:42:46,210 n: module.bert.encoder.layer.10.attention.output.LayerNorm.weight
2022-08-29 22:42:46,210 n: module.bert.encoder.layer.10.attention.output.LayerNorm.bias
2022-08-29 22:42:46,210 n: module.bert.encoder.layer.10.intermediate.dense.weight
2022-08-29 22:42:46,210 n: module.bert.encoder.layer.10.intermediate.dense.bias
2022-08-29 22:42:46,210 n: module.bert.encoder.layer.10.output.dense.weight
2022-08-29 22:42:46,210 n: module.bert.encoder.layer.10.output.dense.bias
2022-08-29 22:42:46,210 n: module.bert.encoder.layer.10.output.LayerNorm.weight
2022-08-29 22:42:46,211 n: module.bert.encoder.layer.10.output.LayerNorm.bias
2022-08-29 22:42:46,211 n: module.bert.encoder.layer.11.attention.self.query.weight
2022-08-29 22:42:46,211 n: module.bert.encoder.layer.11.attention.self.query.bias
2022-08-29 22:42:46,211 n: module.bert.encoder.layer.11.attention.self.key.weight
2022-08-29 22:42:46,211 n: module.bert.encoder.layer.11.attention.self.key.bias
2022-08-29 22:42:46,211 n: module.bert.encoder.layer.11.attention.self.value.weight
2022-08-29 22:42:46,211 n: module.bert.encoder.layer.11.attention.self.value.bias
2022-08-29 22:42:46,211 n: module.bert.encoder.layer.11.attention.output.dense.weight
2022-08-29 22:42:46,211 n: module.bert.encoder.layer.11.attention.output.dense.bias
2022-08-29 22:42:46,211 n: module.bert.encoder.layer.11.attention.output.LayerNorm.weight
2022-08-29 22:42:46,211 n: module.bert.encoder.layer.11.attention.output.LayerNorm.bias
2022-08-29 22:42:46,211 n: module.bert.encoder.layer.11.intermediate.dense.weight
2022-08-29 22:42:46,211 n: module.bert.encoder.layer.11.intermediate.dense.bias
2022-08-29 22:42:46,211 n: module.bert.encoder.layer.11.output.dense.weight
2022-08-29 22:42:46,211 n: module.bert.encoder.layer.11.output.dense.bias
2022-08-29 22:42:46,211 n: module.bert.encoder.layer.11.output.LayerNorm.weight
2022-08-29 22:42:46,211 n: module.bert.encoder.layer.11.output.LayerNorm.bias
2022-08-29 22:42:46,212 n: module.bert.pooler.dense.weight
2022-08-29 22:42:46,212 n: module.bert.pooler.dense.bias
2022-08-29 22:42:46,212 n: module.classifier.weight
2022-08-29 22:42:46,212 n: module.classifier.bias
2022-08-29 22:42:46,212 Total parameters: 109484547
2022-08-29 22:43:19,258 ***** Running evaluation *****
2022-08-29 22:43:19,259   Epoch = 0 iter 99 step
2022-08-29 22:43:19,259   Num examples = 9815
2022-08-29 22:43:19,259   Batch size = 32
2022-08-29 22:43:41,529 ***** Eval results *****
2022-08-29 22:43:41,530   acc = 0.8258787570045848
2022-08-29 22:43:41,530   att_loss = 0.0
2022-08-29 22:43:41,530   cls_loss = 0.11095236595531907
2022-08-29 22:43:41,530   eval_loss = 0.5546451818671211
2022-08-29 22:43:41,530   global_step = 99
2022-08-29 22:43:41,530   loss = 0.11095236595531907
2022-08-29 22:43:41,530   rep_loss = 0.0
2022-08-29 22:43:41,530 ***** Save model *****
2022-08-29 22:43:42,432 Writing example 0 of 9832
2022-08-29 22:43:42,433 *** Example ***
2022-08-29 22:43:42,433 guid: dev_matched-0
2022-08-29 22:43:42,433 tokens: [CLS] your contribution helped make it possible for us to provide our students with a quality education . [SEP] your contributions were of no help with our students ' education . [SEP]
2022-08-29 22:43:42,433 input_ids: 101 2115 6691 3271 2191 2009 2825 2005 2149 2000 3073 2256 2493 2007 1037 3737 2495 1012 102 2115 5857 2020 1997 2053 2393 2007 2256 2493 1005 2495 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-08-29 22:43:42,433 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-08-29 22:43:42,433 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-08-29 22:43:42,433 label: contradiction
2022-08-29 22:43:42,433 label_id: 2
2022-08-29 22:43:48,429 ***** Running mm evaluation *****
2022-08-29 22:43:48,430   Num examples = 9832
2022-08-29 22:43:48,430   Batch size = 32
2022-08-29 22:44:08,812 ***** Eval results *****
2022-08-29 22:44:08,812   acc = 0.8285191212367778
2022-08-29 22:44:08,812   eval_loss = 0.5293935950748719
2022-08-29 22:44:08,812   global_step = 99
2022-08-29 22:44:39,929 ***** Running evaluation *****
2022-08-29 22:44:39,930   Epoch = 0 iter 199 step
2022-08-29 22:44:39,930   Num examples = 9832
2022-08-29 22:44:39,930   Batch size = 32
2022-08-29 22:45:02,435 ***** Eval results *****
2022-08-29 22:45:02,435   acc = 0.8295362082994304
2022-08-29 22:45:02,435   att_loss = 0.0
2022-08-29 22:45:02,435   cls_loss = 0.10994128605828213
2022-08-29 22:45:02,435   eval_loss = 0.49083898443873825
2022-08-29 22:45:02,435   global_step = 199
2022-08-29 22:45:02,435   loss = 0.10994128605828213
2022-08-29 22:45:02,435   rep_loss = 0.0
2022-08-29 22:45:02,435 ***** Save model *****
2022-08-29 22:45:05,636 Writing example 0 of 9832
2022-08-29 22:45:05,637 *** Example ***
2022-08-29 22:45:05,637 guid: dev_matched-0
2022-08-29 22:45:05,637 tokens: [CLS] your contribution helped make it possible for us to provide our students with a quality education . [SEP] your contributions were of no help with our students ' education . [SEP]
2022-08-29 22:45:05,637 input_ids: 101 2115 6691 3271 2191 2009 2825 2005 2149 2000 3073 2256 2493 2007 1037 3737 2495 1012 102 2115 5857 2020 1997 2053 2393 2007 2256 2493 1005 2495 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-08-29 22:45:05,637 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-08-29 22:45:05,637 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-08-29 22:45:05,638 label: contradiction
2022-08-29 22:45:05,638 label_id: 2
2022-08-29 22:45:11,649 ***** Running mm evaluation *****
2022-08-29 22:45:11,649   Num examples = 9832
2022-08-29 22:45:11,649   Batch size = 32
2022-08-29 22:45:31,994 ***** Eval results *****
2022-08-29 22:45:31,994   acc = 0.8295362082994304
2022-08-29 22:45:31,994   eval_loss = 0.49083898443873825
2022-08-29 22:45:31,994   global_step = 199
2022-08-29 22:46:02,749 ***** Running evaluation *****
2022-08-29 22:46:02,749   Epoch = 0 iter 299 step
2022-08-29 22:46:02,749   Num examples = 9832
2022-08-29 22:46:02,749   Batch size = 32
2022-08-29 22:46:25,190 ***** Eval results *****
2022-08-29 22:46:25,190   acc = 0.8310618388934092
2022-08-29 22:46:25,190   att_loss = 0.0
2022-08-29 22:46:25,190   cls_loss = 0.10630440179968757
2022-08-29 22:46:25,190   eval_loss = 0.4839427738220661
2022-08-29 22:46:25,190   global_step = 299
2022-08-29 22:46:25,190   loss = 0.10630440179968757
2022-08-29 22:46:25,191   rep_loss = 0.0
2022-08-29 22:46:25,191 ***** Save model *****
2022-08-29 22:46:28,395 Writing example 0 of 9832
2022-08-29 22:46:28,396 *** Example ***
2022-08-29 22:46:28,396 guid: dev_matched-0
2022-08-29 22:46:28,396 tokens: [CLS] your contribution helped make it possible for us to provide our students with a quality education . [SEP] your contributions were of no help with our students ' education . [SEP]
2022-08-29 22:46:28,396 input_ids: 101 2115 6691 3271 2191 2009 2825 2005 2149 2000 3073 2256 2493 2007 1037 3737 2495 1012 102 2115 5857 2020 1997 2053 2393 2007 2256 2493 1005 2495 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-08-29 22:46:28,396 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-08-29 22:46:28,396 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-08-29 22:46:28,396 label: contradiction
2022-08-29 22:46:28,396 label_id: 2
2022-08-29 22:46:34,394 ***** Running mm evaluation *****
2022-08-29 22:46:34,394   Num examples = 9832
2022-08-29 22:46:34,394   Batch size = 32
2022-08-29 22:46:54,708 ***** Eval results *****
2022-08-29 22:46:54,709   acc = 0.8310618388934092
2022-08-29 22:46:54,709   eval_loss = 0.4839427738220661
2022-08-29 22:46:54,709   global_step = 299
2022-08-29 22:47:25,434 ***** Running evaluation *****
2022-08-29 22:47:25,435   Epoch = 0 iter 399 step
2022-08-29 22:47:25,435   Num examples = 9832
2022-08-29 22:47:25,435   Batch size = 32
2022-08-29 22:47:47,841 ***** Eval results *****
2022-08-29 22:47:47,841   acc = 0.8299430431244914
2022-08-29 22:47:47,841   att_loss = 0.0
2022-08-29 22:47:47,841   cls_loss = 0.1065755066903014
2022-08-29 22:47:47,841   eval_loss = 0.47139383225955744
2022-08-29 22:47:47,841   global_step = 399
2022-08-29 22:47:47,841   loss = 0.1065755066903014
2022-08-29 22:47:47,841   rep_loss = 0.0
2022-08-29 22:48:18,571 ***** Running evaluation *****
2022-08-29 22:48:18,571   Epoch = 0 iter 499 step
2022-08-29 22:48:18,572   Num examples = 9832
2022-08-29 22:48:18,572   Batch size = 32
2022-08-29 22:48:38,857 ***** Eval results *****
2022-08-29 22:48:38,858   acc = 0.8315703824247356
2022-08-29 22:48:38,858   att_loss = 0.0
2022-08-29 22:48:38,858   cls_loss = 0.10563753334010292
2022-08-29 22:48:38,858   eval_loss = 0.4802174800885962
2022-08-29 22:48:38,858   global_step = 499
2022-08-29 22:48:38,858   loss = 0.10563753334010292
2022-08-29 22:48:38,858   rep_loss = 0.0
2022-08-29 22:48:38,858 ***** Save model *****
2022-08-29 22:48:42,068 Writing example 0 of 9832
2022-08-29 22:48:42,069 *** Example ***
2022-08-29 22:48:42,069 guid: dev_matched-0
2022-08-29 22:48:42,069 tokens: [CLS] your contribution helped make it possible for us to provide our students with a quality education . [SEP] your contributions were of no help with our students ' education . [SEP]
2022-08-29 22:48:42,069 input_ids: 101 2115 6691 3271 2191 2009 2825 2005 2149 2000 3073 2256 2493 2007 1037 3737 2495 1012 102 2115 5857 2020 1997 2053 2393 2007 2256 2493 1005 2495 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-08-29 22:48:42,070 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-08-29 22:48:42,070 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-08-29 22:48:42,070 label: contradiction
2022-08-29 22:48:42,070 label_id: 2
2022-08-29 22:48:48,113 ***** Running mm evaluation *****
2022-08-29 22:48:48,113   Num examples = 9832
2022-08-29 22:48:48,113   Batch size = 32
2022-08-29 22:49:10,694 ***** Eval results *****
2022-08-29 22:49:10,694   acc = 0.8315703824247356
2022-08-29 22:49:10,695   eval_loss = 0.4802174800885962
2022-08-29 22:49:10,695   global_step = 499
2022-08-29 22:49:41,888 ***** Running evaluation *****
2022-08-29 22:49:41,888   Epoch = 0 iter 599 step
2022-08-29 22:49:41,889   Num examples = 9832
2022-08-29 22:49:41,889   Batch size = 32
2022-08-29 22:50:02,317 ***** Eval results *****
2022-08-29 22:50:02,318   acc = 0.8342148087876322
2022-08-29 22:50:02,318   att_loss = 0.0
2022-08-29 22:50:02,318   cls_loss = 0.10434180592032824
2022-08-29 22:50:02,318   eval_loss = 0.4771120255788812
2022-08-29 22:50:02,318   global_step = 599
2022-08-29 22:50:02,318   loss = 0.10434180592032824
2022-08-29 22:50:02,318   rep_loss = 0.0
2022-08-29 22:50:02,318 ***** Save model *****
2022-08-29 22:50:05,543 Writing example 0 of 9832
2022-08-29 22:50:05,544 *** Example ***
2022-08-29 22:50:05,544 guid: dev_matched-0
2022-08-29 22:50:05,544 tokens: [CLS] your contribution helped make it possible for us to provide our students with a quality education . [SEP] your contributions were of no help with our students ' education . [SEP]
2022-08-29 22:50:05,544 input_ids: 101 2115 6691 3271 2191 2009 2825 2005 2149 2000 3073 2256 2493 2007 1037 3737 2495 1012 102 2115 5857 2020 1997 2053 2393 2007 2256 2493 1005 2495 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-08-29 22:50:05,544 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-08-29 22:50:05,544 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-08-29 22:50:05,544 label: contradiction
2022-08-29 22:50:05,544 label_id: 2
2022-08-29 22:50:11,574 ***** Running mm evaluation *****
2022-08-29 22:50:11,574   Num examples = 9832
2022-08-29 22:50:11,574   Batch size = 32
2022-08-29 22:50:31,868 ***** Eval results *****
2022-08-29 22:50:31,869   acc = 0.8342148087876322
2022-08-29 22:50:31,869   eval_loss = 0.4771120255788812
2022-08-29 22:50:31,869   global_step = 599
2022-08-29 22:51:05,167 ***** Running evaluation *****
2022-08-29 22:51:05,167   Epoch = 0 iter 699 step
2022-08-29 22:51:05,167   Num examples = 9832
2022-08-29 22:51:05,167   Batch size = 32
2022-08-29 22:51:25,569 ***** Eval results *****
2022-08-29 22:51:25,570   acc = 0.8316720911310008
2022-08-29 22:51:25,570   att_loss = 0.0
2022-08-29 22:51:25,570   cls_loss = 0.10395870149903032
2022-08-29 22:51:25,570   eval_loss = 0.4675095841627229
2022-08-29 22:51:25,570   global_step = 699
2022-08-29 22:51:25,570   loss = 0.10395870149903032
2022-08-29 22:51:25,570   rep_loss = 0.0
2022-08-29 22:51:56,964 ***** Running evaluation *****
2022-08-29 22:51:56,965   Epoch = 0 iter 799 step
2022-08-29 22:51:56,965   Num examples = 9832
2022-08-29 22:51:56,965   Batch size = 32
2022-08-29 22:52:17,362 ***** Eval results *****
2022-08-29 22:52:17,362   acc = 0.8336045565500407
2022-08-29 22:52:17,362   att_loss = 0.0
2022-08-29 22:52:17,362   cls_loss = 0.10383332591126201
2022-08-29 22:52:17,362   eval_loss = 0.46589276015564995
2022-08-29 22:52:17,363   global_step = 799
2022-08-29 22:52:17,363   loss = 0.10383332591126201
2022-08-29 22:52:17,363   rep_loss = 0.0
2022-08-29 22:52:48,744 ***** Running evaluation *****
2022-08-29 22:52:48,744   Epoch = 0 iter 899 step
2022-08-29 22:52:48,744   Num examples = 9832
2022-08-29 22:52:48,745   Batch size = 32
2022-08-29 22:53:11,362 ***** Eval results *****
2022-08-29 22:53:11,362   acc = 0.8318755085435313
2022-08-29 22:53:11,362   att_loss = 0.0
2022-08-29 22:53:11,362   cls_loss = 0.10312917718334379
2022-08-29 22:53:11,362   eval_loss = 0.4804059482094916
2022-08-29 22:53:11,362   global_step = 899
2022-08-29 22:53:11,362   loss = 0.10312917718334379
2022-08-29 22:53:11,363   rep_loss = 0.0
2022-08-29 22:53:42,741 ***** Running evaluation *****
2022-08-29 22:53:42,742   Epoch = 0 iter 999 step
2022-08-29 22:53:42,742   Num examples = 9832
2022-08-29 22:53:42,742   Batch size = 32
2022-08-29 22:54:03,224 ***** Eval results *****
2022-08-29 22:54:03,224   acc = 0.8346216436126932
2022-08-29 22:54:03,224   att_loss = 0.0
2022-08-29 22:54:03,224   cls_loss = 0.10238385680425274
2022-08-29 22:54:03,224   eval_loss = 0.47147793141375116
2022-08-29 22:54:03,224   global_step = 999
2022-08-29 22:54:03,224   loss = 0.10238385680425274
2022-08-29 22:54:03,224   rep_loss = 0.0
2022-08-29 22:54:03,224 ***** Save model *****
2022-08-29 22:54:10,245 Writing example 0 of 9832
2022-08-29 22:54:10,246 *** Example ***
2022-08-29 22:54:10,246 guid: dev_matched-0
2022-08-29 22:54:10,246 tokens: [CLS] your contribution helped make it possible for us to provide our students with a quality education . [SEP] your contributions were of no help with our students ' education . [SEP]
2022-08-29 22:54:10,246 input_ids: 101 2115 6691 3271 2191 2009 2825 2005 2149 2000 3073 2256 2493 2007 1037 3737 2495 1012 102 2115 5857 2020 1997 2053 2393 2007 2256 2493 1005 2495 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-08-29 22:54:10,246 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-08-29 22:54:10,246 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-08-29 22:54:10,246 label: contradiction
2022-08-29 22:54:10,246 label_id: 2
2022-08-29 22:54:16,299 ***** Running mm evaluation *****
2022-08-29 22:54:16,300   Num examples = 9832
2022-08-29 22:54:16,300   Batch size = 32
2022-08-29 22:54:36,694 ***** Eval results *****
2022-08-29 22:54:36,694   acc = 0.8346216436126932
2022-08-29 22:54:36,695   eval_loss = 0.47147793141375116
2022-08-29 22:54:36,695   global_step = 999
2022-08-29 22:55:10,272 ***** Running evaluation *****
2022-08-29 22:55:10,272   Epoch = 0 iter 1099 step
2022-08-29 22:55:10,272   Num examples = 9832
2022-08-29 22:55:10,272   Batch size = 32
2022-08-29 22:55:30,696 ***** Eval results *****
2022-08-29 22:55:30,696   acc = 0.830553295362083
2022-08-29 22:55:30,696   att_loss = 0.0
2022-08-29 22:55:30,696   cls_loss = 0.10247052786320963
2022-08-29 22:55:30,696   eval_loss = 0.4681962386934788
2022-08-29 22:55:30,696   global_step = 1099
2022-08-29 22:55:30,696   loss = 0.10247052786320963
2022-08-29 22:55:30,697   rep_loss = 0.0
2022-08-29 22:56:02,086 ***** Running evaluation *****
2022-08-29 22:56:02,087   Epoch = 0 iter 1199 step
2022-08-29 22:56:02,087   Num examples = 9832
2022-08-29 22:56:02,087   Batch size = 32
2022-08-29 22:56:22,472 ***** Eval results *****
2022-08-29 22:56:22,472   acc = 0.8367575264442636
2022-08-29 22:56:22,472   att_loss = 0.0
2022-08-29 22:56:22,472   cls_loss = 0.10224271875443411
2022-08-29 22:56:22,472   eval_loss = 0.4719845430030451
2022-08-29 22:56:22,472   global_step = 1199
2022-08-29 22:56:22,472   loss = 0.10224271875443411
2022-08-29 22:56:22,473   rep_loss = 0.0
2022-08-29 22:56:22,473 ***** Save model *****
2022-08-29 22:56:25,587 Writing example 0 of 9832
2022-08-29 22:56:25,588 *** Example ***
2022-08-29 22:56:25,588 guid: dev_matched-0
2022-08-29 22:56:25,588 tokens: [CLS] your contribution helped make it possible for us to provide our students with a quality education . [SEP] your contributions were of no help with our students ' education . [SEP]
2022-08-29 22:56:25,588 input_ids: 101 2115 6691 3271 2191 2009 2825 2005 2149 2000 3073 2256 2493 2007 1037 3737 2495 1012 102 2115 5857 2020 1997 2053 2393 2007 2256 2493 1005 2495 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-08-29 22:56:25,588 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-08-29 22:56:25,588 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-08-29 22:56:25,588 label: contradiction
2022-08-29 22:56:25,588 label_id: 2
2022-08-29 22:56:31,610 ***** Running mm evaluation *****
2022-08-29 22:56:31,611   Num examples = 9832
2022-08-29 22:56:31,611   Batch size = 32
2022-08-29 22:56:54,078 ***** Eval results *****
2022-08-29 22:56:54,078   acc = 0.8367575264442636
2022-08-29 22:56:54,078   eval_loss = 0.4719845430030451
2022-08-29 22:56:54,078   global_step = 1199
2022-08-29 22:57:24,878 ***** Running evaluation *****
2022-08-29 22:57:24,878   Epoch = 0 iter 1299 step
2022-08-29 22:57:24,878   Num examples = 9832
2022-08-29 22:57:24,878   Batch size = 32
2022-08-29 22:57:45,200 ***** Eval results *****
2022-08-29 22:57:45,200   acc = 0.8352318958502848
2022-08-29 22:57:45,200   att_loss = 0.0
2022-08-29 22:57:45,200   cls_loss = 0.10177195930797564
2022-08-29 22:57:45,200   eval_loss = 0.46044921313787435
2022-08-29 22:57:45,200   global_step = 1299
2022-08-29 22:57:45,201   loss = 0.10177195930797564
2022-08-29 22:57:45,201   rep_loss = 0.0
2022-08-29 22:58:15,953 ***** Running evaluation *****
2022-08-29 22:58:15,954   Epoch = 0 iter 1399 step
2022-08-29 22:58:15,954   Num examples = 9832
2022-08-29 22:58:15,954   Batch size = 32
2022-08-29 22:58:38,493 ***** Eval results *****
2022-08-29 22:58:38,493   acc = 0.833706265256306
2022-08-29 22:58:38,494   att_loss = 0.0
2022-08-29 22:58:38,494   cls_loss = 0.10184033395851655
2022-08-29 22:58:38,494   eval_loss = 0.4622089250327705
2022-08-29 22:58:38,494   global_step = 1399
2022-08-29 22:58:38,494   loss = 0.10184033395851655
2022-08-29 22:58:38,494   rep_loss = 0.0
2022-08-29 22:59:09,208 ***** Running evaluation *****
2022-08-29 22:59:09,208   Epoch = 0 iter 1499 step
2022-08-29 22:59:09,209   Num examples = 9832
2022-08-29 22:59:09,209   Batch size = 32
2022-08-29 22:59:29,556 ***** Eval results *****
2022-08-29 22:59:29,556   acc = 0.8368592351505288
2022-08-29 22:59:29,556   att_loss = 0.0
2022-08-29 22:59:29,556   cls_loss = 0.10202564064505261
2022-08-29 22:59:29,556   eval_loss = 0.4584506072297499
2022-08-29 22:59:29,556   global_step = 1499
2022-08-29 22:59:29,556   loss = 0.10202564064505261
2022-08-29 22:59:29,556   rep_loss = 0.0
2022-08-29 22:59:29,556 ***** Save model *****
2022-08-29 22:59:32,765 Writing example 0 of 9832
2022-08-29 22:59:32,766 *** Example ***
2022-08-29 22:59:32,766 guid: dev_matched-0
2022-08-29 22:59:32,766 tokens: [CLS] your contribution helped make it possible for us to provide our students with a quality education . [SEP] your contributions were of no help with our students ' education . [SEP]
2022-08-29 22:59:32,766 input_ids: 101 2115 6691 3271 2191 2009 2825 2005 2149 2000 3073 2256 2493 2007 1037 3737 2495 1012 102 2115 5857 2020 1997 2053 2393 2007 2256 2493 1005 2495 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-08-29 22:59:32,766 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-08-29 22:59:32,766 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-08-29 22:59:32,766 label: contradiction
2022-08-29 22:59:32,767 label_id: 2
2022-08-29 22:59:38,778 ***** Running mm evaluation *****
2022-08-29 22:59:38,779   Num examples = 9832
2022-08-29 22:59:38,779   Batch size = 32
2022-08-29 22:59:59,121 ***** Eval results *****
2022-08-29 22:59:59,122   acc = 0.8368592351505288
2022-08-29 22:59:59,122   eval_loss = 0.4584506072297499
2022-08-29 22:59:59,122   global_step = 1499
2022-08-29 23:00:32,025 ***** Running evaluation *****
2022-08-29 23:00:32,025   Epoch = 0 iter 1599 step
2022-08-29 23:00:32,025   Num examples = 9832
2022-08-29 23:00:32,025   Batch size = 32
2022-08-29 23:00:52,384 ***** Eval results *****
2022-08-29 23:00:52,384   acc = 0.8364524003254679
2022-08-29 23:00:52,384   att_loss = 0.0
2022-08-29 23:00:52,384   cls_loss = 0.10201532500630024
2022-08-29 23:00:52,385   eval_loss = 0.46256291097054236
2022-08-29 23:00:52,385   global_step = 1599
2022-08-29 23:00:52,385   loss = 0.10201532500630024
2022-08-29 23:00:52,385   rep_loss = 0.0
2022-08-29 23:01:23,124 ***** Running evaluation *****
2022-08-29 23:01:23,125   Epoch = 0 iter 1699 step
2022-08-29 23:01:23,125   Num examples = 9832
2022-08-29 23:01:23,125   Batch size = 32
2022-08-29 23:01:45,647 ***** Eval results *****
2022-08-29 23:01:45,647   acc = 0.8356387306753458
2022-08-29 23:01:45,647   att_loss = 0.0
2022-08-29 23:01:45,648   cls_loss = 0.10135163152135422
2022-08-29 23:01:45,648   eval_loss = 0.4548073063979482
2022-08-29 23:01:45,648   global_step = 1699
2022-08-29 23:01:45,648   loss = 0.10135163152135422
2022-08-29 23:01:45,648   rep_loss = 0.0
2022-08-29 23:02:17,262 ***** Running evaluation *****
2022-08-29 23:02:17,263   Epoch = 0 iter 1799 step
2022-08-29 23:02:17,263   Num examples = 9832
2022-08-29 23:02:17,263   Batch size = 32
2022-08-29 23:02:39,604 ***** Eval results *****
2022-08-29 23:02:39,604   acc = 0.8356387306753458
2022-08-29 23:02:39,604   att_loss = 0.0
2022-08-29 23:02:39,604   cls_loss = 0.10145600026410245
2022-08-29 23:02:39,604   eval_loss = 0.4670516602670798
2022-08-29 23:02:39,604   global_step = 1799
2022-08-29 23:02:39,604   loss = 0.10145600026410245
2022-08-29 23:02:39,604   rep_loss = 0.0
2022-08-29 23:03:11,093 ***** Running evaluation *****
2022-08-29 23:03:11,094   Epoch = 0 iter 1899 step
2022-08-29 23:03:11,094   Num examples = 9832
2022-08-29 23:03:11,094   Batch size = 32
2022-08-29 23:03:31,502 ***** Eval results *****
2022-08-29 23:03:31,502   acc = 0.8359438567941416
2022-08-29 23:03:31,502   att_loss = 0.0
2022-08-29 23:03:31,502   cls_loss = 0.10140322835987464
2022-08-29 23:03:31,502   eval_loss = 0.4628374884509808
2022-08-29 23:03:31,502   global_step = 1899
2022-08-29 23:03:31,502   loss = 0.10140322835987464
2022-08-29 23:03:31,502   rep_loss = 0.0
2022-08-29 23:04:02,999 ***** Running evaluation *****
2022-08-29 23:04:03,000   Epoch = 0 iter 1999 step
2022-08-29 23:04:03,000   Num examples = 9832
2022-08-29 23:04:03,000   Batch size = 32
2022-08-29 23:04:23,413 ***** Eval results *****
2022-08-29 23:04:23,413   acc = 0.8369609438567941
2022-08-29 23:04:23,413   att_loss = 0.0
2022-08-29 23:04:23,413   cls_loss = 0.10120363968642071
2022-08-29 23:04:23,413   eval_loss = 0.45205825939774513
2022-08-29 23:04:23,413   global_step = 1999
2022-08-29 23:04:23,413   loss = 0.10120363968642071
2022-08-29 23:04:23,413   rep_loss = 0.0
2022-08-29 23:04:23,413 ***** Save model *****
2022-08-29 23:04:26,128 Writing example 0 of 9832
2022-08-29 23:04:26,128 *** Example ***
2022-08-29 23:04:26,129 guid: dev_matched-0
2022-08-29 23:04:26,129 tokens: [CLS] your contribution helped make it possible for us to provide our students with a quality education . [SEP] your contributions were of no help with our students ' education . [SEP]
2022-08-29 23:04:26,129 input_ids: 101 2115 6691 3271 2191 2009 2825 2005 2149 2000 3073 2256 2493 2007 1037 3737 2495 1012 102 2115 5857 2020 1997 2053 2393 2007 2256 2493 1005 2495 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-08-29 23:04:26,129 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-08-29 23:04:26,129 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-08-29 23:04:26,129 label: contradiction
2022-08-29 23:04:26,129 label_id: 2
2022-08-29 23:04:32,173 ***** Running mm evaluation *****
2022-08-29 23:04:32,173   Num examples = 9832
2022-08-29 23:04:32,173   Batch size = 32
2022-08-29 23:04:54,759 ***** Eval results *****
2022-08-29 23:04:54,759   acc = 0.8369609438567941
2022-08-29 23:04:54,759   eval_loss = 0.45205825939774513
2022-08-29 23:04:54,759   global_step = 1999
2022-08-29 23:05:26,070 ***** Running evaluation *****
2022-08-29 23:05:26,070   Epoch = 0 iter 2099 step
2022-08-29 23:05:26,070   Num examples = 9832
2022-08-29 23:05:26,070   Batch size = 32
2022-08-29 23:05:46,417 ***** Eval results *****
2022-08-29 23:05:46,417   acc = 0.8292310821806347
2022-08-29 23:05:46,418   att_loss = 0.0
2022-08-29 23:05:46,418   cls_loss = 0.10134647330495504
2022-08-29 23:05:46,418   eval_loss = 0.47940108288901967
2022-08-29 23:05:46,418   global_step = 2099
2022-08-29 23:05:46,418   loss = 0.10134647330495504
2022-08-29 23:05:46,418   rep_loss = 0.0
2022-08-29 23:06:17,632 ***** Running evaluation *****
2022-08-29 23:06:17,633   Epoch = 0 iter 2199 step
2022-08-29 23:06:17,633   Num examples = 9832
2022-08-29 23:06:17,633   Batch size = 32
2022-08-29 23:06:37,985 ***** Eval results *****
2022-08-29 23:06:37,985   acc = 0.8367575264442636
2022-08-29 23:06:37,985   att_loss = 0.0
2022-08-29 23:06:37,985   cls_loss = 0.10143012103843602
2022-08-29 23:06:37,985   eval_loss = 0.4509930588643659
2022-08-29 23:06:37,986   global_step = 2199
2022-08-29 23:06:37,986   loss = 0.10143012103843602
2022-08-29 23:06:37,986   rep_loss = 0.0
2022-08-29 23:07:11,378 ***** Running evaluation *****
2022-08-29 23:07:11,378   Epoch = 0 iter 2299 step
2022-08-29 23:07:11,378   Num examples = 9832
2022-08-29 23:07:11,378   Batch size = 32
2022-08-29 23:07:31,735 ***** Eval results *****
2022-08-29 23:07:31,735   acc = 0.8389951179820992
2022-08-29 23:07:31,735   att_loss = 0.0
2022-08-29 23:07:31,735   cls_loss = 0.1014137108360181
2022-08-29 23:07:31,735   eval_loss = 0.46262980264599446
2022-08-29 23:07:31,735   global_step = 2299
2022-08-29 23:07:31,735   loss = 0.1014137108360181
2022-08-29 23:07:31,735   rep_loss = 0.0
2022-08-29 23:07:31,736 ***** Save model *****
2022-08-29 23:07:34,961 Writing example 0 of 9832
2022-08-29 23:07:34,962 *** Example ***
2022-08-29 23:07:34,962 guid: dev_matched-0
2022-08-29 23:07:34,962 tokens: [CLS] your contribution helped make it possible for us to provide our students with a quality education . [SEP] your contributions were of no help with our students ' education . [SEP]
2022-08-29 23:07:34,962 input_ids: 101 2115 6691 3271 2191 2009 2825 2005 2149 2000 3073 2256 2493 2007 1037 3737 2495 1012 102 2115 5857 2020 1997 2053 2393 2007 2256 2493 1005 2495 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-08-29 23:07:34,963 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-08-29 23:07:34,963 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-08-29 23:07:34,963 label: contradiction
2022-08-29 23:07:34,963 label_id: 2
2022-08-29 23:07:41,007 ***** Running mm evaluation *****
2022-08-29 23:07:41,008   Num examples = 9832
2022-08-29 23:07:41,008   Batch size = 32
2022-08-29 23:08:01,338 ***** Eval results *****
2022-08-29 23:08:01,338   acc = 0.8389951179820992
2022-08-29 23:08:01,338   eval_loss = 0.46262980264599446
2022-08-29 23:08:01,338   global_step = 2299
2022-08-29 23:08:32,596 ***** Running evaluation *****
2022-08-29 23:08:32,597   Epoch = 0 iter 2399 step
2022-08-29 23:08:32,597   Num examples = 9832
2022-08-29 23:08:32,597   Batch size = 32
2022-08-29 23:08:52,925 ***** Eval results *****
2022-08-29 23:08:52,925   acc = 0.8343165174938975
2022-08-29 23:08:52,926   att_loss = 0.0
2022-08-29 23:08:52,926   cls_loss = 0.10128293255838616
2022-08-29 23:08:52,926   eval_loss = 0.47186907366305203
2022-08-29 23:08:52,926   global_step = 2399
2022-08-29 23:08:52,926   loss = 0.10128293255838616
2022-08-29 23:08:52,926   rep_loss = 0.0
2022-08-29 23:09:26,013 ***** Running evaluation *****
2022-08-29 23:09:26,013   Epoch = 0 iter 2499 step
2022-08-29 23:09:26,013   Num examples = 9832
2022-08-29 23:09:26,013   Batch size = 32
2022-08-29 23:09:46,272 ***** Eval results *****
2022-08-29 23:09:46,272   acc = 0.8326891781936534
2022-08-29 23:09:46,272   att_loss = 0.0
2022-08-29 23:09:46,272   cls_loss = 0.10122167252871742
2022-08-29 23:09:46,272   eval_loss = 0.48327607625200375
2022-08-29 23:09:46,272   global_step = 2499
2022-08-29 23:09:46,272   loss = 0.10122167252871742
2022-08-29 23:09:46,272   rep_loss = 0.0
2022-08-29 23:10:17,057 ***** Running evaluation *****
2022-08-29 23:10:17,057   Epoch = 0 iter 2599 step
2022-08-29 23:10:17,057   Num examples = 9832
2022-08-29 23:10:17,057   Batch size = 32
2022-08-29 23:10:37,316 ***** Eval results *****
2022-08-29 23:10:37,316   acc = 0.8360455655004069
2022-08-29 23:10:37,316   att_loss = 0.0
2022-08-29 23:10:37,316   cls_loss = 0.10118350765312026
2022-08-29 23:10:37,316   eval_loss = 0.4751965476611218
2022-08-29 23:10:37,316   global_step = 2599
2022-08-29 23:10:37,316   loss = 0.10118350765312026
2022-08-29 23:10:37,316   rep_loss = 0.0
2022-08-29 23:11:10,225 ***** Running evaluation *****
2022-08-29 23:11:10,225   Epoch = 0 iter 2699 step
2022-08-29 23:11:10,225   Num examples = 9832
2022-08-29 23:11:10,225   Batch size = 32
2022-08-29 23:11:30,500 ***** Eval results *****
2022-08-29 23:11:30,501   acc = 0.8356387306753458
2022-08-29 23:11:30,501   att_loss = 0.0
2022-08-29 23:11:30,501   cls_loss = 0.10138515513576636
2022-08-29 23:11:30,501   eval_loss = 0.4632594310859968
2022-08-29 23:11:30,501   global_step = 2699
2022-08-29 23:11:30,501   loss = 0.10138515513576636
2022-08-29 23:11:30,501   rep_loss = 0.0
2022-08-29 23:11:56,304 The args: Namespace(aug_train=False, cache_dir='', data_dir='glue_data/STSB', data_url='', do_eval=False, do_lower_case=True, eval_batch_size=32, eval_step=200, gradient_accumulation_steps=1, hidden_act='quad', learning_rate=3e-05, log_path='tmp/distill/STSB/quad_2quad/bert-base-uncased/log.txt', max_seq_length=128, no_cuda=False, num_train_epochs=3.0, output_dir='tmp/distill/STSB/quad_2quad/bert-base-uncased/3e-05_1e-05_8', pred_distill=False, seed=42, softmax_act='2quad', student_model='/home/ubuntu/checkpoints/exp/STSB', task_name='STSB', teacher_model='/home/ubuntu/checkpoints/exp/STSB', temperature=1.0, train_batch_size=8, warmup_proportion=0.1, weight_decay=0.0001)
2022-08-29 23:11:56,304 device: cuda n_gpu: 1
2022-08-29 23:11:56,399 Writing example 0 of 5749
2022-08-29 23:11:56,399 *** Example ***
2022-08-29 23:11:56,399 guid: train-0
2022-08-29 23:11:56,400 tokens: [CLS] a plane is taking off . [SEP] an air plane is taking off . [SEP]
2022-08-29 23:11:56,400 input_ids: 101 1037 4946 2003 2635 2125 1012 102 2019 2250 4946 2003 2635 2125 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-08-29 23:11:56,400 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-08-29 23:11:56,400 segment_ids: 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-08-29 23:11:56,400 label: 5.000
2022-08-29 23:11:56,400 label_id: 5.0
2022-08-29 23:11:56,465 The args: Namespace(aug_train=False, cache_dir='', data_dir='glue_data/STSB', data_url='', do_eval=False, do_lower_case=True, eval_batch_size=32, eval_step=200, gradient_accumulation_steps=1, hidden_act='quad', learning_rate=3e-05, log_path='tmp/distill/STSB/quad_2quad/bert-base-uncased/log.txt', max_seq_length=128, no_cuda=False, num_train_epochs=3.0, output_dir='tmp/distill/STSB/quad_2quad/bert-base-uncased/3e-05_1e-05_32', pred_distill=False, seed=42, softmax_act='2quad', student_model='/home/ubuntu/checkpoints/exp/STSB', task_name='STSB', teacher_model='/home/ubuntu/checkpoints/exp/STSB', temperature=1.0, train_batch_size=32, warmup_proportion=0.1, weight_decay=0.0001)
2022-08-29 23:11:56,465 device: cuda n_gpu: 1
2022-08-29 23:11:56,486 The args: Namespace(aug_train=False, cache_dir='', data_dir='glue_data/QQP', data_url='', do_eval=False, do_lower_case=True, eval_batch_size=32, eval_step=200, gradient_accumulation_steps=1, hidden_act='quad', learning_rate=5e-05, log_path='tmp/distill/QQP/quad_2quad/bert-base-uncased/log.txt', max_seq_length=128, no_cuda=False, num_train_epochs=3.0, output_dir='tmp/distill/QQP/quad_2quad/bert-base-uncased/5e-05_1e-05_32', pred_distill=False, seed=42, softmax_act='2quad', student_model='/home/ubuntu/checkpoints/exp/QQP', task_name='QQP', teacher_model='/home/ubuntu/checkpoints/exp/QQP', temperature=1.0, train_batch_size=32, warmup_proportion=0.1, weight_decay=0.0001)
2022-08-29 23:11:56,486 device: cuda n_gpu: 2
2022-08-29 23:11:56,556 Writing example 0 of 5749
2022-08-29 23:11:56,557 *** Example ***
2022-08-29 23:11:56,557 guid: train-0
2022-08-29 23:11:56,557 tokens: [CLS] a plane is taking off . [SEP] an air plane is taking off . [SEP]
2022-08-29 23:11:56,557 input_ids: 101 1037 4946 2003 2635 2125 1012 102 2019 2250 4946 2003 2635 2125 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-08-29 23:11:56,557 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-08-29 23:11:56,557 segment_ids: 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-08-29 23:11:56,557 label: 5.000
2022-08-29 23:11:56,557 label_id: 5.0
2022-08-29 23:11:58,761 Writing example 0 of 1500
2022-08-29 23:11:58,761 *** Example ***
2022-08-29 23:11:58,762 guid: dev-0
2022-08-29 23:11:58,762 tokens: [CLS] a man with a hard hat is dancing . [SEP] a man wearing a hard hat is dancing . [SEP]
2022-08-29 23:11:58,762 input_ids: 101 1037 2158 2007 1037 2524 6045 2003 5613 1012 102 1037 2158 4147 1037 2524 6045 2003 5613 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-08-29 23:11:58,762 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-08-29 23:11:58,762 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-08-29 23:11:58,762 label: 5.000
2022-08-29 23:11:58,762 label_id: 5.0
2022-08-29 23:11:58,769 Writing example 0 of 363846
2022-08-29 23:11:58,770 *** Example ***
2022-08-29 23:11:58,770 guid: train-133273
2022-08-29 23:11:58,770 tokens: [CLS] how is the life of a math student ? could you describe your own experiences ? [SEP] which level of prep ##ration is enough for the exam j ##lp ##t ##5 ? [SEP]
2022-08-29 23:11:58,770 input_ids: 101 2129 2003 1996 2166 1997 1037 8785 3076 1029 2071 2017 6235 2115 2219 6322 1029 102 2029 2504 1997 17463 8156 2003 2438 2005 1996 11360 1046 14277 2102 2629 1029 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-08-29 23:11:58,770 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-08-29 23:11:58,770 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-08-29 23:11:58,770 label: 0
2022-08-29 23:11:58,770 label_id: 0
2022-08-29 23:11:58,876 Writing example 0 of 1500
2022-08-29 23:11:58,877 *** Example ***
2022-08-29 23:11:58,877 guid: dev-0
2022-08-29 23:11:58,877 tokens: [CLS] a man with a hard hat is dancing . [SEP] a man wearing a hard hat is dancing . [SEP]
2022-08-29 23:11:58,877 input_ids: 101 1037 2158 2007 1037 2524 6045 2003 5613 1012 102 1037 2158 4147 1037 2524 6045 2003 5613 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-08-29 23:11:58,877 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-08-29 23:11:58,877 segment_ids: 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-08-29 23:11:58,877 label: 5.000
2022-08-29 23:11:58,877 label_id: 5.0
2022-08-29 23:11:59,439 Model config {
  "_name_or_path": "bert-base-uncased",
  "architectures": [
    "BertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "crypten": false,
  "finetuning_task": "stsb",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0
  },
  "layer_norm_eps": 1e-12,
  "log_path": "tmp/distill/STSB/quad_2quad/bert-base-uncased/log.txt",
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "pre_trained": "",
  "problem_type": "regression",
  "softmax_act": "softmax",
  "torch_dtype": "float32",
  "training": "",
  "transformers_version": "4.20.0.dev0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

2022-08-29 23:11:59,548 Model config {
  "_name_or_path": "bert-base-uncased",
  "architectures": [
    "BertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "crypten": false,
  "finetuning_task": "stsb",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0
  },
  "layer_norm_eps": 1e-12,
  "log_path": "tmp/distill/STSB/quad_2quad/bert-base-uncased/log.txt",
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "pre_trained": "",
  "problem_type": "regression",
  "softmax_act": "softmax",
  "torch_dtype": "float32",
  "training": "",
  "transformers_version": "4.20.0.dev0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

2022-08-29 23:12:01,465 ***** Running evaluation *****
2022-08-29 23:12:01,465   Epoch = 0 iter 2799 step
2022-08-29 23:12:01,466   Num examples = 9832
2022-08-29 23:12:01,466   Batch size = 32
2022-08-29 23:12:02,162 Loading model /home/ubuntu/checkpoints/exp/STSB/pytorch_model.bin
2022-08-29 23:12:02,264 Loading model /home/ubuntu/checkpoints/exp/STSB/pytorch_model.bin
2022-08-29 23:12:02,499 loading model...
2022-08-29 23:12:02,550 done!
2022-08-29 23:12:02,551 Weights from pretrained model not used in TinyBertForSequenceClassification: ['bert.embeddings.position_ids']
2022-08-29 23:12:02,610 loading model...
2022-08-29 23:12:02,663 done!
2022-08-29 23:12:02,663 Weights from pretrained model not used in TinyBertForSequenceClassification: ['bert.embeddings.position_ids']
2022-08-29 23:12:03,145 Writing example 10000 of 363846
2022-08-29 23:12:07,438 Writing example 20000 of 363846
2022-08-29 23:12:10,112 ***** Teacher evaluation *****
2022-08-29 23:12:10,113 {'pearson': 0.8927170602068698, 'spearmanr': 0.8894144581995493, 'corr': 0.8910657592032096, 'eval_loss': 0.4806266178159004}
2022-08-29 23:12:10,113 Model config {
  "_name_or_path": "bert-base-uncased",
  "architectures": [
    "BertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "crypten": false,
  "finetuning_task": "stsb",
  "gradient_checkpointing": false,
  "hidden_act": "quad",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0
  },
  "layer_norm_eps": 1e-12,
  "log_path": "tmp/distill/STSB/quad_2quad/bert-base-uncased/log.txt",
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "pre_trained": "",
  "problem_type": "regression",
  "softmax_act": "2quad",
  "torch_dtype": "float32",
  "training": "",
  "transformers_version": "4.20.0.dev0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

2022-08-29 23:12:10,143 ***** Teacher evaluation *****
2022-08-29 23:12:10,143 {'pearson': 0.8927170602068698, 'spearmanr': 0.8894144581995493, 'corr': 0.8910657592032096, 'eval_loss': 0.4806266178159004}
2022-08-29 23:12:10,144 Model config {
  "_name_or_path": "bert-base-uncased",
  "architectures": [
    "BertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "crypten": false,
  "finetuning_task": "stsb",
  "gradient_checkpointing": false,
  "hidden_act": "quad",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0
  },
  "layer_norm_eps": 1e-12,
  "log_path": "tmp/distill/STSB/quad_2quad/bert-base-uncased/log.txt",
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "pre_trained": "",
  "problem_type": "regression",
  "softmax_act": "2quad",
  "torch_dtype": "float32",
  "training": "",
  "transformers_version": "4.20.0.dev0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

2022-08-29 23:12:11,969 Writing example 30000 of 363846
2022-08-29 23:12:12,862 Loading model /home/ubuntu/checkpoints/exp/STSB/pytorch_model.bin
2022-08-29 23:12:12,901 Loading model /home/ubuntu/checkpoints/exp/STSB/pytorch_model.bin
2022-08-29 23:12:13,189 loading model...
2022-08-29 23:12:13,238 done!
2022-08-29 23:12:13,238 Weights from pretrained model not used in TinyBertForSequenceClassification: ['bert.embeddings.position_ids']
2022-08-29 23:12:13,244 loading model...
2022-08-29 23:12:13,292 done!
2022-08-29 23:12:13,292 Weights from pretrained model not used in TinyBertForSequenceClassification: ['bert.embeddings.position_ids']
2022-08-29 23:12:13,343 ***** Running training *****
2022-08-29 23:12:13,343   Num examples = 5749
2022-08-29 23:12:13,344   Batch size = 8
2022-08-29 23:12:13,344   Num steps = 35900
2022-08-29 23:12:13,345 n: bert.embeddings.word_embeddings.weight
2022-08-29 23:12:13,345 n: bert.embeddings.position_embeddings.weight
2022-08-29 23:12:13,345 n: bert.embeddings.token_type_embeddings.weight
2022-08-29 23:12:13,345 n: bert.embeddings.LayerNorm.weight
2022-08-29 23:12:13,345 n: bert.embeddings.LayerNorm.bias
2022-08-29 23:12:13,345 n: bert.encoder.layer.0.attention.self.query.weight
2022-08-29 23:12:13,345 n: bert.encoder.layer.0.attention.self.query.bias
2022-08-29 23:12:13,345 n: bert.encoder.layer.0.attention.self.key.weight
2022-08-29 23:12:13,345 n: bert.encoder.layer.0.attention.self.key.bias
2022-08-29 23:12:13,345 n: bert.encoder.layer.0.attention.self.value.weight
2022-08-29 23:12:13,345 n: bert.encoder.layer.0.attention.self.value.bias
2022-08-29 23:12:13,345 n: bert.encoder.layer.0.attention.output.dense.weight
2022-08-29 23:12:13,345 n: bert.encoder.layer.0.attention.output.dense.bias
2022-08-29 23:12:13,345 n: bert.encoder.layer.0.attention.output.LayerNorm.weight
2022-08-29 23:12:13,345 n: bert.encoder.layer.0.attention.output.LayerNorm.bias
2022-08-29 23:12:13,345 n: bert.encoder.layer.0.intermediate.dense.weight
2022-08-29 23:12:13,345 n: bert.encoder.layer.0.intermediate.dense.bias
2022-08-29 23:12:13,346 n: bert.encoder.layer.0.output.dense.weight
2022-08-29 23:12:13,346 n: bert.encoder.layer.0.output.dense.bias
2022-08-29 23:12:13,346 n: bert.encoder.layer.0.output.LayerNorm.weight
2022-08-29 23:12:13,346 n: bert.encoder.layer.0.output.LayerNorm.bias
2022-08-29 23:12:13,346 n: bert.encoder.layer.1.attention.self.query.weight
2022-08-29 23:12:13,346 n: bert.encoder.layer.1.attention.self.query.bias
2022-08-29 23:12:13,346 n: bert.encoder.layer.1.attention.self.key.weight
2022-08-29 23:12:13,346 n: bert.encoder.layer.1.attention.self.key.bias
2022-08-29 23:12:13,346 n: bert.encoder.layer.1.attention.self.value.weight
2022-08-29 23:12:13,346 n: bert.encoder.layer.1.attention.self.value.bias
2022-08-29 23:12:13,346 n: bert.encoder.layer.1.attention.output.dense.weight
2022-08-29 23:12:13,346 n: bert.encoder.layer.1.attention.output.dense.bias
2022-08-29 23:12:13,346 n: bert.encoder.layer.1.attention.output.LayerNorm.weight
2022-08-29 23:12:13,346 n: bert.encoder.layer.1.attention.output.LayerNorm.bias
2022-08-29 23:12:13,346 n: bert.encoder.layer.1.intermediate.dense.weight
2022-08-29 23:12:13,346 n: bert.encoder.layer.1.intermediate.dense.bias
2022-08-29 23:12:13,346 n: bert.encoder.layer.1.output.dense.weight
2022-08-29 23:12:13,346 n: bert.encoder.layer.1.output.dense.bias
2022-08-29 23:12:13,347 n: bert.encoder.layer.1.output.LayerNorm.weight
2022-08-29 23:12:13,347 n: bert.encoder.layer.1.output.LayerNorm.bias
2022-08-29 23:12:13,347 n: bert.encoder.layer.2.attention.self.query.weight
2022-08-29 23:12:13,347 n: bert.encoder.layer.2.attention.self.query.bias
2022-08-29 23:12:13,347 n: bert.encoder.layer.2.attention.self.key.weight
2022-08-29 23:12:13,347 n: bert.encoder.layer.2.attention.self.key.bias
2022-08-29 23:12:13,347 n: bert.encoder.layer.2.attention.self.value.weight
2022-08-29 23:12:13,347 n: bert.encoder.layer.2.attention.self.value.bias
2022-08-29 23:12:13,347 n: bert.encoder.layer.2.attention.output.dense.weight
2022-08-29 23:12:13,347 n: bert.encoder.layer.2.attention.output.dense.bias
2022-08-29 23:12:13,347 n: bert.encoder.layer.2.attention.output.LayerNorm.weight
2022-08-29 23:12:13,347 n: bert.encoder.layer.2.attention.output.LayerNorm.bias
2022-08-29 23:12:13,347 n: bert.encoder.layer.2.intermediate.dense.weight
2022-08-29 23:12:13,347 n: bert.encoder.layer.2.intermediate.dense.bias
2022-08-29 23:12:13,347 n: bert.encoder.layer.2.output.dense.weight
2022-08-29 23:12:13,347 n: bert.encoder.layer.2.output.dense.bias
2022-08-29 23:12:13,347 n: bert.encoder.layer.2.output.LayerNorm.weight
2022-08-29 23:12:13,347 n: bert.encoder.layer.2.output.LayerNorm.bias
2022-08-29 23:12:13,347 n: bert.encoder.layer.3.attention.self.query.weight
2022-08-29 23:12:13,348 n: bert.encoder.layer.3.attention.self.query.bias
2022-08-29 23:12:13,348 n: bert.encoder.layer.3.attention.self.key.weight
2022-08-29 23:12:13,348 n: bert.encoder.layer.3.attention.self.key.bias
2022-08-29 23:12:13,348 n: bert.encoder.layer.3.attention.self.value.weight
2022-08-29 23:12:13,348 n: bert.encoder.layer.3.attention.self.value.bias
2022-08-29 23:12:13,348 n: bert.encoder.layer.3.attention.output.dense.weight
2022-08-29 23:12:13,348 n: bert.encoder.layer.3.attention.output.dense.bias
2022-08-29 23:12:13,348 n: bert.encoder.layer.3.attention.output.LayerNorm.weight
2022-08-29 23:12:13,348 n: bert.encoder.layer.3.attention.output.LayerNorm.bias
2022-08-29 23:12:13,348 n: bert.encoder.layer.3.intermediate.dense.weight
2022-08-29 23:12:13,348 n: bert.encoder.layer.3.intermediate.dense.bias
2022-08-29 23:12:13,348 n: bert.encoder.layer.3.output.dense.weight
2022-08-29 23:12:13,348 n: bert.encoder.layer.3.output.dense.bias
2022-08-29 23:12:13,348 n: bert.encoder.layer.3.output.LayerNorm.weight
2022-08-29 23:12:13,348 n: bert.encoder.layer.3.output.LayerNorm.bias
2022-08-29 23:12:13,348 n: bert.encoder.layer.4.attention.self.query.weight
2022-08-29 23:12:13,348 n: bert.encoder.layer.4.attention.self.query.bias
2022-08-29 23:12:13,348 n: bert.encoder.layer.4.attention.self.key.weight
2022-08-29 23:12:13,348 n: bert.encoder.layer.4.attention.self.key.bias
2022-08-29 23:12:13,349 n: bert.encoder.layer.4.attention.self.value.weight
2022-08-29 23:12:13,349 n: bert.encoder.layer.4.attention.self.value.bias
2022-08-29 23:12:13,349 n: bert.encoder.layer.4.attention.output.dense.weight
2022-08-29 23:12:13,349 n: bert.encoder.layer.4.attention.output.dense.bias
2022-08-29 23:12:13,349 n: bert.encoder.layer.4.attention.output.LayerNorm.weight
2022-08-29 23:12:13,349 n: bert.encoder.layer.4.attention.output.LayerNorm.bias
2022-08-29 23:12:13,349 n: bert.encoder.layer.4.intermediate.dense.weight
2022-08-29 23:12:13,349 n: bert.encoder.layer.4.intermediate.dense.bias
2022-08-29 23:12:13,349 n: bert.encoder.layer.4.output.dense.weight
2022-08-29 23:12:13,349 n: bert.encoder.layer.4.output.dense.bias
2022-08-29 23:12:13,349 n: bert.encoder.layer.4.output.LayerNorm.weight
2022-08-29 23:12:13,349 n: bert.encoder.layer.4.output.LayerNorm.bias
2022-08-29 23:12:13,349 n: bert.encoder.layer.5.attention.self.query.weight
2022-08-29 23:12:13,349 n: bert.encoder.layer.5.attention.self.query.bias
2022-08-29 23:12:13,349 n: bert.encoder.layer.5.attention.self.key.weight
2022-08-29 23:12:13,349 n: bert.encoder.layer.5.attention.self.key.bias
2022-08-29 23:12:13,349 n: bert.encoder.layer.5.attention.self.value.weight
2022-08-29 23:12:13,349 n: bert.encoder.layer.5.attention.self.value.bias
2022-08-29 23:12:13,350 n: bert.encoder.layer.5.attention.output.dense.weight
2022-08-29 23:12:13,350 n: bert.encoder.layer.5.attention.output.dense.bias
2022-08-29 23:12:13,350 n: bert.encoder.layer.5.attention.output.LayerNorm.weight
2022-08-29 23:12:13,350 n: bert.encoder.layer.5.attention.output.LayerNorm.bias
2022-08-29 23:12:13,350 n: bert.encoder.layer.5.intermediate.dense.weight
2022-08-29 23:12:13,350 n: bert.encoder.layer.5.intermediate.dense.bias
2022-08-29 23:12:13,350 n: bert.encoder.layer.5.output.dense.weight
2022-08-29 23:12:13,350 n: bert.encoder.layer.5.output.dense.bias
2022-08-29 23:12:13,350 n: bert.encoder.layer.5.output.LayerNorm.weight
2022-08-29 23:12:13,350 n: bert.encoder.layer.5.output.LayerNorm.bias
2022-08-29 23:12:13,350 n: bert.encoder.layer.6.attention.self.query.weight
2022-08-29 23:12:13,350 n: bert.encoder.layer.6.attention.self.query.bias
2022-08-29 23:12:13,350 n: bert.encoder.layer.6.attention.self.key.weight
2022-08-29 23:12:13,350 n: bert.encoder.layer.6.attention.self.key.bias
2022-08-29 23:12:13,350 n: bert.encoder.layer.6.attention.self.value.weight
2022-08-29 23:12:13,350 n: bert.encoder.layer.6.attention.self.value.bias
2022-08-29 23:12:13,350 n: bert.encoder.layer.6.attention.output.dense.weight
2022-08-29 23:12:13,350 n: bert.encoder.layer.6.attention.output.dense.bias
2022-08-29 23:12:13,350 n: bert.encoder.layer.6.attention.output.LayerNorm.weight
2022-08-29 23:12:13,351 n: bert.encoder.layer.6.attention.output.LayerNorm.bias
2022-08-29 23:12:13,351 n: bert.encoder.layer.6.intermediate.dense.weight
2022-08-29 23:12:13,351 n: bert.encoder.layer.6.intermediate.dense.bias
2022-08-29 23:12:13,351 n: bert.encoder.layer.6.output.dense.weight
2022-08-29 23:12:13,351 n: bert.encoder.layer.6.output.dense.bias
2022-08-29 23:12:13,351 n: bert.encoder.layer.6.output.LayerNorm.weight
2022-08-29 23:12:13,351 n: bert.encoder.layer.6.output.LayerNorm.bias
2022-08-29 23:12:13,351 n: bert.encoder.layer.7.attention.self.query.weight
2022-08-29 23:12:13,351 n: bert.encoder.layer.7.attention.self.query.bias
2022-08-29 23:12:13,351 n: bert.encoder.layer.7.attention.self.key.weight
2022-08-29 23:12:13,351 n: bert.encoder.layer.7.attention.self.key.bias
2022-08-29 23:12:13,351 n: bert.encoder.layer.7.attention.self.value.weight
2022-08-29 23:12:13,351 n: bert.encoder.layer.7.attention.self.value.bias
2022-08-29 23:12:13,351 n: bert.encoder.layer.7.attention.output.dense.weight
2022-08-29 23:12:13,351 n: bert.encoder.layer.7.attention.output.dense.bias
2022-08-29 23:12:13,351 n: bert.encoder.layer.7.attention.output.LayerNorm.weight
2022-08-29 23:12:13,351 n: bert.encoder.layer.7.attention.output.LayerNorm.bias
2022-08-29 23:12:13,351 n: bert.encoder.layer.7.intermediate.dense.weight
2022-08-29 23:12:13,352 n: bert.encoder.layer.7.intermediate.dense.bias
2022-08-29 23:12:13,352 n: bert.encoder.layer.7.output.dense.weight
2022-08-29 23:12:13,352 n: bert.encoder.layer.7.output.dense.bias
2022-08-29 23:12:13,352 n: bert.encoder.layer.7.output.LayerNorm.weight
2022-08-29 23:12:13,352 n: bert.encoder.layer.7.output.LayerNorm.bias
2022-08-29 23:12:13,352 n: bert.encoder.layer.8.attention.self.query.weight
2022-08-29 23:12:13,352 n: bert.encoder.layer.8.attention.self.query.bias
2022-08-29 23:12:13,352 n: bert.encoder.layer.8.attention.self.key.weight
2022-08-29 23:12:13,352 n: bert.encoder.layer.8.attention.self.key.bias
2022-08-29 23:12:13,352 n: bert.encoder.layer.8.attention.self.value.weight
2022-08-29 23:12:13,352 n: bert.encoder.layer.8.attention.self.value.bias
2022-08-29 23:12:13,352 n: bert.encoder.layer.8.attention.output.dense.weight
2022-08-29 23:12:13,352 n: bert.encoder.layer.8.attention.output.dense.bias
2022-08-29 23:12:13,352 n: bert.encoder.layer.8.attention.output.LayerNorm.weight
2022-08-29 23:12:13,352 n: bert.encoder.layer.8.attention.output.LayerNorm.bias
2022-08-29 23:12:13,352 n: bert.encoder.layer.8.intermediate.dense.weight
2022-08-29 23:12:13,352 n: bert.encoder.layer.8.intermediate.dense.bias
2022-08-29 23:12:13,352 n: bert.encoder.layer.8.output.dense.weight
2022-08-29 23:12:13,352 n: bert.encoder.layer.8.output.dense.bias
2022-08-29 23:12:13,353 n: bert.encoder.layer.8.output.LayerNorm.weight
2022-08-29 23:12:13,353 n: bert.encoder.layer.8.output.LayerNorm.bias
2022-08-29 23:12:13,353 n: bert.encoder.layer.9.attention.self.query.weight
2022-08-29 23:12:13,353 n: bert.encoder.layer.9.attention.self.query.bias
2022-08-29 23:12:13,353 n: bert.encoder.layer.9.attention.self.key.weight
2022-08-29 23:12:13,353 n: bert.encoder.layer.9.attention.self.key.bias
2022-08-29 23:12:13,353 n: bert.encoder.layer.9.attention.self.value.weight
2022-08-29 23:12:13,353 n: bert.encoder.layer.9.attention.self.value.bias
2022-08-29 23:12:13,353 n: bert.encoder.layer.9.attention.output.dense.weight
2022-08-29 23:12:13,353 n: bert.encoder.layer.9.attention.output.dense.bias
2022-08-29 23:12:13,353 n: bert.encoder.layer.9.attention.output.LayerNorm.weight
2022-08-29 23:12:13,353 n: bert.encoder.layer.9.attention.output.LayerNorm.bias
2022-08-29 23:12:13,353 n: bert.encoder.layer.9.intermediate.dense.weight
2022-08-29 23:12:13,353 n: bert.encoder.layer.9.intermediate.dense.bias
2022-08-29 23:12:13,353 n: bert.encoder.layer.9.output.dense.weight
2022-08-29 23:12:13,353 n: bert.encoder.layer.9.output.dense.bias
2022-08-29 23:12:13,353 n: bert.encoder.layer.9.output.LayerNorm.weight
2022-08-29 23:12:13,353 n: bert.encoder.layer.9.output.LayerNorm.bias
2022-08-29 23:12:13,353 n: bert.encoder.layer.10.attention.self.query.weight
2022-08-29 23:12:13,354 n: bert.encoder.layer.10.attention.self.query.bias
2022-08-29 23:12:13,354 n: bert.encoder.layer.10.attention.self.key.weight
2022-08-29 23:12:13,354 n: bert.encoder.layer.10.attention.self.key.bias
2022-08-29 23:12:13,354 n: bert.encoder.layer.10.attention.self.value.weight
2022-08-29 23:12:13,354 n: bert.encoder.layer.10.attention.self.value.bias
2022-08-29 23:12:13,354 n: bert.encoder.layer.10.attention.output.dense.weight
2022-08-29 23:12:13,354 n: bert.encoder.layer.10.attention.output.dense.bias
2022-08-29 23:12:13,354 n: bert.encoder.layer.10.attention.output.LayerNorm.weight
2022-08-29 23:12:13,354 n: bert.encoder.layer.10.attention.output.LayerNorm.bias
2022-08-29 23:12:13,354 n: bert.encoder.layer.10.intermediate.dense.weight
2022-08-29 23:12:13,354 n: bert.encoder.layer.10.intermediate.dense.bias
2022-08-29 23:12:13,354 n: bert.encoder.layer.10.output.dense.weight
2022-08-29 23:12:13,354 n: bert.encoder.layer.10.output.dense.bias
2022-08-29 23:12:13,354 n: bert.encoder.layer.10.output.LayerNorm.weight
2022-08-29 23:12:13,354 n: bert.encoder.layer.10.output.LayerNorm.bias
2022-08-29 23:12:13,354 n: bert.encoder.layer.11.attention.self.query.weight
2022-08-29 23:12:13,354 n: bert.encoder.layer.11.attention.self.query.bias
2022-08-29 23:12:13,354 n: bert.encoder.layer.11.attention.self.key.weight
2022-08-29 23:12:13,355 n: bert.encoder.layer.11.attention.self.key.bias
2022-08-29 23:12:13,355 n: bert.encoder.layer.11.attention.self.value.weight
2022-08-29 23:12:13,355 n: bert.encoder.layer.11.attention.self.value.bias
2022-08-29 23:12:13,355 n: bert.encoder.layer.11.attention.output.dense.weight
2022-08-29 23:12:13,355 n: bert.encoder.layer.11.attention.output.dense.bias
2022-08-29 23:12:13,355 n: bert.encoder.layer.11.attention.output.LayerNorm.weight
2022-08-29 23:12:13,355 n: bert.encoder.layer.11.attention.output.LayerNorm.bias
2022-08-29 23:12:13,355 n: bert.encoder.layer.11.intermediate.dense.weight
2022-08-29 23:12:13,355 n: bert.encoder.layer.11.intermediate.dense.bias
2022-08-29 23:12:13,355 n: bert.encoder.layer.11.output.dense.weight
2022-08-29 23:12:13,355 n: bert.encoder.layer.11.output.dense.bias
2022-08-29 23:12:13,355 n: bert.encoder.layer.11.output.LayerNorm.weight
2022-08-29 23:12:13,355 n: bert.encoder.layer.11.output.LayerNorm.bias
2022-08-29 23:12:13,355 n: bert.pooler.dense.weight
2022-08-29 23:12:13,355 n: bert.pooler.dense.bias
2022-08-29 23:12:13,355 n: classifier.weight
2022-08-29 23:12:13,355 n: classifier.bias
2022-08-29 23:12:13,355 Total parameters: 109483009
2022-08-29 23:12:13,379 ***** Running training *****
2022-08-29 23:12:13,379   Num examples = 5749
2022-08-29 23:12:13,379   Batch size = 32
2022-08-29 23:12:13,379   Num steps = 8950
2022-08-29 23:12:13,380 n: bert.embeddings.word_embeddings.weight
2022-08-29 23:12:13,380 n: bert.embeddings.position_embeddings.weight
2022-08-29 23:12:13,380 n: bert.embeddings.token_type_embeddings.weight
2022-08-29 23:12:13,380 n: bert.embeddings.LayerNorm.weight
2022-08-29 23:12:13,380 n: bert.embeddings.LayerNorm.bias
2022-08-29 23:12:13,380 n: bert.encoder.layer.0.attention.self.query.weight
2022-08-29 23:12:13,380 n: bert.encoder.layer.0.attention.self.query.bias
2022-08-29 23:12:13,380 n: bert.encoder.layer.0.attention.self.key.weight
2022-08-29 23:12:13,380 n: bert.encoder.layer.0.attention.self.key.bias
2022-08-29 23:12:13,380 n: bert.encoder.layer.0.attention.self.value.weight
2022-08-29 23:12:13,380 n: bert.encoder.layer.0.attention.self.value.bias
2022-08-29 23:12:13,380 n: bert.encoder.layer.0.attention.output.dense.weight
2022-08-29 23:12:13,381 n: bert.encoder.layer.0.attention.output.dense.bias
2022-08-29 23:12:13,381 n: bert.encoder.layer.0.attention.output.LayerNorm.weight
2022-08-29 23:12:13,381 n: bert.encoder.layer.0.attention.output.LayerNorm.bias
2022-08-29 23:12:13,381 n: bert.encoder.layer.0.intermediate.dense.weight
2022-08-29 23:12:13,381 n: bert.encoder.layer.0.intermediate.dense.bias
2022-08-29 23:12:13,381 n: bert.encoder.layer.0.output.dense.weight
2022-08-29 23:12:13,381 n: bert.encoder.layer.0.output.dense.bias
2022-08-29 23:12:13,381 n: bert.encoder.layer.0.output.LayerNorm.weight
2022-08-29 23:12:13,381 n: bert.encoder.layer.0.output.LayerNorm.bias
2022-08-29 23:12:13,381 n: bert.encoder.layer.1.attention.self.query.weight
2022-08-29 23:12:13,381 n: bert.encoder.layer.1.attention.self.query.bias
2022-08-29 23:12:13,381 n: bert.encoder.layer.1.attention.self.key.weight
2022-08-29 23:12:13,381 n: bert.encoder.layer.1.attention.self.key.bias
2022-08-29 23:12:13,381 n: bert.encoder.layer.1.attention.self.value.weight
2022-08-29 23:12:13,381 n: bert.encoder.layer.1.attention.self.value.bias
2022-08-29 23:12:13,381 n: bert.encoder.layer.1.attention.output.dense.weight
2022-08-29 23:12:13,381 n: bert.encoder.layer.1.attention.output.dense.bias
2022-08-29 23:12:13,382 n: bert.encoder.layer.1.attention.output.LayerNorm.weight
2022-08-29 23:12:13,382 n: bert.encoder.layer.1.attention.output.LayerNorm.bias
2022-08-29 23:12:13,382 n: bert.encoder.layer.1.intermediate.dense.weight
2022-08-29 23:12:13,382 n: bert.encoder.layer.1.intermediate.dense.bias
2022-08-29 23:12:13,382 n: bert.encoder.layer.1.output.dense.weight
2022-08-29 23:12:13,382 n: bert.encoder.layer.1.output.dense.bias
2022-08-29 23:12:13,382 n: bert.encoder.layer.1.output.LayerNorm.weight
2022-08-29 23:12:13,382 n: bert.encoder.layer.1.output.LayerNorm.bias
2022-08-29 23:12:13,382 n: bert.encoder.layer.2.attention.self.query.weight
2022-08-29 23:12:13,382 n: bert.encoder.layer.2.attention.self.query.bias
2022-08-29 23:12:13,382 n: bert.encoder.layer.2.attention.self.key.weight
2022-08-29 23:12:13,382 n: bert.encoder.layer.2.attention.self.key.bias
2022-08-29 23:12:13,382 n: bert.encoder.layer.2.attention.self.value.weight
2022-08-29 23:12:13,382 n: bert.encoder.layer.2.attention.self.value.bias
2022-08-29 23:12:13,382 n: bert.encoder.layer.2.attention.output.dense.weight
2022-08-29 23:12:13,382 n: bert.encoder.layer.2.attention.output.dense.bias
2022-08-29 23:12:13,382 n: bert.encoder.layer.2.attention.output.LayerNorm.weight
2022-08-29 23:12:13,383 n: bert.encoder.layer.2.attention.output.LayerNorm.bias
2022-08-29 23:12:13,383 n: bert.encoder.layer.2.intermediate.dense.weight
2022-08-29 23:12:13,383 n: bert.encoder.layer.2.intermediate.dense.bias
2022-08-29 23:12:13,383 n: bert.encoder.layer.2.output.dense.weight
2022-08-29 23:12:13,383 n: bert.encoder.layer.2.output.dense.bias
2022-08-29 23:12:13,383 n: bert.encoder.layer.2.output.LayerNorm.weight
2022-08-29 23:12:13,383 n: bert.encoder.layer.2.output.LayerNorm.bias
2022-08-29 23:12:13,383 n: bert.encoder.layer.3.attention.self.query.weight
2022-08-29 23:12:13,383 n: bert.encoder.layer.3.attention.self.query.bias
2022-08-29 23:12:13,383 n: bert.encoder.layer.3.attention.self.key.weight
2022-08-29 23:12:13,383 n: bert.encoder.layer.3.attention.self.key.bias
2022-08-29 23:12:13,383 n: bert.encoder.layer.3.attention.self.value.weight
2022-08-29 23:12:13,383 n: bert.encoder.layer.3.attention.self.value.bias
2022-08-29 23:12:13,383 n: bert.encoder.layer.3.attention.output.dense.weight
2022-08-29 23:12:13,383 n: bert.encoder.layer.3.attention.output.dense.bias
2022-08-29 23:12:13,383 n: bert.encoder.layer.3.attention.output.LayerNorm.weight
2022-08-29 23:12:13,383 n: bert.encoder.layer.3.attention.output.LayerNorm.bias
2022-08-29 23:12:13,383 n: bert.encoder.layer.3.intermediate.dense.weight
2022-08-29 23:12:13,384 n: bert.encoder.layer.3.intermediate.dense.bias
2022-08-29 23:12:13,384 n: bert.encoder.layer.3.output.dense.weight
2022-08-29 23:12:13,384 n: bert.encoder.layer.3.output.dense.bias
2022-08-29 23:12:13,384 n: bert.encoder.layer.3.output.LayerNorm.weight
2022-08-29 23:12:13,384 n: bert.encoder.layer.3.output.LayerNorm.bias
2022-08-29 23:12:13,384 n: bert.encoder.layer.4.attention.self.query.weight
2022-08-29 23:12:13,384 n: bert.encoder.layer.4.attention.self.query.bias
2022-08-29 23:12:13,384 n: bert.encoder.layer.4.attention.self.key.weight
2022-08-29 23:12:13,384 n: bert.encoder.layer.4.attention.self.key.bias
2022-08-29 23:12:13,384 n: bert.encoder.layer.4.attention.self.value.weight
2022-08-29 23:12:13,384 n: bert.encoder.layer.4.attention.self.value.bias
2022-08-29 23:12:13,384 n: bert.encoder.layer.4.attention.output.dense.weight
2022-08-29 23:12:13,384 n: bert.encoder.layer.4.attention.output.dense.bias
2022-08-29 23:12:13,384 n: bert.encoder.layer.4.attention.output.LayerNorm.weight
2022-08-29 23:12:13,384 n: bert.encoder.layer.4.attention.output.LayerNorm.bias
2022-08-29 23:12:13,384 n: bert.encoder.layer.4.intermediate.dense.weight
2022-08-29 23:12:13,384 n: bert.encoder.layer.4.intermediate.dense.bias
2022-08-29 23:12:13,385 n: bert.encoder.layer.4.output.dense.weight
2022-08-29 23:12:13,385 n: bert.encoder.layer.4.output.dense.bias
2022-08-29 23:12:13,385 n: bert.encoder.layer.4.output.LayerNorm.weight
2022-08-29 23:12:13,385 n: bert.encoder.layer.4.output.LayerNorm.bias
2022-08-29 23:12:13,385 n: bert.encoder.layer.5.attention.self.query.weight
2022-08-29 23:12:13,385 n: bert.encoder.layer.5.attention.self.query.bias
2022-08-29 23:12:13,385 n: bert.encoder.layer.5.attention.self.key.weight
2022-08-29 23:12:13,385 n: bert.encoder.layer.5.attention.self.key.bias
2022-08-29 23:12:13,385 n: bert.encoder.layer.5.attention.self.value.weight
2022-08-29 23:12:13,385 n: bert.encoder.layer.5.attention.self.value.bias
2022-08-29 23:12:13,385 n: bert.encoder.layer.5.attention.output.dense.weight
2022-08-29 23:12:13,385 n: bert.encoder.layer.5.attention.output.dense.bias
2022-08-29 23:12:13,385 n: bert.encoder.layer.5.attention.output.LayerNorm.weight
2022-08-29 23:12:13,385 n: bert.encoder.layer.5.attention.output.LayerNorm.bias
2022-08-29 23:12:13,385 n: bert.encoder.layer.5.intermediate.dense.weight
2022-08-29 23:12:13,385 n: bert.encoder.layer.5.intermediate.dense.bias
2022-08-29 23:12:13,385 n: bert.encoder.layer.5.output.dense.weight
2022-08-29 23:12:13,385 n: bert.encoder.layer.5.output.dense.bias
2022-08-29 23:12:13,386 n: bert.encoder.layer.5.output.LayerNorm.weight
2022-08-29 23:12:13,386 n: bert.encoder.layer.5.output.LayerNorm.bias
2022-08-29 23:12:13,386 n: bert.encoder.layer.6.attention.self.query.weight
2022-08-29 23:12:13,386 n: bert.encoder.layer.6.attention.self.query.bias
2022-08-29 23:12:13,386 n: bert.encoder.layer.6.attention.self.key.weight
2022-08-29 23:12:13,386 n: bert.encoder.layer.6.attention.self.key.bias
2022-08-29 23:12:13,386 n: bert.encoder.layer.6.attention.self.value.weight
2022-08-29 23:12:13,386 n: bert.encoder.layer.6.attention.self.value.bias
2022-08-29 23:12:13,386 n: bert.encoder.layer.6.attention.output.dense.weight
2022-08-29 23:12:13,386 n: bert.encoder.layer.6.attention.output.dense.bias
2022-08-29 23:12:13,386 n: bert.encoder.layer.6.attention.output.LayerNorm.weight
2022-08-29 23:12:13,386 n: bert.encoder.layer.6.attention.output.LayerNorm.bias
2022-08-29 23:12:13,386 n: bert.encoder.layer.6.intermediate.dense.weight
2022-08-29 23:12:13,386 n: bert.encoder.layer.6.intermediate.dense.bias
2022-08-29 23:12:13,386 n: bert.encoder.layer.6.output.dense.weight
2022-08-29 23:12:13,386 n: bert.encoder.layer.6.output.dense.bias
2022-08-29 23:12:13,386 n: bert.encoder.layer.6.output.LayerNorm.weight
2022-08-29 23:12:13,386 n: bert.encoder.layer.6.output.LayerNorm.bias
2022-08-29 23:12:13,387 n: bert.encoder.layer.7.attention.self.query.weight
2022-08-29 23:12:13,387 n: bert.encoder.layer.7.attention.self.query.bias
2022-08-29 23:12:13,387 n: bert.encoder.layer.7.attention.self.key.weight
2022-08-29 23:12:13,387 n: bert.encoder.layer.7.attention.self.key.bias
2022-08-29 23:12:13,387 n: bert.encoder.layer.7.attention.self.value.weight
2022-08-29 23:12:13,387 n: bert.encoder.layer.7.attention.self.value.bias
2022-08-29 23:12:13,387 n: bert.encoder.layer.7.attention.output.dense.weight
2022-08-29 23:12:13,387 n: bert.encoder.layer.7.attention.output.dense.bias
2022-08-29 23:12:13,387 n: bert.encoder.layer.7.attention.output.LayerNorm.weight
2022-08-29 23:12:13,387 n: bert.encoder.layer.7.attention.output.LayerNorm.bias
2022-08-29 23:12:13,387 n: bert.encoder.layer.7.intermediate.dense.weight
2022-08-29 23:12:13,387 n: bert.encoder.layer.7.intermediate.dense.bias
2022-08-29 23:12:13,387 n: bert.encoder.layer.7.output.dense.weight
2022-08-29 23:12:13,387 n: bert.encoder.layer.7.output.dense.bias
2022-08-29 23:12:13,387 n: bert.encoder.layer.7.output.LayerNorm.weight
2022-08-29 23:12:13,387 n: bert.encoder.layer.7.output.LayerNorm.bias
2022-08-29 23:12:13,387 n: bert.encoder.layer.8.attention.self.query.weight
2022-08-29 23:12:13,387 n: bert.encoder.layer.8.attention.self.query.bias
2022-08-29 23:12:13,388 n: bert.encoder.layer.8.attention.self.key.weight
2022-08-29 23:12:13,388 n: bert.encoder.layer.8.attention.self.key.bias
2022-08-29 23:12:13,388 n: bert.encoder.layer.8.attention.self.value.weight
2022-08-29 23:12:13,388 n: bert.encoder.layer.8.attention.self.value.bias
2022-08-29 23:12:13,388 n: bert.encoder.layer.8.attention.output.dense.weight
2022-08-29 23:12:13,388 n: bert.encoder.layer.8.attention.output.dense.bias
2022-08-29 23:12:13,388 n: bert.encoder.layer.8.attention.output.LayerNorm.weight
2022-08-29 23:12:13,388 n: bert.encoder.layer.8.attention.output.LayerNorm.bias
2022-08-29 23:12:13,388 n: bert.encoder.layer.8.intermediate.dense.weight
2022-08-29 23:12:13,388 n: bert.encoder.layer.8.intermediate.dense.bias
2022-08-29 23:12:13,388 n: bert.encoder.layer.8.output.dense.weight
2022-08-29 23:12:13,388 n: bert.encoder.layer.8.output.dense.bias
2022-08-29 23:12:13,388 n: bert.encoder.layer.8.output.LayerNorm.weight
2022-08-29 23:12:13,388 n: bert.encoder.layer.8.output.LayerNorm.bias
2022-08-29 23:12:13,388 n: bert.encoder.layer.9.attention.self.query.weight
2022-08-29 23:12:13,388 n: bert.encoder.layer.9.attention.self.query.bias
2022-08-29 23:12:13,388 n: bert.encoder.layer.9.attention.self.key.weight
2022-08-29 23:12:13,389 n: bert.encoder.layer.9.attention.self.key.bias
2022-08-29 23:12:13,389 n: bert.encoder.layer.9.attention.self.value.weight
2022-08-29 23:12:13,389 n: bert.encoder.layer.9.attention.self.value.bias
2022-08-29 23:12:13,389 n: bert.encoder.layer.9.attention.output.dense.weight
2022-08-29 23:12:13,389 n: bert.encoder.layer.9.attention.output.dense.bias
2022-08-29 23:12:13,389 n: bert.encoder.layer.9.attention.output.LayerNorm.weight
2022-08-29 23:12:13,389 n: bert.encoder.layer.9.attention.output.LayerNorm.bias
2022-08-29 23:12:13,389 n: bert.encoder.layer.9.intermediate.dense.weight
2022-08-29 23:12:13,389 n: bert.encoder.layer.9.intermediate.dense.bias
2022-08-29 23:12:13,389 n: bert.encoder.layer.9.output.dense.weight
2022-08-29 23:12:13,389 n: bert.encoder.layer.9.output.dense.bias
2022-08-29 23:12:13,389 n: bert.encoder.layer.9.output.LayerNorm.weight
2022-08-29 23:12:13,389 n: bert.encoder.layer.9.output.LayerNorm.bias
2022-08-29 23:12:13,389 n: bert.encoder.layer.10.attention.self.query.weight
2022-08-29 23:12:13,389 n: bert.encoder.layer.10.attention.self.query.bias
2022-08-29 23:12:13,389 n: bert.encoder.layer.10.attention.self.key.weight
2022-08-29 23:12:13,389 n: bert.encoder.layer.10.attention.self.key.bias
2022-08-29 23:12:13,389 n: bert.encoder.layer.10.attention.self.value.weight
2022-08-29 23:12:13,390 n: bert.encoder.layer.10.attention.self.value.bias
2022-08-29 23:12:13,390 n: bert.encoder.layer.10.attention.output.dense.weight
2022-08-29 23:12:13,390 n: bert.encoder.layer.10.attention.output.dense.bias
2022-08-29 23:12:13,390 n: bert.encoder.layer.10.attention.output.LayerNorm.weight
2022-08-29 23:12:13,390 n: bert.encoder.layer.10.attention.output.LayerNorm.bias
2022-08-29 23:12:13,390 n: bert.encoder.layer.10.intermediate.dense.weight
2022-08-29 23:12:13,390 n: bert.encoder.layer.10.intermediate.dense.bias
2022-08-29 23:12:13,390 n: bert.encoder.layer.10.output.dense.weight
2022-08-29 23:12:13,390 n: bert.encoder.layer.10.output.dense.bias
2022-08-29 23:12:13,390 n: bert.encoder.layer.10.output.LayerNorm.weight
2022-08-29 23:12:13,390 n: bert.encoder.layer.10.output.LayerNorm.bias
2022-08-29 23:12:13,390 n: bert.encoder.layer.11.attention.self.query.weight
2022-08-29 23:12:13,390 n: bert.encoder.layer.11.attention.self.query.bias
2022-08-29 23:12:13,390 n: bert.encoder.layer.11.attention.self.key.weight
2022-08-29 23:12:13,390 n: bert.encoder.layer.11.attention.self.key.bias
2022-08-29 23:12:13,390 n: bert.encoder.layer.11.attention.self.value.weight
2022-08-29 23:12:13,390 n: bert.encoder.layer.11.attention.self.value.bias
2022-08-29 23:12:13,390 n: bert.encoder.layer.11.attention.output.dense.weight
2022-08-29 23:12:13,391 n: bert.encoder.layer.11.attention.output.dense.bias
2022-08-29 23:12:13,391 n: bert.encoder.layer.11.attention.output.LayerNorm.weight
2022-08-29 23:12:13,391 n: bert.encoder.layer.11.attention.output.LayerNorm.bias
2022-08-29 23:12:13,391 n: bert.encoder.layer.11.intermediate.dense.weight
2022-08-29 23:12:13,391 n: bert.encoder.layer.11.intermediate.dense.bias
2022-08-29 23:12:13,391 n: bert.encoder.layer.11.output.dense.weight
2022-08-29 23:12:13,391 n: bert.encoder.layer.11.output.dense.bias
2022-08-29 23:12:13,391 n: bert.encoder.layer.11.output.LayerNorm.weight
2022-08-29 23:12:13,391 n: bert.encoder.layer.11.output.LayerNorm.bias
2022-08-29 23:12:13,391 n: bert.pooler.dense.weight
2022-08-29 23:12:13,391 n: bert.pooler.dense.bias
2022-08-29 23:12:13,391 n: classifier.weight
2022-08-29 23:12:13,391 n: classifier.bias
2022-08-29 23:12:13,391 Total parameters: 109483009
2022-08-29 23:12:16,225 Writing example 40000 of 363846
2022-08-29 23:12:20,472 Writing example 50000 of 363846
2022-08-29 23:12:24,375 ***** Eval results *****
2022-08-29 23:12:24,376   acc = 0.8324857607811229
2022-08-29 23:12:24,376   att_loss = 0.0
2022-08-29 23:12:24,376   cls_loss = 0.10135565116927
2022-08-29 23:12:24,376   eval_loss = 0.46461638883240036
2022-08-29 23:12:24,376   global_step = 2799
2022-08-29 23:12:24,376   loss = 0.10135565116927
2022-08-29 23:12:24,376   rep_loss = 0.0
2022-08-29 23:12:25,127 Writing example 60000 of 363846
2022-08-29 23:12:29,395 Writing example 70000 of 363846
2022-08-29 23:12:33,668 Writing example 80000 of 363846
2022-08-29 23:12:37,869 Writing example 90000 of 363846
2022-08-29 23:12:42,631 Writing example 100000 of 363846
2022-08-29 23:12:46,855 Writing example 110000 of 363846
2022-08-29 23:12:49,628 ***** Running evaluation *****
2022-08-29 23:12:49,628   Epoch = 0 iter 199 step
2022-08-29 23:12:49,628   Num examples = 1500
2022-08-29 23:12:49,628   Batch size = 32
2022-08-29 23:12:49,629 ***** Eval results *****
2022-08-29 23:12:49,629   att_loss = 10.42748802391129
2022-08-29 23:12:49,629   cls_loss = 0.0
2022-08-29 23:12:49,629   global_step = 199
2022-08-29 23:12:49,630   loss = 15.034748173239244
2022-08-29 23:12:49,630   rep_loss = 4.6072601744877035
2022-08-29 23:12:49,630 ***** Save model *****
2022-08-29 23:12:51,078 Writing example 120000 of 363846
2022-08-29 23:12:55,323 Writing example 130000 of 363846
2022-08-29 23:12:55,800 ***** Running evaluation *****
2022-08-29 23:12:55,800   Epoch = 0 iter 2899 step
2022-08-29 23:12:55,800   Num examples = 9832
2022-08-29 23:12:55,800   Batch size = 32
2022-08-29 23:13:00,361 Writing example 140000 of 363846
2022-08-29 23:13:04,615 Writing example 150000 of 363846
2022-08-29 23:13:08,868 Writing example 160000 of 363846
2022-08-29 23:13:13,127 Writing example 170000 of 363846
2022-08-29 23:13:16,247 ***** Eval results *****
2022-08-29 23:13:16,247   acc = 0.8331977217249796
2022-08-29 23:13:16,247   att_loss = 0.0
2022-08-29 23:13:16,247   cls_loss = 0.1014268624231029
2022-08-29 23:13:16,247   eval_loss = 0.45401176062787507
2022-08-29 23:13:16,247   global_step = 2899
2022-08-29 23:13:16,247   loss = 0.1014268624231029
2022-08-29 23:13:16,247   rep_loss = 0.0
2022-08-29 23:13:17,363 Writing example 180000 of 363846
2022-08-29 23:13:21,637 Writing example 190000 of 363846
2022-08-29 23:13:26,756 ***** Running evaluation *****
2022-08-29 23:13:26,757   Epoch = 0 iter 399 step
2022-08-29 23:13:26,757   Num examples = 1500
2022-08-29 23:13:26,757   Batch size = 32
2022-08-29 23:13:26,758 ***** Eval results *****
2022-08-29 23:13:26,758   att_loss = 8.414218351058196
2022-08-29 23:13:26,758   cls_loss = 0.0
2022-08-29 23:13:26,759   global_step = 399
2022-08-29 23:13:26,759   loss = 12.335450202301331
2022-08-29 23:13:26,759   rep_loss = 3.9212318637914825
2022-08-29 23:13:26,759 ***** Save model *****
2022-08-29 23:13:26,818 Writing example 200000 of 363846
2022-08-29 23:13:31,099 Writing example 210000 of 363846
2022-08-29 23:13:34,990 ***** Running evaluation *****
2022-08-29 23:13:34,991   Epoch = 1 iter 199 step
2022-08-29 23:13:34,991   Num examples = 1500
2022-08-29 23:13:34,991   Batch size = 32
2022-08-29 23:13:34,992 ***** Eval results *****
2022-08-29 23:13:34,993   att_loss = 7.017033123970032
2022-08-29 23:13:34,993   cls_loss = 0.0
2022-08-29 23:13:34,993   global_step = 199
2022-08-29 23:13:34,993   loss = 10.459678530693054
2022-08-29 23:13:34,993   rep_loss = 3.4426453709602356
2022-08-29 23:13:34,993 ***** Save model *****
2022-08-29 23:13:35,389 Writing example 220000 of 363846
2022-08-29 23:13:39,649 Writing example 230000 of 363846
2022-08-29 23:13:43,891 Writing example 240000 of 363846
2022-08-29 23:13:47,649 ***** Running evaluation *****
2022-08-29 23:13:47,650   Epoch = 0 iter 2999 step
2022-08-29 23:13:47,650   Num examples = 9832
2022-08-29 23:13:47,650   Batch size = 32
2022-08-29 23:13:48,139 Writing example 250000 of 363846
2022-08-29 23:13:52,432 Writing example 260000 of 363846
2022-08-29 23:13:56,713 Writing example 270000 of 363846
2022-08-29 23:14:02,401 Writing example 280000 of 363846
2022-08-29 23:14:06,530 ***** Running evaluation *****
2022-08-29 23:14:06,531   Epoch = 0 iter 599 step
2022-08-29 23:14:06,531   Num examples = 1500
2022-08-29 23:14:06,531   Batch size = 32
2022-08-29 23:14:06,532 ***** Eval results *****
2022-08-29 23:14:06,532   att_loss = 7.569695290420609
2022-08-29 23:14:06,532   cls_loss = 0.0
2022-08-29 23:14:06,532   global_step = 599
2022-08-29 23:14:06,532   loss = 11.196553763642733
2022-08-29 23:14:06,532   rep_loss = 3.6268584867550655
2022-08-29 23:14:06,532 ***** Save model *****
2022-08-29 23:14:06,728 Writing example 290000 of 363846
2022-08-29 23:14:10,084 ***** Eval results *****
2022-08-29 23:14:10,084   acc = 0.8380797396257119
2022-08-29 23:14:10,084   att_loss = 0.0
2022-08-29 23:14:10,084   cls_loss = 0.10137442793671032
2022-08-29 23:14:10,084   eval_loss = 0.46446340076335063
2022-08-29 23:14:10,085   global_step = 2999
2022-08-29 23:14:10,085   loss = 0.10137442793671032
2022-08-29 23:14:10,085   rep_loss = 0.0
2022-08-29 23:14:10,981 Writing example 300000 of 363846
2022-08-29 23:14:15,225 Writing example 310000 of 363846
2022-08-29 23:14:19,485 Writing example 320000 of 363846
2022-08-29 23:14:23,738 Writing example 330000 of 363846
2022-08-29 23:14:28,025 Writing example 340000 of 363846
2022-08-29 23:14:32,301 Writing example 350000 of 363846
2022-08-29 23:14:36,553 Writing example 360000 of 363846
2022-08-29 23:14:40,976 ***** Running evaluation *****
2022-08-29 23:14:40,977   Epoch = 0 iter 3099 step
2022-08-29 23:14:40,977   Num examples = 9832
2022-08-29 23:14:40,977   Batch size = 32
2022-08-29 23:14:43,568 Writing example 0 of 40430
2022-08-29 23:14:43,569 *** Example ***
2022-08-29 23:14:43,569 guid: dev-201359
2022-08-29 23:14:43,569 tokens: [CLS] why are african - americans so beautiful ? [SEP] why are hispanic ##s so beautiful ? [SEP]
2022-08-29 23:14:43,569 input_ids: 101 2339 2024 3060 1011 4841 2061 3376 1029 102 2339 2024 6696 2015 2061 3376 1029 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-08-29 23:14:43,569 input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-08-29 23:14:43,569 segment_ids: 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2022-08-29 23:14:43,569 label: 0
2022-08-29 23:14:43,569 label_id: 0
2022-08-29 23:14:45,772 ***** Running evaluation *****
2022-08-29 23:14:45,772   Epoch = 1 iter 799 step
2022-08-29 23:14:45,772   Num examples = 1500
2022-08-29 23:14:45,773   Batch size = 32
2022-08-29 23:14:45,774 ***** Eval results *****
2022-08-29 23:14:45,774   att_loss = 5.195958149285964
2022-08-29 23:14:45,774   cls_loss = 0.0
2022-08-29 23:14:45,774   global_step = 799
2022-08-29 23:14:45,774   loss = 8.01634637809094
2022-08-29 23:14:45,774   rep_loss = 2.820388243522173
2022-08-29 23:14:45,774 ***** Save model *****
2022-08-29 23:14:47,843 Writing example 10000 of 40430
2022-08-29 23:14:52,128 Writing example 20000 of 40430
2022-08-29 23:14:56,369 Writing example 30000 of 40430
2022-08-29 23:14:58,259 ***** Running evaluation *****
2022-08-29 23:14:58,259   Epoch = 2 iter 399 step
2022-08-29 23:14:58,259   Num examples = 1500
2022-08-29 23:14:58,259   Batch size = 32
2022-08-29 23:14:58,261 ***** Eval results *****
2022-08-29 23:14:58,261   att_loss = 5.870953548245314
2022-08-29 23:14:58,261   cls_loss = 0.0
2022-08-29 23:14:58,261   global_step = 399
2022-08-29 23:14:58,261   loss = 8.99798383945372
2022-08-29 23:14:58,261   rep_loss = 3.1270302621329704
2022-08-29 23:14:58,261 ***** Save model *****
2022-08-29 23:15:00,637 Writing example 40000 of 40430
2022-08-29 23:15:01,250 Model config {
  "_name_or_path": "bert-base-uncased",
  "architectures": [
    "BertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "crypten": false,
  "finetuning_task": "qqp",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "not_duplicate",
    "1": "duplicate"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "duplicate": 1,
    "not_duplicate": 0
  },
  "layer_norm_eps": 1e-12,
  "log_path": "tmp/distill/QQP/quad_2quad/bert-base-uncased/log.txt",
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "pre_trained": "",
  "problem_type": "single_label_classification",
  "softmax_act": "softmax",
  "torch_dtype": "float32",
  "training": "",
  "transformers_version": "4.20.0.dev0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

2022-08-29 23:15:01,368 ***** Eval results *****
2022-08-29 23:15:01,368   acc = 0.8312652563059398
2022-08-29 23:15:01,368   att_loss = 0.0
2022-08-29 23:15:01,368   cls_loss = 0.10139809141465063
2022-08-29 23:15:01,368   eval_loss = 0.4668220948282774
2022-08-29 23:15:01,369   global_step = 3099
2022-08-29 23:15:01,369   loss = 0.10139809141465063
2022-08-29 23:15:01,369   rep_loss = 0.0
2022-08-29 23:15:03,966 Loading model /home/ubuntu/checkpoints/exp/QQP/pytorch_model.bin
2022-08-29 23:15:07,544 loading model...
2022-08-29 23:15:07,589 done!
2022-08-29 23:15:07,589 Weights from pretrained model not used in TinyBertForSequenceClassification: ['bert.embeddings.position_ids']
2022-08-29 23:15:29,690 ***** Running evaluation *****
2022-08-29 23:15:29,691   Epoch = 1 iter 999 step
2022-08-29 23:15:29,691   Num examples = 1500
2022-08-29 23:15:29,691   Batch size = 32
2022-08-29 23:15:29,692 ***** Eval results *****
2022-08-29 23:15:29,692   att_loss = 5.048135913563793
2022-08-29 23:15:29,692   cls_loss = 0.0
2022-08-29 23:15:29,692   global_step = 999
2022-08-29 23:15:29,692   loss = 7.819298296212302
2022-08-29 23:15:29,692   rep_loss = 2.771162403860126
2022-08-29 23:15:29,692 ***** Save model *****
2022-08-29 23:15:32,243 ***** Running evaluation *****
2022-08-29 23:15:32,244   Epoch = 0 iter 3199 step
2022-08-29 23:15:32,244   Num examples = 9832
2022-08-29 23:15:32,244   Batch size = 32
