<!--Copyright 2022 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with
the License. You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on
an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the
specific language governing permissions and limitations under the License.
-->

# Treinamento distribu√≠do com o ü§ó Accelerate

O paralelismo surgiu como uma estrat√©gia para treinar modelos grandes em hardware limitado e aumentar a velocidade
de treinamento em v√°rias √≥rdens de magnitude. Na Hugging Face criamos a biblioteca [ü§ó Accelerate](https://huggingface.co/docs/accelerate/index.html)
para ajudar os usu√°rios a treinar modelos ü§ó Transformers com qualquer configura√ß√£o distribu√≠da, seja em uma m√°quina
com m√∫ltiplos GPUs ou em m√∫ltiplos GPUs distribuidos entre muitas m√°quinas. Neste tutorial, voc√™ ir√° aprender como
personalizar seu la√ßo de treinamento de PyTorch para poder treinar em ambientes distribu√≠dos.

## Configura√ß√£o

De in√≠cio, instale o ü§ó Accelerate:

```bash
pip install accelerate
```

Logo, devemos importar e criar um objeto [`Accelerator`](https://huggingface.co/docs/accelerate/accelerator.html#accelerate.Accelerator).
O `Accelerator` detectar√° autom√°ticamente a configura√ß√£o distribu√≠da dispon√≠vel e inicializar√° todos os
componentes necess√°rios para o treinamento. N√£o h√° necessidade portanto de especificar o dispositivo onde deve colocar seu modelo.

```py
>>> from accelerate import Accelerator

>>> accelerator = Accelerator()
```

## Preparando a acelera√ß√£o

Passe todos os objetos relevantes ao treinamento para o m√©todo [`prepare`](https://huggingface.co/docs/accelerate/accelerator.html#accelerate.Accelerator.prepare).
Isto inclui os DataLoaders de treino e evalua√ß√£o, um modelo e um otimizador:

```py
>>> train_dataloader, eval_dataloader, model, optimizer = accelerator.prepare(
...     train_dataloader, eval_dataloader, model, optimizer
... )
```

## Backward

Por √∫ltimo, substitua o `loss.backward()` padr√£o em seu la√ßo de treinamento com o m√©todo [`backward`](https://huggingface.co/docs/accelerate/accelerator.html#accelerate.Accelerator.backward) do ü§ó Accelerate:

```py
>>> for epoch in range(num_epochs):
...     for batch in train_dataloader:
...         outputs = model(**batch)
...         loss = outputs.loss
...         accelerator.backward(loss)

...         optimizer.step()
...         lr_scheduler.step()
...         optimizer.zero_grad()
...         progress_bar.update(1)
```

Como se poder ver no seguinte c√≥digo, s√≥ precisar√° adicionar quatro linhas de c√≥digo ao seu la√ßo de treinamento
para habilitar o treinamento distribu√≠do!

```diff
+ from accelerate import Accelerator
  from transformers import AdamW, AutoModelForSequenceClassification, get_scheduler

+ accelerator = Accelerator()

  model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)
  optimizer = AdamW(model.parameters(), lr=3e-5)

- device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")
- model.to(device)

+ train_dataloader, eval_dataloader, model, optimizer = accelerator.prepare(
+     train_dataloader, eval_dataloader, model, optimizer
+ )

  num_epochs = 3
  num_training_steps = num_epochs * len(train_dataloader)
  lr_scheduler = get_scheduler(
      "linear",
      optimizer=optimizer,
      num_warmup_steps=0,
      num_training_steps=num_training_steps
  )

  progress_bar = tqdm(range(num_training_steps))

  model.train()
  for epoch in range(num_epochs):
      for batch in train_dataloader:
-         batch = {k: v.to(device) for k, v in batch.items()}
          outputs = model(**batch)
          loss = outputs.loss
-         loss.backward()
+         accelerator.backward(loss)

          optimizer.step()
          lr_scheduler.step()
          optimizer.zero_grad()
          progress_bar.update(1)
```

## Treinamento

Quando tiver adicionado as linhas de c√≥digo relevantes, inicie o treinamento por um script ou notebook como o Colab.

### Treinamento em um Script

Se estiver rodando seu treinamento em um Script, execute o seguinte comando para criar e guardar um arquivo de configura√ß√£o:

```bash
accelerate config
```

Comece o treinamento com:

```bash
accelerate launch train.py
```

### Treinamento em um Notebook

O ü§ó Accelerate pode rodar em um notebook, por exemplo, se estiver planejando usar as TPUs do Google Colab.
Encapsule o c√≥digo respons√°vel pelo treinamento de uma fun√ß√£o e passe-o ao `notebook_launcher`:

```py
>>> from accelerate import notebook_launcher

>>> notebook_launcher(training_function)
```

Para obter mais informa√ß√µes sobre o ü§ó Accelerate e suas numerosas fun√ß√µes, consulte a [documentaci√≥n](https://huggingface.co/docs/accelerate/index.html).
